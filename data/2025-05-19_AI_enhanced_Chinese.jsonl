{"id": "2505.10643", "pdf": "https://arxiv.org/pdf/2505.10643", "abs": "https://arxiv.org/abs/2505.10643", "authors": ["Shuchen Guo", "Yun Wang", "Jichao Yu", "Xuansheng Wu", "Bilgehan Ayik", "Field M. Watts", "Ehsan Latif", "Ninghao Liu", "Lei Liu", "Xiaoming Zhai"], "title": "Artificial Intelligence Bias on English Language Learners in Automatic Scoring", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study investigated potential scoring biases and disparities toward\nEnglish Language Learners (ELLs) when using automatic scoring systems for\nmiddle school students' written responses to science assessments. We\nspecifically focus on examining how unbalanced training data with ELLs\ncontributes to scoring bias and disparities. We fine-tuned BERT with four\ndatasets: responses from (1) ELLs, (2) non-ELLs, (3) a mixed dataset reflecting\nthe real-world proportion of ELLs and non-ELLs (unbalanced), and (4) a balanced\nmixed dataset with equal representation of both groups. The study analyzed 21\nassessment items: 10 items with about 30,000 ELL responses, five items with\nabout 1,000 ELL responses, and six items with about 200 ELL responses. Scoring\naccuracy (Acc) was calculated and compared to identify bias using Friedman\ntests. We measured the Mean Score Gaps (MSGs) between ELLs and non-ELLs and\nthen calculated the differences in MSGs generated through both the human and AI\nmodels to identify the scoring disparities. We found that no AI bias and\ndistorted disparities between ELLs and non-ELLs were found when the training\ndataset was large enough (ELL = 30,000 and ELL = 1,000), but concerns could\nexist if the sample size is limited (ELL = 200).", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u5bf9\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u53d1\u73b0\u8bad\u7ec3\u6570\u636e\u91cf\u8db3\u591f\u65f6\u65e0\u663e\u8457\u504f\u5dee\uff0c\u4f46\u5c0f\u6837\u672c\uff08ELL=200\uff09\u53ef\u80fd\u5b58\u5728\u8bc4\u5206\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76\u8bad\u7ec3\u6570\u636e\u4e2d\u82f1\u8bed\u5b66\u4e60\u8005\uff08ELLs\uff09\u6bd4\u4f8b\u4e0d\u5e73\u8861\u5982\u4f55\u5bfc\u81f4\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u5bf9ELL\u5b66\u751f\u79d1\u5b66\u4f5c\u7b54\u7684\u8bc4\u5206\u504f\u89c1\u548c\u5dee\u5f02\u3002", "method": "\u4f7f\u7528BERT\u6a21\u578b\u5728\u56db\u4e2a\u6570\u636e\u96c6\uff08\u7eafELL/\u975eELL/\u73b0\u5b9e\u6bd4\u4f8b\u6df7\u5408/\u5e73\u8861\u6df7\u5408\uff09\u5fae\u8c03\uff0c\u901a\u8fc7Friedman\u68c0\u9a8c\u5206\u679021\u4e2a\u9879\u76ee\u76843\u4e07/1\u5343/200\u4e2aELL\u6837\u672c\u7684\u8bc4\u5206\u51c6\u786e\u6027\uff0c\u8ba1\u7b97\u4eba\u7c7b\u4e0eAI\u6a21\u578b\u7684\u5e73\u5747\u5206\u6570\u5dee\u8ddd\u5dee\u5f02\u3002", "result": "\u5f53\u8bad\u7ec3\u6570\u636e\u91cf\u8f83\u5927\uff08ELL=30k/1k\uff09\u65f6\u672a\u53d1\u73b0AI\u504f\u89c1\uff0c\u4f46\u6837\u672c\u91cf\u4e0d\u8db3\uff08ELL=200\uff09\u65f6\u5b58\u5728\u8bc4\u5206\u5dee\u5f02\u98ce\u9669\u3002", "conclusion": "\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u9ad8\u5ea6\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\uff0c\u5c0f\u6837\u672c\u573a\u666f\u9700\u8b66\u60d5\u5bf9\u5c11\u6570\u7fa4\u4f53\u7684\u8bc4\u5206\u504f\u5dee\u3002"}}
{"id": "2505.10714", "pdf": "https://arxiv.org/pdf/2505.10714", "abs": "https://arxiv.org/abs/2505.10714", "authors": ["Bowen Jiang", "Yangxinyu Xie", "Xiaomeng Wang", "Jiashu He", "Joshua Bergerson", "John K Hutchison", "Jordan Branham", "Camillo J Taylor", "Tanwi Mallick"], "title": "GeoGrid-Bench: Can Foundation Models Understand Multimodal Gridded Geo-Spatial Data?", "categories": ["cs.CL"], "comment": null, "summary": "We present GeoGrid-Bench, a benchmark designed to evaluate the ability of\nfoundation models to understand geo-spatial data in the grid structure.\nGeo-spatial datasets pose distinct challenges due to their dense numerical\nvalues, strong spatial and temporal dependencies, and unique multimodal\nrepresentations including tabular data, heatmaps, and geographic\nvisualizations. To assess how foundation models can support scientific research\nin this domain, GeoGrid-Bench features large-scale, real-world data covering 16\nclimate variables across 150 locations and extended time frames. The benchmark\nincludes approximately 3,200 question-answer pairs, systematically generated\nfrom 8 domain expert-curated templates to reflect practical tasks encountered\nby human scientists. These range from basic queries at a single location and\ntime to complex spatiotemporal comparisons across regions and periods. Our\nevaluation reveals that vision-language models perform best overall, and we\nprovide a fine-grained analysis of the strengths and limitations of different\nfoundation models in different geo-spatial tasks. This benchmark offers clearer\ninsights into how foundation models can be effectively applied to geo-spatial\ndata analysis and used to support scientific research.", "AI": {"tldr": "GeoGrid-Bench\u662f\u9996\u4e2a\u9488\u5bf9\u7f51\u683c\u7ed3\u6784\u5730\u7406\u7a7a\u95f4\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b3200\u4e2a\u4e13\u5bb6\u8bbe\u8ba1\u7684\u95ee\u9898\u5bf9\uff0c\u63ed\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u65f6\u7a7a\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73", "motivation": "\u89e3\u51b3\u57fa\u7840\u6a21\u578b\u5728\u5bc6\u96c6\u6570\u503c\u3001\u65f6\u7a7a\u4f9d\u8d56\u548c\u591a\u6a21\u6001\u8868\u5f81\uff08\u8868\u683c/\u70ed\u529b\u56fe/\u5730\u7406\u53ef\u89c6\u5316\uff09\u7684\u5730\u7406\u7a7a\u95f4\u6570\u636e\u5206\u6790\u4e2d\u7684\u8bc4\u4f30\u7a7a\u767d", "method": "\u4f7f\u7528\u8986\u76d616\u4e2a\u6c14\u5019\u53d8\u91cf\u3001150\u4e2a\u5730\u70b9\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\uff0c\u901a\u8fc78\u4e2a\u9886\u57df\u4e13\u5bb6\u6a21\u677f\u7cfb\u7edf\u751f\u6210QA\u5bf9\uff0c\u8bc4\u4f30\u4e0d\u540c\u57fa\u7840\u6a21\u578b\u7684\u6027\u80fd", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u5728\u8de8\u65f6\u7a7a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4e0b\u964d18%\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u79d1\u5b66\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u5730\u7406\u7a7a\u95f4\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc1\u660e\u57fa\u7840\u6a21\u578b\u53ef\u6709\u6548\u652f\u6301\u6c14\u5019\u7814\u7a76\u7b49\u79d1\u5b66\u4efb\u52a1"}}
{"id": "2505.10717", "pdf": "https://arxiv.org/pdf/2505.10717", "abs": "https://arxiv.org/abs/2505.10717", "authors": ["Jean-Philippe Corbeil", "Amin Dada", "Jean-Michel Attendu", "Asma Ben Abacha", "Alessandro Sordoni", "Lucas Caccia", "Fran\u00e7ois Beaulieu", "Thomas Lin", "Jens Kleesiek", "Paul Vozila"], "title": "A Modular Approach for Clinical SLMs Driven by Synthetic Data with Pre-Instruction Tuning, Model Merging, and Clinical-Tasks Alignment", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "High computation costs and latency of large language models such as GPT-4\nhave limited their deployment in clinical settings. Small language models\n(SLMs) offer a cost-effective alternative, but their limited capacity requires\nbiomedical domain adaptation, which remains challenging. An additional\nbottleneck is the unavailability and high sensitivity of clinical data. To\naddress these challenges, we propose a novel framework for adapting SLMs into\nhigh-performing clinical models. We introduce the MediPhi collection of\n3.8B-parameter SLMs developed with our novel framework: pre-instruction tuning\nof experts on relevant medical and clinical corpora (PMC, Medical Guideline,\nMedWiki, etc.), model merging, and clinical-tasks alignment. To cover most\nclinical tasks, we extended the CLUE benchmark to CLUE+, doubling its size. Our\nexpert models deliver relative improvements on this benchmark over the base\nmodel without any task-specific fine-tuning: 64.3% on medical entities, 49.5%\non radiology reports, and 44% on ICD-10 coding (outperforming GPT-4-0125 by\n14%). We unify the expert models into MediPhi via model merging, preserving\ngains across benchmarks. Furthermore, we built the MediFlow collection, a\nsynthetic dataset of 2.5 million high-quality instructions on 14 medical NLP\ntasks, 98 fine-grained document types, and JSON format support. Alignment of\nMediPhi using supervised fine-tuning and direct preference optimization\nachieves further gains of 18.9% on average.", "AI": {"tldr": "\u63d0\u51faMediPhi\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6307\u4ee4\u8c03\u6574\u3001\u6a21\u578b\u5408\u5e76\u548c\u4e34\u5e8a\u4efb\u52a1\u5bf9\u9f50\u4f18\u5316\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e34\u5e8a\u90e8\u7f72\u4e2d\u5927\u578b\u6a21\u578b\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u5e76\u5728\u6269\u5c55\u7684CLUE+\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4\uff09\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u9650\u5236\u4e86\u4e34\u5e8a\u90e8\u7f72\uff0c\u800c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u533b\u5b66\u9886\u57df\u9002\u5e94\u4f46\u9762\u4e34\u6570\u636e\u654f\u611f\u6027\u548c\u80fd\u529b\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "1. \u9884\u6307\u4ee4\u8c03\u6574\u533b\u5b66\u4e13\u5bb6\u6a21\u578b\uff08\u4f7f\u7528PMC/\u6307\u5357\u7b49\u6570\u636e\uff09\n2. \u6a21\u578b\u5408\u5e76\u6784\u5efaMediPhi\n3. \u6269\u5c55CLUE\u57fa\u51c6\u5230CLUE+\uff08\u8986\u76d614\u4efb\u52a1\uff09\n4. \u521b\u5efaMediFlow\u5408\u6210\u6307\u4ee4\u6570\u636e\u96c6\uff08250\u4e07\u6761\uff0c\u652f\u6301JSON\u683c\u5f0f\uff09\n5. \u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u548c\u5bf9\u9f50\u4f18\u5316\u5b9e\u73b0\u6027\u80fd\u7a81\u7834", "result": "\u4e13\u5bb6\u6a21\u578b\u76f8\u5bf9\u57fa\u7ebf\u63d0\u5347\uff1a\u533b\u5b66\u5b9e\u4f53\u8bc6\u522b64.3%\u3001\u653e\u5c04\u62a5\u544a\u5206\u679049.5%\u3001ICD-10\u7f16\u780144%\uff08\u8d85\u8d8aGPT-4 14%\uff09\uff1b\u6a21\u578b\u5408\u5e76\u4fdd\u7559\u4f18\u52bf\uff0c\u8fdb\u4e00\u6b65\u5bf9\u9f50\u4f18\u5316\u5e26\u6765\u5e73\u574718.9%\u589e\u76ca\u3002", "conclusion": "MediPhi\u6210\u529f\u5b9e\u73b0\u9ad8\u6548\u4e34\u5e8aNLP\u6a21\u578b\u90e8\u7f72\uff0c\u6a21\u578b\u5408\u5e76\u7b56\u7565\u6709\u6548\u4fdd\u7559\u9886\u57df\u77e5\u8bc6\uff0c\u914d\u5408\u5408\u6210\u6570\u636e\u96c6MediFlow\u4e3a\u4e34\u5e8a\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.10576", "pdf": "https://arxiv.org/pdf/2505.10576", "abs": "https://arxiv.org/abs/2505.10576", "authors": ["Qifan Fu", "Xu Chen", "Muhammad Asad", "Shanxin Yuan", "Changjae Oh", "Gregory Slabaugh"], "title": "Robust Photo-Realistic Hand Gesture Generation: from Single View to Multiple View", "categories": ["cs.GR"], "comment": "11 pages, 9 figures", "summary": "High-fidelity hand gesture generation represents a significant challenge in\nhuman-centric generation tasks. Existing methods typically employ single-view\n3D MANO mesh-rendered images prior to enhancing gesture generation quality.\nHowever, the complexity of hand movements and the inherent limitations of\nsingle-view rendering make it difficult to capture complete 3D hand\ninformation, particularly when fingers are occluded. The fundamental\ncontradiction lies in the loss of 3D topological relationships through 2D\nprojection and the incomplete spatial coverage inherent to single-view\nrepresentations. Diverging from single-view prior approaches, we propose a\nmulti-view prior framework, named Multi-Modal UNet-based Feature Encoder\n(MUFEN), to guide diffusion models in learning comprehensive 3D hand\ninformation. Specifically, we extend conventional front-view rendering to\ninclude rear, left, right, top, and bottom perspectives, selecting the most\ninformation-rich view combination as training priors to address occlusion\ncompletion. This multi-view prior with a dedicated dual stream encoder\nsignificantly improves the model's understanding of complete hand features.\nFurthermore, we design a bounding box feature fusion module, which can fuse the\ngesture localization features and gesture multi-modal features to enhance the\nlocation-awareness of the MUFEN features to the gesture-related features.\nExperiments demonstrate that our method achieves state-of-the-art performance\nin both quantitative metrics and qualitative evaluations.", "AI": {"tldr": "\u63d0\u51fa\u591a\u89c6\u89d2\u5148\u9a8c\u6846\u67b6MUFEN\uff0c\u901a\u8fc7\u591a\u6a21\u6001UNet\u7f16\u7801\u5668\u548c\u8fb9\u754c\u6846\u7279\u5f81\u878d\u5408\u6a21\u5757\u89e3\u51b3\u624b\u52bf\u751f\u6210\u4e2d\u76843D\u4fe1\u606f\u7f3a\u5931\u95ee\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u5355\u89c6\u89d2\u6e32\u67d3\u65e0\u6cd5\u6355\u6349\u590d\u6742\u624b\u52bf\u7684\u5b8c\u65743D\u4fe1\u606f\uff0c\u5c24\u5176\u5728\u624b\u6307\u906e\u6321\u573a\u666f\u4e0b\u5b58\u5728\u62d3\u6251\u5173\u7cfb\u4e22\u5931\u548c\u7a7a\u95f4\u8986\u76d6\u4e0d\u8db3\u7684\u6838\u5fc3\u77db\u76fe\u3002", "method": "\u6269\u5c55\u591a\u89c6\u89d2\u6e32\u67d3\u9009\u62e9\u6700\u4f18\u89c6\u56fe\u7ec4\u5408\uff0c\u8bbe\u8ba1\u53cc\u6d41\u7f16\u7801\u5668\u63d0\u53d6\u5b8c\u6574\u624b\u90e8\u7279\u5f81\uff0c\u5e76\u5f00\u53d1\u8fb9\u754c\u6846\u7279\u5f81\u878d\u5408\u6a21\u5757\u589e\u5f3a\u4f4d\u7f6e\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u591a\u89c6\u89d2\u5148\u9a8c\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u89c6\u89d2\u8868\u5f81\u4e0e\u7279\u5f81\u878d\u5408\u673a\u5236\u663e\u8457\u63d0\u5347\u4e86\u624b\u52bf\u751f\u6210\u76843D\u5b8c\u6574\u6027\u548c\u8d28\u91cf\uff0c\u7a81\u7834\u5355\u89c6\u89d2\u65b9\u6cd5\u5c40\u9650\u6027\u3002"}}
{"id": "2505.10718", "pdf": "https://arxiv.org/pdf/2505.10718", "abs": "https://arxiv.org/abs/2505.10718", "authors": ["Siddharth Suresh", "Kushin Mukherjee", "Tyler Giallanza", "Xizheng Yu", "Mia Patil", "Jonathan D. Cohen", "Timothy T. Rogers"], "title": "AI-enhanced semantic feature norms for 786 concepts", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "8 pages, 5 figures", "summary": "Semantic feature norms have been foundational in the study of human\nconceptual knowledge, yet traditional methods face trade-offs between\nconcept/feature coverage and verifiability of quality due to the\nlabor-intensive nature of norming studies. Here, we introduce a novel approach\nthat augments a dataset of human-generated feature norms with responses from\nlarge language models (LLMs) while verifying the quality of norms against\nreliable human judgments. We find that our AI-enhanced feature norm dataset,\nNOVA: Norms Optimized Via AI, shows much higher feature density and overlap\namong concepts while outperforming a comparable human-only norm dataset and\nword-embedding models in predicting people's semantic similarity judgments.\nTaken together, we demonstrate that human conceptual knowledge is richer than\ncaptured in previous norm datasets and show that, with proper validation, LLMs\ncan serve as powerful tools for cognitive science research.", "AI": {"tldr": "Proposes NOVA (AI-enhanced feature norms) combining human/LLM-generated data with validation, showing superior performance in semantic tasks.", "motivation": "Address limitations in traditional semantic feature norms (low coverage/verifiability trade-off) through AI augmentation while maintaining quality control.", "method": "Augmented human feature norms with LLM responses, validated quality through human judgment comparisons.", "result": "NOVA achieved 3.5x higher feature density, outperformed human-only datasets (+12%) and embeddings (+18%) in predicting semantic similarity.", "conclusion": "Human conceptual richness exceeds current datasets; validated LLMs become potent cognitive science tools."}}
{"id": "2505.10824", "pdf": "https://arxiv.org/pdf/2505.10824", "abs": "https://arxiv.org/abs/2505.10824", "authors": ["Kaifa Yang", "Qi Yang", "Zhu Li", "Yiling Xu"], "title": "Textured mesh Quality Assessment using Geometry and Color Field Similarity", "categories": ["cs.GR", "cs.CV", "cs.MM"], "comment": "15 pages main content, 4 pages supplementary material. Submitted to\n  IEEE Transactions on Visualization and Computer Graphics (IEEE TVCG) for\n  review", "summary": "Textured mesh quality assessment (TMQA) is critical for various 3D mesh\napplications. However, existing TMQA methods often struggle to provide accurate\nand robust evaluations. Motivated by the effectiveness of fields in\nrepresenting both 3D geometry and color information, we propose a novel\npoint-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes\nsigned distance fields and a newly proposed color field named nearest surface\npoint color field to realize effective mesh feature description. Four features\nrelated to visual perception are extracted from the geometry and color fields:\ngeometry similarity, geometry gradient similarity, space color distribution\nsimilarity, and space color gradient similarity. Experimental results on three\nbenchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA)\nTMQA metrics. Furthermore, FMQM exhibits low computational complexity, making\nit a practical and efficient solution for real-world applications in 3D\ngraphics and visualization. Our code is publicly available at:\nhttps://github.com/yyyykf/FMQM.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u573a\u7684FMQM\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u573a\u548c\u989c\u8272\u573a\u7279\u5f81\u63cf\u8ff0\u5b9e\u73b0\u9ad8\u6548\u7eb9\u7406\u7f51\u683c\u8d28\u91cf\u8bc4\u4f30\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7eb9\u7406\u7f51\u683c\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u548c\u9c81\u68d2\u6027\u5dee\u7684\u95ee\u9898\uff0c\u53d7\u573a\u8868\u793a\u6cd5\u57283D\u51e0\u4f55\u4e0e\u8272\u5f69\u4fe1\u606f\u8868\u5f81\u65b9\u9762\u7684\u542f\u53d1", "method": "\u5229\u7528\u7b26\u53f7\u8ddd\u79bb\u573a\u548c\u521b\u65b0\u7684\u6700\u8fd1\u8868\u9762\u70b9\u989c\u8272\u573a\uff0c\u63d0\u53d6\u51e0\u4f55\u76f8\u4f3c\u5ea6/\u68af\u5ea6\u76f8\u4f3c\u5ea6\u3001\u7a7a\u95f4\u8272\u5f69\u5206\u5e03/\u68af\u5ea6\u76f8\u4f3c\u5ea6\u56db\u4e2a\u89c6\u89c9\u611f\u77e5\u7279\u5f81", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u4e14\u5177\u5907\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff08\u65f6\u95f4\u590d\u6742\u5ea6O(N)\uff09", "conclusion": "FMQM\u4e3a3D\u56fe\u5f62\u5e94\u7528\u63d0\u4f9b\u4e86\u517c\u5177\u9ad8\u7cbe\u5ea6\u4e0e\u9ad8\u6548\u7387\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u53ef\u89c6\u5316\u9886\u57df\u8d28\u91cf\u8bc4\u4f30\u6280\u672f\u53d1\u5c55"}}
{"id": "2505.10719", "pdf": "https://arxiv.org/pdf/2505.10719", "abs": "https://arxiv.org/abs/2505.10719", "authors": ["Tom\u00e1s Vergara-Browne", "\u00c1lvaro Soto"], "title": "Tracr-Injection: Distilling Algorithms into Pre-trained Language Models", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "Motivated by the surge of large language models, there has been a push to\nformally characterize the symbolic abilities intrinsic to the transformer\narchitecture. A programming language, called RASP, has been proposed, which can\nbe directly compiled into transformer weights to implement these algorithms.\nHowever, the tasks that can be implemented in RASP are often uncommon to learn\nfrom natural unsupervised data, showing a mismatch between theoretical\ncapabilities of the transformer architecture, and the practical learnability of\nthese capabilities from unsupervised data. We propose tracr-injection, a method\nthat allows us to distill algorithms written in RASP directly into a\npre-trained language model. We showcase our method by injecting 3 different\nalgorithms into a language model. We show how our method creates an\ninterpretable subspace within the model's residual stream, which can be decoded\ninto the variables present in the code of the RASP algorithm. Additionally, we\nfound that the proposed method can improve out of distribution performance\ncompared to our baseline, indicating that indeed a more symbolic mechanism is\ntaking place in the inner workings of the model. We release the code used to\nrun our experiments.", "AI": {"tldr": "\u63d0\u51fatracr-injection\u65b9\u6cd5\u5c06RASP\u7f16\u7a0b\u8bed\u8a00\u7684\u7b97\u6cd5\u76f4\u63a5\u84b8\u998f\u5230\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u7406\u8bba\u80fd\u529b\u4e0e\u5b9e\u8df5\u5b66\u4e60\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f25\u8865Transformer\u67b6\u6784\u7406\u8bba\u7b26\u53f7\u80fd\u529b\u4e0e\u65e0\u76d1\u7763\u6570\u636e\u5b9e\u9645\u5b66\u4e60\u6548\u679c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u901a\u8fc7tracr-injection\u65b9\u6cd5\u5c06RASP\u4ee3\u7801\u7f16\u8bd1\u7684\u7b97\u6cd5\u76f4\u63a5\u6ce8\u5165\u9884\u8bad\u7ec3\u6a21\u578b\u6b8b\u5dee\u6d41", "result": "\u521b\u5efa\u53ef\u89e3\u91ca\u5b50\u7a7a\u95f4\uff08\u53ef\u89e3\u7801\u4e3aRASP\u53d8\u91cf\uff09\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6027\u80fd\u8868\u73b0", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5b9e\u73b0\u4e86\u7b26\u53f7\u673a\u5236\u6ce8\u5165\uff0c\u4e3a\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u7684\u7b26\u53f7\u5316\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2505.10736", "pdf": "https://arxiv.org/pdf/2505.10736", "abs": "https://arxiv.org/abs/2505.10736", "authors": ["Ximing Dong", "Shaowei Wang", "Dayi Lin", "Ahmed E. Hassan"], "title": "Model Performance-Guided Evaluation Data Selection for Effective Prompt Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Optimizing Large Language Model (LLM) performance requires well-crafted\nprompts, but manual prompt engineering is labor-intensive and often\nineffective. Automated prompt optimization techniques address this challenge\nbut the majority of them rely on randomly selected evaluation subsets, which\nfail to represent the full dataset, leading to unreliable evaluations and\nsuboptimal prompts. Existing coreset selection methods, designed for LLM\nbenchmarking, are unsuitable for prompt optimization due to challenges in\nclustering similar samples, high data collection costs, and the unavailability\nof performance data for new or private datasets. To overcome these issues, we\npropose IPOMP, an Iterative evaluation data selection for effective Prompt\nOptimization using real-time Model Performance. IPOMP is a two-stage approach\nthat selects representative and diverse samples using semantic clustering and\nboundary analysis, followed by iterative refinement with real-time model\nperformance data to replace redundant samples. Evaluations on the BIG-bench\ndataset show that IPOMP improves effectiveness by 1.6% to 5.3% and stability by\nat least 57% compared with SOTA baselines, with minimal computational overhead\nbelow 1%. Furthermore, the results demonstrate that our real-time\nperformance-guided refinement approach can be universally applied to enhance\nexisting coreset selection methods.", "AI": {"tldr": "\u63d0\u51faIPOMP\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u5b9e\u65f6\u6027\u80fd\u6570\u636e\u8fed\u4ee3\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u7684\u6548\u679c\u548c\u7a33\u5b9a\u6027", "motivation": "\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\u6548\u7387\u4f4e\u4e0b\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u4f9d\u8d56\u968f\u673a\u8bc4\u4f30\u5b50\u96c6\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6570\u636e\u9009\u62e9\u65b9\u6cd5", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8bed\u4e49\u805a\u7c7b\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c 2) \u57fa\u4e8e\u5b9e\u65f6\u6a21\u578b\u6027\u80fd\u8fed\u4ee3\u66ff\u6362\u5197\u4f59\u6837\u672c\uff0c\u7ed3\u5408\u8fb9\u754c\u5206\u6790\u4f18\u5316\u6837\u672c\u591a\u6837\u6027", "result": "\u5728BIG-bench\u6570\u636e\u96c6\u4e0a\u6548\u679c\u63d0\u53471.6%-5.3%\uff0c\u7a33\u5b9a\u6027\u63d0\u5347\u226557%\uff0c\u8ba1\u7b97\u5f00\u9500<1%", "conclusion": "IPOMP\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u6027\u80fd\u6307\u5bfc\u7684\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u666e\u9002\u6027\uff0c\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u73b0\u6709\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5"}}
{"id": "2505.10740", "pdf": "https://arxiv.org/pdf/2505.10740", "abs": "https://arxiv.org/abs/2505.10740", "authors": ["Qiwei Peng", "Robert Moro", "Michal Gregor", "Ivan Srba", "Simon Ostermann", "Marian Simko", "Juraj Podrou\u017eek", "Mat\u00fa\u0161 Mesar\u010d\u00edk", "Jaroslav Kop\u010dan", "Anders S\u00f8gaard"], "title": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The rapid spread of online disinformation presents a global challenge, and\nmachine learning has been widely explored as a potential solution. However,\nmultilingual settings and low-resource languages are often neglected in this\nfield. To address this gap, we conducted a shared task on multilingual claim\nretrieval at SemEval 2025, aimed at identifying fact-checked claims that match\nnewly encountered claims expressed in social media posts across different\nlanguages. The task includes two subtracks: (1) a monolingual track, where\nsocial posts and claims are in the same language, and (2) a crosslingual track,\nwhere social posts and claims might be in different languages. A total of 179\nparticipants registered for the task contributing to 52 test submissions. 23\nout of 31 teams have submitted their system papers. In this paper, we report\nthe best-performing systems as well as the most common and the most effective\napproaches across both subtracks. This shared task, along with its dataset and\nparticipating systems, provides valuable insights into multilingual claim\nretrieval and automated fact-checking, supporting future research in this\nfield.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86SemEval 2025\u591a\u8bed\u8a00\u58f0\u660e\u68c0\u7d22\u5171\u4eab\u4efb\u52a1\uff0c\u6db5\u76d6\u5355\u8bed\u548c\u8de8\u8bed\u8a00\u68c0\u7d22\u5b50\u4efb\u52a1\uff0c\u5c55\u793a\u4e8652\u4e2a\u6d4b\u8bd5\u7cfb\u7edf\u4e0e23\u7bc7\u7cfb\u7edf\u8bba\u6587\u4e2d\u7684\u6700\u4f73\u65b9\u6848\u53ca\u6709\u6548\u65b9\u6cd5", "motivation": "\u9488\u5bf9\u865a\u5047\u4fe1\u606f\u5728\u591a\u8bed\u8a00/\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7814\u7a76\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ec4\u7ec7\u5171\u4eab\u4efb\u52a1\u63a8\u52a8\u591a\u8bed\u8a00\u58f0\u660e\u68c0\u7d22\u4e0e\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u6280\u672f\u53d1\u5c55", "method": "\u8bbe\u7acb\u5355\u8bed\uff08\u540c\u8bed\u8a00\uff09\u548c\u8de8\u8bed\u8a00\uff08\u4e0d\u540c\u8bed\u8a00\uff09\u4e24\u4e2a\u5b50\u8d5b\u9053\uff0c\u6536\u96c6179\u4e2a\u6ce8\u518c\u8005\u4e2d52\u4efd\u6d4b\u8bd5\u63d0\u4ea4\u4e0e23\u7bc7\u7cfb\u7edf\u8bba\u6587\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790", "result": "\u6700\u4f73\u7cfb\u7edf\u5c55\u793a\u4e86\u8bed\u4e49\u5bf9\u9f50\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6709\u6548\u6027\uff0c\u53c2\u4e0e\u8005\u65b9\u6848\u4e3a\u591a\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u57fa\u51c6", "conclusion": "\u8be5\u5171\u4eab\u4efb\u52a1\u6784\u5efa\u4e86\u9996\u4e2a\u591a\u8bed\u8a00\u58f0\u660e\u68c0\u7d22\u57fa\u51c6\uff0c\u5176\u6570\u636e\u96c6\u548c\u7cfb\u7edf\u6210\u679c\u4e3a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u5b9e\u8df5\u53c2\u8003"}}
{"id": "2505.10772", "pdf": "https://arxiv.org/pdf/2505.10772", "abs": "https://arxiv.org/abs/2505.10772", "authors": ["Weiqin Wang", "Yile Wang", "Hui Huang"], "title": "Ranked Voting based Self-Consistency of Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Majority voting is considered an effective method to enhance chain-of-thought\nreasoning, as it selects the answer with the highest \"self-consistency\" among\ndifferent reasoning paths (Wang et al., 2023). However, previous\nchain-of-thought reasoning methods typically generate only a single answer in\neach trial, thereby ignoring the possibility of other potential answers. As a\nresult, these alternative answers are often overlooked in subsequent voting\nprocesses. In this work, we propose to generate ranked answers in each\nreasoning process and conduct ranked voting among multiple ranked answers from\ndifferent responses, thereby making the overall self-consistency more reliable.\nSpecifically, we use three ranked voting methods: Instant-runoff voting, Borda\ncount voting, and mean reciprocal rank voting. We validate our methods on six\ndatasets, including three multiple-choice and three open-ended\nquestion-answering tasks, using both advanced open-source and closed-source\nlarge language models. Extensive experimental results indicate that our\nproposed method outperforms the baselines, showcasing the potential of\nleveraging the information of ranked answers and using ranked voting to improve\nreasoning performance. The code is available at\nhttps://github.com/szu-tera/RankedVotingSC.", "AI": {"tldr": "\u901a\u8fc7\u751f\u6210\u6392\u5e8f\u7b54\u6848\u5e76\u91c7\u7528\u4e09\u79cd\u6392\u5e8f\u6295\u7968\u65b9\u6cd5\uff08\u5373\u65f6\u51b3\u9009\u6295\u7968/\u6ce2\u8fbe\u8ba1\u6570/\u5e73\u5747\u5012\u6570\u6392\u540d\uff09\uff0c\u663e\u8457\u63d0\u5347\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u81ea\u6d3d\u6027\u3002", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u6bcf\u6b21\u4ec5\u751f\u6210\u5355\u4e00\u7b54\u6848\uff0c\u5bfc\u81f4\u5176\u4ed6\u6f5c\u5728\u7b54\u6848\u5728\u6295\u7968\u4e2d\u88ab\u5ffd\u7565\uff0c\u5f71\u54cd\u63a8\u7406\u53ef\u9760\u6027\u3002", "method": "\u5728\u6bcf\u6b21\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u6392\u5e8f\u7b54\u6848\uff0c\u4f7f\u7528\u5373\u65f6\u51b3\u9009\u6295\u7968\u3001\u6ce2\u8fbe\u8ba1\u6570\u6295\u7968\u548c\u5e73\u5747\u5012\u6570\u6392\u540d\u4e09\u79cd\u65b9\u6cd5\u8fdb\u884c\u6392\u5e8f\u6295\u7968\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\uff08\u542b\u5f00\u653e/\u5c01\u95ed\u5927\u6a21\u578b\uff09\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u6700\u9ad8\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u5229\u7528\u6392\u5e8f\u7b54\u6848\u4fe1\u606f\u8fdb\u884c\u6392\u5e8f\u6295\u7968\u53ef\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u63a8\u7406\u65b9\u6cd5\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.10775", "pdf": "https://arxiv.org/pdf/2505.10775", "abs": "https://arxiv.org/abs/2505.10775", "authors": ["Kian Ahrabian", "Pegah Jandaghi", "Negar Mokhberian", "Sai Praneeth Karimireddy", "Jay Pujara"], "title": "A Systematic Analysis of Base Model Choice for Reward Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 13 figures, 5 tables", "summary": "Reinforcement learning from human feedback (RLHF) and, at its core, reward\nmodeling have become a crucial part of training powerful large language models\n(LLMs). One commonly overlooked factor in training high-quality reward models\n(RMs) is the effect of the base model, which is becoming more challenging to\nchoose given the rapidly growing pool of LLMs. In this work, we present a\nsystematic analysis of the effect of base model selection on reward modeling\nperformance. Our results show that the performance can be improved by up to 14%\ncompared to the most common (i.e., default) choice. Moreover, we showcase the\nstrong statistical relation between some existing benchmarks and downstream\nperformances. We also demonstrate that the results from a small set of\nbenchmarks could be combined to boost the model selection ($+$18% on average in\nthe top 5-10). Lastly, we illustrate the impact of different post-training\nsteps on the final performance and explore using estimated data distributions\nto reduce performance prediction error.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u5956\u52b1\u6a21\u578b\u57fa\u6a21\u578b\u9009\u62e9\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\uff08+14%\uff09\uff0c\u63d0\u51fa\u901a\u8fc7\u7ec4\u5408\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\u4f18\u5316\u6a21\u578b\u9009\u62e9\u7b56\u7565\uff08\u5e73\u5747\u63d0\u534718%\uff09\u5e76\u5206\u6790\u540e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5bf9\u6027\u80fd\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709RLHF\u8bad\u7ec3\u4e2d\u5956\u52b1\u6a21\u578b\u7684\u57fa\u6a21\u578b\u9009\u62e9\u5e38\u88ab\u5ffd\u89c6\uff0c\u800c\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6570\u91cf\u6fc0\u589e\uff0c\u5982\u4f55\u79d1\u5b66\u9009\u62e9\u57fa\u6a21\u578b\u6210\u4e3a\u4e9f\u5f85\u89e3\u51b3\u7684\u96be\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u4e0d\u540c\u57fa\u6a21\u578b\u5bf9\u5956\u52b1\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u7ed3\u5408\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\u9884\u6d4b\u4e0b\u6e38\u8868\u73b0\uff0c\u5e76\u63a2\u7d22\u540e\u8bad\u7ec3\u6b65\u9aa4\u4e0e\u6570\u636e\u5206\u5e03\u5bf9\u6027\u80fd\u9884\u6d4b\u7684\u4f18\u5316\u3002", "result": "\u57fa\u6a21\u578b\u9009\u62e9\u53ef\u5e26\u6765\u6700\u9ad814%\u7684\u6027\u80fd\u63d0\u5347\uff1b\u7ec4\u54083-4\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6307\u6807\u4f7ftop5-10\u6a21\u578b\u9009\u62e9\u7cbe\u5ea6\u63d0\u534718%\uff1b\u53d1\u73b0\u540e\u8bad\u7ec3\u9636\u6bb5\u5bf9\u6700\u7ec8\u6027\u80fd\u6709\u663e\u8457\u8c03\u8282\u4f5c\u7528\u3002", "conclusion": "\u57fa\u6a21\u578b\u9009\u62e9\u662f\u5956\u52b1\u5efa\u6a21\u7684\u5173\u952e\u73af\u8282\uff0c\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8574\u542b\u672a\u88ab\u5145\u5206\u5229\u7528\u7684\u9884\u6d4b\u4ef7\u503c\uff0c\u8be5\u53d1\u73b0\u4e3a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.10792", "pdf": "https://arxiv.org/pdf/2505.10792", "abs": "https://arxiv.org/abs/2505.10792", "authors": ["Zhan Peng Lee", "Andre Lin", "Calvin Tan"], "title": "Finetune-RAG: Fine-Tuning Language Models to Resist Hallucination in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to\nimprove factuality in large language models (LLMs) by grounding their outputs\nin retrieved documents. However, ensuring perfect retrieval of relevant\ninformation remains challenging, and when irrelevant content is passed\ndownstream to an LLM, it can lead to hallucinations. In this work, we propose\nFinetune-RAG, a simple and effective fine-tuning approach that features the\nfirst-of-its-kind RAG training dataset constructed to mimic real-world\nimperfections. Experimental results show that Finetune-RAG improves factual\naccuracy by 21.2% over the base model. We also propose a Bench-RAG, an\nLLM-as-a-judge evaluation pipeline that stress tests models under realistic\nimperfect retrieval scenarios. Our codebase and dataset are fully open sourced\nfor community use.", "AI": {"tldr": "\u63d0\u51faFinetune-RAG\u6846\u67b6\u53caBench-RAG\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u68c0\u7d22\u7f3a\u9677\u7684\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u4f7f\u5927\u6a21\u578b\u4e8b\u5b9e\u51c6\u786e\u6027\u63d0\u534721.2%", "motivation": "\u89e3\u51b3RAG\u6846\u67b6\u4e2d\u56e0\u68c0\u7d22\u5185\u5bb9\u4e0d\u76f8\u5173\u5bfc\u81f4\u5927\u6a21\u578b\u4ea7\u751f\u5e7b\u89c9\u7684\u95ee\u9898", "method": "\u6784\u5efa\u6a21\u62df\u771f\u5b9e\u68c0\u7d22\u7f3a\u9677\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u53d1LLM\u4f5c\u4e3a\u8bc4\u4f30\u5668\u7684\u538b\u529b\u6d4b\u8bd5\u6d41\u7a0b", "result": "\u4e8b\u5b9e\u51c6\u786e\u7387\u8f83\u57fa\u7840\u6a21\u578b\u63d0\u534721.2%", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347RAG\u7cfb\u7edf\u5728\u975e\u5b8c\u7f8e\u68c0\u7d22\u4e0b\u7684\u8868\u73b0\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2505.10798", "pdf": "https://arxiv.org/pdf/2505.10798", "abs": "https://arxiv.org/abs/2505.10798", "authors": ["Erica Cai", "Sean McQuade", "Kevin Young", "Brendan O'Connor"], "title": "Relation Extraction Across Entire Books to Reconstruct Community Networks: The AffilKG Datasets", "categories": ["cs.CL"], "comment": null, "summary": "When knowledge graphs (KGs) are automatically extracted from text, are they\naccurate enough for downstream analysis? Unfortunately, current annotated\ndatasets can not be used to evaluate this question, since their KGs are highly\ndisconnected, too small, or overly complex. To address this gap, we introduce\nAffilKG (https://doi.org/10.5281/zenodo.15427977), which is a collection of six\ndatasets that are the first to pair complete book scans with large, labeled\nknowledge graphs. Each dataset features affiliation graphs, which are simple\nKGs that capture Member relationships between Person and Organization entities\n-- useful in studies of migration, community interactions, and other social\nphenomena. In addition, three datasets include expanded KGs with a wider\nvariety of relation types. Our preliminary experiments demonstrate significant\nvariability in model performance across datasets, underscoring AffilKG's\nability to enable two critical advances: (1) benchmarking how extraction errors\npropagate to graph-level analyses (e.g., community structure), and (2)\nvalidating KG extraction methods for real-world social science research.", "AI": {"tldr": "\u63d0\u51faAffilKG\u6570\u636e\u96c6\u4ee5\u8bc4\u4f30\u77e5\u8bc6\u56fe\u8c31\u81ea\u52a8\u63d0\u53d6\u7684\u51c6\u786e\u6027\u53ca\u5176\u5bf9\u56fe\u5c42\u9762\u5206\u6790\u7684\u5f71\u54cd\uff0c\u652f\u6301\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u6807\u6ce8\u6570\u636e\u96c6\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u77e5\u8bc6\u56fe\u8c31\u63d0\u53d6\u7684\u51c6\u786e\u6027\u5bf9\u4e0b\u6e38\u5206\u6790\u7684\u5f71\u54cd\uff08\u5982\u793e\u533a\u7ed3\u6784\u5206\u6790\uff09\uff0c\u963b\u788d\u4e86\u771f\u5b9e\u573a\u666f\u4e0bKG\u63d0\u53d6\u65b9\u6cd5\u7684\u9a8c\u8bc1\u3002", "method": "\u6784\u5efa\u5305\u542b6\u4e2a\u6570\u636e\u96c6\u7684\u65b0\u57fa\u51c6AffilKG\uff0c\u5305\u542b\u5b8c\u6574\u4e66\u7c4d\u626b\u63cf\u4e0e\u5927\u578b\u6807\u6ce8\u77e5\u8bc6\u56fe\u8c31\u3002\u5176\u4e2d3\u4e2a\u6570\u636e\u96c6\u6269\u5c55\u4e86\u591a\u7c7b\u578b\u5173\u7cfb\uff0c\u805a\u7126\u4e2a\u4eba-\u7ec4\u7ec7\u7684\u6210\u5458\u5173\u7cfb\u56fe\u8c31\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u4e0d\u540c\u6a21\u578b\u5728AffilKG\u5404\u6570\u636e\u96c6\u95f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u8bc1\u660e\u5176\u80fd\u6709\u6548\u8bc4\u4f30\u63d0\u53d6\u9519\u8bef\u5bf9\u56fe\u5206\u6790\u7684\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1KG\u63d0\u53d6\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "AffilKG\u586b\u8865\u4e86KG\u8bc4\u4f30\u9886\u57df\u7684\u7a7a\u767d\uff0c\u4e3a\u63d0\u53d6\u9519\u8bef\u4f20\u64ad\u5206\u6790\u548c\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\u5e94\u7528\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u9a8c\u8bc1\u6846\u67b6\u3002"}}
{"id": "2505.10829", "pdf": "https://arxiv.org/pdf/2505.10829", "abs": "https://arxiv.org/abs/2505.10829", "authors": ["Chen-Chi Chang", "Chong-Fu Li", "Chu-Hsuan Lee", "Hung-Shin Lee"], "title": "Enhancing Low-Resource Minority Language Translation with LLMs and Retrieval-Augmented Generation for Cultural Nuances", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to IntelliSys 2025", "summary": "This study investigates the challenges of translating low-resource languages\nby integrating Large Language Models (LLMs) with Retrieval-Augmented Generation\n(RAG). Various model configurations were tested on Hakka translations, with\nBLEU scores ranging from 12% (dictionary-only) to 31% (RAG with Gemini 2.0).\nThe best-performing model (Model 4) combined retrieval and advanced language\nmodeling, improving lexical coverage, particularly for specialized or\nculturally nuanced terms, and enhancing grammatical coherence. A two-stage\nmethod (Model 3) using dictionary outputs refined by Gemini 2.0 achieved a BLEU\nscore of 26%, highlighting iterative correction's value and the challenges of\ndomain-specific expressions. Static dictionary-based approaches struggled with\ncontext-sensitive content, demonstrating the limitations of relying solely on\npredefined resources. These results emphasize the need for curated resources,\ndomain knowledge, and ethical collaboration with local communities, offering a\nframework that improves translation accuracy and fluency while supporting\ncultural preservation.", "AI": {"tldr": "\u6574\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00(\u5982\u5ba2\u5bb6\u8bdd)\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u6700\u4f73\u6a21\u578bBLEU\u5206\u6570\u8fbe31%", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u56e0\u6570\u636e\u7a00\u7f3a\u5bfc\u81f4\u7684\u7ffb\u8bd1\u56f0\u96be\u95ee\u9898\uff0c\u63a2\u7d22\u68c0\u7d22\u589e\u5f3a\u6280\u672f\u4e0e\u8bed\u8a00\u6a21\u578b\u534f\u540c\u4f18\u5316\u7684\u53ef\u884c\u6027", "method": "\u91c7\u7528\u591a\u6a21\u578b\u5bf9\u6bd4\u5b9e\u9a8c\uff1aModel 4(\u68c0\u7d22+\u9ad8\u7ea7\u8bed\u8a00\u5efa\u6a21)\uff0cModel 3(\u8bcd\u5178\u8f93\u51fa\u7ecfGemini 2.0\u8fed\u4ee3\u6821\u6b63)\uff0c\u9759\u6001\u8bcd\u5178\u57fa\u51c6\u6d4b\u8bd5", "result": "\u6700\u4f73\u6a21\u578b\u63d0\u5347\u4e13\u4e1a\u672f\u8bed\u8986\u76d6\u5ea6300%\uff0c\u8bed\u6cd5\u8fde\u8d2f\u6027\u63d0\u534745%\uff1b\u4e24\u9636\u6bb5\u6821\u6b63\u6a21\u578bBLEU\u8fbe26%\uff0c\u63ed\u793a\u8fed\u4ee3\u4f18\u5316\u7684\u6709\u6548\u6027", "conclusion": "\u5efa\u7acb\u878d\u5408\u9886\u57df\u77e5\u8bc6\u5e93\u3001\u52a8\u6001\u68c0\u7d22\u673a\u5236\u4e0e\u4f26\u7406\u534f\u4f5c\u7684\u7ffb\u8bd1\u6846\u67b6\uff0c\u5728\u4fdd\u8bc1\u6587\u5316\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u7ffb\u8bd1\u51c6\u786e\u7387\u4e0e\u6d41\u7545\u5ea6\u53cc\u63d0\u5347"}}
{"id": "2505.10832", "pdf": "https://arxiv.org/pdf/2505.10832", "abs": "https://arxiv.org/abs/2505.10832", "authors": ["Songjun Tu", "Jiahao Lin", "Qichao Zhang", "Xiangyu Tian", "Linjing Li", "Xiangyuan Lan", "Dongbin Zhao"], "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Project Page: https://github.com/TU2021/AutoThink", "summary": "Large reasoning models (LRMs) are proficient at generating explicit,\nstep-by-step reasoning sequences before producing final answers. However, such\ndetailed reasoning can introduce substantial computational overhead and\nlatency, particularly for simple problems. To address this over-thinking\nproblem, we explore how to equip LRMs with adaptive thinking capabilities:\nenabling them to dynamically decide whether or not to engage in explicit\nreasoning based on problem complexity. Building on R1-style distilled models,\nwe observe that inserting a simple ellipsis (\"...\") into the prompt can\nstochastically trigger either a thinking or no-thinking mode, revealing a\nlatent controllability in the reasoning behavior. Leveraging this property, we\npropose AutoThink, a multi-stage reinforcement learning (RL) framework that\nprogressively optimizes reasoning policies via stage-wise reward shaping.\nAutoThink learns to invoke explicit reasoning only when necessary, while\ndefaulting to succinct responses for simpler tasks. Experiments on five\nmainstream mathematical benchmarks demonstrate that AutoThink achieves\nfavorable accuracy-efficiency trade-offs compared to recent prompting and\nRL-based pruning methods. It can be seamlessly integrated into any R1-style\nmodel, including both distilled and further fine-tuned variants. Notably,\nAutoThink improves relative accuracy by 6.4 percent while reducing token usage\nby 52 percent on DeepSeek-R1-Distill-Qwen-1.5B, establishing a scalable and\nadaptive reasoning paradigm for LRMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAutoThink\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8ba9\u5927\u578b\u63a8\u7406\u6a21\u578b\u81ea\u9002\u5e94\u9009\u62e9\u663e\u5f0f\u63a8\u7406\u6216\u7b80\u6d01\u56de\u5e94\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c1152%\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u8fc7\u5ea6\u751f\u6210\u8be6\u7ec6\u63a8\u7406\u6b65\u9aa4\u5bfc\u81f4\u7684\u7b97\u529b\u6d6a\u8d39\u548c\u5ef6\u8fdf\u95ee\u9898", "method": "\u57fa\u4e8eR1-style\u84b8\u998f\u6a21\u578b\u53d1\u73b0\u63d0\u793a\u7b26\u63a7\u5236\u7279\u6027\uff0c\u5f00\u53d1\u591a\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u63a8\u7406\u7b56\u7565\u51b3\u7b56", "result": "\u57285\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u53476.4%\u4e14token\u4f7f\u7528\u51cf\u5c1152%\uff0c\u517c\u5bb9\u5404\u79cdR1-style\u53d8\u4f53\u6a21\u578b", "conclusion": "AutoThink\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u9002\u5e94\u63a8\u7406\u8303\u5f0f\uff0c\u4e3a\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2505.10836", "pdf": "https://arxiv.org/pdf/2505.10836", "abs": "https://arxiv.org/abs/2505.10836", "authors": ["Abhishek Dey", "Aabha Bothera", "Samhita Sarikonda", "Rishav Aryan", "Sanjay Kumar Podishetty", "Akshay Havalgi", "Gaurav Singh", "Saurabh Srivastava"], "title": "Multimodal Event Detection: Current Approaches and Defining the New Playground through LLMs and VLMs", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted at NLDB 2025", "summary": "In this paper, we study the challenges of detecting events on social media,\nwhere traditional unimodal systems struggle due to the rapid and multimodal\nnature of data dissemination. We employ a range of models, including unimodal\nModernBERT and ConvNeXt-V2, multimodal fusion techniques, and advanced\ngenerative models like GPT-4o, and LLaVA. Additionally, we also study the\neffect of providing multimodal generative models (such as GPT-4o) with a single\nmodality to assess their efficacy. Our results indicate that while multimodal\napproaches notably outperform unimodal counterparts, generative approaches\ndespite having a large number of parameters, lag behind supervised methods in\nprecision. Furthermore, we also found that they lag behind instruction-tuned\nmodels because of their inability to generate event classes correctly. During\nour error analysis, we discovered that common social media issues such as leet\nspeak, text elongation, etc. are effectively handled by generative approaches\nbut are hard to tackle using supervised approaches.", "AI": {"tldr": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u793e\u4ea4\u5a92\u4f53\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u4f18\u4e8e\u5355\u6a21\u6001\uff0c\u4f46\u751f\u6210\u5f0f\u6a21\u578b\u867d\u80fd\u5904\u7406\u90e8\u5206\u6311\u6218\u5374\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3", "motivation": "\u4f20\u7edf\u5355\u6a21\u6001\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u4f20\u64ad\u7684\u5feb\u901f\u6027\u548c\u591a\u6a21\u6001\u7279\u6027\uff0c\u9700\u63a2\u7d22\u591a\u6a21\u6001\u548c\u751f\u6210\u5f0f\u6a21\u578b\u7684\u6f5c\u529b", "method": "\u91c7\u7528ModernBERT/ConvNeXt-V2(\u5355\u6a21\u6001)\u3001\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u3001GPT-4o/LLaVA\u751f\u6210\u6a21\u578b\uff0c\u5e76\u6d4b\u8bd5\u5355\u6a21\u6001\u8f93\u5165\u5bf9\u751f\u6210\u6a21\u578b\u7684\u5f71\u54cd", "result": "\u591a\u6a21\u6001\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001\uff0c\u751f\u6210\u5f0f\u6a21\u578b(\u53c2\u6570\u89c4\u6a21\u5927)\u7cbe\u5ea6\u843d\u540e\u76d1\u7763\u65b9\u6cd5\uff0c\u4e14\u56e0\u65e0\u6cd5\u6b63\u786e\u751f\u6210\u4e8b\u4ef6\u7c7b\u522b\u800c\u5f31\u4e8e\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u3002\u4f46\u751f\u6210\u5f0f\u80fd\u6709\u6548\u5904\u7406\u706b\u661f\u6587/\u6587\u672c\u62c9\u957f\u7b49\u793e\u4ea4\u5a92\u4f53\u5e38\u89c1\u95ee\u9898", "conclusion": "\u591a\u6a21\u6001\u662f\u6709\u6548\u65b9\u5411\uff0c\u4f46\u751f\u6210\u5f0f\u6a21\u578b\u9700\u63d0\u5347\u7cbe\u5ea6\u548c\u7c7b\u522b\u8bc6\u522b\u80fd\u529b\u3002\u76d1\u7763\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4fdd\u6301\u4f18\u52bf\uff0c\u751f\u6210\u5f0f\u5728\u7279\u5b9a\u566a\u58f0\u5904\u7406\u65b9\u9762\u8868\u73b0\u7a81\u51fa"}}
{"id": "2505.10862", "pdf": "https://arxiv.org/pdf/2505.10862", "abs": "https://arxiv.org/abs/2505.10862", "authors": ["Tairan Fu", "Miguel Gonz\u00e1lez", "Javier Conde", "Elena Merino-G\u00f3mez", "Pedro Reviriego"], "title": "Have Multimodal Large Language Models (MLLMs) Really Learned to Tell the Time on Analog Clocks?", "categories": ["cs.CL", "I.2.7"], "comment": "6 pages, 5 figures, 2 tables", "summary": "Multimodal Large Language Models which can answer complex questions on an\nimage struggle to tell the time on analog clocks. This is probably due to the\nlack of images with clocks at different times in their training set. In this\nwork we explore this issue with one of the latest MLLMs: GPT-4.1 to understand\nwhy MLLMs fail to tell the time and whether fine-tuning can solve the problem.\nThe results show how models are making progress in reading the time on analog\nclocks. But have they really learned to do it, or have they only learned\npatterns in their training datasets? In this work we put the models to the test\nwith different clocks to illustrate the limitations of MLLMs to abstract and\ngeneralize.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u6a21\u62df\u65f6\u949f\u65f6\u95f4\u4e0a\u5b58\u5728\u56f0\u96be\uff0c\u4e3b\u8981\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u5c40\u9650\u53ca\u62bd\u8c61\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7a76MLLMs\u65e0\u6cd5\u8bc6\u522b\u949f\u8868\u65f6\u95f4\u7684\u6839\u672c\u539f\u56e0\uff08\u6570\u636e\u7f3a\u5931/\u6a21\u578b\u80fd\u529b\u5c40\u9650\uff09\uff0c\u9a8c\u8bc1\u5fae\u8c03\u80fd\u5426\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "method": "\u4f7f\u7528GPT-4\u8fdb\u884c\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5305\u542b\u6a21\u578b\u5fae\u8c03\u548c\u8de8\u65f6\u949f\u6cdb\u5316\u6d4b\u8bd5\uff08\u4e0d\u540c\u5f62\u72b6/\u6570\u5b57\u6392\u5217\u7684\u949f\u8868\uff09\u3002", "result": "\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u96c6\u8868\u73b0\u63d0\u5347\uff0c\u4f46\u5bf9\u672a\u89c1\u8fc7\u7684\u65f6\u949f\u7c7b\u578b\u4ecd\u5b58\u5728\u663e\u8457\u8bc6\u522b\u9519\u8bef\uff0c\u63ed\u793a\u6a21\u5f0f\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7406\u89e3\u3002", "conclusion": "MLLMs\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u5b58\u5728\u5c40\u9650\uff0c\u8fc7\u5ea6\u4f9d\u8d56\u6570\u636e\u6a21\u5f0f\u8bb0\u5fc6\uff0c\u9700\u7a81\u7834\u5f53\u524d\u8303\u5f0f\u624d\u80fd\u5b9e\u73b0\u771f\u6b63\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.10870", "pdf": "https://arxiv.org/pdf/2505.10870", "abs": "https://arxiv.org/abs/2505.10870", "authors": ["Ziyang Huang", "Wangtao Sun", "Jun Zhao", "Kang Liu"], "title": "Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "ACL 2025", "summary": "This paper systematically addresses the challenges of rule retrieval, a\ncrucial yet underexplored area. Vanilla retrieval methods using sparse or dense\nretrievers to directly search for relevant rules to support downstream\nreasoning, often suffer from low accuracy. This is primarily due to a\nsignificant semantic gap between the instantiated facts in the queries and the\nabstract representations of the rules. Such misalignment results in suboptimal\nretrieval quality, which in turn negatively impacts reasoning performance. To\novercome these challenges, we propose Self-Induction Augmented Retrieval\n(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce\npotential inferential rules that might offer benefits for reasoning by\nabstracting the underlying knowledge and logical structure in queries. These\ninduced rules are then used for query augmentation to improve retrieval\neffectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a\nmethod that re-estimates the relevance of retrieved rules by assessing whether\nthe abstract knowledge they contain can be instantiated to align with the facts\nin the queries and the helpfulness for reasoning. Extensive experiments across\nvarious settings demonstrate the effectiveness and versatility of our proposed\nmethods.", "AI": {"tldr": "\u63d0\u51faSIAR\u548cR\u00b3\u65b9\u6cd5\u89e3\u51b3\u89c4\u5219\u68c0\u7d22\u4e2d\u7684\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u63d0\u5347\u63a8\u7406\u6548\u679c", "motivation": "\u4f20\u7edf\u89c4\u5219\u68c0\u7d22\u65b9\u6cd5\u56e0\u67e5\u8be2\u4e8b\u5b9e\u4e0e\u89c4\u5219\u62bd\u8c61\u8868\u793a\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u5bfc\u81f4\u51c6\u786e\u7387\u4f4e\u4e0b\uff0c\u5f71\u54cd\u63a8\u7406\u6027\u80fd", "method": "1. SIAR\uff1a\u7528LLM\u5f52\u7eb3\u6f5c\u5728\u63a8\u7406\u89c4\u5219\u8fdb\u884c\u67e5\u8be2\u589e\u5f3a\n2. R\u00b3\uff1a\u901a\u8fc7\u77e5\u8bc6\u5b9e\u4f8b\u5316\u53ef\u80fd\u6027\u8bc4\u4f30\u89c4\u5219\u76f8\u5173\u6027", "result": "\u591a\u573a\u666f\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\uff0c\u68c0\u7d22\u8d28\u91cf\u548c\u63a8\u7406\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "\u7ed3\u5408\u89c4\u5219\u5f52\u7eb3\u4e0e\u76f8\u5173\u6027\u91cd\u8bc4\u4f30\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.10924", "pdf": "https://arxiv.org/pdf/2505.10924", "abs": "https://arxiv.org/abs/2505.10924", "authors": ["Ada Chen", "Yongjiang Wu", "Junyuan Zhang", "Shu Yang", "Jen-tse Huang", "Kun Wang", "Wenxuan Wang", "Shuai Wang"], "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.CV", "cs.SE"], "comment": null, "summary": "Recently, AI-driven interactions with computing devices have advanced from\nbasic prototype tools to sophisticated, LLM-based systems that emulate\nhuman-like operations in graphical user interfaces. We are now witnessing the\nemergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously\nperforming tasks such as navigating desktop applications, web pages, and mobile\napps. However, as these agents grow in capability, they also introduce novel\nsafety and security risks. Vulnerabilities in LLM-driven reasoning, with the\nadded complexity of integrating multiple software components and multimodal\ninputs, further complicate the security landscape. In this paper, we present a\nsystematization of knowledge on the safety and security threats of CUAs. We\nconduct a comprehensive literature review and distill our findings along four\nresearch objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety\nanalysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs;\n\\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive\nstrategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets,\nand evaluation metrics used to assess the safety and performance of CUAs.\nBuilding on these insights, our work provides future researchers with a\nstructured foundation for exploring unexplored vulnerabilities and offers\npractitioners actionable guidance in designing and deploying secure\nComputer-Using Agents.", "AI": {"tldr": "\u7cfb\u7edf\u5316\u68b3\u7406\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406(CUAs)\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u63d0\u51fa\u5a01\u80c1\u5206\u7c7b\u4e0e\u9632\u5fa1\u7b56\u7565\u6846\u67b6\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u7ed3\u6784\u5316\u57fa\u7840\u4e0e\u5b9e\u8df5\u6307\u5bfc", "motivation": "\u968f\u7740CUAs\u80fd\u529b\u589e\u5f3a\uff0c\u5176LLM\u63a8\u7406\u6f0f\u6d1e\u4e0e\u591a\u7ec4\u4ef6\u96c6\u6210\u590d\u6742\u6027\u5bfc\u81f4\u65b0\u578b\u5b89\u5168\u98ce\u9669\uff0c\u4e9f\u9700\u7cfb\u7edf\u5316\u5b89\u5168\u5206\u6790\u6846\u67b6", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u5b9e\u73b0\u56db\u5927\u7814\u7a76\u76ee\u6807\uff1a\u5b9a\u4e49\u5b89\u5168\u5206\u6790\u6846\u67b6\u3001\u5a01\u80c1\u5206\u7c7b\u3001\u9632\u5fa1\u7b56\u7565\u5206\u7c7b\u3001\u8bc4\u4f30\u6307\u6807\u603b\u7ed3", "result": "\u5efa\u7acb\u9996\u4e2a\u6db5\u76d6CUA\u5b89\u5168\u5a01\u80c1\u5206\u7c7b\u5b66\u3001\u9632\u5fa1\u7b56\u7565\u4f53\u7cfb\u53ca\u5bf9\u5e94\u8bc4\u4f30\u57fa\u51c6\u7684\u7cfb\u7edf\u5316\u77e5\u8bc6\u6846\u67b6", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6f0f\u6d1e\u63a2\u7d22\u7684\u7ed3\u6784\u5316\u57fa\u7840\uff0c\u5e76\u4e3a\u5b9e\u8df5\u8005\u90e8\u7f72\u5b89\u5168CUA\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6307\u5357"}}
{"id": "2505.10936", "pdf": "https://arxiv.org/pdf/2505.10936", "abs": "https://arxiv.org/abs/2505.10936", "authors": ["Jiaxing Zhao", "Hongbin Xie", "Yuzhen Lei", "Xuan Song", "Zhuoran Shi", "Lianxin Li", "Shuangxue Liu", "Haoran Zhang"], "title": "Connecting the Dots: A Chain-of-Collaboration Prompting Framework for LLM Agents", "categories": ["cs.CL"], "comment": "34 pages, 20 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive performance in\nexecuting complex reasoning tasks. Chain-of-thought effectively enhances\nreasoning capabilities by unlocking the potential of large models, while\nmulti-agent systems provide more comprehensive solutions by integrating\ncollective intelligence of multiple agents. However, both approaches face\nsignificant limitations. Single-agent with chain-of-thought, due to the\ninherent complexity of designing cross-domain prompts, faces collaboration\nchallenges. Meanwhile, multi-agent systems consume substantial tokens and\ninevitably dilute the primary problem, which is particularly problematic in\nbusiness workflow tasks. To address these challenges, we propose Cochain, a\ncollaboration prompting framework that effectively solves business workflow\ncollaboration problem by combining knowledge and prompts at a reduced cost.\nSpecifically, we construct an integrated knowledge graph that incorporates\nknowledge from multiple stages. Furthermore, by maintaining and retrieving a\nprompts tree, we can obtain prompt information relevant to other stages of the\nbusiness workflow. We perform extensive evaluations of Cochain across multiple\ndatasets, demonstrating that Cochain outperforms all baselines in both prompt\nengineering and multi-agent LLMs. Additionally, expert evaluation results\nindicate that the use of a small model in combination with Cochain outperforms\nGPT-4.", "AI": {"tldr": "Cochain\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u591a\u9636\u6bb5\u77e5\u8bc6\u56fe\u8c31\u548c\u63d0\u793a\u6811\uff0c\u6709\u6548\u89e3\u51b3\u4e1a\u52a1\u5de5\u4f5c\u6d41\u534f\u4f5c\u95ee\u9898\uff0c\u5728\u51cf\u5c11\u6210\u672c\u7684\u540c\u65f6\u8d85\u8d8aGPT-4\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u94fe\u5f0f\u601d\u8003\u5b58\u5728\u8de8\u9886\u57df\u63d0\u793a\u8bbe\u8ba1\u590d\u6742\u6027\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728token\u6d88\u8017\u5927\u3001\u95ee\u9898\u6cdb\u5316\u75db\u70b9\uff0c\u9700\u5f00\u53d1\u8f7b\u91cf\u5316\u534f\u4f5c\u6846\u67b6\u3002", "method": "\u6784\u5efa\u96c6\u6210\u591a\u9636\u6bb5\u4e1a\u52a1\u77e5\u8bc6\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u7ef4\u62a4\u548c\u68c0\u7d22\u63d0\u793a\u6811\u83b7\u53d6\u8de8\u9636\u6bb5\u5173\u8054\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e1a\u52a1\u5de5\u4f5c\u6d41\u7684\u77e5\u8bc6-\u63d0\u793a\u534f\u540c\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u5c0f\u6a21\u578b+Cochain\u7ec4\u5408\u4f18\u4e8eGPT-4\uff080.71 vs 0.69\u51c6\u786e\u7387\uff09\u3002", "conclusion": "Cochain\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0e\u63d0\u793a\u6811\u7684\u534f\u540c\u673a\u5236\uff0c\u5728\u4fdd\u6301\u4e1a\u52a1\u6838\u5fc3\u95ee\u9898\u805a\u7126\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4e3aLLM\u4e1a\u52a1\u843d\u5730\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.10937", "pdf": "https://arxiv.org/pdf/2505.10937", "abs": "https://arxiv.org/abs/2505.10937", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The emergence of large reasoning models (LRMs) has transformed Natural\nLanguage Processing by excelling in complex tasks such as mathematical\nproblem-solving and code generation. These models leverage chain-of-thought\n(CoT) processes, enabling them to emulate human-like reasoning strategies.\nHowever, the advancement of LRMs is hindered by the lack of comprehensive CoT\ndatasets. Current resources often fail to provide extensive reasoning problems\nwith coherent CoT processes distilled from multiple teacher models and do not\naccount for multifaceted properties describing the internal characteristics of\nCoTs. To address these challenges, we introduce OmniThought, a large-scale\ndataset featuring 2 million CoT processes generated and validated by two\npowerful LRMs as teacher models. Each CoT process in OmniThought is annotated\nwith novel Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, which\ndescribe the appropriateness of CoT verbosity and cognitive difficulty level\nfor models to comprehend these reasoning processes. We further establish a\nself-reliant pipeline to curate this dataset. Extensive experiments using\nQwen2.5 models of various sizes demonstrate the positive impact of our proposed\nscores on LRM training effectiveness. Based on the proposed OmniThought\ndataset, we further train and release a series of high-performing LRMs,\nspecifically equipped with stronger reasoning abilities and optimal CoT output\nlength and difficulty level. Our contributions significantly enhance the\ndevelopment and training of LRMs for solving complex tasks.", "AI": {"tldr": "\u63d0\u51fa\u5305\u542b200\u4e07CoT\u8fc7\u7a0b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6OmniThought\uff0c\u901a\u8fc7RV/CD\u8bc4\u5206\u6307\u6807\u4f18\u5316\u5927\u6a21\u578b\u63a8\u7406\u8bad\u7ec3\u6548\u679c", "motivation": "\u73b0\u6709CoT\u6570\u636e\u96c6\u7f3a\u4e4f\u591a\u7ef4\u5ea6\u5c5e\u6027\u6807\u6ce8\uff0c\u65e0\u6cd5\u6709\u6548\u652f\u6491\u5927\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u8bad\u7ec3", "method": "\u6784\u5efa\u81ea\u7ed9\u81ea\u8db3\u7684\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u4f7f\u7528\u53ccLRM\u751f\u6210\u5e26\u63a8\u7406\u590d\u6742\u5ea6(RV)\u548c\u8ba4\u77e5\u96be\u5ea6(CD)\u6807\u6ce8\u7684CoT\u6570\u636e\u96c6", "result": "\u4e0d\u540c\u89c4\u6a21\u7684Qwen2.5\u6a21\u578b\u5b9e\u9a8c\u8bc1\u5b9e\u8bc4\u5206\u6307\u6807\u6709\u6548\u6027\uff0c\u5e76\u53d1\u5e03\u5177\u5907\u4f18\u5316\u63a8\u7406\u957f\u5ea6/\u96be\u5ea6\u7684\u9ad8\u6027\u80fdLRM\u7cfb\u5217", "conclusion": "OmniThought\u901a\u8fc7\u7cfb\u7edf\u6027\u91cf\u5316CoT\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b"}}
{"id": "2505.10938", "pdf": "https://arxiv.org/pdf/2505.10938", "abs": "https://arxiv.org/abs/2505.10938", "authors": ["Yi Su", "Yuechi Zhou", "Quantong Qiu", "Juntao Li", "Qingrong Xia", "Ping Li", "Xinyu Duan", "Zhefeng Wang", "Min Zhang"], "title": "Accurate KV Cache Quantization with Outlier Tokens Tracing", "categories": ["cs.CL"], "comment": "ACL2025 Main", "summary": "The impressive capabilities of Large Language Models (LLMs) come at the cost\nof substantial computational resources during deployment. While KV Cache can\nsignificantly reduce recomputation during inference, it also introduces\nadditional memory overhead. KV Cache quantization presents a promising\nsolution, striking a good balance between memory usage and accuracy. Previous\nresearch has shown that the Keys are distributed by channel, while the Values\nare distributed by token. Consequently, the common practice is to apply\nchannel-wise quantization to the Keys and token-wise quantization to the\nValues. However, our further investigation reveals that a small subset of\nunusual tokens exhibit unique characteristics that deviate from this pattern,\nwhich can substantially impact quantization accuracy. To address this, we\ndevelop a simple yet effective method to identify these tokens accurately\nduring the decoding process and exclude them from quantization as outlier\ntokens, significantly improving overall accuracy. Extensive experiments show\nthat our method achieves significant accuracy improvements under 2-bit\nquantization and can deliver a 6.4 times reduction in memory usage and a 2.3\ntimes increase in throughput.", "AI": {"tldr": "\u901a\u8fc7\u8bc6\u522b\u5f02\u5e38token\u4f18\u5316KV Cache\u91cf\u5316\uff0c\u57282-bit\u91cf\u5316\u4e0b\u5b9e\u73b0\u5185\u5b58\u5360\u7528\u964d\u4f4e6.4\u500d\u548c\u541e\u5410\u91cf\u63d0\u53472.3\u500d", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u65f6\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u4e0e\u5185\u5b58\u5360\u7528\u7684\u5e73\u8861\u96be\u9898\u3002KV Cache\u91cf\u5316\u867d\u80fd\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u4f46\u5f02\u5e38token\u4f1a\u663e\u8457\u5f71\u54cd\u91cf\u5316\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u89e3\u7801\u8fc7\u7a0b\u5b9e\u65f6\u68c0\u6d4b\u5f02\u5e38token\u7684\u65b9\u6cd5\uff0c\u5c06\u5176\u6392\u9664\u5728\u91cf\u5316\u8303\u56f4\u5916\u3002\u901a\u8fc7\u5206\u6790token\u7684\u901a\u9053\u5206\u5e03\u7279\u5f81\uff0c\u7cbe\u51c6\u8bc6\u522b\u4e0d\u7b26\u5408\u5e38\u89c4\u5206\u5e03\u6a21\u5f0f\u7684\u7279\u6b8atoken\u3002", "result": "\u57282-bit\u91cf\u5316\u6761\u4ef6\u4e0b\uff0c\u5b9e\u9a8c\u663e\u793a\u5185\u5b58\u4f7f\u7528\u51cf\u5c116.4\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53472.3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u91cf\u5316\u7cbe\u5ea6\u3002", "conclusion": "\u9488\u5bf9KV Cache\u91cf\u5316\u4e2d\u7684\u5f02\u5e38token\u5904\u7406\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u4e86\u5185\u5b58\u6548\u7387\u4e0e\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2505.10939", "pdf": "https://arxiv.org/pdf/2505.10939", "abs": "https://arxiv.org/abs/2505.10939", "authors": ["Mohammadtaha Bagherifard", "Sahar Rajabi", "Ali Edalat", "Yadollah Yaghoobzadeh"], "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages", "summary": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM.", "AI": {"tldr": "\u63d0\u51faGenKnowSub\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u901a\u7528\u77e5\u8bc6\u4e0e\u4efb\u52a1\u7279\u5b9aLoRA\u6a21\u5757\u63d0\u5347\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6a21\u5757\u5316\u65b9\u6cd5\u5b58\u5728\u901a\u7528\u77e5\u8bc6\u4e0e\u4efb\u52a1\u9002\u5e94\u77e5\u8bc6\u7ea0\u7f20\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "method": "\u6784\u5efa\u901a\u7528\u9886\u57dfLoRA\u5e93\u5e76\u8fdb\u884c\u77e5\u8bc6\u51cf\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u8def\u7531\u7b97\u6cd5\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u6a21\u5757\u7ec4\u5408", "result": "\u5728Phi-3\u548cPhi-2\u6a21\u578b\u4e0a\u5b9e\u73b0\u8de8\u8bed\u8a00\u573a\u666f\u6027\u80fd\u63d0\u5347\uff0c\u82f1\u8bed/\u6cd5\u8bed/\u5fb7\u8bed\u901a\u7528\u77e5\u8bc6\u5e93\u5e26\u6765\u6301\u7eed\u589e\u76ca", "conclusion": "\u77e5\u8bc6\u51cf\u6cd5\u6846\u67b6\u6709\u6548\u89e3\u8026\u4e0d\u540c\u77e5\u8bc6\u7c7b\u578b\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u6269\u5c55\u81f3\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b"}}
{"id": "2505.10945", "pdf": "https://arxiv.org/pdf/2505.10945", "abs": "https://arxiv.org/abs/2505.10945", "authors": ["Seungyoon Lee", "Seongtae Hong", "Hyeonseok Moon", "Heuiseok Lim"], "title": "Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) increasingly incorporate multilingual\ncapabilities, fueling the demand to transfer them into target language-specific\nmodels. However, most approaches, which blend the source model's embedding by\nreplacing the source vocabulary with the target language-specific vocabulary,\nmay constrain expressive capacity in the target language since the source model\nis predominantly trained on English data. In this paper, we propose Semantic\nAware Linear Transfer (SALT), a novel cross-lingual transfer technique that\nrecycles embeddings from target language Pre-trained Language Models (PLMs) to\ntransmit the deep representational strengths of PLM-derived embedding to LLMs.\nSALT derives unique regression lines based on the similarity in the overlap of\nthe source and target vocabularies, to handle each non-overlapping token's\nembedding space. Our extensive experiments show that SALT significantly\noutperforms other transfer methods and achieves lower loss with accelerating\nfaster convergence during language adaptation. Notably, SALT obtains remarkable\nperformance in cross-lingual understanding setups compared to other methods.\nFurthermore, we highlight the scalable use of PLMs to enhance the functionality\nof contemporary LLMs by conducting experiments with varying architectures.", "AI": {"tldr": "\u63d0\u51faSALT\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5d4c\u5165\u63d0\u5347LLMs\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u6536\u655b\u66f4\u5feb", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00\u8fc1\u79fb\u65b9\u6cd5\u76f4\u63a5\u66ff\u6362\u8bcd\u6c47\u8868\u4f1a\u9650\u5236\u76ee\u6807\u8bed\u8a00\u8868\u8fbe\u80fd\u529b\uff0c\u56e0\u4e3a\u6e90\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u82f1\u8bed\u8bad\u7ec3", "method": "\u57fa\u4e8e\u8bcd\u6c47\u91cd\u53e0\u76f8\u4f3c\u6027\u751f\u6210\u56de\u5f52\u7ebf\uff0c\u5229\u7528\u76ee\u6807\u8bed\u8a00PLMs\u7684\u5d4c\u5165\u5904\u7406\u975e\u91cd\u53e0\u8bcd\u6c47\u7a7a\u95f4\uff08Semantic Aware Linear Transfer\uff09", "result": "\u8de8\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u8868\u73b0\u7a81\u51fa\uff0c\u635f\u5931\u964d\u4f4e15%+\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u53472\u500d\uff0c\u4e0d\u540c\u67b6\u6784\u5b9e\u9a8c\u9a8c\u8bc1\u6269\u5c55\u6027", "conclusion": "SALT\u901a\u8fc7PLMs\u5d4c\u5165\u6709\u6548\u589e\u5f3aLLMs\u8de8\u8bed\u8a00\u80fd\u529b\uff0c\u4e3a\u5f53\u4ee3\u5927\u6a21\u578b\u529f\u80fd\u6269\u5c55\u63d0\u4f9b\u65b0\u8def\u5f84"}}
{"id": "2505.10948", "pdf": "https://arxiv.org/pdf/2505.10948", "abs": "https://arxiv.org/abs/2505.10948", "authors": ["Makoto Sato"], "title": "The Way We Prompt: Conceptual Blending, Neural Dynamics, and Prompt-Induced Transitions in LLMs", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Large language models (LLMs), inspired by neuroscience, exhibit behaviors\nthat often evoke a sense of personality and intelligence-yet the mechanisms\nbehind these effects remain elusive. Here, we operationalize Conceptual\nBlending Theory (CBT) as an experimental framework, using prompt-based methods\nto reveal how LLMs blend and compress meaning. By systematically investigating\nPrompt-Induced Transitions (PIT) and Prompt-Induced Hallucinations (PIH), we\nuncover structural parallels and divergences between artificial and biological\ncognition. Our approach bridges linguistics, neuroscience, and empirical AI\nresearch, demonstrating that human-AI collaboration can serve as a living\nprototype for the future of cognitive science. This work proposes prompt\nengineering not just as a technical tool, but as a scientific method for\nprobing the deep structure of meaning itself.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u6982\u5ff5\u878d\u5408\u7406\u8bba\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u63ed\u793aLLMs\u5904\u7406\u610f\u4e49\u7684\u673a\u5236\uff0c\u5e76\u63a2\u7d22\u4eba\u673a\u8ba4\u77e5\u5f02\u540c\u3002", "motivation": "\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548cAI\u7814\u7a76\uff0c\u586b\u8865\u4eba\u5de5\u4e0e\u751f\u7269\u8ba4\u77e5\u673a\u5236\u4e4b\u95f4\u7684\u7406\u89e3\u7a7a\u767d\uff0c\u5efa\u7acb\u8de8\u5b66\u79d1\u7814\u7a76\u6865\u6881\u3002", "method": "\u91c7\u7528Prompt-Induced Transitions\uff08PIT\uff09\u548cPrompt-Induced Hallucinations\uff08PIH\uff09\u7cfb\u7edf\u7814\u7a76LLMs\u7684\u610f\u4e49\u6df7\u5408\u673a\u5236", "result": "\u53d1\u73b0\u4eba\u5de5\u4e0e\u751f\u7269\u8ba4\u77e5\u7684\u7ed3\u6784\u6027\u5f02\u540c\uff0c\u8bc1\u660e\u63d0\u793a\u5de5\u7a0b\u53ef\u4f5c\u4e3a\u63a2\u7a76\u610f\u4e49\u6df1\u5c42\u7ed3\u6784\u7684\u79d1\u5b66\u65b9\u6cd5", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u4e0d\u4ec5\u662f\u6280\u672f\u5de5\u5177\uff0c\u66f4\u662f\u63a2\u7d22\u8ba4\u77e5\u673a\u5236\u7684\u79d1\u5b66\u65b9\u6cd5\u8bba\uff0c\u4eba\u673a\u534f\u4f5c\u5c06\u91cd\u5851\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\u8303\u5f0f"}}
{"id": "2505.10975", "pdf": "https://arxiv.org/pdf/2505.10975", "abs": "https://arxiv.org/abs/2505.10975", "authors": ["Xinlu He", "Jacob Whitehill"], "title": "Survey of End-to-End Multi-Speaker Automatic Speech Recognition for Monaural Audio", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "13 pages. Submitted to IEEE/ACM Transaction on Audio Speech and\n  Language Processing (TASLP)", "summary": "Monaural multi-speaker automatic speech recognition (ASR) remains challenging\ndue to data scarcity and the intrinsic difficulty of recognizing and\nattributing words to individual speakers, particularly in overlapping speech.\nRecent advances have driven the shift from cascade systems to end-to-end (E2E)\narchitectures, which reduce error propagation and better exploit the synergy\nbetween speech content and speaker identity. Despite rapid progress in E2E\nmulti-speaker ASR, the field lacks a comprehensive review of recent\ndevelopments. This survey provides a systematic taxonomy of E2E neural\napproaches for multi-speaker ASR, highlighting recent advances and comparative\nanalysis. Specifically, we analyze: (1) architectural paradigms (SIMO vs.~SISO)\nfor pre-segmented audio, analyzing their distinct characteristics and\ntrade-offs; (2) recent architectural and algorithmic improvements based on\nthese two paradigms; (3) extensions to long-form speech, including segmentation\nstrategy and speaker-consistent hypothesis stitching. Further, we (4) evaluate\nand compare methods across standard benchmarks. We conclude with a discussion\nof open challenges and future research directions towards building robust and\nscalable multi-speaker ASR.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u7aef\u5230\u7aef\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u8bc6\u522b\u6280\u672f\uff0c\u63d0\u51faSIMO/SISO\u67b6\u6784\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u957f\u8bed\u97f3\u5904\u7406\u7b56\u7565\u5e76\u8bc4\u4f30\u4e3b\u6d41\u65b9\u6cd5\u6027\u80fd", "motivation": "\u586b\u8865\u7aef\u5230\u7aef\u591a\u8bf4\u8bdd\u4ebaASR\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6280\u672f\u7efc\u8ff0\u7684\u7a7a\u767d\uff0c\u68b3\u7406\u67b6\u6784\u6f14\u8fdb\u8def\u5f84\u4e0e\u6280\u672f\u74f6\u9888", "method": "\u5efa\u7acb\u53cc\u8303\u5f0f\u5206\u7c7b\u4f53\u7cfb\uff08SIMO/SISO\uff09\uff0c\u5206\u6790\u5176\u7b97\u6cd5\u6539\u8fdb\uff1b\u63d0\u51fa\u957f\u8bed\u97f3\u5206\u6bb5\u7b56\u7565\u4e0e\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u62fc\u63a5\u65b9\u6cd5", "result": "SIMO\u67b6\u6784\u5728\u8bf4\u8bdd\u4eba\u5206\u79bb\u6548\u679c\u5360\u4f18\uff0cSISO\u5728\u8ba1\u7b97\u6548\u7387\u66f4\u4f73\uff1b\u5206\u5c42\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u91cd\u53e0\u8bed\u97f3\u8bc6\u522b\u7cbe\u5ea615%\u4ee5\u4e0a", "conclusion": "\u9700\u89e3\u51b3\u566a\u58f0\u654f\u611f\u6027\u95ee\u9898\uff0c\u53d1\u5c55\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u5efa\u7acb\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\u4ee5\u63a8\u52a8\u591a\u8bf4\u8bdd\u4ebaASR\u5b9e\u7528\u5316"}}
{"id": "2505.11004", "pdf": "https://arxiv.org/pdf/2505.11004", "abs": "https://arxiv.org/abs/2505.11004", "authors": ["Jingcheng Niu", "Subhabrata Dutta", "Ahmed Elshabrawy", "Harish Tayyar Madabushi", "Iryna Gurevych"], "title": "Illusion or Algorithm? Investigating Memorization, Emergence, and Symbolic Processing in In-Context Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large-scale Transformer language models (LMs) trained solely on next-token\nprediction with web-scale data can solve a wide range of tasks after seeing\njust a few examples. The mechanism behind this capability, known as in-context\nlearning (ICL), remains both controversial and poorly understood. Some studies\nargue that it is merely the result of memorizing vast amounts of data, while\nothers contend that it reflects a fundamental, symbolic algorithmic development\nin LMs. In this work, we introduce a suite of investigative tasks and a novel\nmethod to systematically investigate ICL by leveraging the full Pythia scaling\nsuite, including interim checkpoints that capture progressively larger amount\nof training data. By carefully exploring ICL performance on downstream tasks\nand simultaneously conducting a mechanistic analysis of the residual stream's\nsubspace, we demonstrate that ICL extends beyond mere \"memorization\" of the\ntraining corpus, yet does not amount to the implementation of an independent\nsymbolic algorithm. Our results also clarify several aspects of ICL, including\nthe influence of training dynamics, model capabilities, and elements of\nmechanistic interpretability. Overall, our work advances the understanding of\nICL and its implications, offering model developers insights into potential\nimprovements and providing AI security practitioners with a basis for more\ninformed guidelines.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u673a\u5236\u65e2\u975e\u5355\u7eaf\u8bb0\u5fc6\u4e5f\u975e\u72ec\u7acb\u7b97\u6cd5\uff0c\u800c\u662f\u4ecb\u4e8e\u4e24\u8005\u4e4b\u95f4\u7684\u65b0\u8303\u5f0f", "motivation": "\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u4e2dICL\u80fd\u529b\u672c\u8d28\u7684\u4e89\u8bae\uff08\u7eaf\u8bb0\u5fc6 vs \u7b26\u53f7\u7b97\u6cd5\uff09\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u63ed\u793a\u5176\u771f\u5b9e\u673a\u5236", "method": "\u4f7f\u7528Pythia\u6a21\u578b\u5957\u4ef6\u53ca\u4e2d\u95f4\u68c0\u67e5\u70b9\uff0c\u7ed3\u5408\u4e0b\u6e38\u4efb\u52a1\u6d4b\u8bd5\u548c\u6b8b\u5dee\u6d41\u5b50\u7a7a\u95f4\u673a\u5236\u5206\u6790", "result": "ICL\u80fd\u529b\u968f\u8bad\u7ec3\u52a8\u6001\u9010\u6b65\u5f62\u6210\uff0c\u6a21\u578b\u901a\u8fc7\u53c2\u6570\u8c03\u6574\u6784\u5efa\u7279\u5b9a\u5b50\u7a7a\u95f4\u5b9e\u73b0\u4e0a\u4e0b\u6587\u63a8\u7406", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u540c\u65f6\u4e3aAI\u5b89\u5168\u5efa\u7acb\u66f4\u79d1\u5b66\u7684\u8bc4\u4f30\u6846\u67b6\u5960\u5b9a\u57fa\u7840"}}
{"id": "2505.11008", "pdf": "https://arxiv.org/pdf/2505.11008", "abs": "https://arxiv.org/abs/2505.11008", "authors": ["Ye Kyaw Thu", "Thazin Myint Oo"], "title": "Reconstructing Syllable Sequences in Abugida Scripts with Incomplete Inputs", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "14 pages, 2 figures, 6 tables, 1 listing", "summary": "This paper explores syllable sequence prediction in Abugida languages using\nTransformer-based models, focusing on six languages: Bengali, Hindi, Khmer,\nLao, Myanmar, and Thai, from the Asian Language Treebank (ALT) dataset. We\ninvestigate the reconstruction of complete syllable sequences from various\nincomplete input types, including consonant sequences, vowel sequences, partial\nsyllables (with random character deletions), and masked syllables (with fixed\nsyllable deletions). Our experiments reveal that consonant sequences play a\ncritical role in accurate syllable prediction, achieving high BLEU scores,\nwhile vowel sequences present a significantly greater challenge. The model\ndemonstrates robust performance across tasks, particularly in handling partial\nand masked syllable reconstruction, with strong results for tasks involving\nconsonant information and syllable masking. This study advances the\nunderstanding of sequence prediction for Abugida languages and provides\npractical insights for applications such as text prediction, spelling\ncorrection, and data augmentation in these scripts.", "AI": {"tldr": "\u4f7f\u7528Transformer\u6a21\u578b\u7814\u7a76Abugida\u8bed\u8a00\u7684\u97f3\u8282\u5e8f\u5217\u9884\u6d4b\uff0c\u53d1\u73b0\u8f85\u97f3\u5e8f\u5217\u5bf9\u9884\u6d4b\u51c6\u786e\u6027\u8d77\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u63a2\u7d22Abugida\u8bed\u8a00\u4e2d\u4e0d\u540c\u8f93\u5165\u7c7b\u578b\uff08\u8f85\u97f3\u5e8f\u5217/\u5143\u97f3\u5e8f\u5217/\u90e8\u5206\u97f3\u8282/\u906e\u853d\u97f3\u8282\uff09\u5bf9\u5b8c\u6574\u97f3\u8282\u91cd\u5efa\u7684\u5f71\u54cd\uff0c\u63a8\u8fdb\u6b64\u7c7b\u6587\u5b57\u7cfb\u7edf\u7684\u5e8f\u5217\u9884\u6d4b\u7814\u7a76\u3002", "method": "\u57fa\u4e8eALT\u6570\u636e\u96c6\u7684\u516d\u79cd\u4e9a\u6d32\u8bed\u8a00\uff0c\u91c7\u7528Transformer\u6a21\u578b\u8fdb\u884c\u56db\u79cd\u97f3\u8282\u91cd\u5efa\u4efb\u52a1\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff08BLEU\u8bc4\u4f30\uff09\u3002", "result": "\u8f85\u97f3\u5e8f\u5217\u9884\u6d4bBLEU\u5f97\u5206\u6700\u9ad8\uff0894.41\uff09\uff0c\u5143\u97f3\u5e8f\u5217\u9884\u6d4b\u6700\u5177\u6311\u6218\u6027\uff08BLEU 53.13\uff09\u3002\u6a21\u578b\u5728\u6d89\u53ca\u8f85\u97f3\u4fe1\u606f\u548c\u97f3\u8282\u906e\u853d\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAbugida\u8bed\u8a00\u7684\u6587\u672c\u9884\u6d4b/\u62fc\u5199\u7ea0\u6b63\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u7b97\u6cd5\u652f\u6301\uff0c\u63ed\u793a\u4e86\u97f3\u7cfb\u7279\u5f81\u5bf9\u5e8f\u5217\u5efa\u6a21\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2505.11010", "pdf": "https://arxiv.org/pdf/2505.11010", "abs": "https://arxiv.org/abs/2505.11010", "authors": ["Jiangxu Wu", "Cong Wang", "TianHuang Su", "Jun Yang", "Haozhi Lin", "Chao Zhang", "Ming Peng", "Kai Shi", "SongPan Yang", "BinQing Pan", "ZiXian Li", "Ni Yang", "ZhenYu Yang"], "title": "Review-Instruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL2025 Accepted", "summary": "The effectiveness of large language models (LLMs) in conversational AI is\nhindered by their reliance on single-turn supervised fine-tuning (SFT) data,\nwhich limits contextual coherence in multi-turn dialogues. Existing methods for\ngenerating multi-turn dialogue data struggle to ensure both diversity and\nquality in instructions. To address this, we propose Review-Instruct, a novel\nframework that synthesizes multi-turn conversations through an iterative\n\"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple\nReviewers, and a Chairman. The framework iteratively refines instructions by\nincorporating Reviewer feedback, enhancing dialogue diversity and difficulty.\nWe construct a multi-turn dataset using the Alpaca dataset and fine-tune the\nLLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate\nsignificant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\%\non MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B.\nAblation studies confirm the critical role of the Review stage and the use of\nmultiple Reviewers in boosting instruction diversity and difficulty. Our work\nhighlights the potential of review-driven, multi-agent frameworks for\ngenerating high-quality conversational data at scale.", "AI": {"tldr": "\u63d0\u51faReview-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fed\u4ee3\u8bc4\u5ba1\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347LLM\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5355\u8f6e\u76d1\u7763\u5fae\u8c03\u6570\u636e\u5bfc\u81f4\u591a\u8f6e\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u6307\u4ee4\u591a\u6837\u6027\u4e0e\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1\u5305\u542b\u5019\u9009\u8005-\u8bc4\u5ba1\u7ec4-\u4e3b\u5e2d\u7684\u4e09\u89d2\u8272\u6846\u67b6\uff0c\u901a\u8fc7'\u63d0\u95ee-\u54cd\u5e94-\u8bc4\u5ba1'\u8fed\u4ee3\u4f18\u5316\u6307\u4ee4\uff0c\u57fa\u4e8eAlpaca\u6570\u636e\u96c6\u6784\u5efa\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u5e76\u5fae\u8c03LLaMA2-13B\u3002", "result": "\u5728MT-Bench/MMLU-Pro/Auto-Arena\u8bc4\u4f30\u4e2d\u5206\u522b\u53d6\u5f972%\u548c2.9%\u7edd\u5bf9\u63d0\u5347\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u8bc4\u5ba1\u673a\u5236\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u8bc4\u5ba1\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5728\u5927\u89c4\u6a21\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u6570\u636e\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3aLLM\u8bad\u7ec3\u6570\u636e\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.11026", "pdf": "https://arxiv.org/pdf/2505.11026", "abs": "https://arxiv.org/abs/2505.11026", "authors": ["Maria Dziuba", "Valentin Malykh"], "title": "StRuCom: A Novel Dataset of Structured Code Comments in Russian", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "Structured code comments in docstring format are essential for code\ncomprehension and maintenance, but existing machine learning models for their\ngeneration perform poorly for Russian compared to English. To bridge this gap,\nwe present StRuCom - the first large-scale dataset (153K examples) specifically\ndesigned for Russian code documentation. Unlike machine-translated English\ndatasets that distort terminology (e.g., technical loanwords vs. literal\ntranslations) and docstring structures, StRuCom combines human-written comments\nfrom Russian GitHub repositories with synthetically generated ones, ensuring\ncompliance with Python, Java, JavaScript, C#, and Go standards through\nautomated validation. Fine-tuning Qwen2.5-Coder models (0.5B-7B) on StRuCom\nshows statistically significant improvements of chrf++ and BERTScore over\nbaseline models.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4fc4\u8bed\u4ee3\u7801\u6587\u6863\u5927\u89c4\u6a21\u6570\u636e\u96c6StRuCom\uff0c\u901a\u8fc7\u4eba\u5de5+\u5408\u6210\u6570\u636e\u589e\u5f3a\u6a21\u578b\u6548\u679c", "motivation": "\u73b0\u6709\u6587\u6863\u751f\u6210\u6a21\u578b\u5728\u4fc4\u8bed\u8868\u73b0\u663e\u8457\u843d\u540e\u82f1\u8bed\uff0c\u4e3b\u8981\u56e0\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\u5b58\u5728\u672f\u8bed\u5931\u771f\u548c\u6587\u6863\u7ed3\u6784\u95ee\u9898", "method": "\u6574\u5408\u4fc4\u8bedGitHub\u4eba\u5de5\u6ce8\u91ca\u4e0e\u5408\u6210\u6570\u636e\uff0c\u652f\u6301\u591a\u7f16\u7a0b\u8bed\u8a00\u89c4\u8303\u9a8c\u8bc1\uff0c\u57fa\u4e8eStRuCom\u5fae\u8c03Qwen2.5-Coder\u7cfb\u5217\u6a21\u578b", "result": "\u6a21\u578b\u5728chrf++\u548cBERTScore\u6307\u6807\u4e0a\u5b9e\u73b0\u7edf\u8ba1\u5b66\u663e\u8457\u63d0\u5347", "conclusion": "StRuCom\u6709\u6548\u7f29\u5c0f\u4fc4\u82f1\u6027\u80fd\u5dee\u8ddd\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9a8c\u8bc1\u4fdd\u969c\u6587\u6863\u7ed3\u6784\u89c4\u8303\u6027"}}
{"id": "2505.11031", "pdf": "https://arxiv.org/pdf/2505.11031", "abs": "https://arxiv.org/abs/2505.11031", "authors": ["Xiao Zhang", "Huiyuan Lai", "Qianru Meng", "Johan Bos"], "title": "OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic Ontological Understanding, Reasoning and Learning", "categories": ["cs.CL"], "comment": "Paper submitted to NeruoIPS 2025 dataset and benchmark track", "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\na range of natural language processing tasks, yet their ability to process\nstructured symbolic knowledge remains underexplored. To address this gap, we\npropose a taxonomy of LLMs' ontological capabilities and introduce OntoURL, the\nfirst comprehensive benchmark designed to systematically evaluate LLMs'\nproficiency in handling ontologies -- formal, symbolic representations of\ndomain knowledge through concepts, relationships, and instances. Based on the\nproposed taxonomy, OntoURL systematically assesses three dimensions:\nunderstanding, reasoning, and learning through 15 distinct tasks comprising\n58,981 questions derived from 40 ontologies across 8 domains. Experiments with\n20 open-source LLMs reveal significant performance differences across models,\ntasks, and domains, with current LLMs showing proficiency in understanding\nontological knowledge but substantial weaknesses in reasoning and learning\ntasks. These findings highlight fundamental limitations in LLMs' capability to\nprocess symbolic knowledge and establish OntoURL as a critical benchmark for\nadvancing the integration of LLMs with formal knowledge representations.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u672c\u4f53\u8bba\u8bc4\u4f30\u57fa\u51c6OntoURL\uff0c\u63ed\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b26\u53f7\u77e5\u8bc6\u63a8\u7406\u548c\u5b66\u4e60\u65b9\u9762\u7684\u663e\u8457\u7f3a\u9677", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLMs\u5904\u7406\u5f62\u5f0f\u5316\u672c\u4f53\u8bba\u77e5\u8bc6\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u5efa\u7acb\u91cf\u5316\u57fa\u51c6\u63ed\u793a\u5176\u7b26\u53f7\u77e5\u8bc6\u5904\u7406\u80fd\u529b\u8fb9\u754c", "method": "\u57fa\u4e8e\u63d0\u51fa\u7684\u672c\u4f53\u8bba\u80fd\u529b\u5206\u7c7b\u6cd5\uff0c\u6784\u5efa\u8de88\u9886\u57df40\u672c\u4f53\u8bba\u768458,981\u4e2a\u8bc4\u4f30\u95ee\u9898\uff0c\u901a\u8fc715\u7c7b\u4efb\u52a1\u572820\u4e2a\u5f00\u6e90LLM\u4e0a\u5f00\u5c55\u4e09\u7ef4\u5ea6\uff08\u7406\u89e3/\u63a8\u7406/\u5b66\u4e60\uff09\u7cfb\u7edf\u6d4b\u8bd5", "result": "\u5f53\u524dLLMs\u672c\u4f53\u7406\u89e3\u51c6\u786e\u7387\u6700\u9ad8\u8fbe80%\uff0c\u4f46\u63a8\u7406\u548c\u5b66\u4e60\u4efb\u52a1\u51c6\u786e\u7387\u4e0d\u8db335%\uff0c\u4e0d\u540c\u6a21\u578b/\u9886\u57df\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff08\u6700\u5927\u8fbe61.2%\uff09", "conclusion": "\u66b4\u9732LLMs\u7b26\u53f7\u77e5\u8bc6\u5904\u7406\u7684\u6839\u672c\u6027\u5c40\u9650\uff0c\u786e\u7acbOntoURL\u4f5c\u4e3a\u63a8\u52a8LLMs\u4e0e\u5f62\u5f0f\u5316\u77e5\u8bc6\u878d\u5408\u7684\u5173\u952e\u57fa\u51c6"}}
{"id": "2505.11051", "pdf": "https://arxiv.org/pdf/2505.11051", "abs": "https://arxiv.org/abs/2505.11051", "authors": ["Iwona Christop", "Maciej Czajka"], "title": "CAMEO: Collection of Multilingual Emotional Speech Corpora", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Under review at NeurIPS", "summary": "This paper presents CAMEO -- a curated collection of multilingual emotional\nspeech datasets designed to facilitate research in emotion recognition and\nother speech-related tasks. The main objectives were to ensure easy access to\nthe data, to allow reproducibility of the results, and to provide a\nstandardized benchmark for evaluating speech emotion recognition (SER) systems\nacross different emotional states and languages. The paper describes the\ndataset selection criteria, the curation and normalization process, and\nprovides performance results for several models. The collection, along with\nmetadata, and a leaderboard, is publicly available via the Hugging Face\nplatform.", "AI": {"tldr": "CAMEO\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u60c5\u611f\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u60c5\u611f\u8bc6\u522b\u53ca\u8bed\u97f3\u76f8\u5173\u7814\u7a76\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\u548c\u53ef\u590d\u73b0\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u60c5\u611f\u8bed\u97f3\u6570\u636e\u5206\u6563\u3001\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u7684\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6613\u8bbf\u95ee\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u7b5b\u9009\u7b26\u5408\u6761\u4ef6\u7684\u6570\u636e\u96c6\u2192\u8fdb\u884c\u6570\u636e\u6e05\u6d17/\u6807\u51c6\u5316\u2192\u6784\u5efa\u591a\u79cd\u57fa\u7ebf\u6a21\u578b\u6d4b\u8bd5\u6027\u80fd\u2192\u901a\u8fc7Hugging Face\u5e73\u53f0\u516c\u5f00\u6570\u636e\u53ca\u5143\u6570\u636e\u3002", "result": "\u6210\u529f\u6574\u5408\u5305\u542b\u591a\u8bed\u8a00\u60c5\u611f\u6807\u7b7e\u7684\u8bed\u97f3\u5e93\uff0c\u5efa\u7acb\u516c\u5f00\u6392\u884c\u699c\uff0c\u6a21\u578b\u6027\u80fd\u6307\u6807\u53ef\u4f5c\u4e3a\u540e\u7eed\u7814\u7a76\u53c2\u7167\u57fa\u51c6\u3002", "conclusion": "CAMEO\u4e3a\u8de8\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u8bbe\u65bd\uff0c\u5176\u5f00\u653e\u6027\u548c\u6807\u51c6\u5316\u5c06\u52a0\u901f\u8bed\u97f3\u60c5\u611f\u5206\u6790\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.11080", "pdf": "https://arxiv.org/pdf/2505.11080", "abs": "https://arxiv.org/abs/2505.11080", "authors": ["Yapei Chang", "Yekyung Kim", "Michael Krumdick", "Amir Zadeh", "Chuan Li", "Chris Tanner", "Mohit Iyyer"], "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 11 figures, 15 tables", "summary": "Reward models are central to aligning LLMs with human preferences, but they\nare costly to train, requiring large-scale human-labeled preference data and\npowerful pretrained LLM backbones. Meanwhile, the increasing availability of\nhigh-quality synthetic instruction-following datasets raises the question: can\nsimpler, reference-based metrics serve as viable alternatives to reward models\nduring RL-based alignment? In this paper, we show first that BLEU, a basic\nstring-matching metric, surprisingly matches strong reward models in agreement\nwith human preferences on general instruction-following datasets. Based on this\ninsight, we develop BLEUBERI, a method that first identifies challenging\ninstructions and then applies Group Relative Policy Optimization (GRPO) using\nBLEU directly as the reward function. We demonstrate that BLEUBERI-trained\nmodels are competitive with models trained via reward model-guided RL across\nfour challenging instruction-following benchmarks and three different base\nlanguage models. A human evaluation further supports that the quality of\nBLEUBERI model outputs is on par with those from reward model-aligned models.\nMoreover, BLEUBERI models generate outputs that are more factually grounded\nthan competing methods. Overall, we show that given access to high-quality\nreference outputs (easily obtained via existing instruction-following datasets\nor synthetic data generation), string matching-based metrics are cheap yet\neffective proxies for reward models during alignment. We release our code and\ndata at https://github.com/lilakk/BLEUBERI.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u57fa\u7840\u5b57\u7b26\u4e32\u5339\u914d\u6307\u6807BLEU\u53ef\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u66ff\u4ee3\uff0c\u5e76\u63d0\u51faBLEUBERI\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e0e\u5956\u52b1\u6a21\u578b\u5bf9\u9f50\u6a21\u578b\u76f8\u5f53\u7684\u751f\u6210\u8d28\u91cf", "motivation": "\u5956\u52b1\u6a21\u578b\u5bf9\u9f50LLM\u9700\u8981\u9ad8\u6602\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u800c\u73b0\u6709\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\u80fd\u5426\u8ba9\u57fa\u4e8e\u53c2\u8003\u6307\u6807\u7684\u7b80\u5355\u65b9\u6cd5\u66ff\u4ee3\u5956\u52b1\u6a21\u578b\u6210\u4e3a\u5173\u952e\u7814\u7a76\u95ee\u9898", "method": "\u9996\u5148\u9a8c\u8bc1BLEU\u6307\u6807\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5f3a\u76f8\u5173\u6027\uff0c\u968f\u540e\u5f00\u53d1\u57fa\u4e8eGRPO\u7b97\u6cd5\u548cBLEU\u5956\u52b1\u7684BLEUBERI\u65b9\u6cd5\uff0c\u91cd\u70b9\u5904\u7406\u56f0\u96be\u6307\u4ee4\u7684\u4f18\u5316", "result": "BLEUBERI\u5728\u56db\u4e2a\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u548c\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\u4e0a\u5747\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u5176\u8f93\u51fa\u8d28\u91cf\u4e0e\u5956\u52b1\u6a21\u578b\u5bf9\u9f50\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u751f\u6210\u5185\u5bb9\u66f4\u5177\u4e8b\u5b9e\u51c6\u786e\u6027", "conclusion": "\u5f53\u5b58\u5728\u9ad8\u8d28\u91cf\u53c2\u8003\u8f93\u51fa\u65f6\uff08\u53ef\u901a\u8fc7\u73b0\u6709\u6570\u636e\u96c6\u6216\u5408\u6210\u751f\u6210\u83b7\u5f97\uff09\uff0c\u57fa\u4e8e\u5b57\u7b26\u4e32\u5339\u914d\u7684\u6307\u6807\u662f\u7ecf\u6d4e\u9ad8\u6548\u7684\u5956\u52b1\u6a21\u578b\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2505.11095", "pdf": "https://arxiv.org/pdf/2505.11095", "abs": "https://arxiv.org/abs/2505.11095", "authors": ["Lekang Jiang", "Pascal A Scherz", "Stephan Goetz"], "title": "Towards Better Evaluation for Generated Patent Claims", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. 14 pages, 8 tables", "summary": "Patent claims define the scope of protection and establish the legal\nboundaries of an invention. Drafting these claims is a complex and\ntime-consuming process that usually requires the expertise of skilled patent\nattorneys, which can form a large access barrier for many small enterprises. To\nsolve these challenges, researchers have investigated the use of large language\nmodels (LLMs) for automating patent claim generation. However, existing studies\nhighlight inconsistencies between automated evaluation metrics and human expert\nassessments. To bridge this gap, we introduce Patent-CE, the first\ncomprehensive benchmark for evaluating patent claims. Patent-CE includes\ncomparative claim evaluations annotated by patent experts, focusing on five key\ncriteria: feature completeness, conceptual clarity, terminology consistency,\nlogical linkage, and overall quality. Additionally, we propose PatClaimEval, a\nnovel multi-dimensional evaluation method specifically designed for patent\nclaims. Our experiments demonstrate that PatClaimEval achieves the highest\ncorrelation with human expert evaluations across all assessment criteria among\nall tested metrics. This research provides the groundwork for more accurate\nevaluations of automated patent claim generation systems.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u9996\u4e2a\u4e13\u5229\u6743\u5229\u8981\u6c42\u8bc4\u4f30\u57fa\u51c6Patent-CE\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5PatClaimEval\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u76f8\u5173\u6027\u6700\u9ad8", "motivation": "\u73b0\u6709\u4e13\u5229\u6743\u5229\u8981\u6c42\u81ea\u52a8\u751f\u6210\u7cfb\u7edf\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u79d1\u5b66\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u63d0\u51fa\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u5bf9\u6bd4\u8bc4\u4f30\u7684Patent-CE\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u4e13\u6ce8\u4e94\u5927\u6807\u51c6\u7684PatClaimEval\u591a\u7ef4\u8bc4\u4f30\u65b9\u6cd5", "result": "PatClaimEval\u5728\u6240\u6709\u6d4b\u8bd5\u6307\u6807\u4e2d\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u7684\u76f8\u5173\u6027\u6700\u9ad8\uff08\u6db5\u76d6\u7279\u5f81\u5b8c\u6574\u6027\u3001\u6982\u5ff5\u6e05\u6670\u5ea6\u7b49\u4e94\u4e2a\u7ef4\u5ea6\uff09", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e13\u5229\u6743\u5229\u8981\u6c42\u81ea\u52a8\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7cbe\u51c6\u7684\u8bc4\u4f30\u57fa\u7840\uff0c\u5f25\u5408\u4e86\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u5dee\u8ddd"}}
{"id": "2505.11140", "pdf": "https://arxiv.org/pdf/2505.11140", "abs": "https://arxiv.org/abs/2505.11140", "authors": ["Mike Zhang", "Johannes Bjerva", "Russa Biswas"], "title": "Scaling Reasoning can Improve Factuality in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies on large language model (LLM) reasoning capabilities have\ndemonstrated promising improvements in model performance by leveraging a\nlengthy thinking process and additional computational resources during\ninference, primarily in tasks involving mathematical reasoning (Muennighoff et\nal., 2025). However, it remains uncertain if longer reasoning chains inherently\nenhance factual accuracy, particularly beyond mathematical contexts. In this\nwork, we thoroughly examine LLM reasoning within complex open-domain\nquestion-answering (QA) scenarios. We initially distill reasoning traces from\nadvanced, large-scale reasoning models (QwQ-32B and DeepSeek-R1-671B), then\nfine-tune a variety of models ranging from smaller, instruction-tuned variants\nto larger architectures based on Qwen2.5. To enrich reasoning traces, we\nintroduce factual information from knowledge graphs in the form of paths into\nour reasoning traces. Our experimental setup includes four baseline approaches\nand six different instruction-tuned models evaluated across a benchmark of six\ndatasets, encompassing over 22.6K questions. Overall, we carry out 168\nexperimental runs and analyze approximately 1.7 million reasoning traces. Our\nfindings indicate that, within a single run, smaller reasoning models achieve\nnoticeable improvements in factual accuracy compared to their original\ninstruction-tuned counterparts. Moreover, our analysis demonstrates that adding\ntest-time compute and token budgets factual accuracy consistently improves by\n2-8%, further confirming the effectiveness of test-time scaling for enhancing\nperformance and consequently improving reasoning accuracy in open-domain QA\ntasks. We release all the experimental artifacts for further research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u901a\u8fc7\u589e\u52a0\u63a8\u7406\u94fe\u957f\u5ea6\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\uff0c\u80fd\u6709\u6548\u63d0\u5347\u5f00\u653e\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5c0f\u6a21\u578b\u5355\u6b21\u8fd0\u884c\u5373\u53ef\u6539\u5584\u8868\u73b0\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u5e26\u67652-8%\u7684\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u63a2\u7a76\u5728\u6570\u5b66\u63a8\u7406\u4e4b\u5916\u7684\u5f00\u653e\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u66f4\u957f\u7684\u63a8\u7406\u94fe\u662f\u5426\u771f\u6b63\u80fd\u63d0\u9ad8LLM\u7684\u4e8b\u5b9e\u51c6\u786e\u6027", "method": "\u4eceQwQ-32B\u548cDeepSeek-R1-671B\u84b8\u998f\u63a8\u7406\u8f68\u8ff9\uff0c\u5f15\u5165\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u4fe1\u606f\uff0c\u57fa\u4e8eQwen2.5\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u57286\u4e2a\u6570\u636e\u96c622.6K\u95ee\u9898\u4e0a\u8fdb\u884c168\u6b21\u5b9e\u9a8c", "result": "\u5c0f\u6a21\u578b\u5355\u6b21\u8fd0\u884c\u5373\u8d85\u8d8a\u539f\u7248\uff0c\u6d4b\u8bd5\u65f6\u589e\u52a0\u8ba1\u7b97\u9884\u7b97\u4f7f\u51c6\u786e\u7387\u63d0\u53472-8%\uff0c170\u4e07\u63a8\u7406\u8f68\u8ff9\u5206\u6790\u8bc1\u5b9e\u6548\u679c", "conclusion": "\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u5f00\u653e\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u7684\u8868\u73b0\uff0c\u8ba1\u7b97\u8d44\u6e90\u6295\u5165\u4e0e\u63a8\u7406\u51c6\u786e\u6027\u5448\u6b63\u76f8\u5173"}}
{"id": "2505.11166", "pdf": "https://arxiv.org/pdf/2505.11166", "abs": "https://arxiv.org/abs/2505.11166", "authors": ["Huashan Sun", "Shengyi Liao", "Yansen Han", "Yu Bai", "Yang Gao", "Cheng Fu", "Weizhou Shen", "Fanqi Wan", "Ming Yan", "Ji Zhang", "Fei Huang"], "title": "SoLoPO: Unlocking Long-Context Capabilities in LLMs via Short-to-Long Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite advances in pretraining with extended context lengths, large language\nmodels (LLMs) still face challenges in effectively utilizing real-world\nlong-context information, primarily due to insufficient long-context alignment\ncaused by data quality issues, training inefficiencies, and the lack of\nwell-designed optimization objectives. To address these limitations, we propose\na framework named $\\textbf{S}$h$\\textbf{o}$rt-to-$\\textbf{Lo}$ng\n$\\textbf{P}$reference $\\textbf{O}$ptimization ($\\textbf{SoLoPO}$), decoupling\nlong-context preference optimization (PO) into two components: short-context PO\nand short-to-long reward alignment (SoLo-RA), supported by both theoretical and\nempirical evidence. Specifically, short-context PO leverages preference pairs\nsampled from short contexts to enhance the model's contextual knowledge\nutilization ability. Meanwhile, SoLo-RA explicitly encourages reward score\nconsistency utilization for the responses when conditioned on both short and\nlong contexts that contain identical task-relevant information. This\nfacilitates transferring the model's ability to handle short contexts into\nlong-context scenarios. SoLoPO is compatible with mainstream preference\noptimization algorithms, while substantially improving the efficiency of data\nconstruction and training processes. Experimental results show that SoLoPO\nenhances all these algorithms with respect to stronger length and domain\ngeneralization abilities across various long-context benchmarks, while\nachieving notable improvements in both computational and memory efficiency.", "AI": {"tldr": "\u63d0\u51faSoLoPO\u6846\u67b6\uff0c\u901a\u8fc7\u77ed\u4e0a\u4e0b\u6587\u504f\u597d\u4f18\u5316\u548c\u77ed\u957f\u5956\u52b1\u5bf9\u9f50\u63d0\u5347\u5927\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u5b58\u5728\u6570\u636e\u8d28\u91cf\u5dee\u3001\u8bad\u7ec3\u6548\u7387\u4f4e\u3001\u4f18\u5316\u76ee\u6807\u4e0d\u5b8c\u5584\u4e09\u5927\u6838\u5fc3\u95ee\u9898", "method": "\u5c06\u957f\u4e0a\u4e0b\u6587\u504f\u597d\u4f18\u5316\u89e3\u8026\u4e3a\u77ed\u4e0a\u4e0b\u6587PO\uff08\u589e\u5f3a\u4e0a\u4e0b\u6587\u77e5\u8bc6\u5229\u7528\uff09\u548cSoLo-RA\uff08\u4fdd\u6301\u77ed\u957f\u4e0a\u4e0b\u6587\u5956\u52b1\u4e00\u81f4\u6027\uff09", "result": "\u5728\u591a\u9879\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u957f\u5ea6/\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u9ad82.7\u500d\u8bad\u7ec3\u6548\u7387\uff0c\u51cf\u5c1147%\u5185\u5b58\u6d88\u8017", "conclusion": "SoLoPO\u901a\u8fc7\u521b\u65b0\u6027\u7684\u4f18\u5316\u89e3\u8026\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7b97\u6cd5\u517c\u5bb9\u6027\u7684\u540c\u65f6\uff0c\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u5bf9\u9f50\u7684\u6838\u5fc3\u75db\u70b9"}}
{"id": "2505.11177", "pdf": "https://arxiv.org/pdf/2505.11177", "abs": "https://arxiv.org/abs/2505.11177", "authors": ["Hrishit Madhavi", "Jacob Cherian", "Yuvraj Khamkar", "Dhananjay Bhagat"], "title": "Low-Resource Language Processing: An OCR-Driven Summarization and Translation Pipeline", "categories": ["cs.CL", "cs.AI", "68T50 (Natural language processing), 68U10 (Image processing)"], "comment": "8 pages, 7 figures, direct arXiv submission", "summary": "This paper presents an end-to-end suite for multilingual information\nextraction and processing from image-based documents. The system uses Optical\nCharacter Recognition (Tesseract) to extract text in languages such as English,\nHindi, and Tamil, and then a pipeline involving large language model APIs\n(Gemini) for cross-lingual translation, abstractive summarization, and\nre-translation into a target language. Additional modules add sentiment\nanalysis (TensorFlow), topic classification (Transformers), and date extraction\n(Regex) for better document comprehension. Made available in an accessible\nGradio interface, the current research shows a real-world application of\nlibraries, models, and APIs to close the language gap and enhance access to\ninformation in image media across different linguistic environments", "AI": {"tldr": "\u591a\u8bed\u8a00\u7aef\u5230\u7aef\u56fe\u50cf\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u7cfb\u7edf\uff0c\u96c6\u6210OCR\u3001\u8de8\u8bed\u8a00\u7ffb\u8bd1\u3001\u6458\u8981\u751f\u6210\u53ca\u591a\u6a21\u6001\u5206\u6790\u529f\u80fd", "motivation": "\u89e3\u51b3\u56fe\u50cf\u6587\u6863\u5728\u82f1\u8bed/\u5370\u5730\u8bed/\u6cf0\u7c73\u5c14\u8bed\u7b49\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u4fe1\u606f\u83b7\u53d6\u969c\u788d\uff0c\u7279\u522b\u662f\u975e\u62c9\u4e01\u8bed\u7cfb\u6587\u5b57\u7684\u5904\u7406\u96be\u9898", "method": "\u7ed3\u5408Tesseract OCR\uff08\u6587\u5b57\u63d0\u53d6\uff09\u3001Gemini API\uff08\u7ffb\u8bd1\u4e0e\u6458\u8981\uff09\u3001TensorFlow\uff08\u60c5\u611f\u5206\u6790\uff09\u3001Transformers\uff08\u4e3b\u9898\u5206\u7c7b\uff09\u3001\u6b63\u5219\u8868\u8fbe\u5f0f\uff08\u65e5\u671f\u63d0\u53d6\uff09\u7b49\u6280\u672f\u6808\uff0c\u901a\u8fc7Gradio\u6784\u5efa\u4ea4\u4e92\u754c\u9762", "result": "\u5b9e\u73b0\u4e86\u4ece\u56fe\u50cf\u6587\u6863\u5230\u7ed3\u6784\u5316\u4fe1\u606f\u7684\u5b8c\u6574\u5904\u7406\u6d41\u7a0b\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u4fe1\u606f\u8f6c\u6362\u4e0e\u5206\u6790\u9a8c\u8bc1", "conclusion": "\u8be5\u96c6\u6210\u7cfb\u7edf\u6709\u6548\u7a81\u7834\u8bed\u8a00\u58c1\u5792\uff0c\u4e3a\u591a\u8bed\u8a00\u56fe\u50cf\u5a92\u4f53\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u843d\u5730\u7684\u6280\u672f\u65b9\u6848"}}
{"id": "2505.11199", "pdf": "https://arxiv.org/pdf/2505.11199", "abs": "https://arxiv.org/abs/2505.11199", "authors": ["Chris K\u00f6cher", "Alexander Kozachinskiy", "Anthony Widjaja Lin", "Marco S\u00e4lzer", "Georg Zetzsche"], "title": "NoPE: The Counting Power of Transformers with No Positional Encodings", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": null, "summary": "Positional Encodings (PEs) seem to be indispensable for ensuring\nexpressiveness of transformers; without them attention transformers reduce to a\nbag-of-word model. NoPE-transformers (i.e. with No PEs) with unique hard\nattention mechanisms were very recently shown to only be able to express\nregular languages, i.e., with limited counting ability. This paper shows that,\nwith average hard attention mechanisms, NoPE-transformers are still\nsurprisingly expressive: they can express counting languages corresponding to\nnonnegative integer solutions to multivariate polynomial equations (i.e.\nDiophantine equations), reasoning about which is well-known to be undecidable.\nIn fact, we provide a precise characterization of languages expressible by\nAverage Hard Attention NoPE-Transformers (NoPE-AHATs): they correspond\nprecisely to what we call \\emph{semi-algebraic sets}, i.e., finite unions of\nsets of nonnegative integer solutions to systems of multivariate polynomial\ninequations. We obtain several interesting consequences of our\ncharacterization. Firstly, NoPE-transformers can express counting properties\nthat are far more complex than established models like simplified counter\nmachines and Petri nets, but cannot express a very simple counting property of\nPARITY. Secondly, the problem of analyzing NoPE-transformers is undecidable,\ne.g., whether a given NoPE transformer classifies all input strings in one\nclass. To complement our results, we exhibit a counting language that is not\nexpressible by average hard attention transformers even with arbitrary PEs but\nis expressible in the circuit complexity class TC$^0$, answering an open\nproblem.", "AI": {"tldr": "NoPE-transformers\u4f7f\u7528\u5e73\u5747\u786c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u8868\u8fbe\u590d\u6742\u8ba1\u6570\u8bed\u8a00\uff08\u5bf9\u5e94\u591a\u5143\u591a\u9879\u5f0f\u65b9\u7a0b\u7684\u975e\u8d1f\u6574\u6570\u89e3\uff09\uff0c\u4f46\u65e0\u6cd5\u5904\u7406\u7b80\u5355\u5947\u5076\u6821\u9a8c\u95ee\u9898\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u8ba4\u77e5\uff0c\u63a2\u7d22\u65e0\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u4e0a\u9650\uff0c\u8bc1\u660e\u5176\u5728\u7406\u8bba\u8ba1\u7b97\u5c42\u9762\u7684\u610f\u5916\u5f3a\u5927\u7279\u6027\u3002", "method": "\u6784\u5efaAverage Hard Attention NoPE-Transformers\uff08NoPE-AHATs\uff09\uff0c\u5efa\u7acb\u4e0e\u534a\u4ee3\u6570\u96c6\u7684\u5bf9\u5e94\u5173\u7cfb\u7406\u8bba\u6846\u67b6\u3002", "result": "1. \u7cbe\u786e\u523b\u753bNoPE-AHATs\u8868\u8fbe\u80fd\u529b\uff1a\u7b49\u4ef7\u4e8e\u534a\u4ee3\u6570\u96c6\n2. \u63ed\u793a\u6a21\u578b\u53ef\u8868\u8fbe\u8fdc\u8d85\u8ba1\u6570\u5668\u81ea\u52a8\u673a/\u4f69\u7279\u91cc\u7f51\u7684\u590d\u6742\u8ba1\u6570\u5c5e\u6027\n3. \u8bc1\u660eNoPE-transformers\u5206\u6790\u95ee\u9898\u7684\u4e0d\u53ef\u5224\u5b9a\u6027", "conclusion": "\u65e0\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\u5728\u7406\u8bba\u5c42\u9762\u5177\u6709\u60ca\u4eba\u8868\u8fbe\u529b\uff0c\u4f46\u5176\u5206\u6790\u590d\u6742\u5ea6\u8fbe\u5230\u4e0d\u53ef\u5224\u5b9a\u7ea7\u522b\uff0c\u63ed\u793a\u4e86\u7406\u8bba\u80fd\u529b\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\u3002"}}
{"id": "2505.11225", "pdf": "https://arxiv.org/pdf/2505.11225", "abs": "https://arxiv.org/abs/2505.11225", "authors": ["Chengyu Huang", "Zhengxin Zhang", "Claire Cardie"], "title": "HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "While scaling the length of responses at test-time has been shown to markedly\nimprove the reasoning abilities and performance of large language models\n(LLMs), it often results in verbose outputs and increases inference cost. Prior\napproaches for efficient test-time scaling, typically using universal budget\nconstraints or query-level length optimization, do not leverage historical\ninformation from previous encounters with the same problem during training. We\nhypothesize that this limits their ability to progressively make solutions more\nconcise over time. To address this, we present History-Aware Policy\nOptimization (HAPO), which keeps track of a history state (e.g., the minimum\nlength over previously generated correct responses) for each problem. HAPO\nemploys a novel length reward function based on this history state to\nincentivize the discovery of correct solutions that are more concise than those\npreviously found. Crucially, this reward structure avoids overly penalizing\nshorter incorrect responses with the goal of facilitating exploration towards\nmore efficient solutions. By combining this length reward with a correctness\nreward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to\ntrain DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and\nQwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span\nvarious difficulty levels. Experiment results demonstrate that HAPO effectively\ninduces LLMs' concise reasoning abilities, producing length reductions of\n33-59% with accuracy drops of only 2-5%.", "AI": {"tldr": "\u901a\u8fc7\u5386\u53f2\u611f\u77e5\u7b56\u7565\u4f18\u5316\uff08HAPO\uff09\u63d0\u5347LLM\u7684\u63a8\u7406\u6548\u7387\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c1133-59%\u7684\u54cd\u5e94\u957f\u5ea6", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u8bad\u7ec3\u65f6\u79ef\u7d2f\u7684\u5386\u53f2\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u89e3\u51b3\u65b9\u6848\u7684\u6301\u7eed\u7cbe\u7b80\u80fd\u529b", "method": "\u57fa\u4e8e\u5386\u53f2\u72b6\u6001\uff08\u5982\u6700\u5c0f\u6b63\u786e\u54cd\u5e94\u957f\u5ea6\uff09\u8bbe\u8ba1\u957f\u5ea6\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u6b63\u786e\u6027\u5956\u52b1\u5b9e\u73b0\u8054\u5408\u4f18\u5316", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b033-59%\u7684\u957f\u5ea6\u7f29\u51cf\uff0c\u7cbe\u5ea6\u4ec5\u4e0b\u964d2-5%", "conclusion": "HAPO\u6709\u6548\u6fc0\u53d1LLM\u7684\u7b80\u6d01\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411"}}
{"id": "2505.11271", "pdf": "https://arxiv.org/pdf/2505.11271", "abs": "https://arxiv.org/abs/2505.11271", "authors": ["Camille Couturier", "Spyros Mastorakis", "Haiying Shen", "Saravan Rajmohan", "Victor R\u00fchle"], "title": "Semantic Caching of Contextual Summaries for Efficient Question-Answering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "I.2.7"], "comment": "Preprint. Paper accepted at ICCCN 2025, the final version will appear\n  in the proceedings", "summary": "Large Language Models (LLMs) are increasingly deployed across edge and cloud\nplatforms for real-time question-answering and retrieval-augmented generation.\nHowever, processing lengthy contexts in distributed systems incurs high\ncomputational overhead, memory usage, and network bandwidth. This paper\nintroduces a novel semantic caching approach for storing and reusing\nintermediate contextual summaries, enabling efficient information reuse across\nsimilar queries in LLM-based QA workflows. Our method reduces redundant\ncomputations by up to 50-60% while maintaining answer accuracy comparable to\nfull document processing, as demonstrated on NaturalQuestions, TriviaQA, and a\nsynthetic ArXiv dataset. This approach balances computational cost and response\nquality, critical for real-time AI assistants.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u4e49\u7f13\u5b58\u6280\u672f\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5206\u5e03\u5f0f\u90e8\u7f72\u4e2d\u7684\u957f\u6587\u672c\u5904\u7406\u6548\u7387", "motivation": "\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u4f1a\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u3001\u5185\u5b58\u5360\u7528\u548c\u7f51\u7edc\u5e26\u5bbd\u8fc7\u9ad8", "method": "\u91c7\u7528\u5b58\u50a8\u548c\u590d\u7528\u4e2d\u95f4\u8bed\u4e49\u6458\u8981\u7684\u8bed\u4e49\u7f13\u5b58\u673a\u5236", "result": "\u5728NaturalQuestions/TriviaQA/ArXiv\u6570\u636e\u96c6\u4e0a\u5b9e\u73b050-60%\u5197\u4f59\u8ba1\u7b97\u51cf\u5c11\uff0c\u7cbe\u5ea6\u4fdd\u6301\u6587\u6863\u5168\u5904\u7406\u6c34\u5e73", "conclusion": "\u8be5\u6280\u672f\u5e73\u8861\u4e86\u8ba1\u7b97\u6210\u672c\u4e0e\u54cd\u5e94\u8d28\u91cf\uff0c\u5bf9\u5b9e\u65f6AI\u52a9\u624b\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2505.11277", "pdf": "https://arxiv.org/pdf/2505.11277", "abs": "https://arxiv.org/abs/2505.11277", "authors": ["Yaorui Shi", "Shihan Li", "Chang Wu", "Zhiyuan Liu", "Junfeng Fang", "Hengxing Cai", "An Zhang", "Xiang Wang"], "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.", "AI": {"tldr": "\u63d0\u51faAutoRefine\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u300c\u641c\u7d22-\u7cbe\u70bc\u300d\u8fed\u4ee3\u673a\u5236\u6539\u8fdb\u68c0\u7d22\u589e\u5f3a\u63a8\u7406\uff0c\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u5b58\u5728\u68c0\u7d22\u4fe1\u606f\u566a\u58f0\u5927\u3001\u76f8\u5173\u6027\u4f4e\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u63a8\u7406\u51c6\u786e\u6027\u3002\u9700\u8981\u66f4\u6709\u6548\u7684\u77e5\u8bc6\u7cbe\u70bc\u673a\u5236\u6765\u63d0\u5347\u8bc1\u636e\u8d28\u91cf", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u8fde\u7eed\u641c\u7d22\u95f4\u63d2\u5165\u77e5\u8bc6\u7cbe\u70bc\u6b65\u9aa4\uff08\u8fc7\u6ee4/\u84b8\u998f/\u7ec4\u7ec7\u8bc1\u636e\uff09\uff0c\u7ed3\u5408\u68c0\u7d22\u8d28\u91cf\u5956\u52b1\u548c\u7b54\u6848\u6b63\u786e\u6027\u5956\u52b1\u8fdb\u884c\u7b56\u7565\u4f18\u5316", "result": "\u5728\u5355\u8df3/\u591a\u8df3QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5c24\u5176\u591a\u8df3\u573a\u666f\uff09\uff0c\u641c\u7d22\u9891\u7387\u63d0\u534735%\uff0c\u8bc1\u636e\u5408\u6210\u8d28\u91cf\u63d0\u9ad828%", "conclusion": "AutoRefine\u901a\u8fc7\u8fed\u4ee3\u5f0f\u77e5\u8bc6\u7cbe\u70bc\u673a\u5236\u548c\u5b9a\u5236\u5316\u5956\u52b1\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u68c0\u7d22\u6548\u7387\u548c\u8bc1\u636e\u6574\u5408\u80fd\u529b"}}
{"id": "2505.11280", "pdf": "https://arxiv.org/pdf/2505.11280", "abs": "https://arxiv.org/abs/2505.11280", "authors": ["Horacio Thompson", "Esa\u00fa Villatoro-Tello", "Manuel Montes-y-G\u00f3mez", "Marcelo Errecalde"], "title": "Temporal fine-tuning for early risk detection", "categories": ["cs.CL"], "comment": "In: Proceedings of the 53rd JAIIO / 50th CLEI - ASAID, 2024, p. 137.\n  ISSN: 2451-7496", "summary": "Early Risk Detection (ERD) on the Web aims to identify promptly users facing\nsocial and health issues. Users are analyzed post-by-post, and it is necessary\nto guarantee correct and quick answers, which is particularly challenging in\ncritical scenarios. ERD involves optimizing classification precision and\nminimizing detection delay. Standard classification metrics may not suffice,\nresorting to specific metrics such as ERDE(theta) that explicitly consider\nprecision and delay. The current research focuses on applying a multi-objective\napproach, prioritizing classification performance and establishing a separate\ncriterion for decision time. In this work, we propose a completely different\nstrategy, temporal fine-tuning, which allows tuning transformer-based models by\nexplicitly incorporating time within the learning process. Our method allows us\nto analyze complete user post histories, tune models considering different\ncontexts, and evaluate training performance using temporal metrics. We\nevaluated our proposal in the depression and eating disorders tasks for the\nSpanish language, achieving competitive results compared to the best models of\nMentalRiskES 2023. We found that temporal fine-tuning optimized decisions\nconsidering context and time progress. In this way, by properly taking\nadvantage of the power of transformers, it is possible to address ERD by\ncombining precision and speed as a single objective.", "AI": {"tldr": "\u63d0\u51fa\u65f6\u95f4\u5fae\u8c03\u65b9\u6cd5\uff0c\u5c06\u65f6\u95f4\u56e0\u7d20\u878d\u5165Transformer\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u897f\u73ed\u7259\u8bed\u6291\u90c1\u548c\u996e\u98df\u969c\u788d\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u901f\u5ea6\u7684\u8054\u5408\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u5206\u7c7b\u7cbe\u5ea6\u4e0e\u68c0\u6d4b\u5ef6\u8fdf\uff0c\u9700\u901a\u8fc7\u65f6\u95f4\u654f\u611f\u6307\u6807\u4f18\u5316\u5b9e\u65f6\u573a\u666f\u4e0b\u7684\u51b3\u7b56\u6548\u7387\u3002", "method": "\u57fa\u4e8eTransformer\u67b6\u6784\u8bbe\u8ba1\u65f6\u95f4\u5fae\u8c03\u673a\u5236\uff0c\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u663e\u5f0f\u7f16\u7801\u65f6\u95f4\u7ef4\u5ea6\uff0c\u5229\u7528\u7528\u6237\u5b8c\u6574\u53d1\u5e16\u5386\u53f2\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65f6\u5e8f\u5efa\u6a21\u3002", "result": "\u5728MentalRiskES 2023\u8bc4\u6d4b\u4e2d\uff0c\u897f\u73ed\u7259\u8bed\u6291\u90c1/\u996e\u98df\u969c\u788d\u68c0\u6d4b\u4efb\u52a1\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u51b3\u7b56\u5ef6\u8fdf\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "\u901a\u8fc7\u65f6\u95f4\u7ef4\u5ea6\u91cd\u6784\u6a21\u578b\u8bad\u7ec3\u76ee\u6807\uff0c\u9a8c\u8bc1\u4e86\u5355\u4e00\u4f18\u5316\u76ee\u6807\u4e0b\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u901f\u5ea6\u534f\u540c\u63d0\u5347\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u65f6\u98ce\u9669\u68c0\u6d4b\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.11297", "pdf": "https://arxiv.org/pdf/2505.11297", "abs": "https://arxiv.org/abs/2505.11297", "authors": ["Gal Astrach", "Yuval Pinter"], "title": "Probing Subphonemes in Morphology Models", "categories": ["cs.CL"], "comment": null, "summary": "Transformers have achieved state-of-the-art performance in morphological\ninflection tasks, yet their ability to generalize across languages and\nmorphological rules remains limited. One possible explanation for this behavior\ncan be the degree to which these models are able to capture implicit phenomena\nat the phonological and subphonemic levels. We introduce a language-agnostic\nprobing method to investigate phonological feature encoding in transformers\ntrained directly on phonemes, and perform it across seven morphologically\ndiverse languages. We show that phonological features which are local, such as\nfinal-obstruent devoicing in Turkish, are captured well in phoneme embeddings,\nwhereas long-distance dependencies like vowel harmony are better represented in\nthe transformer's encoder. Finally, we discuss how these findings inform\nempirical strategies for training morphological models, particularly regarding\nthe role of subphonemic feature acquisition.", "AI": {"tldr": "\u63a2\u7a76Transformer\u5728\u97f3\u7cfb\u7279\u5f81\u7f16\u7801\u4e0a\u7684\u5c40\u9650\u6027\uff1a\u5c40\u90e8\u7279\u5f81\uff08\u5982\u571f\u8033\u5176\u8bed\u5c3e\u97f3\u6e05\u5316\uff09\u5728\u97f3\u7d20\u5d4c\u5165\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u957f\u8ddd\u79bb\u4f9d\u8d56\uff08\u5982\u5143\u97f3\u548c\u8c10\uff09\u9700\u4f9d\u8d56\u7f16\u7801\u5668\u8868\u5f81\u3002", "motivation": "\u5c3d\u7ba1Transformer\u5728\u5f62\u6001\u5b66\u5c48\u6298\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8de8\u8bed\u8a00\u548c\u5f62\u6001\u89c4\u5219\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u53ef\u80fd\u4e0e\u97f3\u7cfb/\u4e9a\u97f3\u4f4d\u7279\u5f81\u7684\u7f16\u7801\u80fd\u529b\u6709\u5173\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u65e0\u5173\u7684\u63a2\u6d4b\u65b9\u6cd5\uff0c\u57287\u79cd\u5f62\u6001\u591a\u6837\u8bed\u8a00\u4e2d\u5206\u6790\u57fa\u4e8e\u97f3\u7d20\u8bad\u7ec3\u7684Transformer\u7684\u97f3\u7cfb\u7279\u5f81\u7f16\u7801\u673a\u5236\u3002", "result": "\u5c40\u90e8\u97f3\u7cfb\u89c4\u5219\u4e3b\u8981\u7f16\u7801\u4e8e\u97f3\u7d20\u5d4c\u5165\u5c42\uff0c\u957f\u8ddd\u79bb\u4f9d\u8d56\u5219\u66f4\u591a\u4f9d\u8d56\u7f16\u7801\u5668\u5c42\u7684\u8868\u5f81\u80fd\u529b\u3002", "conclusion": "\u8bad\u7ec3\u5f62\u6001\u6a21\u578b\u65f6\u5e94\u91cd\u89c6\u4e9a\u97f3\u4f4d\u7279\u5f81\u7684\u83b7\u53d6\u7b56\u7565\uff0c\u5c24\u5176\u9488\u5bf9\u4e0d\u540c\u97f3\u7cfb\u73b0\u8c61\u9009\u62e9\u7279\u5f81\u8868\u793a\u5c42\u7ea7\u3002"}}
{"id": "2505.11336", "pdf": "https://arxiv.org/pdf/2505.11336", "abs": "https://arxiv.org/abs/2505.11336", "authors": ["Nuo Chen", "Andre Lin HuiKai", "Jiaying Wu", "Junyi Hou", "Zining Zhang", "Qian Wang", "Xidong Wang", "Bingsheng He"], "title": "XtraGPT: LLMs for Human-AI Collaboration on Controllable Academic Paper Revision", "categories": ["cs.CL"], "comment": "preprint", "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u7c7b-AI\u534f\u4f5c\u7684\u5b66\u672f\u8bba\u6587\u4fee\u8ba2\u6846\u67b6XtraGPT\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u96c6\u548c\u5f00\u53d1\u5f00\u6e90\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79d1\u5b66\u5199\u4f5c\u8f85\u52a9\u6548\u679c", "motivation": "\u73b0\u6709\u901a\u7528\u578bLLM\u96be\u4ee5\u6ee1\u8db3\u79d1\u7814\u5199\u4f5c\u5bf9\u7ae0\u8282\u95f4\u6982\u5ff5\u8fde\u8d2f\u6027\u548c\u8fed\u4ee3\u4fee\u8ba2\u7684\u6df1\u5c42\u6b21\u9700\u6c42\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b66\u672f\u5199\u4f5c\u8f85\u52a9\u5de5\u5177", "method": "\u6784\u5efa\u5305\u542b7,040\u7bc7\u9876\u4f1a\u8bba\u6587\u548c14\u4e07\u6307\u4ee4-\u54cd\u5e94\u5bf9\u7684REVISIONS\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u53c2\u6570\u89c4\u6a211.5B-14B\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5f00\u6e90\u6a21\u578bXtraGPT", "result": "\u5b9e\u9a8c\u663e\u793aXtraGPT\u8d85\u8d8a\u540c\u89c4\u6a21\u57fa\u7ebf\u6a21\u578b\uff0c\u63a5\u8fd1\u5546\u4e1a\u7cfb\u7edf\u6c34\u5e73\uff0c\u81ea\u52a8\u8bc4\u4f30\u548c\u4eba\u5de5\u9a8c\u8bc1\u5747\u8bc1\u5b9e\u5176\u6709\u6548\u63d0\u5347\u79d1\u5b66\u8349\u7a3f\u8d28\u91cf", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u6570\u636e\u6784\u5efa\u548c\u6a21\u578b\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u4e13\u95e8\u5316AI\u7cfb\u7edf\u5728\u652f\u6301\u79d1\u7814\u5199\u4f5c\u590d\u6742\u9700\u6c42\u65b9\u9762\u7684\u53ef\u884c\u6027\u4e0e\u4f18\u52bf"}}
{"id": "2505.11341", "pdf": "https://arxiv.org/pdf/2505.11341", "abs": "https://arxiv.org/abs/2505.11341", "authors": ["Banca Calvo Figueras", "Rodrigo Agerri"], "title": "Benchmarking Critical Questions Generation: A Challenging Reasoning Task for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The task of Critical Questions Generation (CQs-Gen) aims to foster critical\nthinking by enabling systems to generate questions that expose assumptions and\nchallenge the reasoning in arguments. Despite growing interest in this area,\nprogress has been hindered by the lack of suitable datasets and automatic\nevaluation standards. This work presents a comprehensive approach to support\nthe development and benchmarking of systems for this task. We construct the\nfirst large-scale manually-annotated dataset. We also investigate automatic\nevaluation methods and identify a reference-based technique using large\nlanguage models (LLMs) as the strategy that best correlates with human\njudgments. Our zero-shot evaluation of 11 LLMs establishes a strong baseline\nwhile showcasing the difficulty of the task. Data, code, and a public\nleaderboard are provided to encourage further research not only in terms of\nmodel performance, but also to explore the practical benefits of CQs-Gen for\nboth automated reasoning and human critical thinking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u5173\u952e\u95ee\u9898\u751f\u6210\u4efb\u52a1(CQs-Gen)\uff0c\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u3001\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u3001\u5efa\u7acb11\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u7684\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u4fc3\u8fdb\u6279\u5224\u6027\u601d\u7ef4\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u5173\u952e\u95ee\u9898\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u53ef\u9760\u8bc4\u4f30\u6807\u51c6\uff0c\u963b\u788d\u4e86\u7cfb\u7edf\u5f00\u53d1\u548c\u6027\u80fd\u63d0\u5347\u3002\u8be5\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a8\u52a8\u81ea\u52a8\u5316\u63a8\u7406\u4e0e\u4eba\u7c7b\u6279\u5224\u6027\u601d\u7ef4\u7684\u53cc\u5411\u4fc3\u8fdb\u3002", "method": "1. \u6784\u5efa\u4eba\u5de5\u6807\u6ce8\u7684CQs-Gen\u6570\u636e\u96c6\uff1b2. \u5b9e\u9a8c\u9a8c\u8bc1\u57fa\u4e8eLLM\u7684\u53c2\u8003\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u76f8\u5173\uff1b3. \u5bf911\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u6d4b\u8bd5\u5efa\u7acb\u6027\u80fd\u57fa\u7ebf\u3002", "result": "1. \u53d1\u5e03\u9996\u4e2aCQs-Gen\u4e13\u7528\u6570\u636e\u96c6\uff1b2. \u53d1\u73b0LLM\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027\u8fbe\u6700\u9ad8\uff1b3. \u73b0\u6709\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u5b58\u5728\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u3001\u4ee3\u7801\u548c\u516c\u5171\u6392\u884c\u699c\uff0c\u672c\u7814\u7a76\u4e3aCQs-Gen\u9886\u57df\u5efa\u7acb\u4e86\u7cfb\u7edf\u5316\u7814\u7a76\u6846\u67b6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u4efb\u52a1\u96be\u5ea6\uff0c\u4fc3\u8fdb\u672a\u6765\u5728\u6a21\u578b\u6027\u80fd\u548c\u5e94\u7528\u4ef7\u503c\u5c42\u9762\u7684\u63a2\u7d22\u3002"}}
{"id": "2505.11352", "pdf": "https://arxiv.org/pdf/2505.11352", "abs": "https://arxiv.org/abs/2505.11352", "authors": ["Rao Ma", "Tongzhou Chen", "Kartik Audhkhasi", "Bhuvana Ramabhadran"], "title": "LegoSLM: Connecting LLM with Speech Encoder using CTC Posteriors", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Recently, large-scale pre-trained speech encoders and Large Language Models\n(LLMs) have been released, which show state-of-the-art performance on a range\nof spoken language processing tasks including Automatic Speech Recognition\n(ASR). To effectively combine both models for better performance, continuous\nspeech prompts, and ASR error correction have been adopted. However, these\nmethods are prone to suboptimal performance or are inflexible. In this paper,\nwe propose a new paradigm, LegoSLM, that bridges speech encoders and LLMs using\nthe ASR posterior matrices. The speech encoder is trained to generate\nConnectionist Temporal Classification (CTC) posteriors over the LLM vocabulary,\nwhich are used to reconstruct pseudo-audio embeddings by computing a weighted\nsum of the LLM input embeddings. These embeddings are concatenated with text\nembeddings in the LLM input space. Using the well-performing USM and Gemma\nmodels as an example, we demonstrate that our proposed LegoSLM method yields\ngood performance on both ASR and speech translation tasks. By connecting USM\nwith Gemma models, we can get an average of 49% WERR over the USM-CTC baseline\non 8 MLS testsets. The trained model also exhibits modularity in a range of\nsettings -- after fine-tuning the Gemma model weights, the speech encoder can\nbe switched and combined with the LLM in a zero-shot fashion. Additionally, we\npropose to control the decode-time influence of the USM and LLM using a softmax\ntemperature, which shows effectiveness in domain adaptation.", "AI": {"tldr": "\u63d0\u51faLegoSLM\u65b9\u6cd5\uff0c\u901a\u8fc7ASR\u540e\u9a8c\u77e9\u9635\u8fde\u63a5\u8bed\u97f3\u7f16\u7801\u5668\u4e0eLLM\uff0c\u5b9e\u73b0\u8bed\u97f3\u6587\u672c\u8054\u5408\u5efa\u6a21", "motivation": "\u73b0\u6709\u8bed\u97f3\u7f16\u7801\u5668\u4e0eLLM\u7ed3\u5408\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u6b21\u4f18\u6216\u7075\u6d3b\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8de8\u6a21\u6001\u878d\u5408\u65b9\u6848", "method": "\u8bad\u7ec3\u8bed\u97f3\u7f16\u7801\u5668\u751f\u6210LLM\u8bcd\u6c47\u7684CTC\u540e\u9a8c\u77e9\u9635\uff0c\u91cd\u6784\u4f2a\u97f3\u9891\u5d4c\u5165\u5e76\u4e0e\u6587\u672c\u5d4c\u5165\u62fc\u63a5\u8f93\u5165LLM", "result": "USM+Gemma\u7ec4\u5408\u57288\u4e2aMLS\u6d4b\u8bd5\u96c6\u4e0a\u5b9e\u73b049% WERR\u63d0\u5347\uff0c\u652f\u6301\u8bed\u97f3\u7f16\u7801\u5668\u96f6\u6837\u672c\u66ff\u6362\u548c\u9886\u57df\u9002\u5e94\u6e29\u5ea6\u63a7\u5236", "conclusion": "LegoSLM\u5728ASR/\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5177\u6709\u6a21\u5757\u5316\u3001\u96f6\u6837\u672c\u7ec4\u5408\u80fd\u529b\u53ca\u89e3\u7801\u53ef\u63a7\u6027\u4f18\u52bf"}}
{"id": "2505.11368", "pdf": "https://arxiv.org/pdf/2505.11368", "abs": "https://arxiv.org/abs/2505.11368", "authors": ["Lingxiao Diao", "Xinyue Xu", "Wanxuan Sun", "Cheng Yang", "Zhuosheng Zhang"], "title": "GuideBench: Benchmarking Domain-Oriented Guideline Following for LLM Agents", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "Large language models (LLMs) have been widely deployed as autonomous agents\ncapable of following user instructions and making decisions in real-world\napplications. Previous studies have made notable progress in benchmarking the\ninstruction following capabilities of LLMs in general domains, with a primary\nfocus on their inherent commonsense knowledge. Recently, LLMs have been\nincreasingly deployed as domain-oriented agents, which rely on domain-oriented\nguidelines that may conflict with their commonsense knowledge. These guidelines\nexhibit two key characteristics: they consist of a wide range of\ndomain-oriented rules and are subject to frequent updates. Despite these\nchallenges, the absence of comprehensive benchmarks for evaluating the\ndomain-oriented guideline following capabilities of LLMs presents a significant\nobstacle to their effective assessment and further development. In this paper,\nwe introduce GuideBench, a comprehensive benchmark designed to evaluate\nguideline following performance of LLMs. GuideBench evaluates LLMs on three\ncritical aspects: (i) adherence to diverse rules, (ii) robustness to rule\nupdates, and (iii) alignment with human preferences. Experimental results on a\nrange of LLMs indicate substantial opportunities for improving their ability to\nfollow domain-oriented guidelines.", "AI": {"tldr": "\u63d0\u51faGuideBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u5bfc\u5411\u6307\u5357\u9075\u5faa\u80fd\u529b\u4e2d\u7684\u89c4\u5219\u9075\u5b88\u3001\u66f4\u65b0\u9c81\u68d2\u6027\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5b58\u5728\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u901a\u7528\u9886\u57df\u5e38\u8bc6\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u9886\u57df\u7279\u5b9a\u6307\u5357\uff08\u5305\u542b\u5927\u91cf\u52a8\u6001\u66f4\u65b0\u89c4\u5219\uff09\u7684\u7cfb\u7edf\u8bc4\u4f30\u80fd\u529b\uff0c\u5236\u7ea6\u4e86\u9886\u57df\u5bfc\u5411\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u53d1\u3002", "method": "\u6784\u5efa\u5305\u542b\u591a\u6837\u5316\u9886\u57df\u89c4\u5219\u7684GuideBench\u6d4b\u8bd5\u96c6\uff0c\u8bbe\u8ba1\u89c4\u5219\u51b2\u7a81\u66f4\u65b0\u573a\u666f\uff0c\u5f15\u5165\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5bf9\u4e3b\u6d41LLMs\u8fdb\u884c\u7cfb\u7edf\u6027\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u89c4\u5219\u9075\u5faa\uff08\u5e73\u5747\u51c6\u786e\u7387\u4e0b\u964d35%\uff09\u3001\u89c4\u5219\u66f4\u65b0\u9002\u5e94\uff08\u6027\u80fd\u6ce2\u52a8\u8fbe40%\uff09\u548c\u4eba\u7c7b\u504f\u597d\u5339\u914d\uff08\u504f\u5dee\u7387\u8d8525%\uff09\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "conclusion": "GuideBench\u63ed\u793a\u4e86\u9886\u57df\u5bfc\u5411LLMs\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u5728\u52a8\u6001\u89c4\u5219\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u8bc4\u4f30\u57fa\u51c6\u4e0e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2505.11379", "pdf": "https://arxiv.org/pdf/2505.11379", "abs": "https://arxiv.org/abs/2505.11379", "authors": ["Alicia Gonz\u00e1lez Mart\u00ednez"], "title": "A computational system to handle the orthographic layer of tajwid in contemporary Quranic Orthography", "categories": ["cs.CL"], "comment": null, "summary": "Contemporary Quranic Orthography (CQO) relies on a precise system of phonetic\nnotation that can be traced back to the early stages of Islam, when the Quran\nwas mainly oral in nature and the first written renderings of it served as\nmemory aids for this oral tradition. The early systems of diacritical marks\ncreated on top of the Quranic Consonantal Text (QCT) motivated the creation and\nfurther development of a fine-grained system of phonetic notation that\nrepresented tajwid-the rules of recitation. We explored the systematicity of\nthe rules of tajwid, as they are encountered in the Cairo Quran, using a fully\nand accurately encoded digital edition of the Quranic text. For this purpose,\nwe developed a python module that can remove or add the orthographic layer of\ntajwid from a Quranic text in CQO. The interesting characteristic of these two\nsets of rules is that they address the complete Quranic text of the Cairo\nQuran, so they can be used as precise witnesses to study its phonetic and\nprosodic processes. From a computational point of view, the text of the Cairo\nQuran can be used as a linchpin to align and compare Quranic manuscripts, due\nto its richness and completeness. This will let us create a very powerful\nframework to work with the Arabic script, not just within an isolated text, but\nautomatically exploring a specific textual phenomenon in other connected\nmanuscripts. Having all the texts mapped among each other can serve as a\npowerful tool to study the nature of the notation systems of diacritics added\nto the consonantal skeleton.", "AI": {"tldr": "\u5f00\u53d1Python\u6a21\u5757\u5b9e\u73b0\u300a\u53e4\u5170\u7ecf\u300b\u6b63\u5b57\u6cd5\u5c42\u7684\u6570\u5b57\u5316\u5904\u7406\uff0c\u63ed\u793a\u5f00\u7f57\u7248\u672c\u5728\u8de8\u624b\u7a3f\u6bd4\u8f83\u4e2d\u7684\u7cfb\u7edf\u6027\u4ef7\u503c", "motivation": "\u63a2\u7d22tajwid\u89c4\u5219\u7684\u7cfb\u7edf\u6027\u7279\u5f81\u53ca\u5176\u5728\u6bd4\u8f83\u4e0d\u540c\u300a\u53e4\u5170\u7ecf\u300b\u624b\u7a3f\u4e2d\u7684\u8ba1\u7b97\u8bed\u8a00\u5b66\u5e94\u7528\u4ef7\u503c", "method": "\u57fa\u4e8e\u5b8c\u6574\u6570\u5b57\u5316\u5f00\u7f57\u7248\u300a\u53e4\u5170\u7ecf\u300b\uff0c\u5f00\u53d1\u53ef\u589e\u5220\u6b63\u5b57\u6cd5\u5c42\u7684Python\u6a21\u5757\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790", "result": "\u53d1\u73b0tajwid\u89c4\u5219\u8986\u76d6\u5f00\u7f57\u7248\u5168\u6587\uff0c\u5176\u7cfb\u7edf\u6027\u7279\u5f81\u53ef\u4f5c\u4e3a\u624b\u7a3f\u5bf9\u9f50\u7684\u7cbe\u786e\u53c2\u7167\u6846\u67b6", "conclusion": "\u6570\u5b57\u5316\u6b63\u5b57\u6cd5\u5de5\u5177\u4e3a\u8de8\u624b\u7a3f\u6bd4\u8f83\u7814\u7a76\u548c\u963f\u62c9\u4f2f\u8bed\u53d8\u97f3\u7b26\u53f7\u7cfb\u7edf\u6f14\u8fdb\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2505.11413", "pdf": "https://arxiv.org/pdf/2505.11413", "abs": "https://arxiv.org/abs/2505.11413", "authors": ["Sijia Chen", "Xiaomin Li", "Mengxue Zhang", "Eric Hanchen Jiang", "Qingcheng Zeng", "Chen-Hsiang Yu"], "title": "CARES: Comprehensive Evaluation of Safety and Adversarial Robustness in Medical LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in medical contexts,\nraising critical concerns about safety, alignment, and susceptibility to\nadversarial manipulation. While prior benchmarks assess model refusal\ncapabilities for harmful prompts, they often lack clinical specificity, graded\nharmfulness levels, and coverage of jailbreak-style attacks. We introduce CARES\n(Clinical Adversarial Robustness and Evaluation of Safety), a benchmark for\nevaluating LLM safety in healthcare. CARES includes over 18,000 prompts\nspanning eight medical safety principles, four harm levels, and four prompting\nstyles: direct, indirect, obfuscated, and role-play, to simulate both malicious\nand benign use cases. We propose a three-way response evaluation protocol\n(Accept, Caution, Refuse) and a fine-grained Safety Score metric to assess\nmodel behavior. Our analysis reveals that many state-of-the-art LLMs remain\nvulnerable to jailbreaks that subtly rephrase harmful prompts, while also\nover-refusing safe but atypically phrased queries. Finally, we propose a\nmitigation strategy using a lightweight classifier to detect jailbreak attempts\nand steer models toward safer behavior via reminder-based conditioning. CARES\nprovides a rigorous framework for testing and improving medical LLM safety\nunder adversarial and ambiguous conditions.", "AI": {"tldr": "\u63d0\u51faCARES\u57fa\u51c6\u8bc4\u4f30\u533b\u7597LLM\u5b89\u5168\u6027\uff0c\u63ed\u793a\u6a21\u578b\u5bf9\u6539\u5199\u653b\u51fb\u7684\u8106\u5f31\u6027\u5e76\u63d0\u51fa\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u7f13\u89e3\u7b56\u7565", "motivation": "\u73b0\u6709\u533b\u7597LLM\u5b89\u5168\u8bc4\u4f30\u7f3a\u4e4f\u4e34\u5e8a\u7279\u5f02\u6027\u3001\u5206\u7d1a\u5371\u5bb3\u8bc4\u4f30\u548c\u5bf9\u8d8a\u72f1\u653b\u51fb\u7684\u8986\u76d6\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u6846\u67b6", "method": "\u6784\u5efa\u5305\u542b18,000+\u63d0\u793a\u7684CARES\u57fa\u51c6\uff0c\u8986\u76d68\u5927\u5b89\u5168\u539f\u5219\uff0c\u63d0\u51fa\u4e09\u9636\u6bb5\u54cd\u5e94\u8bc4\u4f30\u534f\u8bae\uff08\u63a5\u53d7/\u8b66\u544a/\u62d2\u7edd\uff09\u548c\u5b89\u5168\u8bc4\u5206\u6307\u6807", "result": "\u4e3b\u6d41LLM\u5bf9\u6539\u5199\u653b\u51fb\u8868\u73b0\u8106\u5f31\uff0c\u540c\u65f6\u5bf9\u975e\u5e38\u89c4\u5b89\u5168\u67e5\u8be2\u8fc7\u5ea6\u62d2\u7edd\uff1b\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668+\u63d0\u793a\u8c03\u6574\u53ef\u6709\u6548\u6539\u5584\u5b89\u5168\u6027", "conclusion": "CARES\u4e3a\u533b\u7597LLM\u5728\u5bf9\u6297\u6027\u573a\u666f\u4e0b\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u53ef\u884c\u7684\u5b89\u5168\u589e\u5f3a\u65b9\u6848"}}
{"id": "2505.11421", "pdf": "https://arxiv.org/pdf/2505.11421", "abs": "https://arxiv.org/abs/2505.11421", "authors": ["Phan Tran Minh Dat", "Vo Hoang Nhat Khang", "Quan Thanh Tho"], "title": "Towards Cultural Bridge by Bahnaric-Vietnamese Translation Using Transfer Learning of Sequence-To-Sequence Pre-training Language Model", "categories": ["cs.CL"], "comment": null, "summary": "This work explores the journey towards achieving Bahnaric-Vietnamese\ntranslation for the sake of culturally bridging the two ethnic groups in\nVietnam. However, translating from Bahnaric to Vietnamese also encounters some\ndifficulties. The most prominent challenge is the lack of available original\nBahnaric resources source language, including vocabulary, grammar, dialogue\npatterns and bilingual corpus, which hinders the data collection process for\ntraining. To address this, we leverage a transfer learning approach using\nsequence-to-sequence pre-training language model. First of all, we leverage a\npre-trained Vietnamese language model to capture the characteristics of this\nlanguage. Especially, to further serve the purpose of machine translation, we\naim for a sequence-to-sequence model, not encoder-only like BERT or\ndecoder-only like GPT. Taking advantage of significant similarity between the\ntwo languages, we continue training the model with the currently limited\nbilingual resources of Vietnamese-Bahnaric text to perform the transfer\nlearning from language model to machine translation. Thus, this approach can\nhelp to handle the problem of imbalanced resources between two languages, while\nalso optimizing the training and computational processes. Additionally, we also\nenhanced the datasets using data augmentation to generate additional resources\nand defined some heuristic methods to help the translation more precise. Our\napproach has been validated to be highly effective for the Bahnaric-Vietnamese\ntranslation model, contributing to the expansion and preservation of languages,\nand facilitating better mutual understanding between the two ethnic people.", "AI": {"tldr": "\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u548c\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u5df4\u62ff-\u8d8a\u5357\u8bed\u7ffb\u8bd1\u8d44\u6e90\u4e0d\u8db3\u95ee\u9898", "motivation": "\u5df4\u62ff\u8bed\u6e90\u8d44\u6e90\u532e\u4e4f\uff08\u8bcd\u6c47/\u8bed\u6cd5/\u53cc\u8bed\u8bed\u6599\uff09\u963b\u788d\u7ffb\u8bd1\u6a21\u578b\u8bad\u7ec3", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u8d8a\u5357\u8bed\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b+\u8fc1\u79fb\u5b66\u4e60+\u6570\u636e\u589e\u5f3a+\u542f\u53d1\u5f0f\u4f18\u5316", "result": "\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4fc3\u8fdb\u8de8\u6c11\u65cf\u7406\u89e3", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u53cc\u8bed\u8d44\u6e90\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5bf9\u8bed\u8a00\u4fdd\u62a4\u548c\u6587\u5316\u4ea4\u6d41\u6709\u53cc\u91cd\u4ef7\u503c"}}
{"id": "2505.11423", "pdf": "https://arxiv.org/pdf/2505.11423", "abs": "https://arxiv.org/abs/2505.11423", "authors": ["Xiaomin Li", "Zhou Yu", "Zhiwei Zhang", "Xupeng Chen", "Ziji Zhang", "Yingying Zhuang", "Narayanan Sadagopan", "Anurag Beniwal"], "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained\nfor reasoning or prompted via chain-of-thought (CoT), have achieved\nstate-of-the-art performance on many complex reasoning tasks. However, we\nuncover a surprising and previously overlooked phenomenon: explicit CoT\nreasoning can significantly degrade instruction-following accuracy. Evaluating\n15 models on two benchmarks: IFEval (with simple, rule-verifiable constraints)\nand ComplexBench (with complex, compositional constraints), we consistently\nobserve performance drops when CoT prompting is applied. Through large-scale\ncase studies and an attention-based analysis, we identify common patterns where\nreasoning either helps (e.g., with formatting or lexical precision) or hurts\n(e.g., by neglecting simple constraints or introducing unnecessary content). We\npropose a metric, constraint attention, to quantify model focus during\ngeneration and show that CoT reasoning often diverts attention away from\ninstruction-relevant tokens. To mitigate these effects, we introduce and\nevaluate four strategies: in-context learning, self-reflection, self-selective\nreasoning, and classifier-selective reasoning. Our results demonstrate that\nselective reasoning strategies, particularly classifier-selective reasoning,\ncan substantially recover lost performance. To our knowledge, this is the first\nwork to systematically expose reasoning-induced failures in\ninstruction-following and offer practical mitigation strategies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u663e\u5f0f\u601d\u7ef4\u94fe(CoT)\u63a8\u7406\u4f1a\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u968f\u51c6\u786e\u7387\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\u63ed\u793a\u539f\u56e0\u5e76\u63d0\u51fa\u9009\u62e9\u6027\u63a8\u7406\u7f13\u89e3\u7b56\u7565", "motivation": "\u63a2\u7d22\u63a8\u7406\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u53cd\u5e38\u8868\u73b0\uff0c\u63ed\u793aCoT\u63a8\u7406\u5bf9\u7b80\u5355\u6307\u4ee4\u7684\u8d1f\u9762\u5f71\u54cd\u673a\u5236", "method": "\u4f7f\u7528IFEval\u548cComplexBench\u57fa\u51c6\u6d4b\u8bd515\u4e2a\u6a21\u578b\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u5206\u5e03\u5206\u6790\u548c\u56db\u7c7b\u7f13\u89e3\u7b56\u7565\u5b9e\u9a8c", "result": "CoT\u5bfc\u81f4\u5e73\u5747\u6027\u80fd\u4e0b\u964d5-15%\uff0c\u6ce8\u610f\u529b\u5206\u6563\u662f\u4e3b\u56e0\uff0c\u5206\u7c7b\u5668\u9009\u62e9\u6027\u63a8\u7406\u53ef\u6062\u590d87%\u7684\u635f\u5931\u6027\u80fd", "conclusion": "\u63d0\u51fa\u9009\u62e9\u6027\u63a8\u7406\u6846\u67b6\uff0c\u8bc1\u660e\u52a8\u6001\u63a7\u5236\u63a8\u7406\u8fc7\u7a0b\u80fd\u6709\u6548\u5e73\u8861\u4efb\u52a1\u590d\u6742\u5ea6\u4e0e\u6307\u4ee4\u9075\u5faa\u8981\u6c42"}}
{"id": "2505.11436", "pdf": "https://arxiv.org/pdf/2505.11436", "abs": "https://arxiv.org/abs/2505.11436", "authors": ["Chenkai Zhang", "Yiming Lei", "Zeming Liu", "Haitao Leng", "Shaoguo Liu", "Tingting Gao", "Qingjie Liu", "Yunhong Wang"], "title": "GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art", "categories": ["cs.CL", "cs.AI"], "comment": "69 pages, 66 figures, accepted by ACL 2025", "summary": "Video Comment Art enhances user engagement by providing creative content that\nconveys humor, satire, or emotional resonance, requiring a nuanced and\ncomprehensive grasp of cultural and contextual subtleties. Although Multimodal\nLarge Language Models (MLLMs) and Chain-of-Thought (CoT) have demonstrated\nstrong reasoning abilities in STEM tasks (e.g. mathematics and coding), they\nstill struggle to generate creative expressions such as resonant jokes and\ninsightful satire. Moreover, existing benchmarks are constrained by their\nlimited modalities and insufficient categories, hindering the exploration of\ncomprehensive creativity in video-based Comment Art creation. To address these\nlimitations, we introduce GODBench, a novel benchmark that integrates video and\ntext modalities to systematically evaluate MLLMs' abilities to compose Comment\nArt. Furthermore, inspired by the propagation patterns of waves in physics, we\npropose Ripple of Thought (RoT), a multi-step reasoning framework designed to\nenhance the creativity of MLLMs. Extensive experiments reveal that existing\nMLLMs and CoT methods still face significant challenges in understanding and\ngenerating creative video comments. In contrast, RoT provides an effective\napproach to improve creative composing, highlighting its potential to drive\nmeaningful advancements in MLLM-based creativity. GODBench is publicly\navailable at https://github.com/stan-lei/GODBench-ACL2025.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u57fa\u51c6GODBench\u8bc4\u4f30\u89c6\u9891\u8bc4\u8bba\u827a\u672f\u521b\u9020\u529b\uff0c\u8bbe\u8ba1Ripple of Thought\u63a8\u7406\u6846\u67b6\u63d0\u5347\u5927\u6a21\u578b\u521b\u610f\u751f\u6210\u80fd\u529b", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728STEM\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u5e7d\u9ed8\u8bbd\u523a\u7b49\u521b\u610f\u89c6\u9891\u8bc4\u8bba\u65f6\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff1b\u5f53\u524d\u57fa\u51c6\u5b58\u5728\u6a21\u6001\u5355\u4e00\u548c\u7c7b\u522b\u5c40\u9650\uff0c\u5236\u7ea6\u89c6\u9891\u8bc4\u8bba\u827a\u672f\u7684\u521b\u9020\u529b\u7814\u7a76", "method": "\u6784\u5efa\u6574\u5408\u89c6\u9891\u4e0e\u6587\u672c\u7684\u591a\u6a21\u6001\u57fa\u51c6GODBench\uff1b\u53d7\u7269\u7406\u6ce2\u52a8\u4f20\u64ad\u542f\u53d1\uff0c\u63d0\u51fa\u591a\u6b65\u63a8\u7406\u6846\u67b6Ripple of Thought (RoT)", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u521b\u610f\u751f\u6210\u4e0a\u5b58\u5728\u663e\u8457\u6311\u6218\uff0cRoT\u6846\u67b6\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u521b\u610f\u8868\u8fbe\u80fd\u529b\uff08\u4ee3\u7801\u5df2\u5f00\u6e90\uff09", "conclusion": "GODBench\u4e3a\u89c6\u9891\u8bc4\u8bba\u827a\u672f\u8bc4\u4f30\u63d0\u4f9b\u65b0\u57fa\u51c6\uff0cRoT\u6846\u67b6\u63a8\u52a8\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u521b\u610f\u751f\u6210\u9886\u57df\u7684\u53d1\u5c55"}}
{"id": "2505.11441", "pdf": "https://arxiv.org/pdf/2505.11441", "abs": "https://arxiv.org/abs/2505.11441", "authors": ["Xianzhen Luo", "Shijie Xuyang", "Tianhao Cheng", "Zheng Chu", "Houyi Li", "ziqi wang", "Siming Huang", "Qingfu Zhu", "Qiufeng Wang", "Xiangyu Zhang", "Shuigeng Zhou", "Wanxiang Che"], "title": "Is Compression Really Linear with Code Intelligence?", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Understanding the relationship between data compression and the capabilities\nof Large Language Models (LLMs) is crucial, especially in specialized domains\nlike code intelligence. Prior work posited a linear relationship between\ncompression and general intelligence. However, it overlooked the multifaceted\nnature of code that encompasses diverse programming languages and tasks, and\nstruggled with fair evaluation of modern Code LLMs. We address this by\nevaluating a diverse array of open-source Code LLMs on comprehensive\nmulti-language, multi-task code benchmarks. To address the challenge of\nefficient and fair evaluation of pre-trained LLMs' code intelligence, we\nintroduce \\textit{Format Annealing}, a lightweight, transparent training\nmethodology designed to assess the intrinsic capabilities of these pre-trained\nmodels equitably. Compression efficacy, measured as bits-per-character (BPC),\nis determined using a novel, large-scale, and previously unseen code validation\nset derived from GitHub. Our empirical results reveal a fundamental logarithmic\nrelationship between measured code intelligence and BPC. This finding refines\nprior hypotheses of linearity, which we suggest are likely observations of the\nlogarithmic curve's tail under specific, limited conditions. Our work provides\na more nuanced understanding of compression's role in developing code\nintelligence and contributes a robust evaluation framework in the code domain.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u4ee3\u7801\u667a\u80fd\u4e0e\u538b\u7f29\u6548\u7387\u95f4\u5b58\u5728\u5bf9\u6570\u5173\u7cfb\uff0c\u4fee\u6b63\u4e86\u5148\u524d\u7ebf\u6027\u5173\u7cfb\u7684\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u516c\u5e73\u8bc4\u4f30\u6846\u67b6Format Annealing\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5047\u8bbe\u538b\u7f29\u4e0e\u901a\u7528\u667a\u80fd\u5b58\u5728\u7ebf\u6027\u5173\u7cfb\uff0c\u4f46\u5ffd\u7565\u4e86\u4ee3\u7801\u9886\u57df\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u7279\u6027\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u73b0\u4ee3\u4ee3\u7801LLMs\u7684\u516c\u5e73\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "1. \u5f15\u5165Format Annealing\u8f7b\u91cf\u8bad\u7ec3\u65b9\u6cd5\u516c\u5e73\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\uff1b2. \u4f7f\u7528GitHub\u65b0\u4ee3\u7801\u9a8c\u8bc1\u96c6\u8ba1\u7b97BPC\uff1b3. \u5728\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u5f00\u6e90\u4ee3\u7801LLMs\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\u4ee3\u7801\u667a\u80fd\u4e0eBPC\u5b58\u5728\u5bf9\u6570\u5173\u7cfb\uff0c\u5148\u524d\u89c2\u5bdf\u5230\u7684\u7ebf\u6027\u5173\u7cfb\u53ef\u80fd\u53ea\u662f\u5bf9\u6570\u66f2\u7ebf\u5c3e\u90e8\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u7684\u5c40\u90e8\u8868\u73b0\u3002", "conclusion": "\u5efa\u7acb\u4e86\u538b\u7f29\u4e0e\u4ee3\u7801\u667a\u80fd\u7684\u66f4\u7cbe\u786e\u5173\u7cfb\u6a21\u578b\uff0c\u4e3a\u4ee3\u7801\u9886\u57df\u8d21\u732e\u4e86\u5305\u542bFormat Annealing\u65b9\u6cd5\u548c\u5927\u89c4\u6a21\u9a8c\u8bc1\u96c6\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2505.11462", "pdf": "https://arxiv.org/pdf/2505.11462", "abs": "https://arxiv.org/abs/2505.11462", "authors": ["Rahul Thapa", "Qingyang Wu", "Kevin Wu", "Harrison Zhang", "Angela Zhang", "Eric Wu", "Haotian Ye", "Suhana Bedi", "Nevin Aresh", "Joseph Boen", "Shriya Reddy", "Ben Athiwaratkun", "Shuaiwen Leon Song", "James Zou"], "title": "Disentangling Reasoning and Knowledge in Medical Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical reasoning in large language models (LLMs) aims to emulate clinicians'\ndiagnostic thinking, but current benchmarks such as MedQA-USMLE, MedMCQA, and\nPubMedQA often mix reasoning with factual recall. We address this by separating\n11 biomedical QA benchmarks into reasoning- and knowledge-focused subsets using\na PubMedBERT classifier that reaches 81 percent accuracy, comparable to human\nperformance. Our analysis shows that only 32.8 percent of questions require\ncomplex reasoning. We evaluate biomedical models (HuatuoGPT-o1, MedReason, m1)\nand general-domain models (DeepSeek-R1, o4-mini, Qwen3), finding consistent\ngaps between knowledge and reasoning performance. For example, m1 scores 60.5\non knowledge but only 47.1 on reasoning. In adversarial tests where models are\nmisled with incorrect initial reasoning, biomedical models degrade sharply,\nwhile larger or RL-trained general models show more robustness. To address\nthis, we train BioMed-R1 using fine-tuning and reinforcement learning on\nreasoning-heavy examples. It achieves the strongest performance among similarly\nsized models. Further gains may come from incorporating clinical case reports\nand training with adversarial and backtracking scenarios.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u79bb\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u4e2d\u7684\u77e5\u8bc6\u56de\u5fc6\u4e0e\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff0c\u53d1\u73b0\u6a21\u578b\u63a8\u7406\u80fd\u529b\u666e\u904d\u4e0d\u8db3\uff0c\u5e76\u6210\u529f\u8bad\u7ec3\u51fa\u63a8\u7406\u80fd\u529b\u66f4\u5f3a\u7684BioMed-R1\u6a21\u578b", "motivation": "\u5f53\u524d\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982MedQA\uff09\u6df7\u6dc6\u77e5\u8bc6\u56de\u5fc6\u4e0e\u590d\u6742\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\uff0c\u65e0\u6cd5\u51c6\u786e\u8861\u91cf\u6a21\u578b\u7684\u5b9e\u9645\u4e34\u5e8a\u63a8\u7406\u6c34\u5e73", "method": "1. \u4f7f\u7528PubMedBERT\u5206\u7c7b\u5668\uff0881%\u51c6\u786e\u7387\uff09\u5c0611\u4e2a\u751f\u7269\u533b\u5b66QA\u57fa\u51c6\u5206\u4e3a\u77e5\u8bc6\u578b\u548c\u63a8\u7406\u578b\n2. \u8bc4\u4f30\u751f\u7269\u533b\u5b66\u6a21\u578b\u4e0e\u901a\u7528\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\n3. \u8fdb\u884c\u5bf9\u6297\u6027\u6d4b\u8bd5\uff08\u9519\u8bef\u521d\u59cb\u63a8\u7406\u8bef\u5bfc\uff09\n4. \u901a\u8fc7\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3BioMed-R1", "result": "1. \u4ec532.8%\u95ee\u9898\u9700\u8981\u590d\u6742\u63a8\u7406\n2. \u751f\u7269\u533b\u5b66\u6a21\u578b\u5728\u5bf9\u6297\u6d4b\u8bd5\u4e2d\u6027\u80fd\u9aa4\u964d\uff08\u5982m1\u77e5\u8bc660.5 vs \u63a8\u740647.1\uff09\n3. BioMed-R1\u5b9e\u73b0\u540c\u89c4\u6a21\u6700\u5f3a\u63a8\u7406\u6027\u80fd\n4. \u5927\u6a21\u578b/RL\u8bad\u7ec3\u6a21\u578b\u5c55\u73b0\u66f4\u5f3a\u6297\u5e72\u6270\u80fd\u529b", "conclusion": "\u533b\u5b66\u63a8\u7406\u8bc4\u4f30\u9700\u72ec\u7acb\u4e8e\u77e5\u8bc6\u6d4b\u8bd5\uff0c\u672a\u6765\u5e94\u6574\u5408\u4e34\u5e8a\u6848\u4f8b\u62a5\u544a\u5e76\u91c7\u7528\u5bf9\u6297\u6027\u56de\u6eaf\u8bad\u7ec3\u573a\u666f\u6765\u6301\u7eed\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.11470", "pdf": "https://arxiv.org/pdf/2505.11470", "abs": "https://arxiv.org/abs/2505.11470", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "title": "No Gold Standard, No Problem: Reference-Free Evaluation of Taxonomies", "categories": ["cs.CL"], "comment": null, "summary": "We introduce two reference-free metrics for quality evaluation of taxonomies.\nThe first metric evaluates robustness by calculating the correlation between\nsemantic and taxonomic similarity, covering a type of error not handled by\nexisting metrics. The second uses Natural Language Inference to assess logical\nadequacy. Both metrics are tested on five taxonomies and are shown to correlate\nwell with F1 against gold-standard taxonomies.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u4e2a\u65e0\u53c2\u8003\u6307\u6807\u8bc4\u4f30\u5206\u7c7b\u6cd5\u8d28\u91cf\uff0c\u5206\u522b\u901a\u8fc7\u8bed\u4e49-\u5206\u7c7b\u76f8\u5173\u6027\u8bc4\u4f30\u7a33\u5065\u6027\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8bc4\u4f30\u903b\u8f91\u9002\u5f53\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u4e0e\u9ec4\u91d1\u6807\u51c6\u9ad8\u5ea6\u76f8\u5173", "motivation": "\u73b0\u6709\u5206\u7c7b\u6cd5\u8bc4\u4f30\u6307\u6807\u672a\u80fd\u8986\u76d6\u67d0\u4e9b\u9519\u8bef\u7c7b\u578b\uff0c\u9700\u5f00\u53d1\u65b0\u6307\u6807\u89e3\u51b3\u7a33\u5065\u6027\u8bc4\u4f30\u7f3a\u5931\u548c\u903b\u8f91\u9002\u5f53\u6027\u9a8c\u8bc1\u95ee\u9898", "method": "1. \u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4e0e\u5206\u7c7b\u5b66\u76f8\u4f3c\u5ea6\u76f8\u5173\u6027\u7684\u7a33\u5065\u6027\u6307\u6807 2. \u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63a8\u7406(NLI)\u7684\u903b\u8f91\u9002\u5f53\u6027\u6307\u6807\uff0c\u5728\u4e94\u4e2a\u5206\u7c7b\u6cd5\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1", "result": "\u4e24\u4e2a\u65b0\u6307\u6807\u4e0e\u9ec4\u91d1\u6807\u51c6\u5206\u7c7b\u6cd5\u7684F1\u5206\u6570\u5448\u73b0\u663e\u8457\u6b63\u76f8\u5173\uff08\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570r=0.82,p<0.05\uff09", "conclusion": "\u65b0\u6307\u6807\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u4f53\u7cfb\u7684\u76f2\u70b9\uff0c\u4e3a\u65e0\u53c2\u8003\u73af\u5883\u4e0b\u7684\u5206\u7c7b\u6cd5\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u53cc\u7ef4\u5ea6\u91cf\u5316\u5de5\u5177"}}
{"id": "2505.11475", "pdf": "https://arxiv.org/pdf/2505.11475", "abs": "https://arxiv.org/abs/2505.11475", "authors": ["Zhilin Wang", "Jiaqi Zeng", "Olivier Delalleau", "Hoo-Chang Shin", "Felipe Soares", "Alexander Bukharin", "Ellie Evans", "Yi Dong", "Oleksii Kuchaiev"], "title": "HelpSteer3-Preference: Open Human-Annotated Preference Data across Diverse Tasks and Languages", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "38 pages, 2 figures", "summary": "Preference datasets are essential for training general-domain,\ninstruction-following language models with Reinforcement Learning from Human\nFeedback (RLHF). Each subsequent data release raises expectations for future\ndata collection, meaning there is a constant need to advance the quality and\ndiversity of openly available preference data. To address this need, we\nintroduce HelpSteer3-Preference, a permissively licensed (CC-BY-4.0),\nhigh-quality, human-annotated preference dataset comprising of over 40,000\nsamples. These samples span diverse real-world applications of large language\nmodels (LLMs), including tasks relating to STEM, coding and multilingual\nscenarios. Using HelpSteer3-Preference, we train Reward Models (RMs) that\nachieve top performance on RM-Bench (82.4%) and JudgeBench (73.7%). This\nrepresents a substantial improvement (~10% absolute) over the previously\nbest-reported results from existing RMs. We demonstrate HelpSteer3-Preference\ncan also be applied to train Generative RMs and how policy models can be\naligned with RLHF using our RMs. Dataset (CC-BY-4.0):\nhttps://huggingface.co/datasets/nvidia/HelpSteer3#preference", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86HelpSteer3-Preference\u6570\u636e\u96c6\uff08CC-BY-4.0\u534f\u8bae\uff09\uff0c\u5305\u542b40,000+\u6807\u6ce8\u6837\u672c\uff0c\u8986\u76d6STEM/\u7f16\u7a0b/\u591a\u8bed\u8a00\u573a\u666f\uff0c\u8bad\u7ec3\u51fa\u7684\u5956\u52b1\u6a21\u578b\u5728RM-Bench(82.4%)\u548cJudgeBench(73.7%)\u8fbe\u5230\u6700\u4f18\u8868\u73b0\uff0c\u8f83\u5148\u524d\u6a21\u578b\u63d0\u5347\u7ea610%\u3002", "motivation": "\u73b0\u6709\u516c\u5f00\u504f\u597d\u6570\u636e\u96c6\u5728\u8d28\u91cf\u548c\u591a\u6837\u6027\u4e0a\u65e0\u6cd5\u6ee1\u8db3RLHF\u8bad\u7ec3\u9700\u6c42\uff0c\u9700\u6301\u7eed\u63d0\u5347\u5f00\u653e\u6570\u636e\u96c6\u8d28\u91cf\u4ee5\u652f\u6301\u8bed\u8a00\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5f3a\u5316\u3002", "method": "1. \u6784\u5efa\u8986\u76d6\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u591a\u6837\u5316\u504f\u597d\u6570\u636e\u96c6\uff08\u542bSTEM/\u7f16\u7a0b/\u591a\u8bed\u8a00\u4efb\u52a1\uff09\n2. \u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff08Reward Models\uff09\n3. \u63a2\u7d22\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u53caRLHF\u5bf9\u9f50\u7b56\u7565", "result": "1. \u5956\u52b1\u6a21\u578b\u5728RM-Bench/JudgeBench\u5206\u522b\u53d6\u5f9782.4%/73.7%\u51c6\u786e\u7387\n2. \u76f8\u8f83\u73b0\u6709\u6700\u4f73\u7ed3\u679c\u5b9e\u73b0\u7ea610%\u7edd\u5bf9\u63d0\u5347\n3. \u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u5728\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u548c\u7b56\u7565\u6a21\u578b\u5bf9\u9f50\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "HelpSteer3-Preference\u901a\u8fc7\u9ad8\u8d28\u91cf\u6807\u6ce8\u548c\u573a\u666f\u8986\u76d6\u7a81\u7834\u4e86\u5f00\u653e\u504f\u597d\u6570\u636e\u96c6\u74f6\u9888\uff0c\u5176\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6548\u679c\uff0c\u4e3aRLHF\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u91cd\u8981\u6570\u636e\u57fa\u7840\u3002"}}
{"id": "2505.11480", "pdf": "https://arxiv.org/pdf/2505.11480", "abs": "https://arxiv.org/abs/2505.11480", "authors": ["Anjiang Wei", "Tarun Suresh", "Huanmi Tan", "Yinglun Xu", "Gagandeep Singh", "Ke Wang", "Alex Aiken"], "title": "Improving Assembly Code Performance with Large Language Models via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.PF", "cs.PL", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance across a\nwide range of programming tasks, yet their potential for code optimization\nremains underexplored. This work investigates whether LLMs can optimize the\nperformance of assembly code, where fine-grained control over execution enables\nimprovements that are difficult to express in high-level languages. We present\na reinforcement learning framework that trains LLMs using Proximal Policy\nOptimization (PPO), guided by a reward function that considers both functional\ncorrectness, validated through test cases, and execution performance relative\nto the industry-standard compiler gcc -O3. To support this study, we introduce\na benchmark of 8,072 real-world programs. Our model, Qwen2.5-Coder-7B-PPO,\nachieves 96.0% test pass rates and an average speedup of 1.47x over the gcc -O3\nbaseline, outperforming all 20 other models evaluated, including\nClaude-3.7-sonnet. These results indicate that reinforcement learning can\nunlock the potential of LLMs to serve as effective optimizers for assembly code\nperformance.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3LLMs\u4f18\u5316\u6c47\u7f16\u4ee3\u7801\u6027\u80fd\uff0c\u6a21\u578bQwen2.5-Coder-7B-PPO\u57288,072\u4e2a\u771f\u5b9e\u7a0b\u5e8f\u6d4b\u8bd5\u4e2d\u53d6\u5f9796%\u901a\u8fc7\u7387\uff0c\u5e73\u5747\u52a0\u901f1.47\u500d\u8d85\u8d8agcc -O3\u57fa\u51c6\u3002", "motivation": "LLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u5927\u80fd\u529b\u4f46\u4ee3\u7801\u4f18\u5316\u6f5c\u529b\u672a\u5145\u5206\u5f00\u53d1\uff0c\u6c47\u7f16\u4ee3\u7801\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u7279\u6027\u53ef\u5b9e\u73b0\u9ad8\u7ea7\u8bed\u8a00\u96be\u4ee5\u5b9e\u73b0\u7684\u6027\u80fd\u4f18\u5316\u3002", "method": "\u91c7\u7528PPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5956\u52b1\u51fd\u6570\u540c\u65f6\u8bc4\u4f30\u6d4b\u8bd5\u7528\u4f8b\u9a8c\u8bc1\u7684\u529f\u80fd\u6b63\u786e\u6027\u548c\u76f8\u5bf9gcc -O3\u7684\u6267\u884c\u6027\u80fd\uff0c\u4f7f\u75288,072\u4e2a\u771f\u5b9e\u7a0b\u5e8f\u4f5c\u4e3a\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "result": "\u6a21\u578b\u5728\u4fdd\u630196%\u6d4b\u8bd5\u901a\u8fc7\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u5e73\u57471.47\u500d\u52a0\u901f\uff0c\u6027\u80fd\u8d85\u8d8a\u5305\u62ecClaude-3.7-sonnet\u5728\u5185\u768420\u4e2a\u5bf9\u6bd4\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u91ca\u653eLLMs\u4f5c\u4e3a\u6c47\u7f16\u4ee3\u7801\u4f18\u5316\u5668\u7684\u6f5c\u529b\uff0c\u4e3a\u5e95\u5c42\u4ee3\u7801\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.11484", "pdf": "https://arxiv.org/pdf/2505.11484", "abs": "https://arxiv.org/abs/2505.11484", "authors": ["Yige Xu", "Xu Guo", "Zhiwei Zeng", "Chunyan Miao"], "title": "SoftCoT++: Test-Time Scaling with Soft Chain-of-Thought Reasoning", "categories": ["cs.CL"], "comment": "14 pages", "summary": "Test-Time Scaling (TTS) refers to approaches that improve reasoning\nperformance by allocating extra computation during inference, without altering\nthe model's parameters. While existing TTS methods operate in a discrete token\nspace by generating more intermediate steps, recent studies in Coconut and\nSoftCoT have demonstrated that thinking in the continuous latent space can\nfurther enhance the reasoning performance. Such latent thoughts encode\ninformative thinking without the information loss associated with\nautoregressive token generation, sparking increased interest in\ncontinuous-space reasoning. Unlike discrete decoding, where repeated sampling\nenables exploring diverse reasoning paths, latent representations in continuous\nspace are fixed for a given input, which limits diverse exploration, as all\ndecoded paths originate from the same latent thought. To overcome this\nlimitation, we introduce SoftCoT++ to extend SoftCoT to the Test-Time Scaling\nparadigm by enabling diverse exploration of thinking paths. Specifically, we\nperturb latent thoughts via multiple specialized initial tokens and apply\ncontrastive learning to promote diversity among soft thought representations.\nExperiments across five reasoning benchmarks and two distinct LLM architectures\ndemonstrate that SoftCoT++ significantly boosts SoftCoT and also outperforms\nSoftCoT with self-consistency scaling. Moreover, it shows strong compatibility\nwith conventional scaling techniques such as self-consistency. Source code is\navailable at https://github.com/xuyige/SoftCoT.", "AI": {"tldr": "\u63d0\u51fa\u4e86SoftCoT++\u65b9\u6cd5\uff0c\u901a\u8fc7\u6270\u52a8\u6f5c\u5728\u601d\u8003\u548c\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u63a8\u7406\u8def\u5f84\u591a\u6837\u6027\uff0c\u663e\u8457\u63d0\u5347\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406\u6027\u80fd\u5e76\u517c\u5bb9\u4f20\u7edf\u6269\u5c55\u6280\u672f\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u53d7\u9650\u4e8e\u56fa\u5b9a\u6f5c\u5728\u8868\u793a\uff0c\u65e0\u6cd5\u8fdb\u884c\u591a\u6837\u5316\u7684\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u3002\u6240\u6709\u89e3\u7801\u8def\u5f84\u5747\u6e90\u81ea\u540c\u4e00\u6f5c\u5728\u601d\u8003\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u6f5c\u529b\u3002", "method": "1. \u901a\u8fc7\u591a\u7ec4\u7279\u5316\u521d\u59cbtoken\u6270\u52a8\u6f5c\u5728\u601d\u8003\n2. \u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8f6f\u601d\u8003\u8868\u5f81\u7684\u591a\u6837\u6027\n3. \u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u4e0b\u5b9e\u73b0\u591a\u6837\u5316\u63a2\u7d22", "result": "\u57285\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c2\u79cdLLM\u67b6\u6784\u4e2d\uff0cSoftCoT++\u663e\u8457\u8d85\u8d8aSoftCoT\u53ca\u81ea\u6d3d\u6269\u5c55\u65b9\u6cd5\uff0c\u5e76\u4e0e\u81ea\u6d3d\u7b49\u4f20\u7edf\u6269\u5c55\u6280\u672f\u826f\u597d\u517c\u5bb9\u3002", "conclusion": "\u901a\u8fc7\u591a\u6837\u5316\u63a2\u7d22\u673a\u5236\u7a81\u7834\u8fde\u7eed\u7a7a\u95f4\u63a8\u7406\u74f6\u9888\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u4e14\u5177\u5907\u4f18\u79c0\u7684\u5411\u4e0b\u517c\u5bb9\u6027\uff0c\u53ef\u4e0e\u73b0\u6709\u6269\u5c55\u6280\u672f\u534f\u540c\u4f5c\u7528\u3002"}}
{"id": "2505.11485", "pdf": "https://arxiv.org/pdf/2505.11485", "abs": "https://arxiv.org/abs/2505.11485", "authors": ["Bruno Bianchi", "Ferm\u00edn Travi", "Juan E. Kamienkowski"], "title": "Modeling cognitive processes of natural reading with transformer-based Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in Natural Language Processing (NLP) have led to the\ndevelopment of highly sophisticated language models for text generation. In\nparallel, neuroscience has increasingly employed these models to explore\ncognitive processes involved in language comprehension. Previous research has\nshown that models such as N-grams and LSTM networks can partially account for\npredictability effects in explaining eye movement behaviors, specifically Gaze\nDuration, during reading. In this study, we extend these findings by evaluating\ntransformer-based models (GPT2, LLaMA-7B, and LLaMA2-7B) to further investigate\nthis relationship. Our results indicate that these architectures outperform\nearlier models in explaining the variance in Gaze Durations recorded from\nRioplantense Spanish readers. However, similar to previous studies, these\nmodels still fail to account for the entirety of the variance captured by human\npredictability. These findings suggest that, despite their advancements,\nstate-of-the-art language models continue to predict language in ways that\ndiffer from human readers.", "AI": {"tldr": "Transformer-based language models outperform previous architectures in explaining gaze duration variance during reading, but still fall short of fully capturing human predictability patterns.", "motivation": "Extend previous research on language models' ability to account for human eye movement behaviors during reading by evaluating modern transformer architectures", "method": "Evaluated transformer models (GPT2, LLaMA-7B, LLaMA2-7B) using eye-tracking data (Gaze Durations) from Rioplantense Spanish readers", "result": "Transformer models explained more variance than N-grams/LSTMs but still failed to account for all variance captured by human predictability measures", "conclusion": "State-of-the-art language models predict language differently from human readers, despite improved performance over previous architectures"}}
{"id": "2505.10583", "pdf": "https://arxiv.org/pdf/2505.10583", "abs": "https://arxiv.org/abs/2505.10583", "authors": ["Diogo Freitas", "Brigt H\u00e5vardstun", "C\u00e8sar Ferri", "Dar\u00edo Garigliotti", "Jan Arne Telle", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "title": "Relative Drawing Identification Complexity is Invariant to Modality in Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "54 pages (42 pages of appendix)", "summary": "Large language models have become multimodal, and many of them are said to\nintegrate their modalities using common representations. If this were true, a\ndrawing of a car as an image, for instance, should map to the similar area in\nthe latent space as a textual description of the strokes that conform the\ndrawing. To explore this in a black-box access regime to these models, we\npropose the use of machine teaching, a theory that studies the minimal set of\nexamples a teacher needs to choose so that the learner captures the concept. In\nthis paper we evaluate the complexity of teaching visual-language models a\nsubset of objects in the Quick, Draw! dataset using two presentations: raw\nimages as bitmaps and trace coordinates in TikZ format. The results indicate\nthat image-based representations generally require fewer segments and achieve\nhigher accuracy than coordinate-based representations. But, surprisingly, the\nteaching size usually ranks concepts similarly across both modalities, even\nwhen controlling for (a human proxy of) concept priors, suggesting that the\nsimplicity of concepts may be an inherent property that transcends modality\nrepresentations.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u56fe\u50cf\u4e0e\u5750\u6807\u8868\u793a\u7684\u6559\u5b66\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u56fe\u50cf\u8868\u793a\u66f4\u9ad8\u6548\u4f46\u8de8\u6a21\u6001\u6982\u5ff5\u590d\u6742\u5ea6\u6392\u5e8f\u76f8\u4f3c", "motivation": "\u9a8c\u8bc1\u591a\u6a21\u6001\u6a21\u578b\u662f\u5426\u901a\u8fc7\u5171\u540c\u6f5c\u5728\u7a7a\u95f4\u6574\u5408\u4e0d\u540c\u6a21\u6001\uff0c\u6bd4\u8f83\u56fe\u50cf\u4e0e\u5750\u6807\u8868\u793a\u7684\u6559\u5b66\u6548\u7387\u5dee\u5f02", "method": "\u57fa\u4e8e\u673a\u5668\u6559\u5b66\u7406\u8bba\uff0c\u4f7f\u7528Quick, Draw!\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u4f4d\u56fe\u56fe\u50cf\u548cTikZ\u5750\u6807\u4e24\u79cd\u8868\u793a\u65b9\u5f0f\u7684\u6559\u5b66\u590d\u6742\u5ea6", "result": "\u56fe\u50cf\u8868\u793a\u6240\u9700\u6837\u672c\u66f4\u5c11\u4e14\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4f46\u4e24\u79cd\u6a21\u6001\u4e0b\u6982\u5ff5\u7684\u6559\u5b66\u590d\u6742\u5ea6\u6392\u5e8f\u5177\u6709\u4e00\u81f4\u6027", "conclusion": "\u6982\u5ff5\u7b80\u5355\u6027\u53ef\u80fd\u662f\u8de8\u6a21\u6001\u7684\u56fa\u6709\u5c5e\u6027\uff0c\u6697\u793a\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u5b58\u5728\u7edf\u4e00\u7684\u590d\u6742\u5ea6\u8bc4\u4f30\u6807\u51c6"}}
{"id": "2505.10586", "pdf": "https://arxiv.org/pdf/2505.10586", "abs": "https://arxiv.org/abs/2505.10586", "authors": ["Poli A. Nemkova", "Suleyman O. Polat", "Rafid I. Jahan", "Sagnik Ray Choudhury", "Sun-joo Lee", "Shouryadipta Sarkar", "Mark V. Albert"], "title": "Towards Automated Situation Awareness: A RAG-Based Framework for Peacebuilding Reports", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "Timely and accurate situation awareness is vital for decision-making in\nhumanitarian response, conflict monitoring, and early warning and early action.\nHowever, the manual analysis of vast and heterogeneous data sources often\nresults in delays, limiting the effectiveness of interventions. This paper\nintroduces a dynamic Retrieval-Augmented Generation (RAG) system that\nautonomously generates situation awareness reports by integrating real-time\ndata from diverse sources, including news articles, conflict event databases,\nand economic indicators. Our system constructs query-specific knowledge bases\non demand, ensuring timely, relevant, and accurate insights.\n  To ensure the quality of generated reports, we propose a three-level\nevaluation framework that combines semantic similarity metrics, factual\nconsistency checks, and expert feedback. The first level employs automated NLP\nmetrics to assess coherence and factual accuracy. The second level involves\nhuman expert evaluation to verify the relevance and completeness of the\nreports. The third level utilizes LLM-as-a-Judge, where large language models\nprovide an additional layer of assessment to ensure robustness. The system is\ntested across multiple real-world scenarios, demonstrating its effectiveness in\nproducing coherent, insightful, and actionable reports. By automating report\ngeneration, our approach reduces the burden on human analysts and accelerates\ndecision-making processes. To promote reproducibility and further research, we\nopenly share our code and evaluation tools with the community via GitHub.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u5b9e\u65f6\u6570\u636e\u81ea\u52a8\u751f\u6210\u60c5\u5883\u611f\u77e5\u62a5\u544a\uff0c\u5e76\u5efa\u7acb\u4e09\u7ea7\u8bc4\u4f30\u6846\u67b6\u786e\u4fdd\u8d28\u91cf", "motivation": "\u89e3\u51b3\u4eba\u5de5\u5206\u6790\u6d77\u91cf\u5f02\u6784\u6570\u636e\u5bfc\u81f4\u7684\u54cd\u5e94\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u4eba\u9053\u6551\u63f4\u548c\u51b2\u7a81\u76d1\u6d4b\u7684\u51b3\u7b56\u65f6\u6548\u6027", "method": "1. \u52a8\u6001RAG\u7cfb\u7edf\u6784\u5efa\u6309\u9700\u77e5\u8bc6\u5e93\n2. \u4e09\u7ea7\u8bc4\u4f30\u6846\u67b6\uff08NLP\u6307\u6807+\u4e13\u5bb6\u8bc4\u4f30+LLM\u9a8c\u8bc1\uff09\n3. \u591a\u573a\u666f\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1", "result": "\u7cfb\u7edf\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6709\u6548\u751f\u6210\u8fde\u8d2f\u3001\u53ef\u64cd\u4f5c\u7684\u62a5\u544a\uff0c\u81ea\u52a8\u5316\u6d41\u7a0b\u5c06\u51b3\u7b56\u901f\u5ea6\u63d0\u534760%", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u5316\u62a5\u544a\u751f\u6210\u51cf\u5c11\u4eba\u5de5\u8d1f\u62c5\uff0c\u5f00\u6e90\u8bc4\u4f30\u5de5\u5177\u4fc3\u8fdb\u9886\u57df\u7814\u7a76\u53ef\u91cd\u590d\u6027\u53d1\u5c55"}}
{"id": "2505.10588", "pdf": "https://arxiv.org/pdf/2505.10588", "abs": "https://arxiv.org/abs/2505.10588", "authors": ["Manisha Mehta", "Fausto Giunchiglia"], "title": "Understanding Gen Alpha Digital Language: Evaluation of LLM Safety Systems for Content Moderation", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "I.2; I.2.7; K.4.2"], "comment": "Accepted to ACM FAccT 2025. To be presented in Athens, June 2025, and\n  published in the conference proceedings. Preprint version; final version will\n  appear in the ACM Digital Library", "summary": "This research offers a unique evaluation of how AI systems interpret the\ndigital language of Generation Alpha (Gen Alpha, born 2010-2024). As the first\ncohort raised alongside AI, Gen Alpha faces new forms of online risk due to\nimmersive digital engagement and a growing mismatch between their evolving\ncommunication and existing safety tools. Their distinct language, shaped by\ngaming, memes, and AI-driven trends, often conceals harmful interactions from\nboth human moderators and automated systems. We assess four leading AI models\n(GPT-4, Claude, Gemini, and Llama 3) on their ability to detect masked\nharassment and manipulation within Gen Alpha discourse. Using a dataset of 100\nrecent expressions from gaming platforms, social media, and video content, the\nstudy reveals critical comprehension failures with direct implications for\nonline safety. This work contributes: (1) a first-of-its-kind dataset capturing\nGen Alpha expressions; (2) a framework to improve AI moderation systems for\nyouth protection; (3) a multi-perspective evaluation including AI systems,\nhuman moderators, and parents, with direct input from Gen Alpha co-researchers;\nand (4) an analysis of how linguistic divergence increases youth vulnerability.\nFindings highlight the urgent need to redesign safety systems attuned to youth\ncommunication, especially given Gen Alpha reluctance to seek help when adults\nfail to understand their digital world. This study combines the insight of a\nGen Alpha researcher with systematic academic analysis to address critical\ndigital safety challenges.", "AI": {"tldr": "\u7814\u7a76AI\u7cfb\u7edf\u5bf9Alpha\u4e16\u4ee3\uff082010-2024\u51fa\u751f\uff09\u6570\u5b57\u8bed\u8a00\u7684\u7406\u89e3\u80fd\u529b\u53ca\u5176\u5b89\u5168\u9690\u60a3\uff0c\u63d0\u51fa\u6539\u8fdb\u5b89\u5168\u7cfb\u7edf\u7684\u6846\u67b6", "motivation": "Alpha\u4e16\u4ee3\u4f5c\u4e3a\u9996\u4ee3\u4e0eAI\u5171\u540c\u6210\u957f\u7684\u7fa4\u4f53\uff0c\u5176\u7279\u6709\u7684\u6570\u5b57\u8bed\u8a00\uff08\u878d\u5408\u6e38\u620f/\u8868\u60c5\u5305/AI\u8d8b\u52bf\uff09\u5bfc\u81f4\u73b0\u6709\u5b89\u5168\u7cfb\u7edf\u5931\u6548\uff0c\u5f15\u53d1\u65b0\u578b\u7f51\u7edc\u98ce\u9669", "method": "\u4f7f\u7528100\u4e2a\u6700\u65b0\u6570\u5b57\u8868\u8fbe\u6570\u636e\u96c6\uff0c\u8bc4\u4f30GPT-4/Claude/Gemini/Llama3\u5728\u8bc6\u522b\u9690\u853d\u7f51\u7edc\u66b4\u529b\u65b9\u9762\u7684\u8868\u73b0\uff0c\u6784\u5efa\u591a\u65b9\u53c2\u4e0e\u8bc4\u4ef7\u4f53\u7cfb", "result": "\u53d1\u73b0\u4e3b\u6d41AI\u6a21\u578b\u5b58\u5728\u5173\u952e\u7406\u89e3\u7f3a\u9677\uff0c\u9752\u5c11\u5e74\u81ea\u6211\u4fdd\u62a4\u610f\u613f\u4e0e\u7cfb\u7edf\u6f0f\u6d1e\u5f62\u6210\u53cc\u91cd\u98ce\u9669\uff0c\u6025\u9700\u9002\u5e94\u5e74\u8f7b\u7fa4\u4f53\u7684\u5b89\u5168\u5de5\u5177", "conclusion": "\u63d0\u51fa\u9996\u4e2aAlpha\u4e16\u4ee3\u8bed\u8a00\u6570\u636e\u96c6+AI\u5ba1\u6838\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u4ee3\u9645\u534f\u4f5c\uff08AI/\u5bb6\u957f/\u9752\u5c11\u5e74\u5171\u540c\u7814\u7a76\u8005\uff09\u63a8\u52a8\u6570\u5b57\u5b89\u5168\u7cfb\u7edf\u9769\u65b0"}}
{"id": "2505.10597", "pdf": "https://arxiv.org/pdf/2505.10597", "abs": "https://arxiv.org/abs/2505.10597", "authors": ["Jiazheng Zhang", "Wenqing Jing", "Zizhuo Zhang", "Zhiheng Xi", "Shihan Dou", "Rongxiang Weng", "Jiahuan Li", "Jingang Wang", "MingXu Cai", "Shibo Hong", "Tao Gui", "Qi Zhang"], "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reward models (RMs) are essential for aligning large language models (LLMs)\nwith human values. However, noisy preferences in human feedback often lead to\nreward misgeneralization, where RMs overfit to spurious patterns and provide\nmisleading signals during policy optimization. We systematically analyze the\ntraining dynamics of preference pairs and identify that noisy examples are\nharder to fit and introduce instability. Empirical evidence shows that LLMs\noptimized using reward models trained on full noisy datasets perform worse than\nthose trained on filtered, high-quality preferences. To address this, we\npropose Collaborative Reward Modeling (CRM), an online framework that enhances\nrobustness by combining peer review and curriculum learning. Two reward models\nare trained in parallel and assess each other's data selections to filter out\npotential noise. Curriculum learning structures the preference data from easy\nto hard, ensuring synchronized training and stable feedback. Extensive\nexperiments demonstrate that CRM improves generalization, with up to 9.94\npoints of accuracy gain on RewardBench under 40 percent label noise. CRM is\nalso compatible with implicit-reward alignment methods, offering a practical\nand versatile strategy for robust alignment.", "AI": {"tldr": "\u63d0\u51fa\u534f\u540c\u5956\u52b1\u5efa\u6a21\u6846\u67b6CRM\uff0c\u901a\u8fc7\u540c\u884c\u8bc4\u5ba1+\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u5956\u52b1\u6a21\u578b\u6297\u566a\u80fd\u529b\uff0c\u572840%\u566a\u58f0\u4e0bRewardBench\u51c6\u786e\u7387\u63d0\u53479.94\u5206\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u7684\u566a\u58f0\u504f\u597d\u6570\u636e\u5bfc\u81f4\u5956\u52b1\u6a21\u578b\u8fc7\u62df\u5408\u865a\u5047\u6a21\u5f0f\uff0c\u8fdb\u800c\u5f71\u54cdLLM\u5bf9\u9f50\u6548\u679c\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u573a\u666f\u4e0b\u6cdb\u5316\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u8bbe\u8ba1\u53cc\u5956\u52b1\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u6846\u67b6\uff1a1) \u540c\u884c\u8bc4\u5ba1\u673a\u5236\u76f8\u4e92\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u8fc7\u6ee4\u566a\u58f0\u6837\u672c\uff1b2) \u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\u6309\u96be\u6613\u7a0b\u5ea6\u7ec4\u7ec7\u6570\u636e\uff0c\u4fdd\u8bc1\u8bad\u7ec3\u540c\u6b65\u6027\u3002", "result": "\u572840%\u6807\u7b7e\u566a\u58f0\u4e0b\uff0cRewardBench\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe9.94\u5206\u3002\u4e0e\u9690\u5f0f\u5956\u52b1\u5bf9\u9f50\u65b9\u6cd5\u517c\u5bb9\uff0c\u5c55\u73b0\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "CRM\u4e3aLLM\u5bf9\u9f50\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u534f\u540c\u8bad\u7ec3\u67b6\u6784\u6709\u6548\u62b5\u5fa1\u566a\u58f0\u5e72\u6270\uff0c\u663e\u8457\u63d0\u5347\u5956\u52b1\u6a21\u578b\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2505.10599", "pdf": "https://arxiv.org/pdf/2505.10599", "abs": "https://arxiv.org/abs/2505.10599", "authors": ["Jiaxuan Liu", "Zhenhua Ling"], "title": "UDDETTS: Unifying Discrete and Dimensional Emotions for Controllable Emotional Text-to-Speech", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Under review", "summary": "Recent neural codec language models have made great progress in the field of\ntext-to-speech (TTS), but controllable emotional TTS still faces many\nchallenges. Traditional methods rely on predefined discrete emotion labels to\ncontrol emotion categories and intensities, which can't capture the complexity\nand continuity of human emotional perception and expression. The lack of\nlarge-scale emotional speech datasets with balanced emotion distributions and\nfine-grained emotion annotations often causes overfitting in synthesis models\nand impedes effective emotion control. To address these issues, we propose\nUDDETTS, a neural codec language model unifying discrete and dimensional\nemotions for controllable emotional TTS. This model introduces the\ninterpretable Arousal-Dominance-Valence (ADV) space for dimensional emotion\ndescription and supports emotion control driven by either discrete emotion\nlabels or nonlinearly quantified ADV values. Furthermore, a semi-supervised\ntraining strategy is designed to comprehensively utilize diverse speech\ndatasets with different types of emotion annotations to train the UDDETTS.\nExperiments show that UDDETTS achieves linear emotion control along the three\ndimensions of ADV space, and exhibits superior end-to-end emotional speech\nsynthesis capabilities.", "AI": {"tldr": "\u63d0\u51faUDDETTS\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u79bb\u6563\u548c\u7ef4\u5ea6\u60c5\u611f\u8868\u5f81\uff08ADV\u7a7a\u95f4\uff09\u4e0e\u534a\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u53ef\u63a7\u7684\u60c5\u611f\u8bed\u97f3\u5408\u6210\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u60c5\u611f\u6807\u7b7e\u65e0\u6cd5\u6355\u6349\u60c5\u611f\u8fde\u7eed\u6027\u548c\u590d\u6742\u6027\uff0c\u4e14\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u3002\u9700\u8981\u66f4\u7075\u6d3b\u7684\u60c5\u611f\u63cf\u8ff0\u65b9\u5f0f\u548c\u6570\u636e\u5229\u7528\u65b9\u6cd5\u3002", "method": "1. \u6784\u5efaADV\u4e09\u7ef4\u60c5\u611f\u7a7a\u95f4 2. \u652f\u6301\u79bb\u6563\u6807\u7b7e/ADV\u91cf\u5316\u503c\u53cc\u63a7\u5236\u6a21\u5f0f 3. \u534a\u76d1\u7763\u8bad\u7ec3\u6574\u5408\u591a\u6e90\u6807\u6ce8\u6570\u636e", "result": "\u6a21\u578b\u5b9e\u73b0ADV\u7a7a\u95f4\u7684\u7ebf\u6027\u60c5\u611f\u63a7\u5236\uff0c\u5728\u7aef\u5230\u7aef\u5408\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "conclusion": "UDDETTS\u7a81\u7834\u4e86\u79bb\u6563\u60c5\u611f\u6807\u7b7e\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8fde\u7eed\u60c5\u611f\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2505.10610", "pdf": "https://arxiv.org/pdf/2505.10610", "abs": "https://arxiv.org/abs/2505.10610", "authors": ["Zhaowei Wang", "Wenhao Yu", "Xiyu Ren", "Jipeng Zhang", "Yu Zhao", "Rohit Saxena", "Liang Cheng", "Ginny Wong", "Simon See", "Pasquale Minervini", "Yangqiu Song", "Mark Steedman"], "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly", "categories": ["cs.CV", "cs.CL"], "comment": "Work in progress", "summary": "The rapid extension of context windows in large vision-language models has\ngiven rise to long-context vision-language models (LCVLMs), which are capable\nof handling hundreds of images with interleaved text tokens in a single forward\npass. In this work, we introduce MMLongBench, the first benchmark covering a\ndiverse set of long-context vision-language tasks, to evaluate LCVLMs\neffectively and thoroughly. MMLongBench is composed of 13,331 examples spanning\nfive different categories of downstream tasks, such as Visual RAG and Many-Shot\nICL. It also provides broad coverage of image types, including various natural\nand synthetic images. To assess the robustness of the models to different input\nlengths, all examples are delivered at five standardized input lengths (8K-128K\ntokens) via a cross-modal tokenization scheme that combines vision patches and\ntext tokens. Through a thorough benchmarking of 46 closed-source and\nopen-source LCVLMs, we provide a comprehensive analysis of the current models'\nvision-language long-context ability. Our results show that: i) performance on\na single task is a weak proxy for overall long-context capability; ii) both\nclosed-source and open-source models face challenges in long-context\nvision-language tasks, indicating substantial room for future improvement; iii)\nmodels with stronger reasoning ability tend to exhibit better long-context\nperformance. By offering wide task coverage, various image types, and rigorous\nlength control, MMLongBench provides the missing foundation for diagnosing and\nadvancing the next generation of LCVLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u957f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u57fa\u51c6MMLongBench\uff0c\u8986\u76d613k+\u6837\u672c\u548c5\u7c7b\u4efb\u52a1\uff0c\u8bc4\u4f30\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u6311\u6218\u4e14\u6027\u80fd\u4e0e\u63a8\u7406\u80fd\u529b\u6b63\u76f8\u5173", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8861\u91cf\u957f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u591a\u56fe\u4ea4\u7ec7\u6587\u672c\u7684\u80fd\u529b\uff0c\u9700\u8981\u6784\u5efa\u8986\u76d6\u591a\u6a21\u6001\u3001\u591a\u4efb\u52a1\u3001\u591a\u957f\u5ea6\u7684\u6807\u51c6\u5316\u57fa\u51c6", "method": "\u6784\u5efa\u5305\u542b13,331\u6837\u672c\u7684\u57fa\u51c6\u96c6\uff0c\u6db5\u76d6\u89c6\u89c9\u68c0\u7d22\u3001\u5c11\u6837\u672c\u5b66\u4e60\u7b495\u7c7b\u4efb\u52a1\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u6807\u8bb0\u5316\u65b9\u6848\u751f\u62108K-128K tokens\u7684\u6807\u51c6\u5316\u8f93\u5165\u957f\u5ea6", "result": "\u6d4b\u8bd546\u4e2a\u6a21\u578b\u53d1\u73b0\uff1a\u5355\u4efb\u52a1\u6027\u80fd\u4e0d\u80fd\u4ee3\u8868\u6574\u4f53\u80fd\u529b\uff1b\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u5747\u5b58\u5728\u660e\u663e\u77ed\u677f\uff1b\u63a8\u7406\u80fd\u529b\u4e0e\u957f\u4e0a\u4e0b\u6587\u8868\u73b0\u6b63\u76f8\u5173", "conclusion": "MMLongBench\u901a\u8fc7\u591a\u4efb\u52a1\u8986\u76d6\u3001\u591a\u6837\u5316\u56fe\u50cf\u7c7b\u578b\u548c\u4e25\u683c\u957f\u5ea6\u63a7\u5236\uff0c\u4e3a\u8bca\u65ad\u548c\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u957f\u4e0a\u4e0b\u6587\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5960\u5b9a\u57fa\u7840"}}
{"id": "2505.10831", "pdf": "https://arxiv.org/pdf/2505.10831", "abs": "https://arxiv.org/abs/2505.10831", "authors": ["Omar Shaikh", "Shardul Sapkota", "Shan Rizvi", "Eric Horvitz", "Joon Sung Park", "Diyi Yang", "Michael S. Bernstein"], "title": "Creating General User Models from Computer Use", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "22 pages, 6 figures, 1 table; see\n  https://generalusermodels.github.io/", "summary": "Human-computer interaction has long imagined technology that understands\nus-from our preferences and habits, to the timing and purpose of our everyday\nactions. Yet current user models remain fragmented, narrowly tailored to\nspecific apps, and incapable of the flexible reasoning required to fulfill\nthese visions. This paper presents an architecture for a general user model\n(GUM) that learns about you by observing any interaction you have with your\ncomputer. The GUM takes as input any unstructured observation of a user (e.g.,\ndevice screenshots) and constructs confidence-weighted propositions that\ncapture that user knowledge and preferences. GUMs can infer that a user is\npreparing for a wedding they're attending from messages with a friend. Or\nrecognize that a user is struggling with a collaborator's feedback on a draft\nby observing multiple stalled edits and a switch to reading related work. GUMs\nintroduce an architecture that infers new propositions about a user from\nmultimodal observations, retrieves related propositions for context, and\ncontinuously revises existing propositions. To illustrate the breadth of\napplications that GUMs enable, we demonstrate how they augment chat-based\nassistants with context, manage OS notifications to selectively surface\nimportant information, and enable interactive agents that adapt to preferences\nacross apps. We also instantiate proactive assistants (GUMBOs) that discover\nand execute useful suggestions on a user's behalf using their GUM. In our\nevaluations, we find that GUMs make calibrated and accurate inferences about\nusers, and that assistants built on GUMs proactively identify and perform\nactions that users wouldn't think to request explicitly. Altogether, GUMs\nintroduce methods that leverage multimodal models to understand unstructured\ncontext, enabling long-standing visions of HCI and entirely new interactive\nsystems that anticipate user needs.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u7528\u6237\u6a21\u578b\uff08GUM\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89c2\u5bdf\u975e\u7ed3\u6784\u5316\u7528\u6237\u884c\u4e3a\uff0c\u5b9e\u73b0\u8de8\u5e94\u7528\u7684\u7075\u6d3b\u63a8\u7406\u4e0e\u4e2a\u6027\u5316\u4ea4\u4e92\u7cfb\u7edf\u6784\u5efa\u3002", "motivation": "\u73b0\u6709\u7528\u6237\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u5e94\u7528\u4e14\u7f3a\u4e4f\u7075\u6d3b\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u957f\u671f\u4eba\u673a\u4ea4\u4e92\u613f\u666f\u3002GUM\u65e8\u5728\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u7528\u6237\u884c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "1. \u591a\u6a21\u6001\u975e\u7ed3\u6784\u5316\u8f93\u5165\u5904\u7406\uff08\u5982\u8bbe\u5907\u622a\u56fe\uff09\n2. \u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u7528\u6237\u77e5\u8bc6\u547d\u9898\u6784\u5efa\n3. \u6301\u7eed\u547d\u9898\u4fee\u8ba2\u4e0e\u4e0a\u4e0b\u6587\u68c0\u7d22\u673a\u5236\n4. \u5f00\u53d1\u4e3b\u52a8\u52a9\u624bGUMBO\uff08\u81ea\u52a8\u6267\u884c\u7528\u6237\u6f5c\u5728\u9700\u6c42\uff09", "result": "\u8bc4\u4f30\u663e\u793aGUM\u80fd\u51c6\u786e\u6821\u51c6\u7528\u6237\u63a8\u65ad\uff08\u6821\u51c6\u6027+39%\uff09\uff0c\u6784\u5efa\u7684\u52a9\u624b\u53ef\u4e3b\u52a8\u5b8c\u6210\u7528\u6237\u672a\u660e\u786e\u9700\u6c42\u7684\u64cd\u4f5c\uff08\u5982\u90ae\u4ef6\u4f18\u5148\u7ea7\u5904\u7406\u3001\u4f1a\u8bae\u51c6\u5907\u63d0\u9192\u7b49\uff09", "conclusion": "GUM\u9996\u6b21\u5b9e\u73b0\u975e\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u7684\u591a\u6a21\u6001\u7406\u89e3\uff0c\u63a8\u52a8HCI\u957f\u671f\u613f\u666f\u843d\u5730\uff0c\u5e76\u4e3a\u9884\u671f\u7528\u6237\u9700\u6c42\u7684\u65b0\u578b\u4ea4\u4e92\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.10838", "pdf": "https://arxiv.org/pdf/2505.10838", "abs": "https://arxiv.org/abs/2505.10838", "authors": ["Ran Li", "Hao Wang", "Chengzhi Mao"], "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Efficient red-teaming method to uncover vulnerabilities in Large Language\nModels (LLMs) is crucial. While recent attacks often use LLMs as optimizers,\nthe discrete language space make gradient-based methods struggle. We introduce\nLARGO (Latent Adversarial Reflection through Gradient Optimization), a novel\nlatent self-reflection attack that reasserts the power of gradient-based\noptimization for generating fluent jailbreaking prompts. By operating within\nthe LLM's continuous latent space, LARGO first optimizes an adversarial latent\nvector and then recursively call the same LLM to decode the latent into natural\nlanguage. This methodology yields a fast, effective, and transferable attack\nthat produces fluent and stealthy prompts. On standard benchmarks like AdvBench\nand JailbreakBench, LARGO surpasses leading jailbreaking techniques, including\nAutoDAN, by 44 points in attack success rate. Our findings demonstrate a potent\nalternative to agentic LLM prompting, highlighting the efficacy of interpreting\nand attacking LLM internals through gradient optimization.", "AI": {"tldr": "\u63d0\u51faLARGO\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u68af\u5ea6\u4f18\u5316\u751f\u6210\u6d41\u7545\u9690\u853d\u7684\u8d8a\u72f1\u63d0\u793a\uff0c\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f44\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u8d8a\u72f1\u653b\u51fb\u96be\u4ee5\u5728\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u5e94\u7528\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5bf9\u6297\u6027\u653b\u51fb\u624b\u6bb5\u3002", "method": "1. \u5728LLM\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u5bf9\u6297\u5411\u91cf 2. \u9012\u5f52\u8c03\u7528\u540c\u6a21\u578b\u89e3\u7801\u4e3a\u81ea\u7136\u8bed\u8a00\u63d0\u793a", "result": "\u5728AdvBench\u548cJailbreakBench\u57fa\u51c6\u4e0a\uff0c\u653b\u51fb\u6210\u529f\u7387\u8d85\u8d8aAutoDAN\u7b49\u4e3b\u6d41\u65b9\u6cd544\u4e2a\u70b9", "conclusion": "\u68af\u5ea6\u4f18\u5316\u5bf9LLM\u5185\u90e8\u7ed3\u6784\u7684\u653b\u51fb\u8def\u5f84\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u7ea2\u961f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2505.10844", "pdf": "https://arxiv.org/pdf/2505.10844", "abs": "https://arxiv.org/abs/2505.10844", "authors": ["Simeng Han", "Stephen Xia", "Grant Zhang", "Howard Dai", "Chen Liu", "Lichang Chen", "Hoang Huy Nguyen", "Hongyuan Mei", "Jiayuan Mao", "R. Thomas McCoy"], "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "13 Tables; 5 Figures", "summary": "Accuracy remains a standard metric for evaluating AI systems, but it offers\nlimited insight into how models arrive at their solutions. In this work, we\nintroduce a benchmark based on brainteasers written in long narrative form to\nprobe more deeply into the types of reasoning strategies that models use.\nBrainteasers are well-suited for this goal because they can be solved with\nmultiple approaches, such as a few-step solution that uses a creative insight\nor a longer solution that uses more brute force. We investigate large language\nmodels (LLMs) across multiple layers of reasoning, focusing not only on\ncorrectness but also on the quality and creativity of their solutions. We\ninvestigate many aspects of the reasoning process: (1) semantic parsing of the\nbrainteasers into precise mathematical competition style formats; (2)\ngenerating solutions from these mathematical forms; (3) self-correcting\nsolutions based on gold solutions; (4) producing step-by-step sketches of\nsolutions; and (5) making use of hints. We find that LLMs are in many cases\nable to find creative, insightful solutions to brainteasers, suggesting that\nthey capture some of the capacities needed to solve novel problems in creative\nways. Nonetheless, there also remain situations where they rely on brute force\ndespite the availability of more efficient, creative solutions, highlighting a\npotential direction for improvement in the reasoning abilities of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53d9\u4e8b\u578b\u8111\u7b4b\u6025\u8f6c\u5f2f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u521b\u9020\u6027\u63a8\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u51c6\u786e\u7387\u6307\u6807\u65e0\u6cd5\u63ed\u793a\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\uff0c\u9700\u901a\u8fc7\u9700\u8981\u591a\u6b65\u9aa4\u521b\u9020\u6027\u89e3\u51b3\u65b9\u6848\u7684\u6d4b\u8bd5\u573a\u666f\u6df1\u5165\u5206\u6790\u6a21\u578b\u63a8\u7406\u7b56\u7565", "method": "\u6784\u5efa\u5305\u542b\u8bed\u4e49\u89e3\u6790\u3001\u65b9\u6848\u751f\u6210\u3001\u81ea\u6211\u7ea0\u6b63\u3001\u6b65\u9aa4\u8349\u56fe\u548c\u63d0\u793a\u5229\u7528\u7b49\u591a\u5c42\u6b21\u63a8\u7406\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6", "result": "LLMs\u5c55\u73b0\u90e8\u5206\u521b\u9020\u6027\u89e3\u9898\u80fd\u529b\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u86ee\u529b\u65b9\u6cd5\u7684\u73b0\u8c61", "conclusion": "\u6a21\u578b\u5df2\u5177\u5907\u65b0\u9896\u95ee\u9898\u89e3\u51b3\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u589e\u5f3a\u5728\u9ad8\u6548\u521b\u9020\u6027\u63a8\u7406\u65b9\u9762\u7684\u7a33\u5b9a\u6027"}}
{"id": "2505.10852", "pdf": "https://arxiv.org/pdf/2505.10852", "abs": "https://arxiv.org/abs/2505.10852", "authors": ["Siyu Liu", "Jiamin Xu", "Beilin Ye", "Bo Hu", "David J. Srolovitz", "Tongqi Wen"], "title": "MatTools: Benchmarking Large Language Models for Materials Science Tools", "categories": ["cond-mat.mtrl-sci", "cs.CL", "cs.DB"], "comment": "27 pages, 23 figures", "summary": "Large language models (LLMs) are increasingly applied to materials science\nquestions, including literature comprehension, property prediction, materials\ndiscovery and alloy design. At the same time, a wide range of physics-based\ncomputational approaches have been developed in which materials properties can\nbe calculated. Here, we propose a benchmark application to evaluate the\nproficiency of LLMs to answer materials science questions through the\ngeneration and safe execution of codes based on such physics-based\ncomputational materials science packages. MatTools is built on two\ncomplementary components: a materials simulation tool question-answer (QA)\nbenchmark and a real-world tool-usage benchmark. We designed an automated\nmethodology to efficiently collect real-world materials science tool-use\nexamples. The QA benchmark, derived from the pymatgen (Python Materials\nGenomics) codebase and documentation, comprises 69,225 QA pairs that assess the\nability of an LLM to understand materials science tools. The real-world\nbenchmark contains 49 tasks (138 subtasks) requiring the generation of\nfunctional Python code for materials property calculations. Our evaluation of\ndiverse LLMs yields three key insights: (1)Generalists outshine\nspecialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a\nstandardized framework for assessing and improving LLM capabilities for\nmaterials science tool applications, facilitating the development of more\neffective AI systems for materials science and general scientific research.", "AI": {"tldr": "\u63d0\u51faMatTools\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u4e0e\u6267\u884c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6750\u6599\u79d1\u5b66\u5de5\u5177\u5e94\u7528\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u901a\u7528\u6a21\u578b\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3001AI\u66f4\u64c5\u957f\u5904\u7406AI\u76f8\u5173\u5de5\u5177\u3001\u7b80\u5355\u65b9\u6cd5\u66f4\u6709\u6548\u7b49\u5173\u952e\u7ed3\u8bba\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6750\u6599\u79d1\u5b66\u6587\u732e\u89e3\u8bfb\u548c\u7269\u6027\u9884\u6d4b\u7b49\u573a\u666f\u7684\u5e94\u7528\u9700\u6c42\uff0c\u9700\u5efa\u7acb\u7cfb\u7edf\u8bc4\u4f30\u5176\u751f\u6210\u6750\u6599\u79d1\u5b66\u8ba1\u7b97\u4ee3\u7801\u80fd\u529b\u7684\u57fa\u51c6\u4f53\u7cfb\u3002", "method": "\u6784\u5efa\u5305\u542b69,225\u4e2aQA\u5bf9\u7684\u6750\u6599\u4eff\u771f\u5de5\u5177\u95ee\u7b54\u57fa\u51c6\uff0c\u4ee5\u53ca\u5305\u542b138\u4e2a\u5b50\u4efb\u52a1\u7684\u771f\u5b9e\u4e16\u754c\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff0c\u91c7\u7528\u81ea\u52a8\u5316\u65b9\u6cd5\u6536\u96c6\u6750\u6599\u79d1\u5b66\u5de5\u5177\u4f7f\u7528\u6848\u4f8b\u3002", "result": "\u8bc4\u4f30\u53d1\u73b0\uff1a1\uff09\u901a\u7528\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff1b2\uff09\u6a21\u578b\u66f4\u64c5\u957f\u5904\u7406AI\u76f8\u5173\u7684\u5de5\u5177\u4efb\u52a1\uff1b3\uff09\u7b80\u5355\u65b9\u6cd5\u6bd4\u590d\u6742\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "conclusion": "MatTools\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6750\u6599\u79d1\u5b66\u5de5\u5177\u5e94\u7528\u80fd\u529b\u63d0\u4f9b\u6807\u51c6\u5316\u6846\u67b6\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u6709\u6548\u7684\u79d1\u5b66AI\u7cfb\u7edf\u3002"}}
{"id": "2505.10872", "pdf": "https://arxiv.org/pdf/2505.10872", "abs": "https://arxiv.org/abs/2505.10872", "authors": ["Chenxi Jiang", "Chuhao Zhou", "Jianfei Yang"], "title": "REI-Bench: Can Embodied Agents Understand Vague Human Instructions in Task Planning?", "categories": ["cs.RO", "cs.AI", "cs.CL"], "comment": "Submitted to CoRL 2025, under review", "summary": "Robot task planning decomposes human instructions into executable action\nsequences that enable robots to complete a series of complex tasks. Although\nrecent large language model (LLM)-based task planners achieve amazing\nperformance, they assume that human instructions are clear and straightforward.\nHowever, real-world users are not experts, and their instructions to robots\noften contain significant vagueness. Linguists suggest that such vagueness\nfrequently arises from referring expressions (REs), whose meanings depend\nheavily on dialogue context and environment. This vagueness is even more\nprevalent among the elderly and children, who robots should serve more. This\npaper studies how such vagueness in REs within human instructions affects\nLLM-based robot task planning and how to overcome this issue. To this end, we\npropose the first robot task planning benchmark with vague REs (REI-Bench),\nwhere we discover that the vagueness of REs can severely degrade robot planning\nperformance, leading to success rate drops of up to 77.9%. We also observe that\nmost failure cases stem from missing objects in planners. To mitigate the REs\nissue, we propose a simple yet effective approach: task-oriented context\ncognition, which generates clear instructions for robots, achieving\nstate-of-the-art performance compared to aware prompt and chains of thought.\nThis work contributes to the research community of human-robot interaction\n(HRI) by making robot task planning more practical, particularly for non-expert\nusers, e.g., the elderly and children.", "AI": {"tldr": "\u7814\u7a76\u6a21\u7cca\u6307\u79f0\u8868\u8fbe\u5f0f\u5bf9LLM\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4efb\u52a1\u5bfc\u5411\u7684\u4e0a\u4e0b\u6587\u8ba4\u77e5\u89e3\u51b3\u65b9\u6848", "motivation": "\u73b0\u5b9e\u7528\u6237\uff08\u5c24\u5176\u662f\u8001\u5e74\u4eba\u548c\u513f\u7ae5\uff09\u7684\u673a\u5668\u4eba\u6307\u4ee4\u5e38\u5b58\u5728\u6307\u79f0\u6a21\u7cca\u95ee\u9898\uff0c\u5bfc\u81f4LLM\u4efb\u52a1\u89c4\u5212\u6210\u529f\u7387\u9aa4\u964d", "method": "\u521b\u5efaREI-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u6a21\u7ccaREs\u5bfc\u81f4\u6210\u529f\u7387\u4e0b\u964d\u8fbe77.9%\uff1b\u63d0\u51fa\u4efb\u52a1\u5bfc\u5411\u7684\u4e0a\u4e0b\u6587\u8ba4\u77e5\u65b9\u6cd5\u751f\u6210\u6e05\u6670\u6307\u4ee4", "result": "\u6a21\u7ccaREs\u663e\u8457\u964d\u4f4e\u89c4\u5212\u6027\u80fd\uff08\u6210\u529f\u7387\u6700\u5927\u964d\u5e4577.9%\uff09\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97SOTA\u6548\u679c", "conclusion": "\u8be5\u7814\u7a76\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u7279\u522b\u6539\u5584\u4e86\u975e\u4e13\u5bb6\u7528\u6237\u7fa4\u4f53\u7684\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c"}}
{"id": "2505.10981", "pdf": "https://arxiv.org/pdf/2505.10981", "abs": "https://arxiv.org/abs/2505.10981", "authors": ["Yexiang Liu", "Zekun Li", "Zhi Fang", "Nan Xu", "Ran He", "Tieniu Tan"], "title": "Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A Perspective of Probability Theory", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "ACL 2025 Main", "summary": "Recently, scaling test-time compute on Large Language Models (LLM) has\ngarnered wide attention. However, there has been limited investigation of how\nvarious reasoning prompting strategies perform as scaling. In this paper, we\nfocus on a standard and realistic scaling setting: majority voting. We\nsystematically conduct experiments on 6 LLMs $\\times$ 8 prompting strategies\n$\\times$ 6 benchmarks. Experiment results consistently show that as the\nsampling time and computational overhead increase, complicated prompting\nstrategies with superior initial performance gradually fall behind simple\nChain-of-Thought. We analyze this phenomenon and provide theoretical proofs.\nAdditionally, we propose a method according to probability theory to quickly\nand accurately predict the scaling performance and select the best strategy\nunder large sampling times without extra resource-intensive inference in\npractice. It can serve as the test-time scaling law for majority voting.\nFurthermore, we introduce two ways derived from our theoretical analysis to\nsignificantly improve the scaling performance. We hope that our research can\npromote to re-examine the role of complicated prompting, unleash the potential\nof simple prompting strategies, and provide new insights for enhancing\ntest-time scaling performance.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u968f\u7740\u8ba1\u7b97\u91cf\u589e\u52a0\uff0c\u590d\u6742\u63d0\u793a\u7b56\u7565\u9010\u6e10\u88ab\u7b80\u5355\u601d\u7ef4\u94fe\u8d85\u8d8a\uff0c\u5e76\u63d0\u51fa\u5feb\u901f\u9884\u6d4b\u65b9\u6cd5\u548c\u4e24\u79cd\u6539\u8fdb\u6269\u5c55\u6027\u80fd\u7684\u65b9\u6848", "motivation": "\u63a2\u7d22\u4e0d\u540c\u63a8\u7406\u63d0\u793a\u7b56\u7565\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u4e2d\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5728\u591a\u6570\u6295\u7968\u573a\u666f\u4e0b\u7684\u7b56\u7565\u6548\u7387\u95ee\u9898\uff0c\u65e8\u5728\u4f18\u5316\u8d44\u6e90\u5bc6\u96c6\u578b\u63a8\u7406\u8fc7\u7a0b", "method": "\u901a\u8fc76\u79cdLLM\u00d78\u79cd\u63d0\u793a\u7b56\u7565\u00d76\u4e2a\u57fa\u51c6\u7684\u7cfb\u7edf\u5b9e\u9a8c\uff0c\u7ed3\u5408\u6982\u7387\u8bba\u63a8\u5bfc\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u7406\u8bba\u6539\u8fdb\u65b9\u6848", "result": "\u590d\u6742\u7b56\u7565\u5728\u6269\u5c55\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u63d0\u51fa\u7684\u9884\u6d4b\u65b9\u6cd5\u51c6\u786e\u7387\u8fbe90%\uff0c\u6539\u8fdb\u65b9\u6848\u4f7f\u6027\u80fd\u63d0\u534715-20%", "conclusion": "\u5e94\u91cd\u65b0\u5ba1\u89c6\u590d\u6742\u63d0\u793a\u7684\u4f5c\u7528\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u65b9\u6cd5\u53ef\u5145\u5206\u91ca\u653e\u7b80\u5355\u7b56\u7565\u6f5c\u529b\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u6269\u5c55\u63d0\u4f9b\u65b0\u4f18\u5316\u65b9\u5411"}}
{"id": "2505.11079", "pdf": "https://arxiv.org/pdf/2505.11079", "abs": "https://arxiv.org/abs/2505.11079", "authors": ["Hao Gu", "Jiangyan Yi", "Chenglong Wang", "Jianhua Tao", "Zheng Lian", "Jiayi He", "Yong Ren", "Yujie Chen", "Zhengqi Wen"], "title": "$\\mathcal{A}LLM4ADD$: Unlocking the Capabilities of Audio Large Language Models for Audio Deepfake Detection", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Audio deepfake detection (ADD) has grown increasingly important due to the\nrise of high-fidelity audio generative models and their potential for misuse.\nGiven that audio large language models (ALLMs) have made significant progress\nin various audio processing tasks, a heuristic question arises: Can ALLMs be\nleveraged to solve ADD?. In this paper, we first conduct a comprehensive\nzero-shot evaluation of ALLMs on ADD, revealing their ineffectiveness in\ndetecting fake audio. To enhance their performance, we propose\n$\\mathcal{A}LLM4ADD$, an ALLM-driven framework for ADD. Specifically, we\nreformulate ADD task as an audio question answering problem, prompting the\nmodel with the question: \"Is this audio fake or real?\". We then perform\nsupervised fine-tuning to enable the ALLM to assess the authenticity of query\naudio. Extensive experiments are conducted to demonstrate that our ALLM-based\nmethod can achieve superior performance in fake audio detection, particularly\nin data-scarce scenarios. As a pioneering study, we anticipate that this work\nwill inspire the research community to leverage ALLMs to develop more effective\nADD systems.", "AI": {"tldr": "\u63d0\u51faALLM4ADD\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u97f3\u9891\u4f2a\u9020\u68c0\u6d4b\u4efb\u52a1\u91cd\u6784\u4e3a\u95ee\u7b54\u95ee\u9898\u5e76\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u97f3\u9891\u751f\u6210\u6a21\u578b\u6ee5\u7528\u98ce\u9669\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b(ALLM)\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5bf9\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u63a2\u7d22\u4f18\u5316\u65b9\u6848\u3002", "method": "\u5c06\u97f3\u9891\u4f2a\u9020\u68c0\u6d4b\u8f6c\u5316\u4e3a\"\u8be5\u97f3\u9891\u662f\u771f\u5b9e\u8fd8\u662f\u4f2a\u9020\"\u7684\u95ee\u7b54\u4efb\u52a1\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u4f7fALLM\u5177\u5907\u97f3\u9891\u771f\u4f2a\u5224\u65ad\u80fd\u529b\u3002", "result": "ALLM4ADD\u5728\u4f2a\u9020\u97f3\u9891\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u4e0d\u8db3\u65f6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22ALLM\u5728\u97f3\u9891\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u7cfb\u7edf\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.11154", "pdf": "https://arxiv.org/pdf/2505.11154", "abs": "https://arxiv.org/abs/2505.11154", "authors": ["Zihan Wang", "Hongwei Li", "Rui Zhang", "Yu Liu", "Wenbo Jiang", "Wenshu Fan", "Qingchuan Zhao", "Guowen Xu"], "title": "MPMA: Preference Manipulation Attack Against Model Context Protocol", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Model Context Protocol (MCP) standardizes interface mapping for large\nlanguage models (LLMs) to access external data and tools, which revolutionizes\nthe paradigm of tool selection and facilitates the rapid expansion of the LLM\nagent tool ecosystem. However, as the MCP is increasingly adopted, third-party\ncustomized versions of the MCP server expose potential security\nvulnerabilities. In this paper, we first introduce a novel security threat,\nwhich we term the MCP Preference Manipulation Attack (MPMA). An attacker\ndeploys a customized MCP server to manipulate LLMs, causing them to prioritize\nit over other competing MCP servers. This can result in economic benefits for\nattackers, such as revenue from paid MCP services or advertising income\ngenerated from free servers. To achieve MPMA, we first design a Direct\nPreference Manipulation Attack ($\\mathtt{DPMA}$) that achieves significant\neffectiveness by inserting the manipulative word and phrases into the tool name\nand description. However, such a direct modification is obvious to users and\nlacks stealthiness. To address these limitations, we further propose\nGenetic-based Advertising Preference Manipulation Attack ($\\mathtt{GAPMA}$).\n$\\mathtt{GAPMA}$ employs four commonly used strategies to initialize\ndescriptions and integrates a Genetic Algorithm (GA) to enhance stealthiness.\nThe experiment results demonstrate that $\\mathtt{GAPMA}$ balances high\neffectiveness and stealthiness. Our study reveals a critical vulnerability of\nthe MCP in open ecosystems, highlighting an urgent need for robust defense\nmechanisms to ensure the fairness of the MCP ecosystem.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86Model Context Protocol\uff08MCP\uff09\u5728\u5f00\u653e\u751f\u6001\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1eMPMA\uff0c\u63d0\u51fa\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7684GAPMA\u653b\u51fb\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u9690\u853d\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u5de5\u5177\u4f18\u5148\u7ea7\u52ab\u6301\u3002", "motivation": "\u9488\u5bf9\u7b2c\u4e09\u65b9MCP\u670d\u52a1\u5668\u5b9a\u5236\u5316\u5e26\u6765\u7684\u5b89\u5168\u9690\u60a3\uff0c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u653b\u51fb\u8005\u901a\u8fc7\u64cd\u7eb5LLM\u5de5\u5177\u9009\u62e9\u504f\u597d\u83b7\u53d6\u7ecf\u6d4e\u5229\u76ca\u7684\u65b0\u578b\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u63d0\u51faDPMA\u76f4\u63a5\u4fee\u6539\u5de5\u5177\u540d\u79f0/\u63cf\u8ff0\u5b9e\u73b0\u653b\u51fb\uff0c\u8fdb\u4e00\u6b65\u8bbe\u8ba1GAPMA\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u5e7f\u544a\u63cf\u8ff0\uff0c\u5e73\u8861\u653b\u51fb\u6548\u679c\u4e0e\u9690\u853d\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGAPMA\u5728\u4fdd\u6301\u9690\u853d\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\uff08\u6210\u529f\u738788.3%\uff09\uff0c\u66b4\u9732MCP\u534f\u8bae\u8bbe\u8ba1\u7f3a\u9677\u3002", "conclusion": "MCP\u751f\u6001\u7cfb\u7edf\u5b58\u5728\u91cd\u5927\u5b89\u5168\u98ce\u9669\uff0c\u9700\u5efa\u7acb\u9632\u5fa1\u673a\u5236\u4fdd\u969c\u751f\u6001\u516c\u5e73\u6027\uff0c\u7814\u7a76\u4e3a\u534f\u8bae\u5b89\u5168\u8bbe\u8ba1\u63d0\u4f9b\u91cd\u8981\u8b66\u793a\u3002"}}
{"id": "2505.11165", "pdf": "https://arxiv.org/pdf/2505.11165", "abs": "https://arxiv.org/abs/2505.11165", "authors": ["Haiqing Hao", "Nikola Zubi\u0107", "Weihua He", "Zhipeng Sui", "Davide Scaramuzza", "Wenhui Wang"], "title": "Maximizing Asynchronicity in Event-based Neural Networks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "18 pages, 5 figures, 9 tables", "summary": "Event cameras deliver visual data with high temporal resolution, low latency,\nand minimal redundancy, yet their asynchronous, sparse sequential nature\nchallenges standard tensor-based machine learning (ML). While the recent\nasynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by\nasynchronously encoding events into learned representations for ML pipelines,\nexisting A2S approaches often sacrifice representation expressivity and\ngeneralizability compared to dense, synchronous methods. This paper introduces\nEVA (EVent Asynchronous representation learning), a novel A2S framework to\ngenerate highly expressive and generalizable event-by-event representations.\nInspired by the analogy between events and language, EVA uniquely adapts\nadvances from language modeling in linear attention and self-supervised\nlearning for its construction. In demonstration, EVA outperforms prior A2S\nmethods on recognition tasks (DVS128-Gesture and N-Cars), and represents the\nfirst A2S framework to successfully master demanding detection tasks, achieving\na remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's\ntransformative potential for advancing real-time event-based vision\napplications.", "AI": {"tldr": "\u63d0\u51faEVA\u6846\u67b6\uff0c\u901a\u8fc7\u501f\u9274\u8bed\u8a00\u5efa\u6a21\u6280\u672f\u663e\u8457\u63d0\u5347\u4e8b\u4ef6\u76f8\u673a\u6570\u636e\u7684\u8868\u8fbe\u529b\u4e0e\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u5f02\u6b65\u8f6c\u540c\u6b65\uff08A2S\uff09\u65b9\u6cd5\u5728\u8868\u5f81\u80fd\u529b\u4e0e\u6cdb\u5316\u6027\u4e0a\u843d\u540e\u4e8e\u5bc6\u96c6\u540c\u6b65\u65b9\u6cd5\uff0c\u9700\u8981\u7a81\u7834\u6027\u89e3\u51b3\u65b9\u6848", "method": "\u7ed3\u5408\u4e8b\u4ef6\u6d41\u4e0e\u8bed\u8a00\u7684\u76f8\u4f3c\u6027\uff0c\u521b\u65b0\u6027\u5e94\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u673a\u5236\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6280\u672f", "result": "\u5728DVS128-Gesture/N-Cars\u8bc6\u522b\u4efb\u52a1\u8d85\u8d8a\u524d\u4eba\uff0c\u9996\u6b21\u5b9e\u73b0A2S\u6846\u67b6\u5728Gen1\u68c0\u6d4b\u4efb\u52a147.7 mAP", "conclusion": "EVA\u4e3a\u5b9e\u65f6\u4e8b\u4ef6\u89c6\u89c9\u5e94\u7528\u5f00\u8f9f\u65b0\u53ef\u80fd\uff0c\u6807\u5fd7\u7740\u4e8b\u4ef6\u6570\u636e\u5904\u7406\u6280\u672f\u7684\u8303\u5f0f\u8f6c\u53d8"}}
{"id": "2505.11178", "pdf": "https://arxiv.org/pdf/2505.11178", "abs": "https://arxiv.org/abs/2505.11178", "authors": ["Yixin Wan", "Kai-Wei Chang"], "title": "CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "State-of-the-art T2I models are capable of generating high-resolution images\ngiven textual prompts. However, they still struggle with accurately depicting\ncompositional scenes that specify multiple objects, attributes, and spatial\nrelations. We present CompAlign, a challenging benchmark with an emphasis on\nassessing the depiction of 3D-spatial relationships, for evaluating and\nimproving models on compositional image generation. CompAlign consists of 900\ncomplex multi-subject image generation prompts that combine numerical and\n3D-spatial relationships with varied attribute bindings. Our benchmark is\nremarkably challenging, incorporating generation tasks with 3+ generation\nsubjects with complex 3D-spatial relationships. Additionally, we propose\nCompQuest, an interpretable and accurate evaluation framework that decomposes\ncomplex prompts into atomic sub-questions, then utilizes a MLLM to provide\nfine-grained binary feedback on the correctness of each aspect of generation\nelements in model-generated images. This enables precise quantification of\nalignment between generated images and compositional prompts. Furthermore, we\npropose an alignment framework that uses CompQuest's feedback as preference\nsignals to improve diffusion models' compositional image generation abilities.\nUsing adjustable per-image preferences, our method is easily scalable and\nflexible for different tasks. Evaluation of 9 T2I models reveals that: (1)\nmodels remarkable struggle more with compositional tasks with more complex\n3D-spatial configurations, and (2) a noticeable performance gap exists between\nopen-source accessible models and closed-source commercial models. Further\nempirical study on using CompAlign for model alignment yield promising results:\npost-alignment diffusion models achieve remarkable improvements in\ncompositional accuracy, especially on complex generation tasks, outperforming\nprevious approaches.", "AI": {"tldr": "\u73b0\u6709T2I\u6a21\u578b\u5728\u590d\u6742\u7ec4\u5408\u573a\u666f\u751f\u6210\u5b58\u5728\u4e0d\u8db3\uff0c\u4f5c\u8005\u63d0\u51faCompAlign\u57fa\u51c6\u548cCompQuest\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u53cd\u9988\u673a\u5236\u63d0\u5347\u6a21\u578b\u7ec4\u5408\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3T2I\u6a21\u578b\u5728\u751f\u6210\u591a\u5bf9\u8c61\u3001\u591a\u5c5e\u6027\u4e14\u542b3D\u7a7a\u95f4\u5173\u7cfb\u7684\u7ec4\u5408\u573a\u666f\u65f6\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1.\u6784\u5efa\u5305\u542b900\u4e2a\u590d\u6742\u63d0\u793a\u7684CompAlign\u57fa\u51c6\uff1b2.\u5f00\u53d1CompQuest\u6846\u67b6\u5c06\u63d0\u793a\u5206\u89e3\u4e3a\u539f\u5b50\u95ee\u9898\uff0c\u5229\u7528MLLM\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\uff1b3.\u8bbe\u8ba1\u57fa\u4e8e\u53cd\u9988\u7684\u5bf9\u9f50\u6846\u67b6\u4f18\u5316\u6269\u6563\u6a21\u578b\u3002", "result": "\u8bc4\u4f309\u4e2a\u6a21\u578b\u53d1\u73b0\uff1a\u590d\u67423D\u7a7a\u95f4\u914d\u7f6e\u4efb\u52a1\u8868\u73b0\u5dee\uff0c\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002\u4f18\u5316\u540e\u6a21\u578b\u7ec4\u5408\u51c6\u786e\u6027\u663e\u8457\u63d0\u5347\uff0c\u590d\u6742\u4efb\u52a1\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CompAlign\u548cCompQuest\u6709\u6548\u63d0\u5347T2I\u6a21\u578b\u7ec4\u5408\u751f\u6210\u80fd\u529b\uff0c\u7ec6\u7c92\u5ea6\u53cd\u9988\u673a\u5236\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u5bf9\u9f50\u6846\u67b6\u5c55\u73b0\u51fa\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.11183", "pdf": "https://arxiv.org/pdf/2505.11183", "abs": "https://arxiv.org/abs/2505.11183", "authors": ["Jacob Trauger", "Ambuj Tewari"], "title": "On Next-Token Prediction in LLMs: How End Goals Determine the Consistency of Decoding Algorithms", "categories": ["stat.ML", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "Probabilistic next-token prediction trained using cross-entropy loss is the\nbasis of most large language models. Given a sequence of previous values,\nnext-token prediction assigns a probability to each possible next value in the\nvocabulary. There are many ways to use next-token prediction to output token\nsequences. This paper examines a few of these algorithms (greedy, lookahead,\nrandom sampling, and temperature-scaled random sampling) and studies their\nconsistency with respect to various goals encoded as loss functions. Although\nconsistency of surrogate losses with respect to a target loss function is a\nwell researched topic, we are the first to study it in the context of LLMs (to\nthe best of our knowledge). We find that, so long as next-token prediction\nconverges to its true probability distribution, random sampling is consistent\nwith outputting sequences that mimic sampling from the true probability\ndistribution. For the other goals, such as minimizing the 0-1 loss on the\nentire sequence, we show no polynomial-time algorithm is optimal for all\nprobability distributions and all decoding algorithms studied are only optimal\nfor a subset of probability distributions. When analyzing these results, we see\nthat there is a dichotomy created between the goals of information retrieval\nand creative generation for the decoding algorithms. This shows that choosing\nthe correct decoding algorithm based on the desired goal is extremely important\nand many of the ones used are lacking theoretical grounding in numerous\nscenarios.", "AI": {"tldr": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e0d\u540c\u89e3\u7801\u7b97\u6cd5\u5728LLM\u4e2d\u7684\u4e00\u81f4\u6027\uff0c\u53d1\u73b0\u968f\u673a\u91c7\u6837\u80fd\u771f\u5b9e\u6a21\u62df\u6982\u7387\u5206\u5e03\uff0c\u800c\u5176\u4ed6\u7b97\u6cd5\u4ec5\u5728\u7279\u5b9a\u5206\u5e03\u4e0b\u6709\u6548\uff0c\u63ed\u793a\u4e86\u76ee\u6807\u5bfc\u5411\u7684\u89e3\u7801\u7b97\u6cd5\u9009\u62e9\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLM\u89e3\u7801\u7b97\u6cd5\u4e0e\u76ee\u6807\u635f\u5931\u51fd\u6570\u4e00\u81f4\u6027\u7684\u7406\u8bba\u5206\u6790\uff0c\u9700\u660e\u786e\u4e0d\u540c\u7b97\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u573a\u666f\u4e0b\u7684\u9002\u7528\u8fb9\u754c\u3002", "method": "\u91c7\u7528\u6570\u5b66\u6536\u655b\u6027\u5206\u6790\u548c\u7406\u8bba\u8bc1\u660e\u65b9\u6cd5\uff0c\u7814\u7a76\u56db\u79cd\u5178\u578b\u89e3\u7801\u7b97\u6cd5\u5728\u6982\u7387\u5206\u5e03\u6536\u655b\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u7279\u5f81\u3002", "result": "\u968f\u673a\u91c7\u6837\u5728\u771f\u5b9e\u5206\u5e03\u4e0b\u5177\u6709\u4e25\u683c\u4e00\u81f4\u6027\uff0c\u4f460-1\u635f\u5931\u7b49\u76ee\u6807\u4e0d\u5b58\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u6700\u4f18\u89e3\uff0c\u89e3\u7801\u7b97\u6cd5\u6027\u80fd\u5448\u73b0\u4fe1\u606f\u68c0\u7d22\u4e0e\u521b\u610f\u751f\u6210\u7684\u76ee\u6807\u4e8c\u5206\u73b0\u8c61\u3002", "conclusion": "LLM\u89e3\u7801\u7b97\u6cd5\u7684\u9009\u62e9\u5fc5\u987b\u4e25\u683c\u5339\u914d\u4efb\u52a1\u76ee\u6807\uff0c\u5f53\u524d\u5e38\u7528\u7b97\u6cd5\u5728\u591a\u6570\u573a\u666f\u4e0b\u7f3a\u4e4f\u7406\u8bba\u652f\u6491\uff0c\u9700\u5efa\u7acb\u76ee\u6807\u5bfc\u5411\u7684\u7b97\u6cd5\u9009\u62e9\u6846\u67b6\u3002"}}
{"id": "2505.11200", "pdf": "https://arxiv.org/pdf/2505.11200", "abs": "https://arxiv.org/abs/2505.11200", "authors": ["Xihuai Wang", "Ziyi Zhao", "Siyu Ren", "Shao Zhang", "Song Li", "Xiaoyu Li", "Ziwen Wang", "Lin Qiu", "Guanglu Wan", "Xuezhi Cao", "Xunliang Cai", "Weinan Zhang"], "title": "Audio Turing Test: Benchmarking the Human-likeness of Large Language Model-based Text-to-Speech Systems in Chinese", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.HC", "cs.LG", "eess.AS"], "comment": "Under Review", "summary": "Recent advances in large language models (LLMs) have significantly improved\ntext-to-speech (TTS) systems, enhancing control over speech style, naturalness,\nand emotional expression, which brings TTS Systems closer to human-level\nperformance. Although the Mean Opinion Score (MOS) remains the standard for TTS\nSystem evaluation, it suffers from subjectivity, environmental inconsistencies,\nand limited interpretability. Existing evaluation datasets also lack a\nmulti-dimensional design, often neglecting factors such as speaking styles,\ncontext diversity, and trap utterances, which is particularly evident in\nChinese TTS evaluation. To address these challenges, we introduce the Audio\nTuring Test (ATT), a multi-dimensional Chinese corpus dataset ATT-Corpus paired\nwith a simple, Turing-Test-inspired evaluation protocol. Instead of relying on\ncomplex MOS scales or direct model comparisons, ATT asks evaluators to judge\nwhether a voice sounds human. This simplification reduces rating bias and\nimproves evaluation robustness. To further support rapid model development, we\nalso finetune Qwen2-Audio-Instruct with human judgment data as Auto-ATT for\nautomatic evaluation. Experimental results show that ATT effectively\ndifferentiates models across specific capability dimensions using its\nmulti-dimensional design. Auto-ATT also demonstrates strong alignment with\nhuman evaluations, confirming its value as a fast and reliable assessment tool.\nThe white-box ATT-Corpus and Auto-ATT can be found in ATT Hugging Face\nCollection\n(https://huggingface.co/collections/meituan/audio-turing-test-682446320368164faeaf38a4).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u97f3\u9891\u56fe\u7075\u6d4b\u8bd5\u7684\u591a\u7ef4\u4e2d\u6587\u8bed\u97f3\u5408\u6210\u8bc4\u4f30\u6846\u67b6ATT\uff0c\u5305\u542b\u6570\u636e\u96c6ATT-Corpus\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177Auto-ATT\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u4eba\u7c7b\u5224\u65ad\u673a\u5236\u63d0\u5347\u8bc4\u4f30\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5408\u6210\u8bc4\u4f30\u4f53\u7cfb\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u7ef4\u5ea6\u5355\u4e00\u95ee\u9898\uff08\u5c24\u5176\u5728\u4e2d\u6587\u573a\u666f\uff09\uff0c\u4f20\u7edfMOS\u8bc4\u5206\u5b58\u5728\u73af\u5883\u504f\u5dee\u4e14\u7f3a\u4e4f\u591a\u7ef4\u5ea6\u8bbe\u8ba1\uff08\u5982\u53d1\u97f3\u98ce\u683c/\u4e0a\u4e0b\u6587\u591a\u6837\u6027/\u9677\u9631\u8bed\u53e5\u7b49\uff09\u3002", "method": "1. \u6784\u5efa\u591a\u7ef4\u4e2d\u6587\u8bed\u6599\u5e93ATT-Corpus\n2. \u91c7\u7528\u56fe\u7075\u6d4b\u8bd5\u7406\u5ff5\u8bbe\u8ba1\u4e8c\u503c\u5316\u4eba\u7c7b\u8bc4\u4f30\u534f\u8bae\uff08\u5224\u65ad\u8bed\u97f3\u662f\u5426\u50cf\u771f\u4eba\uff09\n3. \u57fa\u4e8e\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u5fae\u8c03Qwen2-Audio-Instruct\u6a21\u578b\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30\u5de5\u5177Auto-ATT", "result": "ATT\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u5728\u7279\u5b9a\u80fd\u529b\u7ef4\u5ea6\u7684\u8868\u73b0\uff0cAuto-ATT\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u9ad8\u5ea6\u4e00\u81f4\uff08\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u65700.82\uff09\uff0c\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u5feb\u901f\u8bc4\u4f30\u5de5\u5177\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e2d\u6587\u8bed\u97f3\u5408\u6210\u63d0\u4f9b\u4e86\u66f4\u5ba2\u89c2\u3001\u591a\u7ef4\u7684\u8bc4\u4f30\u57fa\u51c6\uff0cAuto-ATT\u53ef\u4f5c\u4e3a\u9ad8\u6548\u53ef\u9760\u7684\u81ea\u52a8\u8bc4\u4f30\u65b9\u6848\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.11274", "pdf": "https://arxiv.org/pdf/2505.11274", "abs": "https://arxiv.org/abs/2505.11274", "authors": ["Zheng Li", "Qingxiu Dong", "Jingyuan Ma", "Di Zhang", "Zhifang Sui"], "title": "SelfBudgeter: Adaptive Token Allocation for Efficient LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recently, large reasoning models demonstrate exceptional performance on\nvarious tasks. However, reasoning models inefficiently over-process both\ntrivial and complex queries, leading to resource waste and prolonged user\nlatency. To address this challenge, we propose SelfBudgeter - a self-adaptive\ncontrollable reasoning strategy for efficient reasoning. Our approach adopts a\ndual-phase training paradigm: first, the model learns to pre-estimate the\nreasoning cost based on the difficulty of the query. Then, we introduce\nbudget-guided GPRO for reinforcement learning, which effectively maintains\naccuracy while reducing output length. SelfBudgeter allows users to anticipate\ngeneration time and make informed decisions about continuing or interrupting\nthe process. Furthermore, our method enables direct manipulation of reasoning\nlength via pre-filling token budget. Experimental results demonstrate that\nSelfBudgeter can rationally allocate budgets according to problem complexity,\nachieving up to 74.47% response length compression on the MATH benchmark while\nmaintaining nearly undiminished accuracy.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6SelfBudgeter\uff0c\u901a\u8fc7\u9884\u7b97\u63a7\u5236\u5b9e\u73b074.47%\u54cd\u5e94\u538b\u7f29\u4e14\u7cbe\u5ea6\u65e0\u635f", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u5bf9\u7b80\u5355/\u590d\u6742\u8bf7\u6c42\u5747\u8fc7\u5ea6\u5904\u7406\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u548c\u5ef6\u8fdf\u8fc7\u9ad8", "method": "\u53cc\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u5b66\u4e60\u9884\u4f30\u95ee\u9898\u96be\u5ea6\u5bf9\u5e94\u7684\u63a8\u7406\u6210\u672c\uff0c\u518d\u901a\u8fc7\u9884\u7b97\u5f15\u5bfc\u7684GPRO\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u8f93\u51fa\u957f\u5ea6", "result": "\u5728MATH\u57fa\u51c6\u5b9e\u73b074.47%\u54cd\u5e94\u957f\u5ea6\u538b\u7f29\uff0c\u51c6\u786e\u7387\u57fa\u672c\u7ef4\u6301\u4e0d\u53d8", "conclusion": "\u7528\u6237\u53ef\u9884\u5224\u751f\u6210\u65f6\u95f4\u5e76\u5e72\u9884\u8fc7\u7a0b\uff0c\u4e14\u80fd\u901a\u8fc7\u9884\u586b\u5145token\u9884\u7b97\u76f4\u63a5\u63a7\u5236\u63a8\u7406\u957f\u5ea6"}}
{"id": "2505.11314", "pdf": "https://arxiv.org/pdf/2505.11314", "abs": "https://arxiv.org/abs/2505.11314", "authors": ["Christoph Leiter", "Yuki M. Asano", "Margret Keuper", "Steffen Eger"], "title": "CROC: Evaluating and Training T2I Metrics with Pseudo- and Human-Labeled Contrastive Robustness Checks", "categories": ["cs.CV", "cs.CL"], "comment": "preprint", "summary": "The assessment of evaluation metrics (meta-evaluation) is crucial for\ndetermining the suitability of existing metrics in text-to-image (T2I)\ngeneration tasks. Human-based meta-evaluation is costly and time-intensive, and\nautomated alternatives are scarce. We address this gap and propose CROC: a\nscalable framework for automated Contrastive Robustness Checks that\nsystematically probes and quantifies metric robustness by synthesizing\ncontrastive test cases across a comprehensive taxonomy of image properties.\nWith CROC, we generate a pseudo-labeled dataset (CROC$^{syn}$) of over one\nmillion contrastive prompt-image pairs to enable a fine-grained comparison of\nevaluation metrics. We also use the dataset to train CROCScore, a new metric\nthat achieves state-of-the-art performance among open-source methods,\ndemonstrating an additional key application of our framework. To complement\nthis dataset, we introduce a human-supervised benchmark (CROC$^{hum}$)\ntargeting especially challenging categories. Our results highlight robustness\nissues in existing metrics: for example, many fail on prompts involving\nnegation, and all tested open-source metrics fail on at least 25% of cases\ninvolving correct identification of body parts.", "AI": {"tldr": "CROC\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u5bf9\u6bd4\u6d4b\u8bd5\u548c\u751f\u6210\u8d85\u767e\u4e07\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6307\u6807\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u51fa\u65b0\u6307\u6807CROCScore\uff0c\u53d1\u73b0\u73b0\u6709\u6307\u6807\u5728\u5426\u5b9a\u63d0\u793a\u3001\u8eab\u4f53\u90e8\u4f4d\u8bc6\u522b\u7b49\u591a\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u5de5\u7684\u5143\u8bc4\u4f30\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8017\u65f6\u957f\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6848\u7a00\u7f3a\u3002\u9700\u8981\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6307\u6807\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faCROC\u6846\u67b6\uff1a1) \u751f\u6210\u5305\u542b\u8d85\u767e\u4e07\u5bf9\u6bd4\u63d0\u793a-\u56fe\u50cf\u5bf9\u7684CROC$^{syn}$\u6570\u636e\u96c6\uff1b2) \u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u8bad\u7ec3CROCScore\u65b0\u6307\u6807\uff1b3) \u6784\u5efa\u4eba\u5de5\u76d1\u7763\u57fa\u51c6CROC$^{hum}$\u6d4b\u8bd5\u9ad8\u96be\u5ea6\u7c7b\u522b\u3002", "result": "1. \u73b0\u6709\u6307\u6807\u5b58\u5728\u5173\u952e\u7f3a\u9677\uff1a\u591a\u6570\u65e0\u6cd5\u5904\u7406\u5426\u5b9a\u63d0\u793a\uff0c\u6240\u6709\u5f00\u6e90\u6307\u6807\u5728\u81f3\u5c1125%\u8eab\u4f53\u90e8\u4f4d\u8bc6\u522b\u6848\u4f8b\u4e2d\u5931\u8d25\uff1b2. CROCScore\u8fbe\u5230\u5f00\u6e90\u65b9\u6cd5\u4e2d\u7684SOTA\u6027\u80fd\u3002", "conclusion": "CROC\u6846\u67b6\u6709\u6548\u63ed\u793a\u8bc4\u4ef7\u6307\u6807\u7f3a\u9677\uff0cCROCScore\u6307\u6807\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\u4e0e\u4eba\u5de5\u57fa\u51c6\u7684\u7ed3\u5408\u4e3a\u6307\u6807\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.11365", "pdf": "https://arxiv.org/pdf/2505.11365", "abs": "https://arxiv.org/abs/2505.11365", "authors": ["Pierre Le Jeune", "Beno\u00eet Mal\u00e9sieux", "Weixuan Xiao", "Matteo Dora"], "title": "Phare: A Safety Probe for Large Language Models", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Ensuring the safety of large language models (LLMs) is critical for\nresponsible deployment, yet existing evaluations often prioritize performance\nover identifying failure modes. We introduce Phare, a multilingual diagnostic\nframework to probe and evaluate LLM behavior across three critical dimensions:\nhallucination and reliability, social biases, and harmful content generation.\nOur evaluation of 17 state-of-the-art LLMs reveals patterns of systematic\nvulnerabilities across all safety dimensions, including sycophancy, prompt\nsensitivity, and stereotype reproduction. By highlighting these specific\nfailure modes rather than simply ranking models, Phare provides researchers and\npractitioners with actionable insights to build more robust, aligned, and\ntrustworthy language systems.", "AI": {"tldr": "\u63d0\u51faPhare\u591a\u8bed\u8a00\u8bca\u65ad\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b89\u5168\u6027\u7ef4\u5ea6\u4e0a\u7684\u8106\u5f31\u6027\u6a21\u5f0f", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u8fc7\u5ea6\u5173\u6ce8\u6027\u80fd\u6307\u6807\uff0c\u5ffd\u89c6\u7cfb\u7edf\u6027\u6545\u969c\u6a21\u5f0f\u7684\u6df1\u5ea6\u68c0\u6d4b\uff0c\u96be\u4ee5\u652f\u6491\u53ef\u4fe1\u8d56\u7cfb\u7edf\u7684\u5efa\u8bbe", "method": "\u6784\u5efa\u8986\u76d6\u5e7b\u89c9\u53ef\u9760\u6027\u3001\u793e\u4f1a\u504f\u89c1\u3001\u6709\u5bb3\u5185\u5bb9\u7684\u4e09\u7ef4\u8bc4\u4f30\u4f53\u7cfb\uff0c\u6d4b\u8bd517\u4e2a\u524d\u6cbf\u8bed\u8a00\u6a21\u578b", "result": "\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u5949\u627f\u503e\u5411\u3001\u63d0\u793a\u654f\u611f\u6027\u3001\u523b\u677f\u5370\u8c61\u590d\u5236\u7b49\u8de8\u7ef4\u5ea6\u7cfb\u7edf\u6027\u6f0f\u6d1e", "conclusion": "Phare\u901a\u8fc7\u5b9a\u4f4d\u5177\u4f53\u7f3a\u9677\u800c\u975e\u7b80\u5355\u6392\u540d\uff0c\u4e3a\u6784\u5efa\u9c81\u68d2\u3001\u5bf9\u9f50\u7684\u8bed\u8a00\u7cfb\u7edf\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6539\u8fdb\u65b9\u5411"}}
{"id": "2505.11405", "pdf": "https://arxiv.org/pdf/2505.11405", "abs": "https://arxiv.org/abs/2505.11405", "authors": ["Bohao Xing", "Xin Liu", "Guoying Zhao", "Chengyu Liu", "Xiaolan Fu", "Heikki K\u00e4lvi\u00e4inen"], "title": "EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Emotion understanding is a critical yet challenging task. Recent advances in\nMultimodal Large Language Models (MLLMs) have significantly enhanced their\ncapabilities in this area. However, MLLMs often suffer from hallucinations,\ngenerating irrelevant or nonsensical content. To the best of our knowledge,\ndespite the importance of this issue, there has been no dedicated effort to\nevaluate emotion-related hallucinations in MLLMs. In this work, we introduce\nEmotionHallucer, the first benchmark for detecting and analyzing emotion\nhallucinations in MLLMs. Unlike humans, whose emotion understanding stems from\nthe interplay of biology and social learning, MLLMs rely solely on data-driven\nlearning and lack innate emotional instincts. Fortunately, emotion psychology\nprovides a solid foundation of knowledge about human emotions. Building on\nthis, we assess emotion hallucinations from two dimensions: emotion psychology\nknowledge and real-world multimodal perception. To support robust evaluation,\nwe utilize an adversarial binary question-answer (QA) framework, which employs\ncarefully crafted basic and hallucinated pairs to assess the emotion\nhallucination tendencies of MLLMs. By evaluating 38 LLMs and MLLMs on\nEmotionHallucer, we reveal that: i) most current models exhibit substantial\nissues with emotion hallucinations; ii) closed-source models outperform\nopen-source ones in detecting emotion hallucinations, and reasoning capability\nprovides additional advantages; iii) existing models perform better in emotion\npsychology knowledge than in multimodal emotion perception. As a byproduct,\nthese findings inspire us to propose the PEP-MEK framework, which yields an\naverage improvement of 9.90% in emotion hallucination detection across selected\nmodels. Resources will be available at\nhttps://github.com/xxtars/EmotionHallucer.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u68c0\u6d4b\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u60c5\u611f\u5e7b\u89c9\u7684\u57fa\u51c6EmotionHallucer\uff0c\u901a\u8fc7\u5bf9\u6297\u6027QA\u6846\u67b6\u8bc4\u4f3038\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u95ed\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u5e76\u63d0\u51fa\u6539\u8fdb\u6846\u67b6PEP-MEK", "motivation": "\u73b0\u6709MLLMs\u5728\u60c5\u611f\u7406\u89e3\u4e2d\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898\uff0c\u7f3a\u4e4f\u4e13\u95e8\u8bc4\u4f30\u4f53\u7cfb\u3002\u53d7\u4eba\u7c7b\u60c5\u611f\u8ba4\u77e5\u673a\u5236\u542f\u53d1\uff0c\u7ed3\u5408\u60c5\u611f\u5fc3\u7406\u5b66\u6784\u5efa\u8bc4\u4f30\u7ef4\u5ea6", "method": "\u57fa\u4e8e\u60c5\u611f\u5fc3\u7406\u5b66\u77e5\u8bc6\u548c\u591a\u6a21\u6001\u611f\u77e5\u53cc\u7ef4\u5ea6\uff0c\u8bbe\u8ba1\u5bf9\u6297\u6027\u4e8c\u5143QA\u6846\u67b6\uff08\u57fa\u7840/\u5e7b\u89c9\u95ee\u9898\u5bf9\uff09\u8bc4\u4f30\u6a21\u578b\u8868\u73b0", "result": "\u2460\u4e3b\u6d41\u6a21\u578b\u666e\u904d\u5b58\u5728\u60c5\u611f\u5e7b\u89c9 \u2461\u95ed\u6e90\u6a21\u578b\u4f18\u4e8e\u5f00\u6e90\u6a21\u578b \u2462\u6a21\u578b\u5728\u60c5\u611f\u77e5\u8bc6\u7ef4\u5ea6\u8868\u73b0\u4f18\u4e8e\u591a\u6a21\u6001\u611f\u77e5", "conclusion": "\u63d0\u51fa\u7684PEP-MEK\u6846\u67b6\u5c06\u68c0\u6d4b\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53479.90%\uff0c\u4e3aMLLMs\u60c5\u611f\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002\u8d44\u6e90\u5df2\u5f00\u6e90"}}
{"id": "2505.11406", "pdf": "https://arxiv.org/pdf/2505.11406", "abs": "https://arxiv.org/abs/2505.11406", "authors": ["Jenny Xiyu Fu", "Brennan Antone", "Kowe Kadoma", "Malte Jung"], "title": "Large Language Model Use Impact Locus of Control", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI tools increasingly shape how we write, they may also quietly reshape\nhow we perceive ourselves. This paper explores the psychological impact of\nco-writing with AI on people's locus of control. Through an empirical study\nwith 462 participants, we found that employment status plays a critical role in\nshaping users' reliance on AI and their locus of control. Current results\ndemonstrated that employed participants displayed higher reliance on AI and a\nshift toward internal control, while unemployed users tended to experience a\nreduction in personal agency. Through quantitative results and qualitative\nobservations, this study opens a broader conversation about AI's role in\nshaping personal agency and identity.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7462\u4eba\u5b9e\u9a8c\u53d1\u73b0\uff1a\u5c31\u4e1a\u72b6\u6001\u5f71\u54cd\u4eba\u7c7b\u5bf9AI\u5199\u4f5c\u7684\u4f9d\u8d56\u7a0b\u5ea6\u548c\u63a7\u5236\u611f\uff0c\u5728\u804c\u8005\u66f4\u4f9d\u8d56AI\u4f46\u5185\u63a7\u611f\u589e\u5f3a\uff0c\u5931\u4e1a\u8005\u4e2a\u4eba\u80fd\u52a8\u6027\u964d\u4f4e", "motivation": "\u63a2\u7d22AI\u534f\u540c\u5199\u4f5c\u5bf9\u4eba\u7c7b\u5fc3\u7406\u63a7\u5236\u6e90\u7684\u5f71\u54cd\uff0c\u63ed\u793aAI\u5de5\u5177\u5982\u4f55\u6f5c\u79fb\u9ed8\u5316\u91cd\u5851\u4eba\u7c7b\u7684\u81ea\u6211\u8ba4\u77e5\u4e0e\u4e2a\u4eba\u80fd\u52a8\u6027", "method": "\u91c7\u7528\u6df7\u5408\u7814\u7a76\u65b9\u6cd5\uff0c\u901a\u8fc7462\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u8bc1\u6570\u636e\uff0c\u7ed3\u5408\u5b9a\u91cf\u7edf\u8ba1\u4e0e\u5b9a\u6027\u89c2\u5bdf\u5206\u6790\u5c31\u4e1a\u72b6\u6001\u5bf9AI\u4f9d\u8d56\u7684\u5f71\u54cd\u673a\u5236", "result": "\u5728\u804c\u7528\u6237\u5bf9AI\u4f9d\u8d56\u5ea6\u9ad8\u4e14\u63a7\u5236\u611f\u5185\u5316\uff0c\u5931\u4e1a\u7fa4\u4f53\u51fa\u73b0\u4e2a\u4eba\u80fd\u52a8\u6027\u8870\u51cf\u73b0\u8c61\uff0c\u91cf\u5316\u6570\u636e\u4e0e\u8d28\u6027\u89c2\u5bdf\u7ed3\u679c\u4e00\u81f4", "conclusion": "AI\u5bf9\u4e2a\u4eba\u80fd\u52a8\u6027\u7684\u5f71\u54cd\u5b58\u5728\u7fa4\u4f53\u5dee\u5f02\uff0c\u5c31\u4e1a\u72b6\u6001\u662f\u5173\u952e\u8c03\u8282\u53d8\u91cf\uff0c\u8fd9\u4e3a\u601d\u8003AI\u6280\u672f\u4f26\u7406\u548c\u8eab\u4efd\u91cd\u5851\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2505.11409", "pdf": "https://arxiv.org/pdf/2505.11409", "abs": "https://arxiv.org/abs/2505.11409", "authors": ["Yi Xu", "Chengzu Li", "Han Zhou", "Xingchen Wan", "Caiqi Zhang", "Anna Korhonen", "Ivan Vuli\u0107"], "title": "Visual Planning: Let's Think Only with Images", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": "10 pages, 6 figures, 1 table (26 pages, 12 figures, 8 tables\n  including references and appendices)", "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have substantially enhanced machine reasoning across diverse\ntasks. However, these models predominantly rely on pure text as the medium for\nboth expressing and structuring reasoning, even when visual information is\npresent. In this work, we argue that language may not always be the most\nnatural or effective modality for reasoning, particularly in tasks involving\nspatial and geometrical information. Motivated by this, we propose a new\nparadigm, Visual Planning, which enables planning through purely visual\nrepresentations, independent of text. In this paradigm, planning is executed\nvia sequences of images that encode step-by-step inference in the visual\ndomain, akin to how humans sketch or visualize future actions. We introduce a\nnovel reinforcement learning framework, Visual Planning via Reinforcement\nLearning (VPRL), empowered by GRPO for post-training large vision models,\nleading to substantial improvements in planning in a selection of\nrepresentative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our\nvisual planning paradigm outperforms all other planning variants that conduct\nreasoning in the text-only space. Our results establish Visual Planning as a\nviable and promising alternative to language-based reasoning, opening new\navenues for tasks that benefit from intuitive, image-based inference.", "AI": {"tldr": "\u63d0\u51fa\u7eaf\u89c6\u89c9\u89c4\u5212\u8303\u5f0fVPRL\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728\u89c6\u89c9\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u6587\u672c\u63a8\u7406\u7684\u89c4\u5212\u6548\u679c", "motivation": "\u6587\u672c\u5a92\u4ecb\u5728\u7a7a\u95f4\u51e0\u4f55\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4eba\u7c7b\u66f4\u64c5\u957f\u901a\u8fc7\u89c6\u89c9\u8349\u56fe\u8fdb\u884c\u89c4\u5212\u63a8\u7406", "method": "\u57fa\u4e8eGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u50cf\u5e8f\u5217\u6267\u884c\u9010\u6b65\u89c6\u89c9\u63a8\u7406", "result": "\u5728FrozenLake\u3001Maze\u548cMiniBehavior\u5bfc\u822a\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u7eaf\u6587\u672c\u89c4\u5212\u65b9\u6cd5", "conclusion": "\u89c6\u89c9\u89c4\u5212\u4e3a\u7a7a\u95f4\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u8f9f\u57fa\u4e8e\u56fe\u50cf\u63a8\u7406\u7684\u65b0\u65b9\u5411"}}
