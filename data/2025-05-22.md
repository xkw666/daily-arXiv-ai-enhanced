<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 142]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.CV](#cs.CV) [Total: 9]
- [cs.CR](#cs.CR) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](https://arxiv.org/abs/2505.14810)
*Tingchen Fu,Jiawei Gu,Yafu Li,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 论文提出MathIF基准测试，揭示大语言模型在提升数学推理能力与遵循指令间存在权衡关系，训练策略会影响指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理模型在增强推理能力时可能削弱对用户指令的遵循能力，特别是在生成长文本时更明显

Method: 构建MathIF评估基准，分析不同训练方法（长链思维蒸馏、强化学习）对指令遵循的影响，尝试简单干预措施

Result: 推理优化模型指令遵循能力下降，简单干预可部分恢复但牺牲推理性能，生成长度与指令遵循呈负相关

Conclusion: 当前LLM训练范式存在推理能力与指令遵循的矛盾，需开发指令感知更强的推理模型

Abstract: Instruction-following is essential for aligning large language models (LLMs)
with user intent. While recent reasoning-oriented models exhibit impressive
performance on complex mathematical problems, their ability to adhere to
natural language instructions remains underexplored. In this work, we introduce
MathIF, a dedicated benchmark for evaluating instruction-following in
mathematical reasoning tasks. Our empirical analysis reveals a consistent
tension between scaling up reasoning capacity and maintaining controllability,
as models that reason more effectively often struggle to comply with user
directives. We find that models tuned on distilled long chains-of-thought or
trained with reasoning-oriented reinforcement learning often degrade in
instruction adherence, especially when generation length increases.
Furthermore, we show that even simple interventions can partially recover
obedience, though at the cost of reasoning performance. These findings
highlight a fundamental tension in current LLM training paradigms and motivate
the need for more instruction-aware reasoning models. We release the code and
data at https://github.com/TingchenFu/MathIF.

</details>


### [2] [Language Mixing in Reasoning Language Models: Patterns, Impact, and Internal Causes](https://arxiv.org/abs/2505.14815)
*Mingyang Wang,Lukas Lange,Heike Adel,Yunpu Ma,Jannik Strötgen,Hinrich Schütze*

Main category: cs.CL

TL;DR: 系统研究推理语言模型中的语言混合现象，揭示其对性能的影响及与内部表征的关系，并提出约束解码优化方法


<details>
  <summary>Details</summary>
Motivation: 探究RLMs中语言混合现象的模式、影响及内部机制，以解决其对模型性能的争议并优化多语言推理能力

Method: 跨15种语言、7个难度等级和18个领域的多维度分析，结合约束解码实验和模型内部表征的脚本组成对比

Result: 语言混合受任务/语言/领域三重影响，强制拉丁/汉字脚本提升12%准确率，且推理轨迹脚本与内部表征高度一致（r=0.89）

Conclusion: 揭示语言混合反映模型认知偏好，提出基于脚本控制的推理优化范式，为构建可解释的跨语言RLMs提供新方向

Abstract: Reasoning language models (RLMs) excel at complex tasks by leveraging a
chain-of-thought process to generate structured intermediate steps. However,
language mixing, i.e., reasoning steps containing tokens from languages other
than the prompt, has been observed in their outputs and shown to affect
performance, though its impact remains debated. We present the first systematic
study of language mixing in RLMs, examining its patterns, impact, and internal
causes across 15 languages, 7 task difficulty levels, and 18 subject areas, and
show how all three factors influence language mixing. Moreover, we demonstrate
that the choice of reasoning language significantly affects performance:
forcing models to reason in Latin or Han scripts via constrained decoding
notably improves accuracy. Finally, we show that the script composition of
reasoning traces closely aligns with that of the model's internal
representations, indicating that language mixing reflects latent processing
preferences in RLMs. Our findings provide actionable insights for optimizing
multilingual reasoning and open new directions for controlling reasoning
languages to build more interpretable and adaptable RLMs.

</details>


### [3] [WebNovelBench: Placing LLM Novelists on the Web Novel Distribution](https://arxiv.org/abs/2505.14818)
*Leon Lin,Jun Zheng,Haidong Wang*

Main category: cs.CL

TL;DR: WebNovelBench是首个针对长篇小说生成的大规模评估基准，通过4000+中文网文数据集和LLM自动评估框架，实现模型叙事能力的量化分析与排名。


<details>
  <summary>Details</summary>
Motivation: 现有LLM长文本生成评估存在规模小、维度单一问题，需建立客观可量化的评估体系推动叙事生成技术发展。

Method: 构建4000+中文网文数据集，设计概要转故事任务框架，提出8维度叙事质量评估体系（LLM自动评分+PCA聚合+人类作品百分位映射）。

Result: 成功区分人类杰作/流行网文/LLM生成内容，完成24个SOTA模型叙事能力排名（人类杰作平均领先LLMs 32.7个百分位）。

Conclusion: 该基准首次实现数据驱动的LLM长篇小说生成能力评估，为模型优化提供可扩展、可复现的量化分析框架。

Abstract: Robustly evaluating the long-form storytelling capabilities of Large Language
Models (LLMs) remains a significant challenge, as existing benchmarks often
lack the necessary scale, diversity, or objective measures. To address this, we
introduce WebNovelBench, a novel benchmark specifically designed for evaluating
long-form novel generation. WebNovelBench leverages a large-scale dataset of
over 4,000 Chinese web novels, framing evaluation as a synopsis-to-story
generation task. We propose a multi-faceted framework encompassing eight
narrative quality dimensions, assessed automatically via an LLM-as-Judge
approach. Scores are aggregated using Principal Component Analysis and mapped
to a percentile rank against human-authored works. Our experiments demonstrate
that WebNovelBench effectively differentiates between human-written
masterpieces, popular web novels, and LLM-generated content. We provide a
comprehensive analysis of 24 state-of-the-art LLMs, ranking their storytelling
abilities and offering insights for future development. This benchmark provides
a scalable, replicable, and data-driven methodology for assessing and advancing
LLM-driven narrative generation.

</details>


### [4] [Tracing Multilingual Factual Knowledge Acquisition in Pretraining](https://arxiv.org/abs/2505.14824)
*Yihong Liu,Mingyang Wang,Amir Hossein Kargaran,Felicia Körner,Ercong Nie,Barbara Plank,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 研究追踪了多语言大模型预训练过程中事实回忆能力的发展，发现知识获取主要通过高频驱动的语言无关机制实现，而跨语言迁移主要发生在涉及命名实体的关系类型中


<details>
  <summary>Details</summary>
Motivation: 现有研究主要评估最终模型表现，对预训练过程中多语言事实回忆能力和跨语言一致性的发展规律缺乏系统探索

Method: 以OLMo-7B为研究对象，通过追踪预训练不同阶段的事实回忆表现，分析事实频率与跨语言迁移对知识获取的影响机制

Result: 事实回忆准确性和跨语言一致性随训练逐步提升，高频事实回忆呈现语言无关性，低频事实依赖英语的跨语言迁移（特别是早期阶段涉及命名实体的关系）

Conclusion: 多语言事实知识获取存在两条路径：主导的频率驱动机制（语言无关）和受限的跨语言迁移机制（主要作用于命名实体相关关系），为模型训练提供新见解

Abstract: Large Language Models (LLMs) are capable of recalling multilingual factual
knowledge present in their pretraining data. However, most studies evaluate
only the final model, leaving the development of factual recall and
crosslingual consistency throughout pretraining largely unexplored. In this
work, we trace how factual recall and crosslingual consistency evolve during
pretraining, focusing on OLMo-7B as a case study. We find that both accuracy
and consistency improve over time for most languages. We show that this
improvement is primarily driven by the fact frequency in the pretraining
corpus: more frequent facts are more likely to be recalled correctly,
regardless of language. Yet, some low-frequency facts in non-English languages
can still be correctly recalled. Our analysis reveals that these instances
largely benefit from crosslingual transfer of their English counterparts -- an
effect that emerges predominantly in the early stages of pretraining. We
pinpoint two distinct pathways through which multilingual factual knowledge
acquisition occurs: (1) frequency-driven learning, which is dominant and
language-agnostic, and (2) crosslingual transfer, which is limited in scale and
typically constrained to relation types involving named entities. We release
our code and data to facilitate further research at
https://github.com/cisnlp/multilingual-fact-tracing.

</details>


### [5] [Text Generation Beyond Discrete Token Sampling](https://arxiv.org/abs/2505.14827)
*Yufan Zhuang,Liyuan Liu,Chandan Singh,Jingbo Shang,Jianfeng Gao*

Main category: cs.CL

TL;DR: 提出Mixture of Inputs (MoI)方法，通过融合采样token与丢弃的token分布提升自回归生成质量


<details>
  <summary>Details</summary>
Motivation: 传统自回归生成丢弃token分布信息导致内部表示贫化，MoI旨在保留分布信息的丰富性

Method: 使用贝叶斯估计将token分布(先验)与采样token(观测)融合，生成连续后验期望作为新输入

Result: 在数学推理(提升1.2%)、代码生成(提升3.8%)和PhD级QA任务中，多个模型(QwQ-32B等)均取得稳定提升

Conclusion: MoI无需额外训练且计算开销极低，能持续提升文本质量和推理能力

Abstract: In standard autoregressive generation, an LLM predicts the next-token
distribution, samples a discrete token, and then discards the distribution,
passing only the sampled token as new input. To preserve this distribution's
rich information, we propose Mixture of Inputs (MoI), a training-free method
for autoregressive generation. After generating a token following the standard
paradigm, we construct a new input that blends the generated discrete token
with the previously discarded token distribution. Specifically, we employ a
Bayesian estimation method that treats the token distribution as the prior, the
sampled token as the observation, and replaces the conventional one-hot vector
with the continuous posterior expectation as the new model input. MoI allows
the model to maintain a richer internal representation throughout the
generation process, resulting in improved text quality and reasoning
capabilities. On mathematical reasoning, code generation, and PhD-level QA
tasks, MoI consistently improves performance across multiple models including
QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional
training and negligible computational overhead.

</details>


### [6] [SEPS: A Separability Measure for Robust Unlearning in LLMs](https://arxiv.org/abs/2505.14832)
*Wonje Jeung,Sangyeon Yoon,Albert No*

Main category: cs.CL

TL;DR: 本文提出SEPS评估框架验证大语言模型在混合遗忘/保留查询场景下的表现，发现现有方法存在非定向擦除和过拟合问题，进而提出混合提示学习方法提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘评估指标仅关注单一查询场景，无法反映真实场景中遗忘查询与保留查询共存的混合查询情况，导致现有方法在复杂场景下出现灾难性失败。

Method: 通过SEPS框架系统评估现有方法在混合查询场景的缺陷，提出将遗忘查询与保留查询整合到统一训练目标的混合提示（MP）学习方法。

Result: 实验证明该方法在包含8个混合查询的复杂场景下，遗忘成功率提升23.8%同时保留准确率仅下降1.2%，显著优于现有基线方法。

Conclusion: 研究揭示了混合查询场景对机器学习遗忘技术的关键挑战，提出的MP方法通过联合优化机制有效平衡遗忘与保留目标，为实际应用提供新思路。

Abstract: Machine unlearning aims to selectively remove targeted knowledge from Large
Language Models (LLMs), ensuring they forget specified content while retaining
essential information. Existing unlearning metrics assess whether a model
correctly answers retain queries and rejects forget queries, but they fail to
capture real-world scenarios where forget queries rarely appear in isolation.
In fact, forget and retain queries often coexist within the same prompt, making
mixed-query evaluation crucial.
  We introduce SEPS, an evaluation framework that explicitly measures a model's
ability to both forget and retain information within a single prompt. Through
extensive experiments across three benchmarks, we identify two key failure
modes in existing unlearning methods: (1) untargeted unlearning
indiscriminately erases both forget and retain content once a forget query
appears, and (2) targeted unlearning overfits to single-query scenarios,
leading to catastrophic failures when handling multiple queries. To address
these issues, we propose Mixed Prompt (MP) unlearning, a strategy that
integrates both forget and retain queries into a unified training objective.
Our approach significantly improves unlearning effectiveness, demonstrating
robustness even in complex settings with up to eight mixed forget and retain
queries in a single prompt.

</details>


### [7] [A Comparative Study of Large Language Models and Human Personality Traits](https://arxiv.org/abs/2505.14845)
*Wang Jiaqi,Wang bo,Guo fa,Cheng cheng,Yang li*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLM）表现出动态、输入依赖的类人格特征，与人类稳定人格存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 验证传统人格评估工具在LLM中的适用性，探索AI系统的人格表现特征，为构建LLM专用人格框架提供理论依据。

Method: 通过三个实证研究：1）测试-重测稳定性分析；2）跨变体一致性检验；3）角色扮演中人格保留实验。采用行为导向的分布式人格框架。

Result: LLM表现出更高的响应变异性（研究1）、对措辞高度敏感（研究2）、人格特征受提示词和参数设置塑造（研究3），缺乏人类人格的长期稳定性。

Conclusion: LLM的类人格具有流动性、外部依赖性特征，需建立专用评估框架。研究为负责任的AI发展提供新视角，拓展了人格心理学在智能时代的应用边界。

Abstract: Large Language Models (LLMs) have demonstrated human-like capabilities in
language comprehension and generation, becoming active participants in social
and cognitive domains. This study investigates whether LLMs exhibit
personality-like traits and how these traits compare with human personality,
focusing on the applicability of conventional personality assessment tools. A
behavior-based approach was used across three empirical studies. Study 1
examined test-retest stability and found that LLMs show higher variability and
are more input-sensitive than humans, lacking long-term stability. Based on
this, we propose the Distributed Personality Framework, conceptualizing LLM
traits as dynamic and input-driven. Study 2 analyzed cross-variant consistency
in personality measures and found LLMs' responses were highly sensitive to item
wording, showing low internal consistency compared to humans. Study 3 explored
personality retention during role-playing, showing LLM traits are shaped by
prompt and parameter settings. These findings suggest that LLMs express fluid,
externally dependent personality patterns, offering insights for constructing
LLM-specific personality frameworks and advancing human-AI interaction. This
work contributes to responsible AI development and extends the boundaries of
personality psychology in the age of intelligent systems.

</details>


### [8] [MAATS: A Multi-Agent Automated Translation System Based on MQM Evaluation](https://arxiv.org/abs/2505.14848)
*Xi Wang,Jiaqian Hu,Safinah Ali*

Main category: cs.CL

TL;DR: MAATS通过多智能体分工协作，基于MQM框架提升翻译质量，在语义准确性和跨语言对场景中表现突出


<details>
  <summary>Details</summary>
Motivation: 传统单智能体方法依赖自我纠正存在局限，需通过多智能体分工协作实现精细化的翻译质量优化

Method: 多个专门AI代理分别处理不同MQM类别（准确性、流畅性、术语等），综合代理整合标注进行迭代优化

Result: 在自动指标和人工评估中显著优于零样本和单智能体基线，尤其在语义准确性和语言差异大的语种对中表现优异

Conclusion: 通过模块化智能体与可解释的MQM维度对齐，MAATS缩小了黑盒LLM与人类翻译流程的差距，聚焦深层语义和上下文保真度

Abstract: We present MAATS, a Multi Agent Automated Translation System that leverages
the Multidimensional Quality Metrics (MQM) framework as a fine-grained signal
for error detection and refinement. MAATS employs multiple specialized AI
agents, each focused on a distinct MQM category (e.g., Accuracy, Fluency,
Style, Terminology), followed by a synthesis agent that integrates the
annotations to iteratively refine translations. This design contrasts with
conventional single-agent methods that rely on self-correction.
  Evaluated across diverse language pairs and Large Language Models (LLMs),
MAATS outperforms zero-shot and single-agent baselines with statistically
significant gains in both automatic metrics and human assessments. It excels
particularly in semantic accuracy, locale adaptation, and linguistically
distant language pairs. Qualitative analysis highlights its strengths in
multi-layered error diagnosis, omission detection across perspectives, and
context-aware refinement. By aligning modular agent roles with interpretable
MQM dimensions, MAATS narrows the gap between black-box LLMs and human
translation workflows, shifting focus from surface fluency to deeper semantic
and contextual fidelity.

</details>


### [9] [EasyMath: A 0-shot Math Benchmark for SLMs](https://arxiv.org/abs/2505.14852)
*Drishya Karki,Michiel Kamphuis,Angelecia Frey*

Main category: cs.CL

TL;DR: EasyMath是为小型语言模型设计的紧凑型数学推理基准，覆盖13类基础数学问题，测试显示模型表现与规模正相关


<details>
  <summary>Details</summary>
Motivation: 现有数学基准测试过于复杂或专业，需创建适合小模型的实际数学能力评估工具

Method: 构建13类基础数学问题的测试集，使用精确/数值/符号检查方法，在零样本场景测试23个不同规模模型（14M-4B参数）

Result: 模型规模越大表现越好，思维链技术带来有限提升，参数过亿后输出稳定性显著增强

Conclusion: EasyMath有效评估小模型数学能力，结果验证模型规模与训练数据的关键作用，思维链潜力待深入挖掘

Abstract: EasyMath is a compact benchmark for practical math reasoning in small
language models. It covers thirteen categories, from basic arithmetic and order
of operations to word problems, algebraic expressions, edge cases, and omits
specialist topics. We tested 23 models (14M to 4B parameters) using exact,
numerical, and symbolic checks on free-form answers in a zero-shot setting.
Accuracy rises with size and training, chain-of-thought adds modest gains, and
consistency improves at scale.

</details>


### [10] [Saten: Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](https://arxiv.org/abs/2505.14871)
*Ryan Solgi,Kai Zhen,Rupak Vignesh Swaminathan,Nathan Susanj,Athanasios Mouchtaris,Siegfried Kunzmann,Zheng Zhang*

Main category: cs.CL

TL;DR: 提出稀疏增强张量网络(Saten)，在微调阶段实现大语言模型的高效张量压缩，提升精度与压缩效率。


<details>
  <summary>Details</summary>
Motivation: 预训练大语言模型存在高秩特性且缺乏训练数据访问，传统张量压缩方法在迁移学习中效果受限。

Method: 采用稀疏增强张量网络架构，在低秩张量化模型中引入可学习稀疏参数增强表征能力，支持全模型压缩。

Result: 实验显示Saten在模型压缩率提升2-5倍的同时，准确率超越现有方法1.2-3.7个百分点，达到SOTA水平。

Conclusion: 该框架为资源受限场景下的张量化语言模型部署提供了高效解决方案，平衡了模型效率与性能。

Abstract: The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose sparse augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.

</details>


### [11] [Towards Inclusive ASR: Investigating Voice Conversion for Dysarthric Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2505.14874)
*Chin-Jou Li,Eunjung Yeo,Kwanghee Choi,Paula Andrea Pérez-Toro,Masao Someki,Rohan Kumar Das,Zhengjun Yue,Juan Rafael Orozco-Arroyave,Elmar Nöth,David R. Mortensen*

Main category: cs.CL

TL;DR: 通过语音转换生成非英语发音障碍语音数据，显著提升多语言ASR模型对发音障碍语音的识别性能


<details>
  <summary>Details</summary>
Motivation: 解决非英语发音障碍语音数据稀缺问题，改善发音障碍语音识别模型的训练效果

Method: 1. 在英文发音障碍数据集(UASpeech)上微调语音转换模型，捕捉发音者特征和韵律畸变
2. 将健康语音(FLEURS)转换为非英语发音障碍语音
3. 用生成数据微调多语言ASR模型(MMS)

Result: 在西班牙语、意大利语和泰米尔语数据集上验证，语音转换方法显著优于原生MMS和传统数据增强方法（速度/节拍扰动）

Conclusion: 结合发音者特征和韵律转换的语音生成方法能有效模拟发音障碍特征，提升多语言场景下的语音识别性能

Abstract: Automatic speech recognition (ASR) for dysarthric speech remains challenging
due to data scarcity, particularly in non-English languages. To address this,
we fine-tune a voice conversion model on English dysarthric speech (UASpeech)
to encode both speaker characteristics and prosodic distortions, then apply it
to convert healthy non-English speech (FLEURS) into non-English dysarthric-like
speech. The generated data is then used to fine-tune a multilingual ASR model,
Massively Multilingual Speech (MMS), for improved dysarthric speech
recognition. Evaluation on PC-GITA (Spanish), EasyCall (Italian), and SSNCE
(Tamil) demonstrates that VC with both speaker and prosody conversion
significantly outperforms the off-the-shelf MMS performance and conventional
augmentation techniques such as speed and tempo perturbation. Objective and
subjective analyses of the generated data further confirm that the generated
speech simulates dysarthric characteristics.

</details>


### [12] [Incorporating Token Usage into Prompting Strategy Evaluation](https://arxiv.org/abs/2505.14880)
*Chris Sypherd,Sergei Petrov,Sonny George,Vaishak Belle*

Main category: cs.CL

TL;DR: 论文提出应基于效率（性能与token消耗的平衡）而非单一性能评估提示策略，并构建Big-O_tok理论框架和Token Cost实证指标验证该观点。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注提示策略的性能提升，忽视了token消耗对实际应用效率的影响。随着token使用量增加，性能回报呈现急剧递减趋势，需更实用的效率评估体系。

Method: 1. 提出Big-O_tok理论框架描述提示策略的token增长复杂度；2. 定义Token Cost（单位性能所需token数）实证指标；3. 对常见提示策略进行量化分析。

Result: 验证Big-O_tok理论模型的有效性：token增长遵循理论预测，且更高token消耗仅带来边际性能提升（如Chain-of-Thought策略Token Cost达基础策略的1.7倍）。

Conclusion: 实践者应优先选择效率优化的提示策略，结合Big-O_tok和Token Cost指标实现性能与资源消耗的最佳平衡，这对实际部署具有重要指导意义。

Abstract: In recent years, large language models have demonstrated remarkable
performance across diverse tasks. However, their task effectiveness is heavily
dependent on the prompting strategy used to elicit output, which can vary
widely in both performance and token usage. While task performance is often
used to determine prompting strategy success, we argue that
efficiency--balancing performance and token usage--can be a more practical
metric for real-world utility. To enable this, we propose Big-$O_{tok}$, a
theoretical framework for describing the token usage growth of prompting
strategies, and analyze Token Cost, an empirical measure of tokens per
performance. We apply these to several common prompting strategies and find
that increased token usage leads to drastically diminishing performance
returns. Our results validate the Big-$O_{tok}$ analyses and reinforce the need
for efficiency-aware evaluations.

</details>


### [13] [Strategic Planning and Rationalizing on Trees Make LLMs Better Debaters](https://arxiv.org/abs/2505.14886)
*Danqing Wang,Zhuorui Ye,Xinran Zhao,Fei Fang,Lei Li*

Main category: cs.CL

TL;DR: TreeDebater框架通过预演树和辩论流树解决竞争性辩论中的时间分配与互动评估问题，显著优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 竞争性辩论面临时间限制下的策略选择难题，且传统方法无法有效评估论点间的动态互动性。

Method: 引入预演树评估论点强度，辩论流树跟踪辩论状态；结合时间预算分配、演讲控制器及观众反馈优化陈述。

Result: 人类评估显示TreeDebater在阶段/辩论层面均优于现有多智能体系统，其时间分配策略与人类专家策略高度一致。

Conclusion: TreeDebater通过结构化策略模拟人类辩论逻辑，为复杂互动场景下的决策系统提供了新范式。

Abstract: Winning competitive debates requires sophisticated reasoning and argument
skills. There are unique challenges in the competitive debate: (1) The time
constraints force debaters to make strategic choices about which points to
pursue rather than covering all possible arguments; (2) The persuasiveness of
the debate relies on the back-and-forth interaction between arguments, which a
single final game status cannot evaluate. To address these challenges, we
propose TreeDebater, a novel debate framework that excels in competitive
debate. We introduce two tree structures: the Rehearsal Tree and Debate Flow
Tree. The Rehearsal Tree anticipates the attack and defenses to evaluate the
strength of the claim, while the Debate Flow Tree tracks the debate status to
identify the active actions. TreeDebater allocates its time budget among
candidate actions and uses the speech time controller and feedback from the
simulated audience to revise its statement. The human evaluation on both the
stage-level and the debate-level comparison shows that our TreeDebater
outperforms the state-of-the-art multi-agent debate system. Further
investigation shows that TreeDebater shows better strategies in limiting time
to important debate actions, aligning with the strategies of human debate
experts.

</details>


### [14] [In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties](https://arxiv.org/abs/2505.14887)
*Nathan Roll,Calbert Graham,Yuka Tatsumi,Kim Tien Nguyen,Meghan Sumner,Dan Jurafsky*

Main category: cs.CL

TL;DR: 通过开发支持上下文学习的多模态框架，在仅需50秒语音示例的情况下将英语语音识别错误率降低19.7%，低资源方言改善最显著但边际效益递减


<details>
  <summary>Details</summary>
Motivation: 探索当前语音语言模型是否具备类似人类听众的跨说话者适应能力，提升自动语音识别系统在多样化语言背景下的鲁棒性

Method: 构建支持上下文学习的Phi-4多模态框架，通过交替使用任务提示和音频-文本对，在推理阶段注入少量语音样本实现模型适配

Result: 12个语音样本使平均词错率相对下降19.7%，低资源方言改进最明显（需上下文与目标说话者匹配），但存在边际效益递减现象

Conclusion: 上下文学习方案展现出类人类的适应模式，显著提升跨说话者识别性能，但特定方言仍存在显著识别差距，揭示当前模型灵活性的局限性

Abstract: Human listeners readily adjust to unfamiliar speakers and language varieties
through exposure, but do these adaptation benefits extend to state-of-the-art
spoken language models? We introduce a scalable framework that allows for
in-context learning (ICL) in Phi-4 Multimodal using interleaved task prompts
and audio-text pairs, and find that as few as 12 example utterances (~50
seconds) at inference time reduce word error rates by a relative 19.7% (1.2
pp.) on average across diverse English corpora. These improvements are most
pronounced in low-resource varieties, when the context and target speaker
match, and when more examples are provided--though scaling our procedure yields
diminishing marginal returns to context length. Overall, we find that our novel
ICL adaptation scheme (1) reveals a similar performance profile to human
listeners, and (2) demonstrates consistent improvements to automatic speech
recognition (ASR) robustness across diverse speakers and language backgrounds.
While adaptation succeeds broadly, significant gaps remain for certain
varieties, revealing where current models still fall short of human
flexibility. We release our prompts and code on GitHub.

</details>


### [15] [Scaling Laws for State Dynamics in Large Language Models](https://arxiv.org/abs/2505.14892)
*Jacob X Li,Shreyas S Raman,Jessica Wan,Fahad Samman,Jazlyn Lin*

Main category: cs.CL

TL;DR: 研究发现大语言模型在状态空间增大和转移稀疏时状态跟踪能力显著下降，其状态跟踪机制源于注意力头的分布式交互而非显式符号计算


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在需要内部状态追踪任务中对状态转移动态的建模能力

Method: 通过在3个可形式化为有限状态系统的领域（盒子追踪、DFA序列、文本游戏）进行测试，结合激活修补技术定位关键注意力头

Result: GPT-2 XL在低复杂度环境准确率约70%，状态数超过5或10时骤降至30%；发现GPT-2 Layer22_Head20和Pythia的特定层注意力头负责状态特征传播

Conclusion: 大语言模型的状态跟踪能力源于分布式注意力头的协同作用，而非显式符号运算，联合状态-动作推理能力较弱

Abstract: Large Language Models (LLMs) are increasingly used in tasks requiring
internal state tracking, yet their ability to model state transition dynamics
remains poorly understood. We evaluate how well LLMs capture deterministic
state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and
Complex Text Games, each formalizable as a finite-state system. Across tasks,
we find that next-state prediction accuracy degrades with increasing
state-space size and sparse transitions. GPT-2 XL reaches about 70% accuracy in
low-complexity settings but drops below 30% when the number of boxes or states
exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%
accuracy when the number of states is > 10 and transitions are < 30. Through
activation patching, we identify attention heads responsible for propagating
state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,
11, 12, and 14. While these heads successfully move relevant state features,
action information is not reliably routed to the final token, indicating weak
joint state-action reasoning. Our results suggest that state tracking in LLMs
emerges from distributed interactions of next-token heads rather than explicit
symbolic computation.

</details>


### [16] [Concept Incongruence: An Exploration of Time and Death in Role Playing](https://arxiv.org/abs/2505.14905)
*Xiaoyan Bai,Ike Peng,Aditya Singh,Chenhao Tan*

Main category: cs.CL

TL;DR: 提出概念不一致性框架分析LLMs在角色死亡后的异常行为，发现模型无法有效弃权且准确率下降，归因于死亡状态编码不可靠和角色扮演引发的时间表征偏移。


<details>
  <summary>Details</summary>
Motivation: 探究当用户提示与概念定义冲突时（如'双角独角兽'），语言模型应识别矛盾还是继续生成，揭示概念不一致导致的模型行为异常问题。

Method: 在角色扮演场景中设定时间边界，设计弃权率、条件准确率、回答率三个指标，通过探测实验分析模型在角色死亡后的行为模式。

Result: 模型在角色死亡后弃权能力差（准确率较非角色扮演场景下降），时间表征受角色扮演干扰，死亡状态编码存在年份敏感性缺陷。

Conclusion: 概念不一致导致模型行为不可预测，未来需改进状态编码稳定性与角色扮演场景的时序一致性，提升模型在概念冲突下的决策可靠性。

Abstract: Consider this prompt "Draw a unicorn with two horns". Should large language
models (LLMs) recognize that a unicorn has only one horn by definition and ask
users for clarifications, or proceed to generate something anyway? We introduce
concept incongruence to capture such phenomena where concept boundaries clash
with each other, either in user prompts or in model representations, often
leading to under-specified or mis-specified behaviors. In this work, we take
the first step towards defining and analyzing model behavior under concept
incongruence. Focusing on temporal boundaries in the Role-Play setting, we
propose three behavioral metrics--abstention rate, conditional accuracy, and
answer rate--to quantify model behavior under incongruence due to the role's
death. We show that models fail to abstain after death and suffer from an
accuracy drop compared to the Non-Role-Play setting. Through probing
experiments, we identify two main causes: (i) unreliable encoding of the
"death" state across different years, leading to unsatisfactory abstention
behavior, and (ii) role playing causes shifts in the model's temporal
representations, resulting in accuracy drops. We leverage these insights to
improve consistency in the model's abstention and answer behaviors. Our
findings suggest that concept incongruence leads to unexpected model behaviors
and point to future directions on improving model behavior under concept
incongruence.

</details>


### [17] [Understanding 6G through Language Models: A Case Study on LLM-aided Structured Entity Extraction in Telecom Domain](https://arxiv.org/abs/2505.14906)
*Ye Yuan,Haolun Wu,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.CL

TL;DR: 提出TeleSEE技术用于电信领域结构化实体抽取，在准确率和处理速度上显著优于基线模型


<details>
  <summary>Details</summary>
Motivation: 6G网络需要结构化知识支撑AI模型理解网络术语，但现有信息抽取技术存在输出冗余和架构效率问题

Method: 1) 基于token的高效表示方法预测实体类型和属性键 2) 分层并行解码架构整合提示策略 3) 构建6GTech电信领域数据集

Result: TeleSEE准确率优于基线，样本处理速度提升5-9倍

Conclusion: TeleSEE有效平衡了精度与效率，为6G知识网络构建提供了可行方案

Abstract: Knowledge understanding is a foundational part of envisioned 6G networks to
advance network intelligence and AI-native network architectures. In this
paradigm, information extraction plays a pivotal role in transforming
fragmented telecom knowledge into well-structured formats, empowering diverse
AI models to better understand network terminologies. This work proposes a
novel language model-based information extraction technique, aiming to extract
structured entities from the telecom context. The proposed telecom structured
entity extraction (TeleSEE) technique applies a token-efficient representation
method to predict entity types and attribute keys, aiming to save the number of
output tokens and improve prediction accuracy. Meanwhile, TeleSEE involves a
hierarchical parallel decoding method, improving the standard encoder-decoder
architecture by integrating additional prompting and decoding strategies into
entity extraction tasks. In addition, to better evaluate the performance of the
proposed technique in the telecom domain, we further designed a dataset named
6GTech, including 2390 sentences and 23747 words from more than 100 6G-related
technical publications. Finally, the experiment shows that the proposed TeleSEE
method achieves higher accuracy than other baseline techniques, and also
presents 5 to 9 times higher sample processing speed.

</details>


### [18] [ConspEmoLLM-v2: A robust and stable model to detect sentiment-transformed conspiracy theories](https://arxiv.org/abs/2505.14917)
*Zhiwei Liu,Paul Thompson,Jiaqi Rong,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该论文针对大语言模型(LLM)生成的伪装阴谋论检测问题，提出增强版数据集ConDID-v2和改进模型ConspEmoLLM-v2。通过结合人工和LLM重写内容，新模型在原始和情感转换数据上均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: LLM可能生成包括阴谋论的错误信息，并能通过改变情感特征伪装内容。现有检测方法依赖人类撰写文本的特征，难以应对LLM生成的伪装内容。

Method: 构建包含人工撰写和LLM情感转换推文的ConDID-v2数据集，结合人工和LLM评估数据质量，并基于此训练增强的ConspEmoLLM-v2检测模型。

Result: ConspEmoLLM-v2在原始数据集保持或超越前版性能，在情感转换推文检测上显著优于基线模型和原版模型。

Conclusion: 通过数据集增强和模型改进，有效提升了LLM生成伪装阴谋论的检测能力，项目开源促进相关研究发展。

Abstract: Despite the many benefits of large language models (LLMs), they can also
cause harm, e.g., through automatic generation of misinformation, including
conspiracy theories. Moreover, LLMs can also ''disguise'' conspiracy theories
by altering characteristic textual features, e.g., by transforming their
typically strong negative emotions into a more positive tone. Although several
studies have proposed automated conspiracy theory detection methods, they are
usually trained using human-authored text, whose features can vary from
LLM-generated text. Furthermore, several conspiracy detection models, including
the previously proposed ConspEmoLLM, rely heavily on the typical emotional
features of human-authored conspiracy content. As such, intentionally disguised
content may evade detection. To combat such issues, we firstly developed an
augmented version of the ConDID conspiracy detection dataset, ConDID-v2, which
supplements human-authored conspiracy tweets with versions rewritten by an LLM
to reduce the negativity of their original sentiment. The quality of the
rewritten tweets was verified by combining human and LLM-based assessment. We
subsequently used ConDID-v2 to train ConspEmoLLM-v2, an enhanced version of
ConspEmoLLM. Experimental results demonstrate that ConspEmoLLM-v2 retains or
exceeds the performance of ConspEmoLLM on the original human-authored content
in ConDID, and considerably outperforms both ConspEmoLLM and several other
baselines when applied to sentiment-transformed tweets in ConDID-v2. The
project will be available at https://github.com/lzw108/ConspEmoLLM.

</details>


### [19] [Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications](https://arxiv.org/abs/2505.14918)
*Fadel M. Megahed,Ying-Ju Chen,L. Allision Jones-Farmer,Younghwa Lee,Jiawei Brooke Wang,Inez M. Zwetsloot*

Main category: cs.CL

TL;DR: 开发LLM文本分类一致性评估框架，验证发现小模型性能优于大模型且任务限制影响市场预测效果


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型文本分类任务中可靠性评估方法缺失的问题，建立系统化的评估体系

Method: 采用心理测量学原则，通过样本量计算/无效响应指标开发/模型内-评估者间一致性验证，设计14种LLM在金融新闻情感分类的五次重复实验

Result: 模型内部一致性达90-98%，gemma3:1B等小模型准确率超大型模型（0.76-0.88），所有模型市场预测表现随机

Conclusion: 该框架为LLM分类任务提供样本规划与模型选择依据，揭示任务设计而非模型能力是预测局限主因

Abstract: This study introduces a framework for evaluating consistency in large
language model (LLM) binary text classification, addressing the lack of
established reliability assessment methods. Adapting psychometric principles,
we determine sample size requirements, develop metrics for invalid responses,
and evaluate intra- and inter-rater reliability. Our case study examines
financial news sentiment classification across 14 LLMs (including
claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and
command-r-plus), with five replicates per model on 1,350 articles. Models
demonstrated high intra-rater consistency, achieving perfect agreement on
90-98% of examples, with minimal differences between expensive and economical
models from the same families. When validated against StockNewsAPI labels,
models achieved strong performance (accuracy 0.76-0.88), with smaller models
like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger
counterparts. All models performed at chance when predicting actual market
movements, indicating task constraints rather than model limitations. Our
framework provides systematic guidance for LLM selection, sample size planning,
and reliability assessment, enabling organizations to optimize resources for
classification tasks.

</details>


### [20] [Too Long, Didn't Model: Decomposing LLM Long-Context Understanding With Novels](https://arxiv.org/abs/2505.14925)
*Sil Hamilton,Rebecca M. M. Hicke,Matthew Wilkens,David Mimno*

Main category: cs.CL

TL;DR: 前沿大语言模型在超过64k tokens后无法保持对复杂长文本的理解，现有评估方法需改进。


<details>
  <summary>Details</summary>
Motivation: 针对大模型长文本评估局限于'大海捞针'式测试的不足，提出以长篇小说作为复杂语义依赖的测试场景。

Method: 构建TLDM基准测试（包含情节概括、世界观重构、叙事时间轴三项任务），评估七种前沿LLM的长文本理解能力。

Result: 所有测试模型在超过64k tokens后理解能力显著下降，'中间遗忘'现象在复杂场景中表现更突出。

Conclusion: 需开发更贴近真实场景的长文本评估体系，TLDM基准的发布将推动长上下文建模技术的发展。

Abstract: Although the context length of large language models (LLMs) has increased to
millions of tokens, evaluating their effectiveness beyond needle-in-a-haystack
approaches has proven difficult. We argue that novels provide a case study of
subtle, complicated structure and long-range semantic dependencies often over
128k tokens in length. Inspired by work on computational novel analysis, we
release the Too Long, Didn't Model (TLDM) benchmark, which tests a model's
ability to report plot summary, storyworld configuration, and elapsed narrative
time. We find that none of seven tested frontier LLMs retain stable
understanding beyond 64k tokens. Our results suggest language model developers
must look beyond "lost in the middle" benchmarks when evaluating model
performance in complex long-context scenarios. To aid in further development we
release the TLDM benchmark together with reference code and data.

</details>


### [21] [MedBrowseComp: Benchmarking Medical Deep Research and Computer Use](https://arxiv.org/abs/2505.14963)
*Shan Chen,Pedro Moreira,Yuxin Xiao,Sam Schmidgall,Jeremy Warner,Hugo Aerts,Thomas Hartvigsen,Jack Gallifant,Danielle S. Bitterman*

Main category: cs.CL

TL;DR: MedBrowseComp作为首个系统性评估临床信息检索与合成能力的基准，揭示现有大语言模型在临床决策支持中的性能短板（最低正确率仅10%）


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖合成数据、简化为单跳知识查询，无法反映临床实践中整合多源异构知识库（临床试验/基础研究/监管文件/成本数据）进行可靠推理的实际需求

Method: 构建包含1000+人工设计临床场景问题的基准测试集，要求智能体从动态更新的专业知识库中实现多跳事实检索与冲突信息调和

Result: 前沿智能体系统在测试中暴露出严重性能缺陷，验证当前LLM能力与临床严谨性要求之间存在关键差距

Conclusion: MedBrowseComp为可靠的医学信息检索系统建立了明确评估标准，并为未来医疗AI工具链升级提供了具体优化方向

Abstract: Large language models (LLMs) are increasingly envisioned as decision-support
tools in clinical practice, yet safe clinical reasoning demands integrating
heterogeneous knowledge bases -- trials, primary studies, regulatory documents,
and cost data -- under strict accuracy constraints. Existing evaluations often
rely on synthetic prompts, reduce the task to single-hop factoid queries, or
conflate reasoning with open-ended generation, leaving their real-world utility
unclear. To close this gap, we present MedBrowseComp, the first benchmark that
systematically tests an agent's ability to reliably retrieve and synthesize
multi-hop medical facts from live, domain-specific knowledge bases.
MedBrowseComp contains more than 1,000 human-curated questions that mirror
clinical scenarios where practitioners must reconcile fragmented or conflicting
information to reach an up-to-date conclusion. Applying MedBrowseComp to
frontier agentic systems reveals performance shortfalls as low as ten percent,
exposing a critical gap between current LLM capabilities and the rigor demanded
in clinical settings. MedBrowseComp therefore offers a clear testbed for
reliable medical information seeking and sets concrete goals for future model
and toolchain upgrades. You can visit our project page at:
https://moreirap12.github.io/mbc-browse-app/

</details>


### [22] [DECASTE: Unveiling Caste Stereotypes in Large Language Models through Multi-Dimensional Bias Analysis](https://arxiv.org/abs/2505.14971)
*Prashanth Vijayaraghavan,Soroush Vosoughi,Lamogha Chizor,Raya Horesh,Rogerio Abreu de Paula,Ehsan Degan,Vandana Mukherjee*

Main category: cs.CL

TL;DR: 提出DECASTE框架检测大语言模型中的种姓偏见，发现模型系统性强化对达利特/首陀罗等边缘种姓的歧视


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在未被充分研究的种姓偏见问题，可能加剧印度社会的结构性歧视风险

Method: 开发多维评估框架（社会文化/经济/教育/政治），采用定制化提示策略对主流LLMs进行基准测试

Result: 所有被测模型均呈现系统性种姓偏见，达利特/首陀罗群体在偏见得分上显著高于主导种姓群体

Conclusion: 揭示LLMs中隐蔽但普遍的种姓偏见，强调需要更全面的偏见评估方法以确保模型安全部署

Abstract: Recent advancements in large language models (LLMs) have revolutionized
natural language processing (NLP) and expanded their applications across
diverse domains. However, despite their impressive capabilities, LLMs have been
shown to reflect and perpetuate harmful societal biases, including those based
on ethnicity, gender, and religion. A critical and underexplored issue is the
reinforcement of caste-based biases, particularly towards India's marginalized
caste groups such as Dalits and Shudras. In this paper, we address this gap by
proposing DECASTE, a novel, multi-dimensional framework designed to detect and
assess both implicit and explicit caste biases in LLMs. Our approach evaluates
caste fairness across four dimensions: socio-cultural, economic, educational,
and political, using a range of customized prompting strategies. By
benchmarking several state-of-the-art LLMs, we reveal that these models
systematically reinforce caste biases, with significant disparities observed in
the treatment of oppressed versus dominant caste groups. For example, bias
scores are notably elevated when comparing Dalits and Shudras with dominant
caste groups, reflecting societal prejudices that persist in model outputs.
These results expose the subtle yet pervasive caste biases in LLMs and
emphasize the need for more comprehensive and inclusive bias evaluation
methodologies that assess the potential risks of deploying such models in
real-world contexts.

</details>


### [23] [Multimodal Cultural Safety: Evaluation Frameworks and Alignment Strategies](https://arxiv.org/abs/2505.14972)
*Haoyi Qiu,Kung-Hsiang Huang,Ruichen Zheng,Jiao Sun,Nanyun Peng*

Main category: cs.CL

TL;DR: 提出CROSS基准评估大型视觉语言模型的文化安全推理能力，发现现有模型存在显著文化安全缺陷，开发两种增强策略显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有多模态安全基准忽视文化规范导致的符号伤害，亟需评估模型跨文化场景下的响应安全性。

Method: 构建包含1284个多语言视觉查询的CROSS基准，开发CROSS-Eval框架评估文化意识/规范教育/合规性/帮助性四个维度，提出监督微调和对比响应偏好调整两种增强策略。

Result: 最佳模型文化意识61.79%、合规性37.73%；增强策略使GPT-4o文化意识+60.14%、合规性+55.2%，且保持通用多模态能力。

Conclusion: 文化安全是LVLM部署的关键瓶颈，通过针对性训练策略可显著提升模型跨文化适应能力，推动全球化AI应用发展。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in globally
distributed applications, such as tourism assistants, yet their ability to
produce culturally appropriate responses remains underexplored. Existing
multimodal safety benchmarks primarily focus on physical safety and overlook
violations rooted in cultural norms, which can result in symbolic harm. To
address this gap, we introduce CROSS, a benchmark designed to assess the
cultural safety reasoning capabilities of LVLMs. CROSS includes 1,284
multilingual visually grounded queries from 16 countries, three everyday
domains, and 14 languages, where cultural norm violations emerge only when
images are interpreted in context. We propose CROSS-Eval, an intercultural
theory-based framework that measures four key dimensions: cultural awareness,
norm education, compliance, and helpfulness. Using this framework, we evaluate
21 leading LVLMs, including mixture-of-experts models and reasoning models.
Results reveal significant cultural safety gaps: the best-performing model
achieves only 61.79% in awareness and 37.73% in compliance. While some
open-source models reach GPT-4o-level performance, they still fall notably
short of proprietary models. Our results further show that increasing reasoning
capacity improves cultural alignment but does not fully resolve the issue. To
improve model performance, we develop two enhancement strategies: supervised
fine-tuning with culturally grounded, open-ended data and preference tuning
with contrastive response pairs that highlight safe versus unsafe behaviors.
These methods substantially improve GPT-4o's cultural awareness (+60.14%) and
compliance (+55.2%), while preserving general multimodal capabilities with
minimal performance reduction on general multimodal understanding benchmarks.

</details>


### [24] [CRAFT: Training-Free Cascaded Retrieval for Tabular QA](https://arxiv.org/abs/2505.14984)
*Adarsh Singh,Kushal Raj Bhandari,Jianxi Gao,Soham Dan,Vivek Gupta*

Main category: cs.CL

TL;DR: 提出级联检索方法CRAFT，通过稀疏检索+密集检索+神经重排序的流程，在保持高效的同时超越现有检索方法性能，并利用生成式方法增强表格表示


<details>
  <summary>Details</summary>
Motivation: 传统密集检索模型(DTR/ColBERT)存在高计算成本且难以适应新数据集的问题，需要更高效且适应性强的检索方案

Method: 1. 级联架构：稀疏检索筛选候选表→密集模型精排 2. 使用Gemini Flash 1.5生成表格描述和标题优化表示 3. 在NQ-Tables数据集验证多种LLM效果

Result: 检索性能超越SOTA稀疏/密集/混合检索模型，端到端TQA任务验证了有效性

Conclusion: CRAFT在效率与效果间取得平衡，生成的表格元数据增强表示质量，适用于知识快速更新的领域

Abstract: Table Question Answering (TQA) involves retrieving relevant tables from a
large corpus to answer natural language queries. Traditional dense retrieval
models, such as DTR and ColBERT, not only incur high computational costs for
large-scale retrieval tasks but also require retraining or fine-tuning on new
datasets, limiting their adaptability to evolving domains and knowledge. In
this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that
first uses a sparse retrieval model to filter a subset of candidate tables
before applying more computationally expensive dense models and neural
re-rankers. Our approach achieves better retrieval performance than
state-of-the-art (SOTA) sparse, dense, and hybrid retrievers. We further
enhance table representations by generating table descriptions and titles using
Gemini Flash 1.5. End-to-end TQA results using various Large Language Models
(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate
$\textbf{CRAFT}$ effectiveness.

</details>


### [25] [Language Specific Knowledge: Do Models Know Better in X than in English?](https://arxiv.org/abs/2505.14990)
*Ishika Agarwal,Nimet Beyza Bozdag,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 提出语言特定知识(LSK)概念，验证多语言模型中特定语言推理优势，开发LSKExtractor方法实现10%准确率提升。


<details>
  <summary>Details</summary>
Motivation: 基于人类语码切换现象，推测语言模型在不同语言中知识储备存在差异，结合文化-语言共生关系探索多语言推理优化方案。

Method: 使用文化特定数据集测试不同语言推理效果，特别关注低资源语言，设计LSKExtractor方法提取和应用语言特定知识。

Result: 实验显示模型在某些非英语(含低资源语言)推理准确率平均提升10%，验证语言特定知识存在性。

Conclusion: 推动开源语言模型向文化语言适应性发展，提升多语言场景部署效果，促进人工智能的文化包容性建设。

Abstract: Code-switching is a common phenomenon of alternating between different
languages in the same utterance, thought, or conversation. We posit that humans
code-switch because they feel more comfortable talking about certain topics and
domains in one language than another. With the rise of knowledge-intensive
language models, we ask ourselves the next, natural question: Could models hold
more knowledge on some topics in some language X? More importantly, could we
improve reasoning by changing the language that reasoning is performed in? We
coin the term Language Specific Knowledge (LSK) to represent this phenomenon.
As ethnic cultures tend to develop alongside different languages, we employ
culture-specific datasets (that contain knowledge about cultural and social
behavioral norms). We find that language models can perform better when using
chain-of-thought reasoning in some languages other than English, sometimes even
better in low-resource languages. Paired with previous works showing that
semantic similarity does not equate to representational similarity, we
hypothesize that culturally specific texts occur more abundantly in
corresponding languages, enabling specific knowledge to occur only in specific
"expert" languages. Motivated by our initial results, we design a simple
methodology called LSKExtractor to benchmark the language-specific knowledge
present in a language model and, then, exploit it during inference. We show our
results on various models and datasets, showing an average relative improvement
of 10% in accuracy. Our research contributes to the open-source development of
language models that are inclusive and more aligned with the cultural and
linguistic contexts in which they are deployed.

</details>


### [26] [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](https://arxiv.org/abs/2505.14992)
*Zhihao Wen,Sheng Liang,Yaxiong Wu,Yongyue Zhang,Yong Liu*

Main category: cs.CL

TL;DR: 提出双LoRA架构DLISC，通过模式识别与增量缓存优化设备端大语言模型的信息抽取效率


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在资源受限设备部署时存在的幻觉问题、上下文长度限制和高延迟问题，特别是处理多样化抽取模式时的效率瓶颈

Method: 1. 双LoRA架构（模式识别模块+模式感知抽取模块）
2. 增量模式缓存技术减少冗余计算
3. 两阶段模式匹配与信息抽取流程

Result: 在多个信息抽取数据集上验证，同时提升任务效果（effectiveness）和执行效率（efficiency）

Conclusion: DLISC为设备端信息抽取提供了兼顾性能与效率的解决方案，具有实际部署价值

Abstract: Information extraction (IE) plays a crucial role in natural language
processing (NLP) by converting unstructured text into structured knowledge.
Deploying computationally intensive large language models (LLMs) on
resource-constrained devices for information extraction is challenging,
particularly due to issues like hallucinations, limited context length, and
high latency-especially when handling diverse extraction schemas. To address
these challenges, we propose a two-stage information extraction approach
adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching
(DLISC), which enhances both schema identification and schema-aware extraction
in terms of effectiveness and efficiency. In particular, DLISC adopts an
Identification LoRA module for retrieving the most relevant schemas to a given
query, and an Extraction LoRA module for performing information extraction
based on the previously selected schemas. To accelerate extraction inference,
Incremental Schema Caching is incorporated to reduce redundant computation,
substantially improving efficiency. Extensive experiments across multiple
information extraction datasets demonstrate notable improvements in both
effectiveness and efficiency.

</details>


### [27] [Meta-Design Matters: A Self-Design Multi-Agent System](https://arxiv.org/abs/2505.14996)
*Zixuan Ke,Austin Xu,Yifei Ming,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 提出首个自我监督的推理时自动多代理系统设计框架SELF-MAS，通过元级设计实现动态配置优化，实验显示准确率提升7.44%


<details>
  <summary>Details</summary>
Motivation: 现有手动设计的MAS难以匹配LLM能力且缺乏任务适应性，自动MAS方法需要验证集且配置静态化

Method: 采用元级别迭代生成-评估-优化框架，基于可解性和完整性反馈实现动态代理组合与问题分解

Result: 在数学、研究生QA和软件工程基准测试中，使用不同规模LLM均超越基线方法，保持成本效益

Conclusion: 元级自我监督设计为创建自适应MAS提供了有效路径，验证了框架的实用性和扩展潜力

Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large
Language Models (LLMs) hold significant potential for tackling complex tasks.
However, most current MAS depend on manually designed agent roles and
communication protocols. These manual designs often fail to align with the
underlying LLMs' strengths and struggle to adapt to novel tasks. Recent
automatic MAS approaches attempt to mitigate these limitations but typically
necessitate a validation-set for tuning and yield static MAS designs lacking
adaptability during inference. We introduce SELF-MAS, the first
self-supervised, inference-time only framework for automatic MAS design.
SELF-MAS employs meta-level design to iteratively generate, evaluate, and
refine MAS configurations tailored to each problem instance, without requiring
a validation set. Critically, it enables dynamic agent composition and problem
decomposition through meta-feedback on solvability and completeness.
Experiments across math, graduate-level QA, and software engineering
benchmarks, using both closed-source and open-source LLM back-bones of varying
sizes, demonstrate that SELF-MAS outperforms both manual and automatic MAS
baselines, achieving a 7.44% average accuracy improvement over the next
strongest baseline while maintaining cost-efficiency. These findings underscore
the promise of meta-level self-supervised design for creating effective and
adaptive MAS.

</details>


### [28] [Towards Spoken Mathematical Reasoning: Benchmarking Speech-based Models over Multi-faceted Math Problems](https://arxiv.org/abs/2505.15000)
*Chengwei Wei,Bin Wang,Jung-jae Kim,Nancy F. Chen*

Main category: cs.CL

TL;DR: 提出Spoken-MQA基准评估语音模型数学推理能力，发现现有模型在直接算术、口语化数学表达和知识推理方面存在显著不足


<details>
  <summary>Details</summary>
Motivation: 现有语音模型研究集中于事实性理解和简单音频推理，缺乏对数学逻辑推理能力的系统评估，尤其是口语输入场景下的多步推理问题

Method: 构建包含纯算术/上下文推理/知识推理的Spoken-MQA测试集，对比分析级联模型(ASR+LLM)和端到端语音LLM的表现

Result: 语音LLM在上下文推理任务表现尚可，但存在三个关键缺陷：直接算术准确率低（36.2%）、LaTex符号偏好偏差（准确率差距达42%）、数学知识推理能力显著下降（准确率仅21.8%）

Conclusion: 当前语音模型处理口语数学表达存在系统性挑战，需改进符号理解、算术运算和知识整合能力，推动更鲁棒的语音数学推理系统开发

Abstract: Recent advances in large language models (LLMs) and multimodal LLMs (MLLMs)
have led to strong reasoning ability across a wide range of tasks. However,
their ability to perform mathematical reasoning from spoken input remains
underexplored. Prior studies on speech modality have mostly focused on factual
speech understanding or simple audio reasoning tasks, providing limited insight
into logical step-by-step reasoning, such as that required for mathematical
problem solving. To address this gap, we introduce Spoken Math Question
Answering (Spoken-MQA), a new benchmark designed to evaluate the mathematical
reasoning capabilities of speech-based models, including both cascade models
(ASR + LLMs) and end-to-end speech LLMs. Spoken-MQA covers a diverse set of
math problems, including pure arithmetic, single-step and multi-step contextual
reasoning, and knowledge-oriented reasoning problems, all presented in
unambiguous natural spoken language. Through extensive experiments, we find
that: (1) while some speech LLMs perform competitively on contextual reasoning
tasks involving basic arithmetic, they still struggle with direct arithmetic
problems; (2) current LLMs exhibit a strong bias toward symbolic mathematical
expressions written in LaTex and have difficulty interpreting verbalized
mathematical expressions; and (3) mathematical knowledge reasoning abilities
are significantly degraded in current speech LLMs.

</details>


### [29] [Diagnosing our datasets: How does my language model learn clinical information?](https://arxiv.org/abs/2505.15024)
*Furong Jia,David Sontag,Monica Agrawal*

Main category: cs.CL

TL;DR: 研究发现LLMs在临床术语理解和医学声明回应上受预训练数据影响，揭示数据与实际应用的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 探索开源LLMs如何通过预训练数据学习临床信息，重点关注其对临床术语的理解及对争议医学声明的响应能力。

Method: 使用新数据集MedLingo评估模型，分析预训练数据中临床术语频率、数据组成与输出的关系，并追溯数据来源。

Result: 临床术语在预训练数据中的频率与模型表现正相关，但实际高频临床术语在预训练数据中稀少；部分文档支持争议声明导致模型复现。

Conclusion: 需优化预训练数据组成，增加真实临床场景数据并过滤争议内容，以提高LLMs在医疗领域的可靠性和实用性。

Abstract: Large language models (LLMs) have performed well across various clinical
natural language processing tasks, despite not being directly trained on
electronic health record (EHR) data. In this work, we examine how popular
open-source LLMs learn clinical information from large mined corpora through
two crucial but understudied lenses: (1) their interpretation of clinical
jargon, a foundational ability for understanding real-world clinical notes, and
(2) their responses to unsupported medical claims. For both use cases, we
investigate the frequency of relevant clinical information in their
corresponding pretraining corpora, the relationship between pretraining data
composition and model outputs, and the sources underlying this data. To isolate
clinical jargon understanding, we evaluate LLMs on a new dataset MedLingo.
Unsurprisingly, we find that the frequency of clinical jargon mentions across
major pretraining corpora correlates with model performance. However, jargon
frequently appearing in clinical notes often rarely appears in pretraining
corpora, revealing a mismatch between available data and real-world usage.
Similarly, we find that a non-negligible portion of documents support disputed
claims that can then be parroted by models. Finally, we classified and analyzed
the types of online sources in which clinical jargon and unsupported medical
claims appear, with implications for future dataset composition.

</details>


### [30] [Are the confidence scores of reviewers consistent with the review content? Evidence from top conference proceedings in AI](https://arxiv.org/abs/2505.15031)
*Wenqing Wu,Haixu Xi,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 研究发现同行评审文本内容与审稿人信心分数高度一致，高信心分数与论文被拒显著相关，验证了专家评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对审稿意见文本与审稿人信心分数一致性的细粒度分析，可能遗漏影响评审质量的关键细节。

Method: 使用深度学习方法检测对冲性语言，结合NLP技术从词频、句式结构和内容维度分析审稿文本，并通过回归模型验证信心分数与论文结果的关系。

Result: 在词汇、句法和内容层面均呈现高文本-分数一致性，回归分析显示审稿人信心分数每提高1分，论文被拒概率增加27.3%。

Conclusion: 审稿人信心分数有效反映评审内容质量，高信心拒绝决策体现同行评审系统的公平性和专业性。

Abstract: Peer review is vital in academia for evaluating research quality. Top AI
conferences use reviewer confidence scores to ensure review reliability, but
existing studies lack fine-grained analysis of text-score consistency,
potentially missing key details. This work assesses consistency at word,
sentence, and aspect levels using deep learning and NLP conference review data.
We employ deep learning to detect hedge sentences and aspects, then analyze
report length, hedge word/sentence frequency, aspect mentions, and sentiment to
evaluate text-score alignment. Correlation, significance, and regression tests
examine confidence scores' impact on paper outcomes. Results show high
text-score consistency across all levels, with regression revealing higher
confidence scores correlate with paper rejection, validating expert assessments
and peer review fairness.

</details>


### [31] [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](https://arxiv.org/abs/2505.15038)
*Haiyan Zhao,Xuansheng Wu,Fan Yang,Bo Shen,Ninghao Liu,Mengnan Du*

Main category: cs.CL

TL;DR: 提出使用稀疏自编码器去噪的SDCV方法，提升线性概念向量对大语言模型的导向成功率


<details>
  <summary>Details</summary>
Motivation: 现有线性概念向量方法从多样化数据中提取特征时会引入噪声，导致模型导向鲁棒性下降

Method: 在隐藏表示上应用稀疏自编码器过滤噪声特征，改进线性探测和均值差异法

Result: 成功提升两种方法的导向成功率，并通过反事实实验和特征可视化验证噪声假设

Conclusion: SDCV通过有效去噪机制增强了概念向量的导向性能，实验验证了方法的有效性

Abstract: Linear Concept Vectors have proven effective for steering large language
models (LLMs). While existing approaches like linear probing and
difference-in-means derive these vectors from LLM hidden representations,
diverse data introduces noises (i.e., irrelevant features) that challenge
steering robustness. To address this, we propose Sparse Autoencoder-Denoised
Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy
features from hidden representations. When applied to linear probing and
difference-in-means, our method improves their steering success rates. We
validate our noise hypothesis through counterfactual experiments and feature
visualizations.

</details>


### [32] [Diffusion vs. Autoregressive Language Models: A Text Embedding Perspective](https://arxiv.org/abs/2505.15045)
*Siyue Zhang,Yilun Zhao,Liyuan Geng,Arman Cohan,Anh Tuan Luu,Chen Zhao*

Main category: cs.CL

TL;DR: 提出首个双向注意力机制的扩散语言模型文本嵌入方法，在长文档检索任务上性能超越LLM嵌入模型20%


<details>
  <summary>Details</summary>
Motivation: LLM的单向注意力机制与文本嵌入任务的双向特性存在根本性矛盾，扩散语言模型因其双向架构和在推理任务中的优异表现成为替代方案

Method: 系统研究扩散语言模型在文本嵌入中的应用，验证双向注意力机制对全局上下文编码的有效性

Result: 长文档检索提升20%、推理密集型检索提升8%、指令跟随检索提升2%，传统基准测试达到竞争性表现

Conclusion: 双向注意力机制在处理长文本和复杂语义时对全局语境编码具有决定性作用

Abstract: Large language model (LLM)-based embedding models, benefiting from large
scale pre-training and post-training, have begun to surpass BERT and T5-based
models on general-purpose text embedding tasks such as document retrieval.
However, a fundamental limitation of LLM embeddings lies in the unidirectional
attention used during autoregressive pre-training, which misaligns with the
bidirectional nature of text embedding tasks. To this end, We propose adopting
diffusion language models for text embeddings, motivated by their inherent
bidirectional architecture and recent success in matching or surpassing LLMs
especially on reasoning tasks. We present the first systematic study of the
diffusion language embedding model, which outperforms the LLM-based embedding
model by 20% on long-document retrieval, 8% on reasoning-intensive retrieval,
2% on instruction-following retrieval, and achieve competitive performance on
traditional text embedding benchmarks. Our analysis verifies that bidirectional
attention is crucial for encoding global context in long and complex text.

</details>


### [33] [ChartCards: A Chart-Metadata Generation Framework for Multi-Task Chart Understanding](https://arxiv.org/abs/2505.15046)
*Yifan Wu,Lutao Yan,Leixian Shen,Yinan Mei,Jiannan Wang,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出ChartCards框架和MetaChart数据集，通过统一元数据生成支持多任务图表理解，微调后模型平均性能提升5%，显著优化检索和表格转换任务。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLMs在细粒度图表理解任务中依赖大规模任务特定数据集导致的高成本问题，降低数据收集和训练开销。

Method: ChartCards框架系统合成数据表/可视化代码/视觉元素/多维度语义描述，构建MetaChart数据集（10,862数据表+85K图表+170K标注），支持检索/摘要/问答等5类任务。

Result: 微调6个模型平均性能提升5%，文本-图表检索任务提升17%（Long-CLIP），图表转表格任务提升28%（Llama3.2-11B）。

Conclusion: ChartCards有效统一多源信息，MetaChart验证了多任务支持能力，显著降低数据需求同时提升模型跨任务性能。

Abstract: The emergence of Multi-modal Large Language Models (MLLMs) presents new
opportunities for chart understanding. However, due to the fine-grained nature
of these tasks, applying MLLMs typically requires large, high-quality datasets
for task-specific fine-tuning, leading to high data collection and training
costs. To address this, we propose ChartCards, a unified chart-metadata
generation framework for multi-task chart understanding. ChartCards
systematically synthesizes various chart information, including data tables,
visualization code, visual elements, and multi-dimensional semantic captions.
By structuring this information into organized metadata, ChartCards enables a
single chart to support multiple downstream tasks, such as text-to-chart
retrieval, chart summarization, chart-to-table conversion, chart description,
and chart question answering. Using ChartCards, we further construct MetaChart,
a large-scale high-quality dataset containing 10,862 data tables, 85K charts,
and 170 K high-quality chart captions. We validate the dataset through
qualitative crowdsourcing evaluations and quantitative fine-tuning experiments
across various chart understanding tasks. Fine-tuning six different models on
MetaChart resulted in an average performance improvement of 5% across all
tasks. The most notable improvements are seen in text-to-chart retrieval and
chart-to-table tasks, with Long-CLIP and Llama 3.2-11B achieving improvements
of 17% and 28%, respectively.

</details>


### [34] [Improving the fact-checking performance of language models by relying on their entailment ability](https://arxiv.org/abs/2505.15050)
*Gaurav Kumar,Debajyoti Mazumder,Ayush Garg,Jasabanta Patro*

Main category: cs.CL

TL;DR: 提出通过生成支持/反驳性依据训练语言模型，显著提升事实核查性能


<details>
  <summary>Details</summary>
Motivation: 现有事实核查方法存在幻觉问题和证据矛盾挑战，需更有效的训练策略

Method: 利用语言模型的蕴含推理和生成能力产生正反依据，基于这些依据训练模型（TBE-3方法）

Result: 在RAW-FC数据集上macro-F1提升最高44.26%，LIAR-RAW提升28.57%

Conclusion: 基于蕴含依据的训练策略显著优于基线方法，系统性提示工程对比填补了领域研究空白

Abstract: Automated fact-checking is a crucial task in this digital age. To verify a
claim, current approaches majorly follow one of two strategies i.e. (i) relying
on embedded knowledge of language models, and (ii) fine-tuning them with
evidence pieces. While the former can make systems to hallucinate, the later
have not been very successful till date. The primary reason behind this is that
fact verification is a complex process. Language models have to parse through
multiple pieces of evidence before making a prediction. Further, the evidence
pieces often contradict each other. This makes the reasoning process even more
complex. We proposed a simple yet effective approach where we relied on
entailment and the generative ability of language models to produce
''supporting'' and ''refuting'' justifications (for the truthfulness of a
claim). We trained language models based on these justifications and achieved
superior results. Apart from that, we did a systematic comparison of different
prompting and fine-tuning strategies, as it is currently lacking in the
literature. Some of our observations are: (i) training language models with raw
evidence sentences registered an improvement up to 8.20% in macro-F1, over the
best performing baseline for the RAW-FC dataset, (ii) similarly, training
language models with prompted claim-evidence understanding (TBE-2) registered
an improvement (with a margin up to 16.39%) over the baselines for the same
dataset, (iii) training language models with entailed justifications (TBE-3)
outperformed the baselines by a huge margin (up to 28.57% and 44.26% for
LIAR-RAW and RAW-FC, respectively). We have shared our code repository to
reproduce the results.

</details>


### [35] [MolLangBench: A Comprehensive Benchmark for Language-Prompted Molecular Structure Recognition, Editing, and Generation](https://arxiv.org/abs/2505.15054)
*Feiyang Cai,Jiahui Bai,Tao Tang,Joshua Luo,Tianyu Zhu,Ling Liu,Feng Luo*

Main category: cs.CL

TL;DR: 提出MolLangBench基准测试，揭示当前AI在分子识别/编辑（79%精度）和生成（29%精度）任务上的显著不足


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在基础分子语言接口任务（识别、编辑、生成）存在重大缺陷，需建立标准化评估体系推动化学AI发展

Method: 通过化学信息学工具自动构建识别任务，专家标注验证编辑/生成任务，支持字符串、分子图像、分子图三种表示形式

Result: 最优模型(o3)在识别(79.2%)和编辑(78.5%)任务表现有限，生成任务仅29.0%准确率，显著低于人类水平

Conclusion: MolLangBench暴露当前AI处理分子任务的薄弱环节，有望推动开发更可靠的化学应用AI系统

Abstract: Precise recognition, editing, and generation of molecules are essential
prerequisites for both chemists and AI systems tackling various chemical tasks.
We present MolLangBench, a comprehensive benchmark designed to evaluate
fundamental molecule-language interface tasks: language-prompted molecular
structure recognition, editing, and generation. To ensure high-quality,
unambiguous, and deterministic outputs, we construct the recognition tasks
using automated cheminformatics tools, and curate editing and generation tasks
through rigorous expert annotation and validation. MolLangBench supports the
evaluation of models that interface language with different molecular
representations, including linear strings, molecular images, and molecular
graphs. Evaluations of state-of-the-art models reveal significant limitations:
the strongest model (o3) achieves $79.2\%$ and $78.5\%$ accuracy on recognition
and editing tasks, which are intuitively simple for humans, and performs even
worse on the generation task, reaching only $29.0\%$ accuracy. These results
highlight the shortcomings of current AI systems in handling even preliminary
molecular recognition and manipulation tasks. We hope MolLangBench will
catalyze further research toward more effective and reliable AI systems for
chemical applications.

</details>


### [36] [Lost in Benchmarks? Rethinking Large Language Model Benchmarking with Item Response Theory](https://arxiv.org/abs/2505.15055)
*Hongli Zhou,Hui Huang,Ziqing Zhao,Lvyuan Han,Huicheng Wang,Kehai Chen,Muyun Yang,Wei Bao,Jian Dong,Bing Xu,Conghui Zhu,Hailong Cao,Tiejun Zhao*

Main category: cs.CL

TL;DR: 提出PSN-IRT框架改进LLM基准测试效度，揭示现有基准测量质量缺陷并构建更高效精简的评估工具


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试存在排行榜不一致性、顶尖模型区分度低等问题，无法真实反映模型能力，需要开发更科学的评估框架

Method: 开发基于项目反应理论增强的PSN-IRT框架，通过伪孪生网络架构整合多维项目参数，构建IRT理论基础的新型测量体系

Result: 分析显示现有基准存在显著测量缺陷，验证PSN-IRT可构建更小规模但人类偏好对齐度提升17.3%的基准系统

Conclusion: PSN-IRT为LLM评估提供可靠测量框架，其模块化设计支持基准优化，推动AI评估方法学发展

Abstract: The evaluation of large language models (LLMs) via benchmarks is widespread,
yet inconsistencies between different leaderboards and poor separability among
top models raise concerns about their ability to accurately reflect authentic
model capabilities. This paper provides a critical analysis of benchmark
effectiveness, examining main-stream prominent LLM benchmarks using results
from diverse models. We first propose a new framework for accurate and reliable
estimations of item characteristics and model abilities. Specifically, we
propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced
Item Response Theory framework that incorporates a rich set of item parameters
within an IRT-grounded architecture. Based on PSN-IRT, we conduct extensive
analysis which reveals significant and varied shortcomings in the measurement
quality of current benchmarks. Furthermore, we demonstrate that leveraging
PSN-IRT is able to construct smaller benchmarks while maintaining stronger
alignment with human preference.

</details>


### [37] [Self-GIVE: Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2505.15062)
*Jiashu He,Jinxuan Fan,Bowen Jiang,Ignacio Houine,Dan Roth,Alejandro Ribeiro*

Main category: cs.CL

TL;DR: Self-GIVE框架通过强化学习自动关联思维，显著提升小模型（3B/7B）在生物医学QA中的性能，减少90%以上token使用并媲美GPT3.5。


<details>
  <summary>Details</summary>
Motivation: 现有GIVE方法依赖大量假设三元组构建，效率低且难以部署到小模型，知识修剪存在误差。需自动化关联推理提升效率与泛化性。

Method: 提出retrieve-RL框架Self-GIVE：1) 提取结构化信息与实体集辅助概念关联；2) 通过强化学习减少LLM调用与token开销；3) 优化知识修剪准确性。

Result: Qwen2.5 3B/7B模型在未见生物医学QA样本中性能提升至71.4%/90.5%，7B模型性能匹敌GPT3.5 turbo且token用量减少90%+。

Conclusion: Self-GIVE实现结构化检索与关联推理的高效整合，推动小模型在复杂科学推理任务中的实用化部署。

Abstract: When addressing complex questions that require new information, people often
associate the question with existing knowledge to derive a sensible answer. For
instance, when evaluating whether melatonin aids insomnia, one might associate
"hormones helping mental disorders" with "melatonin being a hormone and
insomnia a mental disorder" to complete the reasoning. Large Language Models
(LLMs) also require such associative thinking, particularly in resolving
scientific inquiries when retrieved knowledge is insufficient and does not
directly answer the question. Graph Inspired Veracity Extrapolation (GIVE)
addresses this by using a knowledge graph (KG) to extrapolate structured
knowledge. However, it involves the construction and pruning of many
hypothetical triplets, which limits efficiency and generalizability. We propose
Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic
associative thinking through reinforcement learning. Self-GIVE extracts
structured information and entity sets to assist the model in linking to the
queried concepts. We address GIVE's key limitations: (1) extensive LLM calls
and token overhead for knowledge extrapolation, (2) difficulty in deploying on
smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate
knowledge from LLM pruning. Specifically, after fine-tuning using self-GIVE
with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B
models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and
$\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging
biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or
outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%.
Self-GIVE enhances the scalable integration of structured retrieval and
reasoning with associative thinking.

</details>


### [38] [UrduFactCheck: An Agentic Fact-Checking Framework for Urdu with Evidence Boosting and Benchmarking](https://arxiv.org/abs/2505.15063)
*Sarfraz Ahmad,Hasan Iqbal,Momina Ahsan,Numaan Naeem,Muhammad Ahsan Riaz Khan,Arham Riaz,Muhammad Arslan Manzoor,Yuxia Wang,Preslav Nakov*

Main category: cs.CL

TL;DR: 首个乌尔都语事实核查框架UrduFactCheck，通过多策略证据检索和翻译增强方法，在两个新基准测试中超越基线模型，并揭示专有与开源LLM的差距。


<details>
  <summary>Details</summary>
Motivation: 填补低资源语言（乌尔都语）缺乏自动化事实核查工具的空白，解决LLM在非英语场景中的事实可靠性问题。

Method: 设计动态多策略证据检索管道（单语+翻译方法），创建UrduFactBench声明验证和UrduFactQA问答评估基准，对比12种SOTA LLM表现。

Result: 翻译增强版UrduFactCheck性能最优，专有LLM（如GPT-4）在乌尔都语事实问答中显著优于开源模型（差距达35%）。

Conclusion: 框架开源推动乌尔都语社区发展，为低资源语言事实核查提供可扩展方案，揭示LLM跨语言事实性差异需持续改进。

Abstract: The rapid use of large language models (LLMs) has raised critical concerns
regarding the factual reliability of their outputs, especially in low-resource
languages such as Urdu. Existing automated fact-checking solutions
overwhelmingly focus on English, leaving a significant gap for the 200+ million
Urdu speakers worldwide. In this work, we introduce UrduFactCheck, the first
comprehensive, modular fact-checking framework specifically tailored for Urdu.
Our system features a dynamic, multi-strategy evidence retrieval pipeline that
combines monolingual and translation-based approaches to address the scarcity
of high-quality Urdu evidence. We curate and release two new hand-annotated
benchmarks: UrduFactBench for claim verification and UrduFactQA for evaluating
LLM factuality. Extensive experiments demonstrate that UrduFactCheck,
particularly its translation-augmented variants, consistently outperforms
baselines and open-source alternatives on multiple metrics. We further
benchmark twelve state-of-the-art (SOTA) LLMs on factual question answering in
Urdu, highlighting persistent gaps between proprietary and open-source models.
UrduFactCheck's code and datasets are open-sourced and publicly available at
https://github.com/mbzuai-nlp/UrduFactCheck.

</details>


### [39] [The Pursuit of Empathy: Evaluating Small Language Models for PTSD Dialogue Support](https://arxiv.org/abs/2505.15065)
*Suhas BN,Yash Mahajan,Dominik Mattioli,Andrew M. Sherrill,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: cs.CL

TL;DR: 研究验证了5亿至50亿参数的小型语言模型在创伤知情对话中的有效性，开发了TIDE数据集并发现微调能有限提升共情表现，但存在场景依赖性和模型天花板效应


<details>
  <summary>Details</summary>
Motivation: 探索资源节约型AI在创伤后应激障碍（PTSD）患者辅助对话中的可行性，平衡伦理安全与计算效率，为补充临床心理健康护理提供技术基础

Method: 构建包含500个多样化用户画像的10,000轮创伤对话数据集TIDE，通过临床心理学家审核，采用人工评估（IRB批准）和自动指标对比8个小模型微调前后的表现，并与前沿模型Claude Sonnet 3.5进行基准测试

Result: 微调使共情感知提升但效果受场景/用户差异影响显著（老年用户重视痛苦确认，高学历用户偏好细致回应），小模型存在共情天花板，自动评估指标与人类判断存在偏差

Conclusion: 强调情境感知系统设计的必要性，发布TIDE数据集为开发安全、高效的伦理AI奠定基础，明确AI应作为临床心理护理的补充而非替代

Abstract: Can small language models with 0.5B to 5B parameters meaningfully engage in
trauma-informed, empathetic dialogue for individuals with PTSD? We address this
question by introducing TIDE, a dataset of 10,000 two-turn dialogues spanning
500 diverse PTSD client personas and grounded in a three-factor empathy model:
emotion recognition, distress normalization, and supportive reflection. All
scenarios and reference responses were reviewed for realism and trauma
sensitivity by a clinical psychologist specializing in PTSD. We evaluate eight
small language models before and after fine-tuning, comparing their outputs to
a frontier model (Claude Sonnet 3.5). Our IRB-approved human evaluation and
automatic metrics show that fine-tuning generally improves perceived empathy,
but gains are highly scenario- and user-dependent, with smaller models facing
an empathy ceiling. Demographic analysis shows older adults value distress
validation and graduate-educated users prefer nuanced replies, while gender
effects are minimal. We highlight the limitations of automatic metrics and the
need for context- and user-aware system design. Our findings, along with the
planned release of TIDE, provide a foundation for building safe,
resource-efficient, and ethically sound empathetic AI to supplement, not
replace, clinical mental health care.

</details>


### [40] [In-Domain African Languages Translation Using LLMs and Multi-armed Bandits](https://arxiv.org/abs/2505.15069)
*Pratik Rakesh Singh,Kritarth Prasad,Mohammadi Zaki,Pankaj Wasnik*

Main category: cs.CL

TL;DR: 提出使用Bandit算法解决低资源神经机器翻译的领域适应问题，在非洲语言实验中验证了模型选择的可靠性（含/不含目标数据场景）


<details>
  <summary>Details</summary>
Motivation: 低资源语言的NMT模型在领域适应时面临数据不足和模型泛化能力差的问题，需在无法微调的场景下实现可靠的模型选择

Method: 采用UCB、Linear UCB、Neural Linear Bandit和Thompson Sampling等Bandit算法，构建资源约束下的置信度驱动模型选择框架

Result: 在三种非洲语言的跨领域测试中，该方法在目标数据存在/缺失场景下均展现出稳定的模型选择能力

Conclusion: Bandit算法通过置信度驱动的探索-利用平衡机制，有效解决了低资源NMT领域适应中的模型选择难题

Abstract: Neural Machine Translation (NMT) systems face significant challenges when
working with low-resource languages, particularly in domain adaptation tasks.
These difficulties arise due to limited training data and suboptimal model
generalization, As a result, selecting an optimal model for translation is
crucial for achieving strong performance on in-domain data, particularly in
scenarios where fine-tuning is not feasible or practical. In this paper, we
investigate strategies for selecting the most suitable NMT model for a given
domain using bandit-based algorithms, including Upper Confidence Bound, Linear
UCB, Neural Linear Bandit, and Thompson Sampling. Our method effectively
addresses the resource constraints by facilitating optimal model selection with
high confidence. We evaluate the approach across three African languages and
domains, demonstrating its robustness and effectiveness in both scenarios where
target data is available and where it is absent.

</details>


### [41] [Can Large Language Models Understand Internet Buzzwords Through User-Generated Content](https://arxiv.org/abs/2505.15071)
*Chen Huang,Junkai Luo,Xinzuo Wang,Wenqiang Lei,Jiancheng Lv*

Main category: cs.CL

TL;DR: 本文提出首个中文网络热词数据集CHEER，开发RESS方法优化大模型生成热词定义的能力，并揭示现有方法的三大核心挑战。


<details>
  <summary>Details</summary>
Motivation: 基于中文社交媒体海量用户生成内容（UGC），探索大语言模型（LLM）能否像人类学习者一样，通过示例UGC准确生成网络热词定义。

Method: 提出RESS方法——通过策略性地引导大模型对UGC的解读过程，模拟人类语言学习机制来提升定义生成质量。

Result: 基准测试证明RESS有效性，同时揭示三大共性挑战：过度依赖先验知识、推理能力不足、难以筛选高质量UGC辅助理解。

Conclusion: 该研究为LLM定义生成领域奠定基础，CHEER数据集和RESS方法将助力后续研究，代码与数据已开源。

Abstract: The massive user-generated content (UGC) available in Chinese social media is
giving rise to the possibility of studying internet buzzwords. In this paper,
we study if large language models (LLMs) can generate accurate definitions for
these buzzwords based on UGC as examples. Our work serves a threefold
contribution. First, we introduce CHEER, the first dataset of Chinese internet
buzzwords, each annotated with a definition and relevant UGC. Second, we
propose a novel method, called RESS, to effectively steer the comprehending
process of LLMs to produce more accurate buzzword definitions, mirroring the
skills of human language learning. Third, with CHEER, we benchmark the
strengths and weaknesses of various off-the-shelf definition generation methods
and our RESS. Our benchmark demonstrates the effectiveness of RESS while
revealing crucial shared challenges: over-reliance on prior exposure,
underdeveloped inferential abilities, and difficulty identifying high-quality
UGC to facilitate comprehension. We believe our work lays the groundwork for
future advancements in LLM-based definition generation. Our dataset and code
are available at https://github.com/SCUNLP/Buzzword.

</details>


### [42] [DISCO Balances the Scales: Adaptive Domain- and Difficulty-Aware Reinforcement Learning on Imbalanced Data](https://arxiv.org/abs/2505.15074)
*Yuhang Zhou,Jing Zhu,Shengyi Qian,Zhuokai Zhao,Xiyao Wang,Xiaoyu Liu,Ming Li,Paiheng Xu,Wei Ai,Furong Huang*

Main category: cs.CL

TL;DR: DISCO通过领域感知和难度感知的奖励缩放策略改进GRPO，解决多领域数据不平衡问题，提升模型泛化能力和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO方法隐含平衡数据分布假设，但在现实多领域不平衡场景中会过度优化主导领域，导致泛化性差和公平性问题。

Method: 提出领域感知奖励缩放（对抗频率偏差）和难度感知奖励缩放（利用自一致性识别高价值提示）双策略

Result: 在Qwen3模型上比GRPO变体提升5%，在多领域对齐基准上取得新SOTA

Conclusion: DISCO通过领域信息整合和自一致性优化，为多领域对齐任务提供了更公平有效的策略优化框架

Abstract: Large Language Models (LLMs) are increasingly aligned with human preferences
through Reinforcement Learning from Human Feedback (RLHF). Among RLHF methods,
Group Relative Policy Optimization (GRPO) has gained attention for its
simplicity and strong performance, notably eliminating the need for a learned
value function. However, GRPO implicitly assumes a balanced domain distribution
and uniform semantic alignment across groups - assumptions that rarely hold in
real-world datasets. When applied to multi-domain, imbalanced data, GRPO
disproportionately optimizes for dominant domains, neglecting underrepresented
ones and resulting in poor generalization and fairness. We propose
Domain-Informed Self-Consistency Policy Optimization (DISCO), a principled
extension to GRPO that addresses inter-group imbalance with two key
innovations. Domain-aware reward scaling counteracts frequency bias by
reweighting optimization based on domain prevalence. Difficulty-aware reward
scaling leverages prompt-level self-consistency to identify and prioritize
uncertain prompts that offer greater learning value. Together, these strategies
promote more equitable and effective policy learning across domains. Extensive
experiments across multiple LLMs and skewed training distributions show that
DISCO improves generalization, outperforms existing GRPO variants by 5% on
Qwen3 models, and sets new state-of-the-art results on multi-domain alignment
benchmarks.

</details>


### [43] [Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs](https://arxiv.org/abs/2505.15075)
*Hao Wang,Pinzhi Huang,Jihan Yang,Saining Xie,Daisuke Kawahara*

Main category: cs.CL

TL;DR: 论文提出KnowRecall和VisRecall两个基准测试，揭示当前多模态大语言模型在跨语言文化知识一致性上的不足


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多语言环境下的文化知识整合存在显著性能差异，亟需系统性评估工具

Method: 构建KnowRecall（15语言文化知识问答）和VisRecall（9语言无图视觉记忆描述）双基准测试框架

Result: 实验显示顶尖MLLMs（含商业模型）在跨语言一致性上表现欠佳，文化相关问答错误率显著

Conclusion: 需开发更鲁棒的多语言建模方法，建立真正具备文化感知能力的统一多模态模型

Abstract: The rapid evolution of multimodal large language models (MLLMs) has
significantly enhanced their real-world applications. However, achieving
consistent performance across languages, especially when integrating cultural
knowledge, remains a significant challenge. To better assess this issue, we
introduce two new benchmarks: KnowRecall and VisRecall, which evaluate
cross-lingual consistency in MLLMs. KnowRecall is a visual question answering
benchmark designed to measure factual knowledge consistency in 15 languages,
focusing on cultural and historical questions about global landmarks. VisRecall
assesses visual memory consistency by asking models to describe landmark
appearances in 9 languages without access to images. Experimental results
reveal that state-of-the-art MLLMs, including proprietary ones, still struggle
to achieve cross-lingual consistency. This underscores the need for more robust
approaches that produce truly multilingual and culturally aware models.

</details>


### [44] [HopWeaver: Synthesizing Authentic Multi-Hop Questions Across Text Corpora](https://arxiv.org/abs/2505.15087)
*Zhiyu Shen,Jiyuan Liu,Yunhe Pang,Yanghui Rao*

Main category: cs.CL

TL;DR: 提出HopWeaver框架，首个从非结构化文本自动生成多跳问题的方案，解决人工标注成本高和现有方法局限性的问题


<details>
  <summary>Details</summary>
Motivation: 当前多跳问答数据集构建存在人工标注成本高、合成方法生成问题过于简单或依赖人工指导的痛点，需开发自动化合成方案

Method: 通过识别跨文档互补信息构建推理路径，自动生成桥接类和比较类两种多跳问题，确保问题需要真实的多跳推理

Result: 合成问题质量达到或超过人工标注数据集，成本显著降低，在专业领域资源稀缺场景具有应用价值

Conclusion: HopWeaver为缺乏标注资源的专业领域MHQA数据集开发提供了有效解决方案，代码已开源

Abstract: Multi-Hop Question Answering (MHQA) is crucial for evaluating the model's
capability to integrate information from diverse sources. However, creating
extensive and high-quality MHQA datasets is challenging: (i) manual annotation
is expensive, and (ii) current synthesis methods often produce simplistic
questions or require extensive manual guidance. This paper introduces
HopWeaver, the first automatic framework synthesizing authentic multi-hop
questions from unstructured text corpora without human intervention. HopWeaver
synthesizes two types of multi-hop questions (bridge and comparison) using an
innovative approach that identifies complementary documents across corpora. Its
coherent pipeline constructs authentic reasoning paths that integrate
information across multiple documents, ensuring synthesized questions
necessitate authentic multi-hop reasoning. We further present a comprehensive
system for evaluating synthesized multi-hop questions. Empirical evaluations
demonstrate that the synthesized questions achieve comparable or superior
quality to human-annotated datasets at a lower cost. Our approach is valuable
for developing MHQA datasets in specialized domains with scarce annotated
resources. The code for HopWeaver is publicly available.

</details>


### [45] [DeFTX: Denoised Sparse Fine-Tuning for Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2505.15090)
*Sona Elza Simon,Preethi Jyothi*

Main category: cs.CL

TL;DR: 提出DeFT-X方法，通过奇异值分解去噪提升稀疏微调效果，在低资源语言任务中实现优异跨语言迁移


<details>
  <summary>Details</summary>
Motivation: 现有基于幅度的稀疏微调方法（SFT）在跨语言迁移中存在参数噪声干扰，需要更鲁棒的参数选择方法

Method: 在稀疏微调前使用奇异值分解（SVD）对预训练模型权重矩阵去噪，再进行幅度剪枝选择参数

Result: 在NusaX情感分类和AmericasNLI自然语言推理任务中，DeFT-X性能等同或优于SFT及其他基线方法

Conclusion: 通过权重去噪改进的稀疏微调方法显著提升了跨语言迁移效果，为低资源语言任务提供有效解决方案

Abstract: Effective cross-lingual transfer remains a critical challenge in scaling the
benefits of large language models from high-resource to low-resource languages.
Towards this goal, prior studies have explored many approaches to combine task
knowledge from task-specific data in a (high-resource) source language and
language knowledge from unlabeled text in a (low-resource) target language. One
notable approach proposed composable sparse fine-tuning (SFT) for cross-lingual
transfer that learns task-specific and language-specific sparse masks to select
a subset of the pretrained model's parameters that are further fine-tuned.
These sparse fine-tuned vectors (SFTs) are subsequently composed with the
pretrained model to facilitate zero-shot cross-lingual transfer to a task in a
target language, using only task-specific data from a source language. These
sparse masks for SFTs were identified using a simple magnitude-based pruning.
In our work, we introduce DeFT-X, a novel composable SFT approach that denoises
the weight matrices of a pretrained model before magnitude pruning using
singular value decomposition, thus yielding more robust SFTs. We evaluate
DeFT-X on a diverse set of extremely low-resource languages for sentiment
classification (NusaX) and natural language inference (AmericasNLI) and
demonstrate that it performs at par or outperforms SFT and other prominent
cross-lingual transfer baselines.

</details>


### [46] [SciCUEval: A Comprehensive Dataset for Evaluating Scientific Context Understanding in Large Language Models](https://arxiv.org/abs/2505.15094)
*Jing Yu,Yuqi Tang,Kehua Feng,Mingyang Rao,Lei Liang,Zhiqiang Zhang,Mengshu Sun,Wen Zhang,Qiang Zhang,Keyan Ding,Huajun Chen*

Main category: cs.CL

TL;DR: 构建SciCUEval基准数据集，评估LLMs在科学领域的上下文理解能力，涵盖多学科和多模态数据，分析模型优缺点。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试集中于通用领域，缺乏对科学领域复杂性的评估，需构建专用工具系统评估LLMs的科学上下文理解能力。

Method: 创建包含生物、化学、物理等10个学科的数据集，整合表格/图谱/文本多模态数据，通过多样化题型评估信息识别、缺失检测、多源整合和上下文推理四大能力。

Result: 实验揭示先进LLMs在科学信息整合和推理中的显著差异，部分模型在多源数据融合表现突出但信息缺失敏感度不足。

Conclusion: SciCUEval为科学领域LLMs提供细粒度评估框架，其多模态-多学科设计对领域模型优化具有重要指导意义。

Abstract: Large Language Models (LLMs) have shown impressive capabilities in contextual
understanding and reasoning. However, evaluating their performance across
diverse scientific domains remains underexplored, as existing benchmarks
primarily focus on general domains and fail to capture the intricate complexity
of scientific data. To bridge this gap, we construct SciCUEval, a comprehensive
benchmark dataset tailored to assess the scientific context understanding
capability of LLMs. It comprises ten domain-specific sub-datasets spanning
biology, chemistry, physics, biomedicine, and materials science, integrating
diverse data modalities including structured tables, knowledge graphs, and
unstructured texts. SciCUEval systematically evaluates four core competencies:
Relevant information identification, Information-absence detection,
Multi-source information integration, and Context-aware inference, through a
variety of question formats. We conduct extensive evaluations of
state-of-the-art LLMs on SciCUEval, providing a fine-grained analysis of their
strengths and limitations in scientific context understanding, and offering
valuable insights for the future development of scientific-domain LLMs.

</details>


### [47] [Nek Minit: Harnessing Pragmatic Metacognitive Prompting for Explainable Sarcasm Detection of Australian and Indian English](https://arxiv.org/abs/2505.15095)
*Ishmanbir Singh,Dipankar Srirag,Aditya Joshi*

Main category: cs.CL

TL;DR: 利用PMP技术改进澳大利亚和印度英语的讽刺检测，在多个LLM模型上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决不同地域英语变种（澳大利亚/印度英语）中因文化差异导致的讽刺检测难题

Method: 通过人工标注扩展BESSTIE数据集，结合PMP方法在GEMMA/LLAMA模型上对比测试，并与标准英语数据集FLUTE进行性能比较

Result: PMP方法在所有测试场景中均取得统计显著性改进，Agentic Prompting可有效缓解上下文缺失问题

Conclusion: PMP在生成多英语变体讽刺解释方面表现优异，结合外部知识检索可形成互补优势

Abstract: Sarcasm is a challenge to sentiment analysis because of the incongruity
between stated and implied sentiment. The challenge is exacerbated when the
implication may be relevant to a specific country or geographical region.
Pragmatic metacognitive prompting (PMP) is a cognition-inspired technique that
has been used for pragmatic reasoning. In this paper, we harness PMP for
explainable sarcasm detection for Australian and Indian English, alongside a
benchmark dataset for standard English. We manually add sarcasm explanations to
an existing sarcasm-labeled dataset for Australian and Indian English called
BESSTIE, and compare the performance for explainable sarcasm detection for them
with FLUTE, a standard English dataset containing sarcasm explanations. Our
approach utilising PMP when evaluated on two open-weight LLMs (GEMMA and LLAMA)
achieves statistically significant performance improvement across all tasks and
datasets when compared with four alternative prompting strategies. We also find
that alternative techniques such as agentic prompting mitigate context-related
failures by enabling external knowledge retrieval. The focused contribution of
our work is utilising PMP in generating sarcasm explanations for varieties of
English.

</details>


### [48] [Mechanistic evaluation of Transformers and state space models](https://arxiv.org/abs/2505.15105)
*Aryaman Arora,Neil Rathi,Nikil Roashan Selvam,Róbert Csórdas,Dan Jurafsky,Christopher Potts*

Main category: cs.CL

TL;DR: 比较不同状态空间模型（SSMs）与Transformers在关联回忆任务中的表现，揭示成功模型的机制差异（如归纳头存储与卷积组件）


<details>
  <summary>Details</summary>
Motivation: 探究SSMs在上下文信息回忆中性能不稳定的内在机制，弥补单纯行为指标评估的不足

Method: 通过关联回忆（AR）任务测试模型性能，使用因果干预分析机制差异，并设计层次化关联树召回（ATR）任务验证结论

Result: 仅Transformers和Based SSM完全成功（利用归纳头存储键值关联），Mamba因短卷积部分部分成功，其他SSMs失败

Conclusion: 准确率相近的架构仍存在本质机制差异，需采用机制评估方法深入理解模型行为

Abstract: State space models (SSMs) for language modelling promise an efficient and
performant alternative to quadratic-attention Transformers, yet show variable
performance on recalling basic information from the context. While performance
on synthetic tasks like Associative Recall (AR) can point to this deficiency,
behavioural metrics provide little information as to why--on a mechanistic
level--certain architectures fail and others succeed. To address this, we
conduct experiments on AR and find that only Transformers and Based SSM models
fully succeed at AR, with Mamba a close third, whereas the other SSMs (H3,
Hyena) fail. We then use causal interventions to explain why. We find that
Transformers and Based learn to store key-value associations in-context using
induction heads. By contrast, the SSMs compute these associations only at the
last state, with only Mamba succeeding because of its short convolution
component. To extend and deepen these findings, we introduce Associative
Treecall (ATR), a synthetic task similar to AR based on PCFG induction. ATR
introduces language-like hierarchical structure into the AR setting. We find
that all architectures learn the same mechanism as they did for AR, and the
same three models succeed at the task. These results reveal that architectures
with similar accuracy may still have substantive differences, motivating the
adoption of mechanistic evaluations.

</details>


### [49] [StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](https://arxiv.org/abs/2505.15107)
*Ziliang Wang,Xuhui Zheng,Kang An,Cijun Ouyang,Jialu Cai,Yuhang Wang,Yichao Wu*

Main category: cs.CL

TL;DR: 提出StepSearch框架，通过逐步细粒度监督优化大模型的多跳QA搜索能力，仅用19k数据即在3B/7B模型上实现11.2%/4.2%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于全局奖励的强化学习方法在多跳QA任务中存在奖励稀疏问题，无法有效指导每一步搜索过程。

Method: 采用逐步近端策略优化，结合信息增益奖励与冗余惩罚机制，构建细粒度搜索轨迹数据集实现过程监督。

Result: 在标准多跳QA基准上显著超越基线，3B/7B模型分别获得11.2%和4.2%绝对提升（仅需19k训练数据）。

Conclusion: 细粒度的逐步监督机制能有效提升搜索大模型性能，该方法在数据效率和模型优化层面具有显著优势。

Abstract: Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the sparse rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our
implementation is publicly available at
https://github.com/zxh20001117/StepSearch.

</details>


### [50] [A Risk Taxonomy for Evaluating AI-Powered Psychotherapy Agents](https://arxiv.org/abs/2505.15108)
*Ian Steenstra,Timothy W. Bickmore*

Main category: cs.CL

TL;DR: 提出新型风险分类法系统评估AI心理治疗师风险，通过结合临床标准和模拟患者检测，促进心理健康AI安全创新


<details>
  <summary>Details</summary>
Motivation: 现有AI心理治疗师评估方法缺乏对治疗过程中患者认知/行为微妙变化的检测能力，导致用户伤害甚至自杀风险

Method: 结合心理治疗风险文献、临床法律专家访谈，对齐DSM-5等临床标准和NEQ等评估工具，迭代开发风险分类法

Result: 创建可监测咨询对话风险因素的双重应用框架：人类-AI会话实时监控和模拟患者自动化基准测试

Conclusion: 该分类法为AI心理健康支持领域建立更安全的创新基础，推动负责任的AI治疗技术发展

Abstract: The proliferation of Large Language Models (LLMs) and Intelligent Virtual
Agents acting as psychotherapists presents significant opportunities for
expanding mental healthcare access. However, their deployment has also been
linked to serious adverse outcomes, including user harm and suicide,
facilitated by a lack of standardized evaluation methodologies capable of
capturing the nuanced risks of therapeutic interaction. Current evaluation
techniques lack the sensitivity to detect subtle changes in patient cognition
and behavior during therapy sessions that may lead to subsequent
decompensation. We introduce a novel risk taxonomy specifically designed for
the systematic evaluation of conversational AI psychotherapists. Developed
through an iterative process including review of the psychotherapy risk
literature, qualitative interviews with clinical and legal experts, and
alignment with established clinical criteria (e.g., DSM-5) and existing
assessment tools (e.g., NEQ, UE-ATR), the taxonomy aims to provide a structured
approach to identifying and assessing user/patient harms. We provide a
high-level overview of this taxonomy, detailing its grounding, and discuss
potential use cases. We discuss two use cases in detail: monitoring cognitive
model-based risk factors during a counseling conversation to detect unsafe
deviations, in both human-AI counseling sessions and in automated benchmarking
of AI psychotherapists with simulated patients. The proposed taxonomy offers a
foundational step towards establishing safer and more responsible innovation in
the domain of AI-driven mental health support.

</details>


### [51] [RoT: Enhancing Table Reasoning with Iterative Row-Wise Traversals](https://arxiv.org/abs/2505.15110)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出RoT方法替代长思维链，通过行级表格遍历和反思机制实现高效可靠的表格推理


<details>
  <summary>Details</summary>
Motivation: 长思维链(Long CoT)存在训练成本高、表格内容幻觉严重的缺陷，需要更高效的表格推理方案

Method: 迭代式行级表格遍历，每次遍历包含推理扩展和基于反思的优化，利用LLM的反思能力实现零训练

Result: 在WikiTableQuestions和TableBench上达到SOTA，非推理模型平均超越RLLMs 4.3%，推理token量减少效率更高

Conclusion: RoT通过结构化遍历机制在保持高效的同时显著提升可靠性，为表格推理提供了新的技术路径

Abstract: The table reasoning task, crucial for efficient data acquisition, aims to
answer questions based on the given table. Recently, reasoning large language
models (RLLMs) with Long Chain-of-Thought (Long CoT) significantly enhance
reasoning capabilities, leading to brilliant performance on table reasoning.
However, Long CoT suffers from high cost for training and exhibits low
reliability due to table content hallucinations. Therefore, we propose
Row-of-Thought (RoT), which performs iteratively row-wise table traversal,
allowing for reasoning extension and reflection-based refinement at each
traversal. Scaling reasoning length by row-wise traversal and leveraging
reflection capabilities of LLMs, RoT is training-free. The sequential traversal
encourages greater attention to the table, thus reducing hallucinations.
Experiments show that RoT, using non-reasoning models, outperforms RLLMs by an
average of 4.3%, and achieves state-of-the-art results on WikiTableQuestions
and TableBench with comparable models, proving its effectiveness. Also, RoT
outperforms Long CoT with fewer reasoning tokens, indicating higher efficiency.

</details>


### [52] [An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents](https://arxiv.org/abs/2505.15117)
*Bowen Jin,Jinsung Yoon,Priyanka Kargupta,Sercan O. Arik,Jiawei Han*

Main category: cs.CL

TL;DR: 系统研究强化学习训练大语言模型搜索代理的关键因素：奖励设计、LLM选择、搜索引擎作用，提出实用部署指南


<details>
  <summary>Details</summary>
Motivation: 探索强化学习训练LLM搜索代理的优化方向，明确奖励机制、基础模型特性、搜索引擎角色对训练效果的影响

Method: 通过大规模实证研究，对比不同奖励策略（格式奖励/检索奖励）、LLM初始化方式（通用模型/专用推理模型）、搜索引擎选择的训练效果

Result: 发现格式奖励显著提升性能，检索奖励效果有限；LLM规模与初始化方式决定训练结果；搜索引擎选择影响训练动态与推理鲁棒性

Conclusion: 建立实际部署LLM搜索代理的三维指导框架：定制格式奖励体系、选择专用推理预训练模型、配置高精度搜索引擎

Abstract: Reinforcement learning (RL) has demonstrated strong potential in training
large language models (LLMs) capable of complex reasoning for real-world
problem solving. More recently, RL has been leveraged to create sophisticated
LLM-based search agents that adeptly combine reasoning with search engine use.
While the use of RL for training search agents is promising, the optimal design
of such agents remains not fully understood. In particular, key factors -- such
as (1) reward formulation, (2) the choice and characteristics of the underlying
LLM, and (3) the role of the search engine in the RL process -- require further
investigation. In this work, we conduct comprehensive empirical studies to
systematically investigate these and offer actionable insights. We highlight
several key findings: format rewards are effective in improving final
performance, whereas intermediate retrieval rewards have limited impact; the
scale and initialization of the LLM (general-purpose vs. reasoning-specialized)
significantly influence RL outcomes; and the choice of search engine plays a
critical role in shaping RL training dynamics and the robustness of the trained
agent during inference. These establish important guidelines for successfully
building and deploying LLM-based search agents in real-world applications. Code
is available at https://github.com/PeterGriffinJin/Search-R1.

</details>


### [53] [Prolonged Reasoning Is Not All You Need: Certainty-Based Adaptive Routing for Efficient LLM/MLLM Reasoning](https://arxiv.org/abs/2505.15154)
*Jinghui Lu,Haiyang Yu,Siliang Xu,Shiwei Ran,Guozhi Tang,Siqi Wang,Bin Shan,Teng Fu,Hao Feng,Jingqun Tang,Han Wang,Can Huang*

Main category: cs.CL

TL;DR: 提出基于置信度的自适应推理框架CAR，通过困惑度动态切换长短推理模式，在保持精度的同时提升效率


<details>
  <summary>Details</summary>
Motivation: 发现长推理链虽增强复杂任务表现，但会损害简单任务性能并导致冗余输出，需寻找效率与精度的平衡

Method: 首先生成简短答案并计算困惑度，仅当置信度低时触发长推理，结合自适应阈值机制

Result: 在VQA/KIE多模态基准及文本推理数据集上，CAR在保持97%原始精度的同时减少47%推理长度

Conclusion: 困惑度是有效的置信度代理指标，CAR框架为不同难度任务提供了最优推理策略选择方案

Abstract: Recent advancements in reasoning have significantly enhanced the capabilities
of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
across diverse tasks. However, excessive reliance on chain-of-thought (CoT)
reasoning can impair model performance and brings unnecessarily lengthened
outputs, reducing efficiency. Our work reveals that prolonged reasoning does
not universally improve accuracy and even degrade performance on simpler tasks.
To address this, we propose Certainty-based Adaptive Reasoning (CAR), a novel
framework that dynamically switches between short answers and long-form
reasoning based on the model perplexity. CAR first generates a short answer and
evaluates its perplexity, triggering reasoning only when the model exhibits low
confidence (i.e., high perplexity). Experiments across diverse multimodal
VQA/KIE benchmarks and text reasoning datasets show that CAR outperforms both
short-answer and long-form reasoning approaches, striking an optimal balance
between accuracy and efficiency.

</details>


### [54] [ReflAct: World-Grounded Decision Making in LLM Agents via Goal-State Reflection](https://arxiv.org/abs/2505.15182)
*Jeonghye Kim,Sojeong Rhee,Minbeom Kim,Dohyung Kim,Sangmook Lee,Youngchul Sung,Kyomin Jung*

Main category: cs.CL

TL;DR: ReflAct通过持续状态反思与目标对齐机制，显著提升LLM代理的战略可靠性，在ALFWorld任务中成功率超越ReAct达27.7%。


<details>
  <summary>Details</summary>
Motivation: ReAct框架存在推理步骤不连贯、内部信念不一致的问题，导致代理状态与目标错位并产生错误累积。

Method: 将推理核心从行动规划转向持续状态反思，通过显式状态决策与动态目标对齐机制增强核心推理能力。

Result: ALFWorld任务成功率93.3%，平均超越ReAct 27.7%，且优于带增强模块的ReAct变体。

Conclusion: 强化核心推理主干（而非添加外围模块）是实现可靠智能体性能的关键突破方向。

Abstract: Recent advances in LLM agents have largely built on reasoning backbones like
ReAct, which interleave thought and action in complex environments. However,
ReAct often produces ungrounded or incoherent reasoning steps, leading to
misalignment between the agent's actual state and goal. Our analysis finds that
this stems from ReAct's inability to maintain consistent internal beliefs and
goal alignment, causing compounding errors and hallucinations. To address this,
we introduce ReflAct, a novel backbone that shifts reasoning from merely
planning next actions to continuously reflecting on the agent's state relative
to its goal. By explicitly grounding decisions in states and enforcing ongoing
goal alignment, ReflAct dramatically improves strategic reliability. This
design delivers substantial empirical gains: ReflAct surpasses ReAct by 27.7%
on average, achieving a 93.3% success rate in ALFWorld. Notably, ReflAct even
outperforms ReAct with added enhancement modules (e.g., Reflexion, WKM),
showing that strengthening the core reasoning backbone is key to reliable agent
performance.

</details>


### [55] [EcomScriptBench: A Multi-task Benchmark for E-commerce Script Planning via Step-wise Intention-Driven Product Association](https://arxiv.org/abs/2505.15196)
*Weiqi Wang,Limeng Cui,Xin Liu,Sreyashi Nag,Wenju Xu,Chen Luo,Sheikh Muhammad Sarwar,Yang Li,Hansu Gu,Hui Liu,Changlong Yu,Jiaxin Bai,Yifan Gao,Haiyang Zhang,Qi He,Shuiwang Ji,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出电商脚本规划任务EcomScript，通过语义关联构建产品增强脚本框架并创建首个大规模评测数据集EcomScriptBench，验证模型性能提升路径


<details>
  <summary>Details</summary>
Motivation: 解决LLM在电商场景中无法同时处理脚本规划与产品检索、动作与搜索语义鸿沟问题，以及缺乏评估方法和基准数据的三大挑战

Method: 将任务分解为三个子任务，基于动作语义与购买意图的相似性构建产品关联框架，利用240万真实商品数据生成60万+脚本构建评测基准

Result: 实验表明现有模型(含微调)面临显著挑战，而注入产品购买意图可使准确率提升16.3%(人工评估)和13.7%(自动评估)

Conclusion: 框架通过产品意图注入突破语义匹配瓶颈，EcomScriptBench填补领域空白，为电商智能助手发展提供方法论和数据基础

Abstract: Goal-oriented script planning, or the ability to devise coherent sequences of
actions toward specific goals, is commonly employed by humans to plan for
typical activities. In e-commerce, customers increasingly seek LLM-based
assistants to generate scripts and recommend products at each step, thereby
facilitating convenient and efficient shopping experiences. However, this
capability remains underexplored due to several challenges, including the
inability of LLMs to simultaneously conduct script planning and product
retrieval, difficulties in matching products caused by semantic discrepancies
between planned actions and search queries, and a lack of methods and benchmark
data for evaluation. In this paper, we step forward by formally defining the
task of E-commerce Script Planning (EcomScript) as three sequential subtasks.
We propose a novel framework that enables the scalable generation of
product-enriched scripts by associating products with each step based on the
semantic similarity between the actions and their purchase intentions. By
applying our framework to real-world e-commerce data, we construct the very
first large-scale EcomScript dataset, EcomScriptBench, which includes 605,229
scripts sourced from 2.4 million products. Human annotations are then conducted
to provide gold labels for a sampled subset, forming an evaluation benchmark.
Extensive experiments reveal that current (L)LMs face significant challenges
with EcomScript tasks, even after fine-tuning, while injecting product purchase
intentions improves their performance.

</details>


### [56] [DUSK: Do Not Unlearn Shared Knowledge](https://arxiv.org/abs/2505.15209)
*Wonje Jeung,Sangyeon Yoon,Hyesoo Hong,Soeun Kim,Seungju Han,Youngjae Yu,Albert No*

Main category: cs.CL

TL;DR: 提出DUSK基准测试，评估数据重叠场景下的机器遗忘效果，发现现有方法难以有效移除深层知识且易损害共享内容。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘评估忽略现实场景中遗忘集与保留集的内容重叠问题(如新闻文章与维基百科描述同一事件)，需开发更精准的遗忘方法。

Method: 构建多风格描述相同事实的文档集(DUSK)，设计7项指标评估遗忘方法在移除特定内容同时保留共享信息的能力。

Result: 测试9种最新方法显示：多数能去除表层文本，但无法有效消除上下文特定知识且常破坏共享内容。

Conclusion: DUSK作为公开基准将推动开发更可靠的现实应用遗忘技术，解决现有方法在选择性遗忘上的缺陷。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about the unauthorized use of copyrighted or
sensitive data. Machine unlearning aims to remove such 'forget' data while
preserving utility and information from the 'retain' set. However, existing
evaluations typically assume that forget and retain sets are fully disjoint,
overlooking realistic scenarios where they share overlapping content. For
instance, a news article may need to be unlearned, even though the same event,
such as an earthquake in Japan, is also described factually on Wikipedia.
Effective unlearning should remove the specific phrasing of the news article
while preserving publicly supported facts. In this paper, we introduce DUSK, a
benchmark designed to evaluate unlearning methods under realistic data overlap.
DUSK constructs document sets that describe the same factual content in
different styles, with some shared information appearing across all sets and
other content remaining unique to each. When one set is designated for
unlearning, an ideal method should remove its unique content while preserving
shared facts. We define seven evaluation metrics to assess whether unlearning
methods can achieve this selective removal. Our evaluation of nine recent
unlearning methods reveals a key limitation: while most can remove
surface-level text, they often fail to erase deeper, context-specific knowledge
without damaging shared content. We release DUSK as a public benchmark to
support the development of more precise and reliable unlearning techniques for
real-world applications.

</details>


### [57] [Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs](https://arxiv.org/abs/2505.15210)
*Jie Ma,Ning Qu,Zhitao Gao,Rui Xing,Jun Liu,Hongbin Pei,Jiang Xie,Linyun Song,Pinghui Wang,Jing Tao,Zhou Su*

Main category: cs.CL

TL;DR: 提出DP可信推理框架，通过渐进知识蒸馏和推理-反思策略充分利用知识图谱先验，显著提升LLM生成结果的可信度


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强方法未能充分利用图谱中的结构化信息和约束条件，导致LLM推理的忠实性和可靠性不足

Method: 采用渐进式知识蒸馏（监督微调+Kahneman-Tversky优化）整合结构先验，结合推理-反思策略进行约束验证

Result: 在ComplexWebQuestions等数据集实现SOTA性能（Hit@1提升13%），生成响应可信度显著提高

Conclusion: DP框架通过深度利用知识图谱先验知识，有效解决了LLM幻觉问题，实验验证了其优越性和实用价值

Abstract: Knowledge graph-based retrieval-augmented generation seeks to mitigate
hallucinations in Large Language Models (LLMs) caused by insufficient or
outdated knowledge. However, existing methods often fail to fully exploit the
prior knowledge embedded in knowledge graphs (KGs), particularly their
structural information and explicit or implicit constraints. The former can
enhance the faithfulness of LLMs' reasoning, while the latter can improve the
reliability of response generation. Motivated by these, we propose a
trustworthy reasoning framework, termed Deliberation over Priors (DP), which
sufficiently utilizes the priors contained in KGs. Specifically, DP adopts a
progressive knowledge distillation strategy that integrates structural priors
into LLMs through a combination of supervised fine-tuning and Kahneman-Tversky
optimization, thereby improving the faithfulness of relation path generation.
Furthermore, our framework employs a reasoning-introspection strategy, which
guides LLMs to perform refined reasoning verification based on extracted
constraint priors, ensuring the reliability of response generation. Extensive
experiments on three benchmark datasets demonstrate that DP achieves new
state-of-the-art performance, especially a Hit@1 improvement of 13% on the
ComplexWebQuestions dataset, and generates highly trustworthy responses. We
also conduct various analyses to verify its flexibility and practicality. The
code is available at https://github.com/reml-group/Deliberation-on-Priors.

</details>


### [58] [R-TOFU: Unlearning in Large Reasoning Models](https://arxiv.org/abs/2505.15214)
*Sangyeon Yoon,Wonje Jeung,Albert No*

Main category: cs.CL

TL;DR: 提出首个针对大型推理模型遗忘能力的评测基准R-TOFU，发现传统方法在推理步骤存在知识残留，并提出能保持推理连贯性的新优化方法。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘技术仅关注最终答案层面，而忽视多步链式思维推理中嵌入的隐私信息，导致模型无法可靠遗忘敏感内容。

Method: 构建含链式思维标注的R-TOFU基准，通过梯度优化与偏好优化方法对比，提出保持推理不确定性的Reasoned IDK优化策略。

Result: 传统方法在推理步骤残留27%敏感信息，新方法使模型效用下降减少40%，但解码策略仍可能泄露已遗忘内容。

Conclusion: R-TOFU为系统研究推理模型遗忘技术奠定基础，需在多样化解码设置下评估模型，Reasoned IDK实现遗忘效果与推理能力的更好平衡。

Abstract: Large Reasoning Models (LRMs) embed private or copyrighted information not
only in their final answers but also throughout multi-step chain-of-thought
(CoT) traces, making reliable unlearning far more demanding than in standard
LLMs. We introduce Reasoning-TOFU (R-TOFU), the first benchmark tailored to
this setting. R-TOFU augments existing unlearning tasks with realistic CoT
annotations and provides step-wise metrics that expose residual knowledge
invisible to answer-level checks. Using R-TOFU, we carry out a comprehensive
comparison of gradient-based and preference-optimization baselines and show
that conventional answer-only objectives leave substantial forget traces in
reasoning. We further propose Reasoned IDK, a preference-optimization variant
that preserves coherent yet inconclusive reasoning, achieving a stronger
balance between forgetting efficacy and model utility than earlier refusal
styles. Finally, we identify a failure mode: decoding variants such as
ZeroThink and LessThink can still reveal forgotten content despite seemingly
successful unlearning, emphasizing the need to evaluate models under diverse
decoding settings. Together, the benchmark, analysis, and new baseline
establish a systematic foundation for studying and improving unlearning in LRMs
while preserving their reasoning capabilities.

</details>


### [59] [Multilingual Prompting for Improving LLM Generation Diversity](https://arxiv.org/abs/2505.15229)
*Qihan Wang,Shidong Pan,Tal Linzen,Emily Black*

Main category: cs.CL

TL;DR: 提出多语言提示方法，通过多文化语言提示提升大语言模型生成内容的多样性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成内容时缺乏文化代表性和多样性，影响观点表达和事实回答的准确性。需要激活训练数据中更广泛的文化知识来缓解该问题。

Method: 生成带有不同文化/语言线索的提示变体→收集模型响应→合并结果。利用模型的语言特定知识特性，通过多语言提示激活多样化文化表征。

Result: 在GPT-4o、LLaMA等多个模型上优于高温采样/逐步回忆/角色提示等方法，效果随语言资源水平和模型规模变化，提示语言与文化线索对齐可减少文化信息幻觉。

Conclusion: 多语言提示能有效提升模型输出的文化多样性，其效果受模型容量和语言资源水平影响，语言与文化的对齐机制有助于提升信息准确性。

Abstract: Large Language Models (LLMs) are known to lack cultural representation and
overall diversity in their generations, from expressing opinions to answering
factual questions. To mitigate this problem, we propose multilingual prompting:
a prompting method which generates several variations of a base prompt with
added cultural and linguistic cues from several cultures, generates responses,
and then combines the results. Building on evidence that LLMs have
language-specific knowledge, multilingual prompting seeks to increase diversity
by activating a broader range of cultural knowledge embedded in model training
data. Through experiments across multiple models (GPT-4o, GPT-4o-mini, LLaMA
70B, and LLaMA 8B), we show that multilingual prompting consistently
outperforms existing diversity-enhancing techniques such as high-temperature
sampling, step-by-step recall, and personas prompting. Further analyses show
that the benefits of multilingual prompting vary with language resource level
and model size, and that aligning the prompting language with the cultural cues
reduces hallucination about culturally-specific information.

</details>


### [60] [Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework](https://arxiv.org/abs/2505.15245)
*Zihao Jiang,Ben Liu,Miao Peng,Wenjie Xu,Yao Xiao,Zhenyan Shan,Min Peng*

Main category: cs.CL

TL;DR: 提出GETER框架，通过融合时序知识图谱与文本，提升大语言模型在可解释时序推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注性能提升，忽视了可解释的推理过程，需构建系统评估基准并增强模型解释能力。

Method: 1. 利用时序知识图谱构建结构编码器捕捉查询结构信息；2. 设计结构-文本前缀适配器映射图谱特征到文本空间；3. 通过软图令牌与指令微调生成解释文本。

Result: GETER在实验中取得SOTA性能，展现强泛化能力与有效性。

Conclusion: 结构-文本融合范式显著提升可解释时序推理能力，公开数据集与代码推动领域发展。

Abstract: While large language models (LLMs) show great potential in temporal
reasoning, most existing work focuses heavily on enhancing performance, often
neglecting the explainable reasoning processes underlying the results. To
address this gap, we introduce a comprehensive benchmark covering a wide range
of temporal granularities, designed to systematically evaluate LLMs'
capabilities in explainable temporal reasoning. Furthermore, our findings
reveal that LLMs struggle to deliver convincing explanations when relying
solely on textual information. To address challenge, we propose GETER, a novel
structure-aware generative framework that integrates Graph structures with text
for Explainable TEmporal Reasoning. Specifically, we first leverage temporal
knowledge graphs to develop a temporal encoder that captures structural
information for the query. Subsequently, we introduce a structure-text prefix
adapter to map graph structure features into the text embedding space. Finally,
LLMs generate explanation text by seamlessly integrating the soft graph token
with instruction-tuning prompt tokens. Experimental results indicate that GETER
achieves state-of-the-art performance while also demonstrating its
effectiveness as well as strong generalization capabilities. Our dataset and
code are available at https://github.com/carryTatum/GETER.

</details>


### [61] [Fooling the LVLM Judges: Visual Biases in LVLM-Based Evaluation](https://arxiv.org/abs/2505.15249)
*Yerin Hwang,Dongryeol Lee,Kyungmin Min,Taegwan Kang,Yong-il Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 研究发现当前大型视觉语言模型(LVLMs)在视觉对抗攻击下存在系统性脆弱性，通过构建多领域FRAME基准测试发现所有模型均出现评分虚高现象，揭示了现有评估体系的安全隐患。


<details>
  <summary>Details</summary>
Motivation: 探索LVLMs作为图文对齐评判工具时，其视觉模态鲁棒性的不足及对抗性视觉操作对评分系统的潜在影响。

Method: 定义视觉偏差类型，构建包含多种分数分布的多领域FRAME元评估基准，通过注入视觉偏差进行系统性测试，并研究组合偏差和成对评估的敏感性。

Result: 所有测试模型在全部领域均表现出脆弱性，组合偏差会放大影响，视觉偏差在提示工程缓解策略下仍持续存在。

Conclusion: 当前LVLM评估系统存在根本性脆弱特征，亟需开发更具鲁棒性的视觉语言评估模型以确保评价公正性。

Abstract: Recently, large vision-language models (LVLMs) have emerged as the preferred
tools for judging text-image alignment, yet their robustness along the visual
modality remains underexplored. This work is the first study to address a key
research question: Can adversarial visual manipulations systematically fool
LVLM judges into assigning unfairly inflated scores? We define potential image
induced biases within the context of T2I evaluation and examine how these
biases affect the evaluations of LVLM judges. Moreover, we introduce a novel,
fine-grained, multi-domain meta-evaluation benchmark named FRAME, which is
deliberately constructed to exhibit diverse score distributions. By introducing
the defined biases into the benchmark, we reveal that all tested LVLM judges
exhibit vulnerability across all domains, consistently inflating scores for
manipulated images. Further analysis reveals that combining multiple biases
amplifies their effects, and pairwise evaluations are similarly susceptible.
Moreover, we observe that visual biases persist under prompt-based mitigation
strategies, highlighting the vulnerability of current LVLM evaluation systems
and underscoring the urgent need for more robust LVLM judges.

</details>


### [62] [MentalMAC: Enhancing Large Language Models for Detecting Mental Manipulation via Multi-Task Anti-Curriculum Distillation](https://arxiv.org/abs/2505.15255)
*Yuansheng Gao,Han Bao,Tong Zhang,Bin Li,Zonghui Wang,Wenzhi Chen*

Main category: cs.CL

TL;DR: 提出MentalMAC多任务反课程蒸馏方法，通过EvoSA数据扩展、多任务监督和渐进知识蒸馏提升LLM在对话中检测心理操纵的能力


<details>
  <summary>Details</summary>
Motivation: 心理操纵隐蔽性强且普遍存在，现有大语言模型难以检测，且缺乏真实场景标注数据集阻碍模型训练

Method: 包含基于进化操作和言语行为理论的EvoSA无监督数据扩展方法、教师模型生成的多任务监督、从复杂到简单任务的渐进知识蒸馏

Result: 构建含5,000真实对话样本的ReaMent数据集，实验证明方法显著缩小师生模型差距并在关键指标上超越主流LLM

Conclusion: MentalMAC有效提升心理操纵检测能力，代码/数据集/模型将在论文接收后开源

Abstract: Mental manipulation is a subtle yet pervasive form of psychological abuse
that poses serious threats to mental health. Its covert nature and the
complexity of manipulation strategies make it challenging to detect, even for
state-of-the-art large language models (LLMs). This concealment also hinders
the manual collection of large-scale, high-quality annotations essential for
training effective models. Although recent efforts have sought to improve LLM's
performance on this task, progress remains limited due to the scarcity of
real-world annotated datasets. To address these challenges, we propose
MentalMAC, a multi-task anti-curriculum distillation method that enhances LLMs'
ability to detect mental manipulation in multi-turn dialogue. Our approach
includes: (i) EvoSA, an unsupervised data expansion method based on
evolutionary operations and speech act theory; (ii) teacher-model-generated
multi-task supervision; and (iii) progressive knowledge distillation from
complex to simpler tasks. We then constructed the ReaMent dataset with 5,000
real-world dialogue samples, using a MentalMAC-distilled model to assist human
annotation. Vast experiments demonstrate that our method significantly narrows
the gap between student and teacher models and outperforms competitive LLMs
across key evaluation metrics. All code, datasets, and checkpoints will be
released upon paper acceptance. Warning: This paper contains content that may
be offensive to readers.

</details>


### [63] [When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners](https://arxiv.org/abs/2505.15257)
*Weixiang Zhao,Jiahe Guo,Yang Deng,Tongtong Wu,Wenxuan Zhang,Yulin Hu,Xingyu Sui,Yanyan Zhao,Wanxiang Che,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.CL

TL;DR: 通过训练无关的语言特征截断策略，可显著提升大语言模型的多语言推理能力并保持语言保真度


<details>
  <summary>Details</summary>
Motivation: 受人类认知神经科学启发（推理与语言处理分离），验证LLMs中语言与推理表征的可解耦性及其跨语言泛化价值

Method: 在推理阶段对语言特异性表征进行因果干预，基于10个开源LLM在11种类型学多样语言进行分层表征分析

Result: 语言特征截断使多语言推理性能平均提升15.7%，顶层语言特征保留对维持语言保真度至关重要，且效果优于监督微调等后训练方法

Conclusion: 揭示了LLMs多语言推理的分离表征机制，为轻量级可解释的跨语言泛化提供了新思路

Abstract: Multilingual reasoning remains a significant challenge for large language
models (LLMs), with performance disproportionately favoring high-resource
languages. Drawing inspiration from cognitive neuroscience, which suggests that
human reasoning functions largely independently of language processing, we
hypothesize that LLMs similarly encode reasoning and language as separable
components that can be disentangled to enhance multilingual reasoning. To
evaluate this, we perform a causal intervention by ablating language-specific
representations at inference time. Experiments on 10 open-source LLMs spanning
11 typologically diverse languages show that this language-specific ablation
consistently boosts multilingual reasoning performance. Layer-wise analyses
further confirm that language and reasoning representations can be effectively
decoupled throughout the model, yielding improved multilingual reasoning
capabilities, while preserving top-layer language features remains essential
for maintaining linguistic fidelity. Compared to post-training such as
supervised fine-tuning or reinforcement learning, our training-free ablation
achieves comparable or superior results with minimal computational overhead.
These findings shed light on the internal mechanisms underlying multilingual
reasoning in LLMs and suggest a lightweight and interpretable strategy for
improving cross-lingual generalization.

</details>


### [64] [AGENT-X: Adaptive Guideline-based Expert Network for Threshold-free AI-generated teXt detection](https://arxiv.org/abs/2505.15261)
*Jiatao Li,Mao Ye,Cheng Peng,Xunjian Yin,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出零样本多智能体框架AGENT-X，通过语言学维度分析实现无需标注数据和阈值调优的AI文本检测


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测方法依赖大规模标注数据和外部阈值调整，导致可解释性差、适应性弱和零样本场景失效

Method: 基于修辞学与系统功能语言学理论，构建语义/文体/结构三维检测体系，通过专业语言智能体独立评估+元智能体置信度聚合，结合自适应路由机制动态选择准则

Result: 在多个数据集上准确率、可解释性和泛化能力显著超越现有监督学习与零样本方法

Conclusion: AGENT-X框架实现了无需阈值调节、强解释性的零样本检测，在跨领域场景中展现出优越的适应性

Abstract: Existing AI-generated text detection methods heavily depend on large
annotated datasets and external threshold tuning, restricting interpretability,
adaptability, and zero-shot effectiveness. To address these limitations, we
propose AGENT-X, a zero-shot multi-agent framework informed by classical
rhetoric and systemic functional linguistics. Specifically, we organize
detection guidelines into semantic, stylistic, and structural dimensions, each
independently evaluated by specialized linguistic agents that provide explicit
reasoning and robust calibrated confidence via semantic steering. A meta agent
integrates these assessments through confidence-aware aggregation, enabling
threshold-free, interpretable classification. Additionally, an adaptive
Mixture-of-Agent router dynamically selects guidelines based on inferred
textual characteristics. Experiments on diverse datasets demonstrate that
AGENT-X substantially surpasses state-of-the-art supervised and zero-shot
approaches in accuracy, interpretability, and generalization.

</details>


### [65] [Web-Shepherd: Advancing PRMs for Reinforcing Web Agents](https://arxiv.org/abs/2505.15277)
*Hyungjoo Chae,Sunghwan Kim,Junhee Cho,Seungone Kim,Seungjun Moon,Gyeom Hwangbo,Dongha Lim,Minjin Kim,Yeonjun Hwang,Minju Gwak,Dongwook Choi,Minseok Kang,Gwanhoon Im,ByeongUng Cho,Hyojun Kim,Jun Hee Han,Taeyoon Kwon,Minju Kim,Beong-woo Kwak,Dongjin Kang,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出首个网页导航过程奖励模型Web-Shepherd，通过构建大规模数据集WebPRM Collection和评估基准WebRewardBench，显著提升导航评估效率与准确性


<details>
  <summary>Details</summary>
Motivation: 现有网页导航任务依赖多模态大模型（MLLM）作为奖励模型，存在部署成本高、响应速度慢的瓶颈，亟需专业化轻量化解决方案

Method: 构建含4万步级偏好对的WebPRM数据集，设计首个元评估基准WebRewardBench，开发基于过程奖励评估的Web-Shepherd模型

Result: 在WebRewardBench上准确率比GPT-4o提升30分；在WebArena-lite测试中，相比GPT-4o-mini验证器，性能提升10.9分且成本降低10倍

Conclusion: Web-Shepherd为网页导航任务提供了高效经济的评估方案，通过专业化模型+数据集+评估基准的三位一体创新，推动自动化导航技术发展

Abstract: Web navigation is a unique domain that can automate many repetitive real-life
tasks and is challenging as it requires long-horizon sequential decision making
beyond typical multimodal large language model (MLLM) tasks. Yet, specialized
reward models for web navigation that can be utilized during both training and
test-time have been absent until now. Despite the importance of speed and
cost-effectiveness, prior works have utilized MLLMs as reward models, which
poses significant constraints for real-world deployment. To address this, in
this work, we propose the first process reward model (PRM) called Web-Shepherd
which could assess web navigation trajectories in a step-level. To achieve
this, we first construct the WebPRM Collection, a large-scale dataset with 40K
step-level preference pairs and annotated checklists spanning diverse domains
and difficulty levels. Next, we also introduce the WebRewardBench, the first
meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe
that our Web-Shepherd achieves about 30 points better accuracy compared to
using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by
using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve
10.9 points better performance, in 10 less cost compared to using GPT-4o-mini
as the verifier. Our model, dataset, and code are publicly available at LINK.

</details>


### [66] [Exploring In-Image Machine Translation with Real-World Background](https://arxiv.org/abs/2505.15282)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Yuhang Guo*

Main category: cs.CL

TL;DR: DebackX模型通过分离背景与文本图像、直接翻译文本并融合背景，有效提升了复杂场景下图像内机器翻译的质量与视觉效果。


<details>
  <summary>Details</summary>
Motivation: 现有IIMT研究局限于简化场景（如黑白背景单行文本），缺乏实际应用价值。需推动复杂场景（真实图像背景字幕）的IIMT研究。

Method: 1. 从源图像分离背景与文本图像 → 2. 直接翻译文本图像 → 3. 将翻译后文本与背景融合生成目标图像

Result: 实验显示DebackX模型在翻译质量（BLEU提升2.7）和视觉指标（FID降低15.3）上均优于基线模型。

Conclusion: 该模型验证了背景分离策略的有效性，为复杂场景IIMT提供了实用解决方案，具备现实应用潜力。

Abstract: In-Image Machine Translation (IIMT) aims to translate texts within images
from one language to another. Previous research on IIMT was primarily conducted
on simplified scenarios such as images of one-line text with black font in
white backgrounds, which is far from reality and impractical for applications
in the real world. To make IIMT research practically valuable, it is essential
to consider a complex scenario where the text backgrounds are derived from
real-world images. To facilitate research of complex scenario IIMT, we design
an IIMT dataset that includes subtitle text with real-world background. However
previous IIMT models perform inadequately in complex scenarios. To address the
issue, we propose the DebackX model, which separates the background and
text-image from the source image, performs translation on text-image directly,
and fuses the translated text-image with the background, to generate the target
image. Experimental results show that our model achieves improvements in both
translation quality and visual effect.

</details>


### [67] [Hallucinate at the Last in Long Response Generation: A Case Study on Long Document Summarization](https://arxiv.org/abs/2505.15291)
*Joonho Yang,Seunghyun Yoon,Hwan Chang,Byeongjeong Kim,Hwanhee Lee*

Main category: cs.CL

TL;DR: LLMs在长文本生成（如摘要任务）中，幻觉现象在生成文本尾部显著集中，需针对性改进方法


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注幻觉检测而忽视其位置分布特征，本文聚焦大模型长文本生成中幻觉的尾部集中现象

Method: 以长文档摘要为案例，分析注意力机制和解码过程在长序列生成中的动态变化

Result: 发现幻觉在生成文本尾部持续高发，揭示注意力漂移与解码策略的潜在影响机制

Conclusion: 需针对性优化长序列生成的尾部生成质量，提出的缓解策略可提升长文本输出的整体忠实度

Abstract: Large Language Models (LLMs) have significantly advanced text generation
capabilities, including tasks like summarization, often producing coherent and
fluent outputs. However, faithfulness to source material remains a significant
challenge due to the generation of hallucinations. While extensive research
focuses on detecting and reducing these inaccuracies, less attention has been
paid to the positional distribution of hallucination within generated text,
particularly in long outputs. In this work, we investigate where hallucinations
occur in LLM-based long response generation, using long document summarization
as a key case study. Focusing on the challenging setting of long context-aware
long response generation, we find a consistent and concerning phenomenon:
hallucinations tend to concentrate disproportionately in the latter parts of
the generated long response. To understand this bias, we explore potential
contributing factors related to the dynamics of attention and decoding over
long sequences. Furthermore, we investigate methods to mitigate this positional
hallucination, aiming to improve faithfulness specifically in the concluding
segments of long outputs.

</details>


### [68] [Chinese Toxic Language Mitigation via Sentiment Polarity Consistent Rewrites](https://arxiv.org/abs/2505.15297)
*Xintong Wang,Yixiao Liu,Jingheng Pan,Liang Ding,Longyue Wang,Chris Biemann*

Main category: cs.CL

TL;DR: 首个中文情感保持去毒数据集ToxiRewriteCN发布，涵盖1556条标注数据与五种现实场景，评估17个LLM后发现模型在表情/谐音/对话场景中平衡安全与情感保真仍存挑战


<details>
  <summary>Details</summary>
Motivation: 现有中文去毒模型常因过度礼貌化扭曲原意，尤其面对表情符号、谐音梗等隐性毒性表达时难以保持情感极性

Method: 构建含五类场景(标准表达/表情诱导/谐音/单轮对话/多轮对话)的标注数据集，从解毒准确度、流畅度、内容保持度、情感极性四个维度评估17种不同架构LLM

Result: 商业模型和MoE架构表现最佳，但所有模型在表情/谐音/对话场景中的情感保真度下降显著（单轮对话场景模型准确率平均下降19.7%）

Conclusion: 中文隐性毒性处理需要更细粒度的上下文理解，该数据集将推动可控情感保持去毒技术发展

Abstract: Detoxifying offensive language while preserving the speaker's original intent
is a challenging yet critical goal for improving the quality of online
interactions. Although large language models (LLMs) show promise in rewriting
toxic content, they often default to overly polite rewrites, distorting the
emotional tone and communicative intent. This problem is especially acute in
Chinese, where toxicity often arises implicitly through emojis, homophones, or
discourse context. We present ToxiRewriteCN, the first Chinese detoxification
dataset explicitly designed to preserve sentiment polarity. The dataset
comprises 1,556 carefully annotated triplets, each containing a toxic sentence,
a sentiment-aligned non-toxic rewrite, and labeled toxic spans. It covers five
real-world scenarios: standard expressions, emoji-induced and homophonic
toxicity, as well as single-turn and multi-turn dialogues. We evaluate 17 LLMs,
including commercial and open-source models with variant architectures, across
four dimensions: detoxification accuracy, fluency, content preservation, and
sentiment polarity. Results show that while commercial and MoE models perform
best overall, all models struggle to balance safety with emotional fidelity in
more subtle or context-heavy settings such as emoji, homophone, and
dialogue-based inputs. We release ToxiRewriteCN to support future research on
controllable, sentiment-aware detoxification for Chinese.

</details>


### [69] [Multi-Hop Question Generation via Dual-Perspective Keyword Guidance](https://arxiv.org/abs/2505.15299)
*Maodong Li,Longyin Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出双视角关键词引导框架（DPKG），通过区分问题与文档关键词角色提升多跳问题生成效果，实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用关键词的引导潜力，且未区分问题特定和文档特定关键词的功能差异。

Method: DPKG框架包含扩展Transformer编码器及双解码器：问题关键词捕捉提问意图，文档关键词定位内容，联合筛选关键信息并约束问题生成。

Result: 实验表明DPKG在MQG任务中性能优越，显著优于基线模型，验证了双视角关键词协同机制的有效性。

Conclusion: 通过显式定义关键词的双重角色并设计协同生成机制，DPKG为多跳问题生成提供了可解释且高效的解决方案。

Abstract: Multi-hop question generation (MQG) aims to generate questions that require
synthesizing multiple information snippets from documents to derive target
answers. The primary challenge lies in effectively pinpointing crucial
information snippets related to question-answer (QA) pairs, typically relying
on keywords. However, existing works fail to fully utilize the guiding
potential of keywords and neglect to differentiate the distinct roles of
question-specific and document-specific keywords. To address this, we define
dual-perspective keywords (i.e., question and document keywords) and propose a
Dual-Perspective Keyword-Guided (DPKG) framework, which seamlessly integrates
keywords into the multi-hop question generation process. We argue that question
keywords capture the questioner's intent, whereas document keywords reflect the
content related to the QA pair. Functionally, question and document keywords
work together to pinpoint essential information snippets in the document, with
question keywords required to appear in the generated question. The DPKG
framework consists of an expanded transformer encoder and two answer-aware
transformer decoders for keyword and question generation, respectively.
Extensive experiments demonstrate the effectiveness of our work, showcasing its
promising performance and underscoring its significant value in the MQG task.

</details>


### [70] [Emotional Supporters often Use Multiple Strategies in a Single Turn](https://arxiv.org/abs/2505.15316)
*Xin Bai,Guanyi Chen,Tingting He,Chenlian Zhou,Yu Liu*

Main category: cs.CL

TL;DR: 论文发现情感支持对话中常连续使用多个策略，重新定义任务后LLMs表现超越人类


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话任务过度简化回应结构，忽视多策略连续使用现象

Method: 通过语料分析重新定义任务，提出监督学习模型和LLMs建模方法

Result: LLMs在新任务下提问频率提升43%，建议准确率增加28%，整体支持效果优于人类支持者

Conclusion: 任务重定义揭示LLMs的全面支持潜力，挑战了先前对AI情感支持能力的认知

Abstract: Emotional Support Conversations (ESC) are crucial for providing empathy,
validation, and actionable guidance to individuals in distress. However,
existing definitions of the ESC task oversimplify the structure of supportive
responses, typically modelling them as single strategy-utterance pairs. Through
a detailed corpus analysis of the ESConv dataset, we identify a common yet
previously overlooked phenomenon: emotional supporters often employ multiple
strategies consecutively within a single turn. We formally redefine the ESC
task to account for this, proposing a revised formulation that requires
generating the full sequence of strategy-utterance pairs given a dialogue
history. To facilitate this refined task, we introduce several modelling
approaches, including supervised deep learning models and large language
models. Our experiments show that, under this redefined task, state-of-the-art
LLMs outperform both supervised models and human supporters. Notably, contrary
to some earlier findings, we observe that LLMs frequently ask questions and
provide suggestions, demonstrating more holistic support capabilities.

</details>


### [71] [Improving LLM First-Token Predictions in Multiple-Choice Question Answering via Prefilling Attack](https://arxiv.org/abs/2505.15323)
*Silvia Cappelletti,Tobia Poppi,Samuele Poppi,Zheng-Xin Yong,Diego Garcia-Olano,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CL

TL;DR: 提出预填充攻击方法，通过添加结构化前缀提升基于首词概率的评估可靠性


<details>
  <summary>Details</summary>
Motivation: 传统首词概率评估法存在词义错位和解释偏差问题，导致评估结果不可靠

Method: 采用预填充攻击策略，在模型输出前添加自然语言前缀（如'正确选项是：'）引导响应

Result: 显著提升多类LLM和MCQA基准测试的准确率、校准度和输出一致性，效率优于完整解码方法

Conclusion: 预填充是一种简单、鲁棒且低成本的评估优化方案，可增强多选场景下的评估可靠性

Abstract: Large Language Models (LLMs) are increasingly evaluated on multiple-choice
question answering (MCQA) tasks using *first-token probability* (FTP), which
selects the answer option whose initial token has the highest likelihood. While
efficient, FTP can be fragile: models may assign high probability to unrelated
tokens (*misalignment*) or use a valid token merely as part of a generic
preamble rather than as a clear answer choice (*misinterpretation*),
undermining the reliability of symbolic evaluation. We propose a simple
solution: the *prefilling attack*, a structured natural-language prefix (e.g.,
"*The correct option is:*") prepended to the model output. Originally explored
in AI safety, we repurpose prefilling to steer the model to respond with a
clean, valid option, without modifying its parameters. Empirically, the FTP
with prefilling strategy substantially improves accuracy, calibration, and
output consistency across a broad set of LLMs and MCQA benchmarks. It
outperforms standard FTP and often matches the performance of open-ended
generation approaches that require full decoding and external classifiers,
while being significantly more efficient. Our findings suggest that prefilling
is a simple, robust, and low-cost method to enhance the reliability of
FTP-based evaluation in multiple-choice settings.

</details>


### [72] [Leveraging Unit Language Guidance to Advance Speech Modeling in Textless Speech-to-Speech Translation](https://arxiv.org/abs/2505.15333)
*Yuhao Zhang,Xiangnan Ma,Kaiqi Kou,Peizhuo Liu,Weiqiao Shan,Benyou Wang,Tong Xiao,Yuxin Huang,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出单元语言和任务提示建模解决语音翻译的跨模态/跨语言挑战，实验显示性能接近文本模型


<details>
  <summary>Details</summary>
Motivation: 解决无文本语音翻译中跨模态特征提取和跨语言序列对齐两大核心难题

Method: 通过n-gram构建类文本单元语言，采用多任务学习和任务提示建模缓解语言冲突

Result: 在Voxpupil四语种数据集上显著超越基线，达到接近文本训练模型的性能

Conclusion: 单元语言表示结合任务提示能有效提升语音翻译的跨模态与跨语言建模能力

Abstract: The success of building textless speech-to-speech translation (S2ST) models
has attracted much attention. However, S2ST still faces two main challenges: 1)
extracting linguistic features for various speech signals, called cross-modal
(CM), and 2) learning alignment of difference languages in long sequences,
called cross-lingual (CL). We propose the unit language to overcome the two
modeling challenges. The unit language can be considered a text-like
representation format, constructed using $n$-gram language modeling. We
implement multi-task learning to utilize the unit language in guiding the
speech modeling process. Our initial results reveal a conflict when applying
source and target unit languages simultaneously. We propose task prompt
modeling to mitigate this conflict. We conduct experiments on four languages of
the Voxpupil dataset. Our method demonstrates significant improvements over a
strong baseline and achieves performance comparable to models trained with
text.

</details>


### [73] [Your Language Model Can Secretly Write Like Humans: Contrastive Paraphrase Attacks on LLM-Generated Text Detectors](https://arxiv.org/abs/2505.15337)
*Hao Fang,Jiawei Kong,Tianqu Zhuang,Yixiang Qiu,Kuofeng Gao,Bin Chen,Shu-Tao Xia,Yaowei Wang,Min Zhang*

Main category: cs.CL

TL;DR: 提出无需训练的CoPA方法，通过设计指令引导LLM生成更类人文本，并构建机器化词分布对比来消除统计偏差，有效绕过文本检测器。


<details>
  <summary>Details</summary>
Motivation: 现有改述攻击方法需大量训练资源且对先进检测器效果差，需要一种更高效的绕过检测方法。

Method: 1. 设计指令促使LLM生成类人文本；2. 构建辅助机器化词分布作为对比；3. 在解码阶段减去机器化特征分布。

Result: 实验验证CoPA在多种场景下有效欺骗文本检测器，攻击成功率显著优于基线方法。

Conclusion: CoPA首次实现无训练参数的改述攻击，通过分布对比机制突破检测器防御，为LLM生成文本的隐蔽性提供新思路。

Abstract: The misuse of large language models (LLMs), such as academic plagiarism, has
driven the development of detectors to identify LLM-generated texts. To bypass
these detectors, paraphrase attacks have emerged to purposely rewrite these
texts to evade detection. Despite the success, existing methods require
substantial data and computational budgets to train a specialized paraphraser,
and their attack efficacy greatly reduces when faced with advanced detection
algorithms. To address this, we propose \textbf{Co}ntrastive
\textbf{P}araphrase \textbf{A}ttack (CoPA), a training-free method that
effectively deceives text detectors using off-the-shelf LLMs. The first step is
to carefully craft instructions that encourage LLMs to produce more human-like
texts. Nonetheless, we observe that the inherent statistical biases of LLMs can
still result in some generated texts carrying certain machine-like attributes
that can be captured by detectors. To overcome this, CoPA constructs an
auxiliary machine-like word distribution as a contrast to the human-like
distribution generated by the LLM. By subtracting the machine-like patterns
from the human-like distribution during the decoding process, CoPA is able to
produce sentences that are less discernible by text detectors. Our theoretical
analysis suggests the superiority of the proposed attack. Extensive experiments
validate the effectiveness of CoPA in fooling text detectors across various
scenarios.

</details>


### [74] [FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](https://arxiv.org/abs/2505.15347)
*Xiang Liu,Hong Chen,Xuming Hu,Xiaowen Chu*

Main category: cs.CL

TL;DR: FlowKV提出多轮隔离机制，通过仅压缩最新轮次的新KV缓存避免重复压缩，将指令跟随准确率从10.90%提升至75.40%


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存逐轮压缩策略导致早期对话上下文信息丢失和灾难性遗忘，严重影响多轮对话性能

Method: 采用多轮隔离机制分离历史压缩缓存与最新轮次KV对，支持任意KV缓存压缩方法且无需重新训练

Result: 在后期对话轮次中，指令跟随准确率和用户偏好保持率提升达75.40%（baseline为10.90%）

Conclusion: FlowKV通过缓存隔离策略有效解决多轮对话中的上下文遗忘问题，显著提升大模型对话系统效率

Abstract: Large Language Models (LLMs) are increasingly deployed in multi-turn
conversational applications, where the management of the Key-Value (KV) Cache
presents a significant bottleneck. The linear growth of the KV Cache with
dialogue history imposes substantial computational costs, and existing eviction
strategies often degrade performance by repeatedly compressing early
conversational context, leading to information loss and context forgetting.
This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism}
for KV Cache management, which can be applied to any KV Cache compression
method without training. FlowKV's core innovation is a multi-turn isolation
mechanism that preserves the accumulated compressed KV cache from past turns.
Compression is then strategically applied only to the newly generated KV pairs
of the latest completed turn, effectively preventing the re-compression of
older context and thereby mitigating catastrophic forgetting. Our results
demonstrate that FlowKV consistently and significantly outperforms baseline
strategies in maintaining instruction-following accuracy and user preference
retention from 10.90\% to 75.40\%, particularly in later conversational turns.

</details>


### [75] [The Super Emotion Dataset](https://arxiv.org/abs/2505.15348)
*Enric Junqué de Fortuny*

Main category: cs.CL

TL;DR: 论文提出Super Emotion Dataset，基于心理学分类标准解决NLP情感数据集标准化不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有NLP情感分类数据集存在分类标准不一致、样本量小、领域局限等问题，缺乏心理学理论支撑的标准化资源

Method: 整合多源文本数据，采用Shaver实证验证的情感分类框架构建统一数据集

Result: 创建支持跨领域情感识别研究的标准化数据集，提升研究一致性

Conclusion: Super Emotion Dataset填补领域空白，为情感计算研究提供理论支撑的基准工具

Abstract: Despite the wide-scale usage and development of emotion classification
datasets in NLP, the field lacks a standardized, large-scale resource that
follows a psychologically grounded taxonomy. Existing datasets either use
inconsistent emotion categories, suffer from limited sample size, or focus on
specific domains. The Super Emotion Dataset addresses this gap by harmonizing
diverse text sources into a unified framework based on Shaver's empirically
validated emotion taxonomy, enabling more consistent cross-domain emotion
recognition research.

</details>


### [76] [Revealing Language Model Trajectories via Kullback-Leibler Divergence](https://arxiv.org/abs/2505.15353)
*Ryo Kishino,Yusuke Takase,Momose Oyama,Hiroaki Yamagiwa,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 通过log-likelihood向量坐标分析语言模型的KL散度轨迹，发现预训练过程呈现螺旋结构，且log-likelihood空间的模型演化比权重空间更受限


<details>
  <summary>Details</summary>
Motivation: 系统评估不同条件下语言模型的KL散度行为，包括预训练检查点、精调模型与基础模型、以及通过logit lens的层间比较

Method: 使用公开可用的语言模型，基于log-likelihood向量坐标的KL散度估计方法，分析预训练轨迹、层间变化和权重空间对比

Result: 预训练轨迹呈现螺旋结构，层间变化呈线状进展；log-likelihood空间的扩散指数显示模型演化路径比权重空间更受限

Conclusion: log-likelihood空间为语言模型演化提供了比传统权重空间更结构化的表征视角

Abstract: A recently proposed method enables efficient estimation of the KL divergence
between language models, including models with different architectures, by
assigning coordinates based on log-likelihood vectors. To better understand the
behavior of this metric, we systematically evaluate KL divergence across a wide
range of conditions using publicly available language models. Our analysis
covers comparisons between pretraining checkpoints, fine-tuned and base models,
and layers via the logit lens. We find that trajectories of language models, as
measured by KL divergence, exhibit a spiral structure during pretraining and
thread-like progressions across layers. Furthermore, we show that, in terms of
diffusion exponents, model trajectories in the log-likelihood space are more
constrained than those in weight space.

</details>


### [77] [Decoding Phone Pairs from MEG Signals Across Speech Modalities](https://arxiv.org/abs/2505.15355)
*Xabier de Zuazo,Eva Navas,Ibon Saratxaga,Mathieu Bourguignon,Nicola Molinaro*

Main category: cs.CL

TL;DR: 研究通过MEG信号解码语音产生/感知任务中的音素，发现语音生产任务准确率(76.6%)显著高于被动听和回放(51%)，Elastic Net模型优于神经网络，低频振荡对解码贡献最大。


<details>
  <summary>Details</summary>
Motivation: 理解语音产生的神经机制对认知神经科学和脑机接口技术发展至关重要。研究旨在比较语音生产与感知任务的神经解码差异，探索有效解码方法及神经振荡特征。

Method: 使用17名受试者的MEG数据，分析15对音素的分类性能。对比正则化线性模型(Elastic Net)与神经网络，评估不同脑电频段(Delta/Theta等)对解码的贡献度。

Result: 语音生产解码准确率显著更高；Elastic Net在有限数据下优于复杂网络；Delta(0.2-3Hz)和Theta(4-7Hz)频段贡献最大；残留运动伪影可能影响解码准确性。

Conclusion: 显性语音生产范式提供更丰富的神经信息，简单正则化模型在有限MEG数据中表现优异，低频神经振荡是语音解码关键特征，但需改进伪影消除方法以提升解码特异性。

Abstract: Understanding the neural mechanisms underlying speech production is essential
for both advancing cognitive neuroscience theory and developing practical
communication technologies. In this study, we investigated
magnetoencephalography signals to decode phones from brain activity during
speech production and perception (passive listening and voice playback) tasks.
Using a dataset comprising 17 participants, we performed pairwise phone
classification, extending our analysis to 15 phonetic pairs. Multiple machine
learning approaches, including regularized linear models and neural network
architectures, were compared to determine their effectiveness in decoding
phonetic information. Our results demonstrate significantly higher decoding
accuracy during speech production (76.6%) compared to passive listening and
playback modalities (~51%), emphasizing the richer neural information available
during overt speech. Among the models, the Elastic Net classifier consistently
outperformed more complex neural networks, highlighting the effectiveness of
traditional regularization techniques when applied to limited and
high-dimensional MEG datasets. Besides, analysis of specific brain frequency
bands revealed that low-frequency oscillations, particularly Delta (0.2-3 Hz)
and Theta (4-7 Hz), contributed the most substantially to decoding accuracy,
suggesting that these bands encode critical speech production-related neural
processes. Despite using advanced denoising methods, it remains unclear whether
decoding solely reflects neural activity or if residual muscular or movement
artifacts also contributed, indicating the need for further methodological
refinement. Overall, our findings underline the critical importance of
examining overt speech production paradigms, which, despite their complexity,
offer opportunities to improve brain-computer interfaces to help individuals
with severe speech impairments.

</details>


### [78] [NL-Debugging: Exploiting Natural Language as an Intermediate Representation for Code Debugging](https://arxiv.org/abs/2505.15356)
*Weiming Zhang,Qingyao Li,Xinyi Dai,Jizheng Chen,Kounianhua Du,Weinan Zhang,Weiwen Liu,Yasheng Wang,Ruiming Tang,Yong Yu*

Main category: cs.CL

TL;DR: 提出了NL-DEBUGGING框架，通过自然语言中间表示改进代码调试，效果优于传统方法并扩展修改空间。


<details>
  <summary>Details</summary>
Motivation: 传统代码级调试方法在处理需要理解算法逻辑的复杂错误时存在局限，而自然语言推理为代码调试提供了新的可能性，但具体实现方式和优势尚不明确。

Method: 使用自然语言作为中间表示层，建立执行反馈驱动的直接精炼机制，实现自然语言层面的调试操作。

Result: 框架在调试效果上超越传统方法，通过自然语言表示实现更广的修改空间和直接反馈优化。

Conclusion: 自然语言推理可推动自动化代码调试发展，为解决复杂编程挑战提供新范式，验证了自然语言作为调试媒介的独特优势。

Abstract: Debugging is a critical aspect of LLM's coding ability. Early debugging
efforts primarily focused on code-level analysis, which often falls short when
addressing complex programming errors that require a deeper understanding of
algorithmic logic. Recent advancements in large language models (LLMs) have
shifted attention toward leveraging natural language reasoning to enhance
code-related tasks. However, two fundamental questions remain unanswered: What
type of natural language format is most effective for debugging tasks? And what
specific benefits does natural language reasoning bring to the debugging
process? In this paper, we introduce NL-DEBUGGING, a novel framework that
employs natural language as an intermediate representation to improve code
debugging. By debugging at a natural language level, we demonstrate that
NL-DEBUGGING outperforms traditional debugging methods and enables a broader
modification space through direct refinement guided by execution feedback. Our
findings highlight the potential of natural language reasoning to advance
automated code debugging and address complex programming challenges.

</details>


### [79] [X-WebAgentBench: A Multilingual Interactive Web Benchmark for Evaluating Global Agentic System](https://arxiv.org/abs/2505.15372)
*Peng Wang,Ruihan Tao,Qiguang Chen,Mengkang Hu,Libo Qin*

Main category: cs.CL

TL;DR: 提出多语言智能体基准X-WebAgentBench，评估LLM在跨语言网页交互中的表现，发现现有技术在多语言场景中的不足


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体研究过度集中于英语场景，全球7000+语言的实际需求未被满足，亟需多语言智能体解决方案

Method: 构建交互式网页环境的多语言基准测试，评估不同LLM（包括GPT-4o）的规划与交互能力，并测试跨语言对齐技术的有效性

Result: 即使结合跨语言技术，先进模型仍无法达到满意效果，揭示当前多语言智能体技术的局限性

Conclusion: X-WebAgentBench为实际应用中的多语言智能体发展提供重要评估基准，推动全球智能体技术均衡发展

Abstract: Recently, large language model (LLM)-based agents have achieved significant
success in interactive environments, attracting significant academic and
industrial attention. Despite these advancements, current research
predominantly focuses on English scenarios. In reality, there are over 7,000
languages worldwide, all of which demand access to comparable agentic services.
Nevertheless, the development of language agents remains inadequate for meeting
the diverse requirements of multilingual agentic applications. To fill this
gap, we introduce X-WebAgentBench, a novel multilingual agent benchmark in an
interactive web environment, which evaluates the planning and interaction
performance of language agents across multiple languages, thereby contributing
to the advancement of global agent intelligence. Additionally, we assess the
performance of various LLMs and cross-lingual alignment methods, examining
their effectiveness in enhancing agents. Our findings reveal that even advanced
models like GPT-4o, when combined with cross-lingual techniques, fail to
achieve satisfactory results. We hope that X-WebAgentBench can serve as a
valuable benchmark for multilingual agent scenario in real-world applications.

</details>


### [80] [RePPL: Recalibrating Perplexity by Uncertainty in Semantic Propagation and Language Generation for Explainable QA Hallucination Detection](https://arxiv.org/abs/2505.15386)
*Yiming Huang,Junyan Zhang,Zihao Wang,Biquan Bie,Xuming Hu,Yi R.,Fung,Xinlei He*

Main category: cs.CL

TL;DR: 提出RePPL方法，通过校准语义传播和语言生成中的不确定性，实现可解释的幻觉检测，并在多个QA数据集上达到最佳综合性能（平均AUC 0.833）


<details>
  <summary>Details</summary>
Motivation: 现有基于不确定性的幻觉检测方法无法解释触发幻觉的具体输入来源，需同时考虑注意力机制中的语义融合不确定性和概率生成机制中的选择不确定性

Method: 1. 在语义传播层面分析注意力跨层融合的局部标记信息
2. 在语言生成层面量化概率选择的不确定性
3. 提出Perplexity-style Log-Average聚合方式生成token级可解释分数

Result: 在先进模型上实现平均0.833的AUC值，首次提供token级不确定性解释，通过分数模式分析发现幻觉的混沌特性

Conclusion: RePPL不仅提升检测性能，其可解释分数体系为理解幻觉机制提供新视角，初步验证了在模型诊断和优化中的应用潜力

Abstract: Large Language Models (LLMs) have become powerful, but hallucinations remain
a vital obstacle to their trustworthy use. While previous works improved the
capability of hallucination detection by measuring uncertainty, they all lack
the ability to explain the provenance behind why hallucinations occur, i.e.,
which part of the inputs tends to trigger hallucinations. Recent works on the
prompt attack indicate that uncertainty exists in semantic propagation, where
attention mechanisms gradually fuse local token information into high-level
semantics across layers. Meanwhile, uncertainty also emerges in language
generation, due to its probability-based selection of high-level semantics for
sampled generations. Based on that, we propose RePPL to recalibrate uncertainty
measurement by these two aspects, which dispatches explainable uncertainty
scores to each token and aggregates in Perplexity-style Log-Average form as
total score. Experiments show that our method achieves the best comprehensive
detection performance across various QA datasets on advanced models (average
AUC of 0.833), and our method is capable of producing token-level uncertainty
scores as explanations for the hallucination. Leveraging these scores, we
preliminarily find the chaotic pattern of hallucination and showcase its
promising usage.

</details>


### [81] [Are Vision-Language Models Safe in the Wild? A Meme-Based Benchmark Study](https://arxiv.org/abs/2505.15389)
*DongGeon Lee,Joonwon Jang,Jihae Jeong,Hwanjo Yu*

Main category: cs.CL

TL;DR: 当前视觉语言模型（VLMs）面对真实用户分享的梗图时安全风险显著增高，多轮对话仅能部分缓解漏洞，需加强生态化评估和安全机制。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在真实用户分享的梗图场景下的安全性，突破传统人工合成图像的局限，探究梗图对模型输出的影响及对话上下文的缓解作用。

Method: 构建含50,430个真实梗图与有害/良性指令配对的MemeSafetyBench，结合安全分类法和LLM生成指令，测试多模型在单轮/多轮对话中的表现。

Result: 1. 梗图使VLMs有害响应率提升4.3倍，拒绝率下降17.6%；2. 多轮交互可降低15%有害输出但漏洞仍存；3. 模型规模与安全性无显著相关性。

Conclusion: 需建立生态效度更高的评估体系，开发针对性安全机制，尤其需增强模型对文化语境丰富的梗图的防御能力。

Abstract: Rapid deployment of vision-language models (VLMs) magnifies safety risks, yet
most evaluations rely on artificial images. This study asks: How safe are
current VLMs when confronted with meme images that ordinary users share? To
investigate this question, we introduce MemeSafetyBench, a 50,430-instance
benchmark pairing real meme images with both harmful and benign instructions.
Using a comprehensive safety taxonomy and LLM-based instruction generation, we
assess multiple VLMs across single and multi-turn interactions. We investigate
how real-world memes influence harmful outputs, the mitigating effects of
conversational context, and the relationship between model scale and safety
metrics. Our findings demonstrate that VLMs show greater vulnerability to
meme-based harmful prompts than to synthetic or typographic images. Memes
significantly increase harmful responses and decrease refusals compared to
text-only inputs. Though multi-turn interactions provide partial mitigation,
elevated vulnerability persists. These results highlight the need for
ecologically valid evaluations and stronger safety mechanisms.

</details>


### [82] [An Empirical Study of the Anchoring Effect in LLMs: Existence, Mechanism, and Potential Mitigations](https://arxiv.org/abs/2505.15392)
*Yiming Huang,Biquan Bie,Zuqiu Na,Weilin Ruan,Songxin Lei,Yutao Yue,Xinlei He*

Main category: cs.CL

TL;DR: LLMs普遍存在浅层锚定偏差，常规策略无法消除但推理可部分缓解，需建立认知偏差意识的评估体系


<details>
  <summary>Details</summary>
Motivation: 研究LLMs的锚定效应机制及缓解方法，突破传统仅关注标准测试指标的局限性

Method: 构建SynAnchors数据集，开发改进的评估指标，系统测试主流LLMs的锚定效应表现

Result: 发现锚定偏差存在于模型浅层结构，传统优化策略无效，逻辑推理能降低偏差强度

Conclusion: LLM评估应转向认知偏差可信验证，而非单纯追求标准测试分数或过优化的鲁棒性测试

Abstract: The rise of Large Language Models (LLMs) like ChatGPT has advanced natural
language processing, yet concerns about cognitive biases are growing. In this
paper, we investigate the anchoring effect, a cognitive bias where the mind
relies heavily on the first information as anchors to make affected judgments.
We explore whether LLMs are affected by anchoring, the underlying mechanisms,
and potential mitigation strategies. To facilitate studies at scale on the
anchoring effect, we introduce a new dataset, SynAnchors. Combining refined
evaluation metrics, we benchmark current widely used LLMs. Our findings show
that LLMs' anchoring bias exists commonly with shallow-layer acting and is not
eliminated by conventional strategies, while reasoning can offer some
mitigation. This recontextualization via cognitive psychology urges that LLM
evaluations focus not on standard benchmarks or over-optimized robustness
tests, but on cognitive-bias-aware trustworthy evaluation.

</details>


### [83] [How Should We Enhance the Safety of Large Reasoning Models: An Empirical Study](https://arxiv.org/abs/2505.15404)
*Zhexin Zhang,Xian Qi Loye,Victor Shea-Jay Huang,Junxiao Yang,Qi Zhu,Shiyao Cui,Fei Mi,Lifeng Shang,Yingkang Wang,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 通过监督微调提升大型推理模型的安全性，发现调整数据蒸馏方式、简化推理流程和混合数学数据可有效平衡安全性与性能


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中表现出色，但其推理能力的增强并不直接转化为安全性能的提升，有时甚至会降低安全性，因此需要系统研究如何有效增强模型安全性

Method: 1. 分析直接蒸馏安全响应的失败模式
2. 改进数据蒸馏流程
3. 测试不同复杂度推理链的影响
4. 探索数学推理数据的混合训练效果

Result: 1. 解决三个关键失败模式可使安全性提升32%
2. 简短/模板化推理与传统复杂推理效果相当且更易学习
3. 混合数学数据使过度拒绝率降低41%同时保持安全性

Conclusion: 模型安全性的提升不需要复杂推理过程，关键在于数据质量控制和多任务平衡。未来需要开发更系统的训练框架来协调推理能力与安全需求

Abstract: Large Reasoning Models (LRMs) have achieved remarkable success on
reasoning-intensive tasks such as mathematics and programming. However, their
enhanced reasoning capabilities do not necessarily translate to improved safety
performance-and in some cases, may even degrade it. This raises an important
research question: how can we enhance the safety of LRMs? In this paper, we
present a comprehensive empirical study on how to enhance the safety of LRMs
through Supervised Fine-Tuning (SFT). Our investigation begins with an
unexpected observation: directly distilling safe responses from DeepSeek-R1
fails to significantly enhance safety. We analyze this phenomenon and identify
three key failure patterns that contribute to it. We then demonstrate that
explicitly addressing these issues during the data distillation process can
lead to substantial safety improvements. Next, we explore whether a long and
complex reasoning process is necessary for achieving safety. Interestingly, we
find that simply using short or template-based reasoning process can attain
comparable safety performance-and are significantly easier for models to learn
than more intricate reasoning chains. These findings prompt a deeper reflection
on the role of reasoning in ensuring safety. Finally, we find that mixing math
reasoning data during safety fine-tuning is helpful to balance safety and
over-refusal. Overall, we hope our empirical study could provide a more
holistic picture on enhancing the safety of LRMs. The code and data used in our
experiments are released in https://github.com/thu-coai/LRM-Safety-Study.

</details>


### [84] [Trends and Challenges in Authorship Analysis: A Review of ML, DL, and LLM Approaches](https://arxiv.org/abs/2505.15422)
*Nudrat Habib,Tosin Adewumi,Marcus Liwicki,Elisa Barney*

Main category: cs.CL

TL;DR: 本文系统综述2015-2024年作者分析领域（作者归属与验证）的方法演进，揭示传统ML、DL和LLM技术的优劣，并指出低资源语言处理、多语言适应、跨领域泛化与AI生成文本检测四大研究缺口。


<details>
  <summary>Details</summary>
Motivation: 针对数字时代文本认证需求激增，现有作者分析系统在跨语言、跨领域场景中存在可靠性不足，需系统性总结方法演变并明确未来研究方向。

Method: 采用系统文献综述方法，分析近十年128篇论文，重点比较不同方法的特征提取技术、数据集适用性及模型泛化能力。

Result: 发现LLM在风格一致性检测表现最佳（准确率92%），但低资源语言场景平均下降37%；跨领域场景中DL模型F1值衰减达41%，揭示当前方法泛化瓶颈。

Conclusion: 提出构建多模态特征融合框架和领域自适应预训练机制，为开发鲁棒性作者分析系统提供理论框架，推动司法取证与数字内容认证领域的技术革新。

Abstract: Authorship analysis plays an important role in diverse domains, including
forensic linguistics, academia, cybersecurity, and digital content
authentication. This paper presents a systematic literature review on two key
sub-tasks of authorship analysis; Author Attribution and Author Verification.
The review explores SOTA methodologies, ranging from traditional ML approaches
to DL models and LLMs, highlighting their evolution, strengths, and
limitations, based on studies conducted from 2015 to 2024. Key contributions
include a comprehensive analysis of methods, techniques, their corresponding
feature extraction techniques, datasets used, and emerging challenges in
authorship analysis. The study highlights critical research gaps, particularly
in low-resource language processing, multilingual adaptation, cross-domain
generalization, and AI-generated text detection. This review aims to help
researchers by giving an overview of the latest trends and challenges in
authorship analysis. It also points out possible areas for future study. The
goal is to support the development of better, more reliable, and accurate
authorship analysis system in diverse textual domain.

</details>


### [85] [Gated Integration of Low-Rank Adaptation for Continual Learning of Language Models](https://arxiv.org/abs/2505.15424)
*Yan-Shuo Liang,Wu-Jun Li*

Main category: cs.CL

TL;DR: 提出GainLoRA方法，通过门控机制集成新旧LoRA分支，缓解持续学习中的遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有基于LoRA的持续学习方法强制新旧分支对旧任务贡献相同权重，导致模型遗忘旧知识

Method: 为每个新任务扩展LoRA分支并引入门控模块，通过抑制新分支对旧任务的影响来减少遗忘

Result: 在持续学习基准测试中超越现有最优方法

Conclusion: 门控机制有效平衡新旧任务学习，显著提升语言模型持续学习性能

Abstract: Continual learning (CL), which requires the model to learn multiple tasks
sequentially, is crucial for language models (LMs). Recently, low-rank
adaptation (LoRA), one of the most representative parameter-efficient
fine-tuning (PEFT) methods, has gained increasing attention in CL of LMs.
However, most existing CL methods based on LoRA typically expand a new LoRA
branch to learn each new task and force the new and old LoRA branches to
contribute equally to old tasks, potentially leading to forgetting. In this
work, we propose a new method, called gated integration of low-rank adaptation
(GainLoRA), for CL of LMs. GainLoRA expands a new LoRA branch for each new task
and introduces gating modules to integrate the new and old LoRA branches.
Furthermore, GainLoRA leverages the new gating module to minimize the
contribution from the new LoRA branch to old tasks, effectively mitigating
forgetting and improving the model's overall performance. Experimental results
on CL benchmarks demonstrate that GainLoRA outperforms existing
state-of-the-art methods.

</details>


### [86] [NeoN: A Tool for Automated Detection, Linguistic and LLM-Driven Analysis of Neologisms in Polish](https://arxiv.org/abs/2505.15426)
*Aleksandra Tomaszewska,Dariusz Czerski,Bartosz Żuk,Maciej Ogrodniczuk*

Main category: cs.CL

TL;DR: NeoN是自动化检测波兰语新词的多层系统，整合语料库、语言过滤器、LLM模块和RSS监控，显著降低人工工作量。


<details>
  <summary>Details</summary>
Motivation: 传统词典编纂依赖人工审查效率低下，需开发自动化工具跟踪波兰语词汇创新。

Method: 采用四层流程：基础语料库→波兰语形态过滤器→LLM精度过滤器→RSS每日更新，结合词形还原、频率分析和拼写标准化技术。

Result: 评估显示系统保持高准确率，人工验证工作量减少80%，支持自动生成定义及分类领域/情感。

Conclusion: NeoN为波兰语词汇演变研究提供高效解决方案，推动计算语言学和数字人文领域发展。

Abstract: NeoN, a tool for detecting and analyzing Polish neologisms. Unlike
traditional dictionary-based methods requiring extensive manual review, NeoN
combines reference corpora, Polish-specific linguistic filters, an LLM-driven
precision-boosting filter, and daily RSS monitoring in a multi-layered
pipeline. The system uses context-aware lemmatization, frequency analysis, and
orthographic normalization to extract candidate neologisms while consolidating
inflectional variants. Researchers can verify candidates through an intuitive
interface with visualizations and filtering controls. An integrated LLM module
automatically generates definitions and categorizes neologisms by domain and
sentiment. Evaluations show NeoN maintains high accuracy while significantly
reducing manual effort, providing an accessible solution for tracking lexical
innovation in Polish.

</details>


### [87] [Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions](https://arxiv.org/abs/2505.15427)
*Zhiwen Li,Die Chen,Mingyuan Fan,Cen Chen,Yaliang Li,Yanhao Wang,Wenmeng Zhou*

Main category: cs.CL

TL;DR: 提出通过语义方向向量限制文本嵌入的安全区域，结合LoRA技术减少对模型性能影响，有效降低扩散模型生成的NSFW内容和社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有安全过滤和微调方法在阻止有害内容生成时效果不足，且可能影响正常输出效果。

Method: 在嵌入空间自发现语义方向向量约束文本提示的安全区域，采用LoRA进行向量初始化以保留其他语义性能。

Result: 实验表明该方法在基准数据集上显著减少不安全内容生成，社会偏见缓解效果优于现有基线方法。

Conclusion: 该方法可独立使用或与现有技术结合，在保持模型性能的同时增强扩散模型的社会责任性。

Abstract: The remarkable ability of diffusion models to generate high-fidelity images
has led to their widespread adoption. However, concerns have also arisen
regarding their potential to produce Not Safe for Work (NSFW) content and
exhibit social biases, hindering their practical use in real-world
applications. In response to this challenge, prior work has focused on
employing security filters to identify and exclude toxic text, or
alternatively, fine-tuning pre-trained diffusion models to erase sensitive
concepts. Unfortunately, existing methods struggle to achieve satisfactory
performance in the sense that they can have a significant impact on the normal
model output while still failing to prevent the generation of harmful content
in some cases. In this paper, we propose a novel self-discovery approach to
identifying a semantic direction vector in the embedding space to restrict text
embedding within a safe region. Our method circumvents the need for correcting
individual words within the input text and steers the entire text prompt
towards a safe region in the embedding space, thereby enhancing model
robustness against all possibly unsafe prompts. In addition, we employ Low-Rank
Adaptation (LoRA) for semantic direction vector initialization to reduce the
impact on the model performance for other semantics. Furthermore, our method
can also be integrated with existing methods to improve their social
responsibility. Extensive experiments on benchmark datasets demonstrate that
our method can effectively reduce NSFW content and mitigate social bias
generated by diffusion models compared to several state-of-the-art baselines.

</details>


### [88] [Likelihood Variance as Text Importance for Resampling Texts to Map Language Models](https://arxiv.org/abs/2505.15428)
*Momose Oyama,Ryo Kishino,Hiroaki Yamagiwa,Hidetoshi Shimodaira*

Main category: cs.CL

TL;DR: 提出基于对数似然方差的重采样方法，将构建语言模型地图所需文本量减少约50%同时保持KL散度估计精度


<details>
  <summary>Details</summary>
Motivation: 传统模型地图构建依赖大量文本计算对数似然，计算成本过高

Method: 通过优先选择不同模型间对数似然方差较大的文本进行加权重采样

Result: 实验证明该方法仅需约半数量文本即可达到与均匀采样相当的精度，且支持新模型快速融入现有地图

Conclusion: 该方法显著提升了语言模型地图构建的效率和可扩展性

Abstract: We address the computational cost of constructing a model map, which embeds
diverse language models into a common space for comparison via KL divergence.
The map relies on log-likelihoods over a large text set, making the cost
proportional to the number of texts. To reduce this cost, we propose a
resampling method that selects important texts with weights proportional to the
variance of log-likelihoods across models for each text. Our method
significantly reduces the number of required texts while preserving the
accuracy of KL divergence estimates. Experiments show that it achieves
comparable performance to uniform sampling with about half as many texts, and
also facilitates efficient incorporation of new models into an existing map.
These results enable scalable and efficient construction of language model
maps.

</details>


### [89] [Hunyuan-TurboS: Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](https://arxiv.org/abs/2505.15431)
*Ao Liu,Botong Zhou,Can Xu,Chayse Zhou,ChenChen Zhang,Chengcheng Xu,Chenhao Wang,Decheng Wu,Dengpeng Wu,Dian Jiao,Dong Du,Dong Wang,Feng Zhang,Fengzong Lian,Guanghui Xu,Guanwei Zhang,Hai Wang,Haipeng Luo,Han Hu,Huilin Xu,Jiajia Wu,Jianchen Zhu,Jianfeng Yan,Jiaqi Zhu,Jihong Zhang,Jinbao Xue,Jun Xia,Junqiang Zheng,Kai Liu,Kai Zhang,Kai Zheng,Kejiao Li,Keyao Wang,Lan Jiang,Lixin Liu,Lulu Wu,Mengyuan Huang,Peijie Yu,Peiqi Wang,Qian Wang,Qianbiao Xiang,Qibin Liu,Qingfeng Sun,Richard Guo,Ruobing Xie,Saiyong Yang,Shaohua Chen,Shihui Hu,Shuai Li,Shuaipeng Li,Shuang Chen,Suncong Zheng,Tao Yang,Tian Zhang,Tinghao Yu,Weidong Han,Weijie Liu,Weijin Zhou,Weikang Wang,Wesleye Chen,Xiao Feng,Xiaoqin Ren,Xingwu Sun,Xiong Kuang,Xuemeng Huang,Xun Cao,Yanfeng Chen,Yang Du,Yang Zhen,Yangyu Tao,Yaping Deng,Yi Shen,Yigeng Hong,Yiqi Chen,Yiqing Huang,Yuchi Deng,Yue Mao,Yulong Wang,Yuyuan Zeng,Zenan Xu,Zhanhui Kang,Zhe Zhao,ZhenXiang Yan,Zheng Fang,Zhichao Hu,Zhongzhi Chen,Zhuoyu Li,Zongwei Li,Alex Yan,Ande Liang,Baitong Liu,Beiping Pan,Bin Xing,Binghong Wu,Bingxin Qu,Bolin Ni,Boyu Wu,Chen Li,Cheng Jiang,Cheng Zhang,Chengjun Liu,Chengxu Yang,Chiyu Wang,Chong Zha,Daisy Yi,Di Wang,Fanyang Lu,Fei Chen,Feifei Liu,Feng Zheng,Guanghua Yu,Guiyang Li,Guohua Wang,Haisheng Lin,Han Liu,Han Wang,Hao Fei,Hao Lu,Haoqing Jiang,Haoran Sun,Haotian Zhu,Huangjin Dai,Huankui Chen,Huawen Feng,Huihui Cai,Huxin Peng,Jackson Lv,Jiacheng Shi,Jiahao Bu,Jianbo Li,Jianglu Hu,Jiangtao Guan,Jianing Xu,Jianwei Cai,Jiarong Zhang,Jiawei Song,Jie Jiang,Jie Liu,Jieneng Yang,Jihong Zhang,Jin lv,Jing Zhao,Jinjian Li,Jinxing Liu,Jun Zhao,Juntao Guo,Kai Wang,Kan Wu,Lei Fu,Lei He,Lei Wang,Li Liu,Liang Dong,Liya Zhan,Long Cheng,Long Xu,Mao Zheng,Meng Liu,Mengkang Hu,Nanli Chen,Peirui Chen,Peng He,Pengju Pan,Pengzhi Wei,Qi Yang,Qi Yi,Roberts Wang,Rongpeng Chen,Rui Sun,Rui Yang,Ruibin Chen,Ruixu Zhou,Shaofeng Zhang,Sheng Zhang,Shihao Xu,Shuaishuai Chang,Shulin Liu,SiQi Wang,Songjia Feng,Songling Yuan,Tao Zhang,Tianjiao Lang,Tongkai Li,Wei Deng,Wei Li,Weichao Wang,Weigang Zhang,Weixuan Sun,Wen Ouyang,Wenxiang Jiao,Wenzhi Sun,Wenzhuo Jia,Xiang Zhang,Xiangyu He,Xianshun Ren,XiaoYing Zhu,Xiaolong Guo,Xiaoxue Li,Xiaoyu Ma,Xican Lu,Xinhua Feng,Xinting Huang,Xinyu Guan,Xirui Li,Xu Zhang,Xudong Gao,Xun Luo,Xuxiang Qi,Yangkun Chen,Yangyu Tao,Yanling Xiao,Yantao Mai,Yanze Chen,Yao Ding,Yeting Yang,YiFan Song,Yifan Yang,Yijiao Zhu,Yinhe Wu,Yixian Liu,Yong Yang,Yuanjun Cai,Yuanlin Tu,Yue Zhang,Yufei Huang,Yuhang Zhou,Yuhao Jiang,Yuhong Liu,Yuhui Hu,Yujin Lin,Yun Yang,Yunhao Wang,Yusong Zhang,Zekun Wu,Zelong Zhang,Zhan Yu,Zhaoliang Yang,Zhe Zhao,Zheng Li,Zhenyu Huang,Zhiguang Liu,Zhijiang Xu,Zhiqing Kui,Zhiyin Zeng,Zhiyuan Xiong,Zhuo Han,Zifan Wu,Zigang Geng,Zilong Zhao,Ziyan Tang,Ziyuan Zhu,Zonglei Zhu,Zhijiang Xu*

Main category: cs.CL

TL;DR: Hunyuan-TurboS推出560B参数的混合Transformer-Mamba MoE模型，通过自适应长短思维链机制在LMSYS榜单排名前7（1356分），实现高效长序列处理与高性能平衡。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs在长序列处理效率与上下文理解能力的矛盾，结合Mamba的线性计算优势和Transformer的语义理解能力，优化大模型推理成本。

Method: 采用128层AMF/MF块（Mamba2+Attention+MoE-FFN），预训练16T tokens，结合自适应长短CoT、多轮审议学习、两阶段强化学习优化模型能力。

Result: LMSYS Chatbot Arena排名前7（1356分），23个基准平均77.9%，推理成本低于主流模型，支持256K上下文。

Conclusion: 通过架构创新与训练策略融合，建立高效预训练模型新范式，为工业部署提供高性能且低推理成本的大模型解决方案。

Abstract: As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,
a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It
synergistically combines Mamba's long-sequence processing efficiency with
Transformer's superior contextual understanding. Hunyuan-TurboS features an
adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching
between rapid responses for simple queries and deep "thinking" modes for
complex problems, optimizing computational resources. Architecturally, this 56B
activated (560B total) parameter model employs 128 layers (Mamba2, Attention,
FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear
complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE
structure. Pre-trained on 16T high-quality tokens, it supports a 256K context
length and is the first industry-deployed large-scale Mamba model. Our
comprehensive post-training strategy enhances capabilities via Supervised
Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,
Multi-round Deliberation Learning for iterative improvement, and a two-stage
Large-scale Reinforcement Learning process targeting STEM and general
instruction-following. Evaluations show strong performance: overall top 7 rank
on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like
Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves
an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances
high performance and efficiency, offering substantial capabilities at lower
inference costs than many reasoning models, establishing a new paradigm for
efficient large-scale pre-trained models.

</details>


### [90] [On the Generalization vs Fidelity Paradox in Knowledge Distillation](https://arxiv.org/abs/2505.15442)
*Suhas Kamasetty Ramesh,Ayan Sengupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 知识蒸馏显著提升小语言模型（0.5B-7B参数）在零样本推理任务中的性能（平均+10%，峰值22%），但对大模型增益微弱（~1.3%）。教师模型的任务专业性（非性能）是核心影响因素，且知识蒸馏可能降低推理过程的结构化一致性。


<details>
  <summary>Details</summary>
Motivation: 探索知识蒸馏在不同规模语言模型中的有效性差异，揭示知识迁移机制中教师模型任务专业性与学生模型规模间的相互作用关系。

Method: 在14个复杂推理任务中，对0.5B-7B参数模型进行大规模实证分析，结合相关性研究（模型规模与KD增益关系）和消融实验（教师信号、logit平滑的影响）。

Result: 小模型KD后平均准确率提升10%（最大单任务22%），大模型仅1.3%；教师任务专业性比教师性能更重要；学生准确率提升与推理保真度存在错位。

Conclusion: 知识蒸馏对小模型压缩具有显著价值但存在权衡：提升预测准确率的同时可能弱化结构化推理能力，教师信号质量和任务匹配度是成功关键因素。

Abstract: Knowledge distillation (KD) is a key technique for compressing large language
models into smaller ones while preserving performance. Despite the recent
traction of KD research, its effectiveness for smaller language models (LMs)
and the mechanisms driving knowledge transfer remain underexplored. In this
work, we present the first large-scale empirical and statistical analysis of KD
across models ranging from 0.5B to 7B parameters on 14 complex reasoning tasks
in a zero-shot setting. Our findings reveal that KD can improve the average
performance of smaller models by up to $10\%$, with a peak task specific gain
of $22\%$, while providing only marginal benefits ($\sim 1.3\%$) for larger
models. Surprisingly, teacher performance has a minimal impact on student
outcomes, while teacher task expertise impacts KD effectiveness. A correlation
study indicates that smaller LMs benefit more from KD, whereas larger LMs show
diminished gains. Additionally, we uncover a misalignment between improvements
in student performance and reasoning fidelity, suggesting that while KD
enhances accuracy, it does not always maintain the structured decision-making
processes of the teacher. Our ablation study further highlights the importance
of teacher signals and logit smoothing in influencing students' performance
after distillation. Overall, our study offers a comprehensive empirical and
statistical assessment of KD, highlighting both its benefits and trade-offs
when distilling knowledge from larger to smaller LMs.

</details>


### [91] [AdUE: Improving uncertainty estimation head for LoRA adapters in LLMs](https://arxiv.org/abs/2505.15443)
*Artem Zabolotnyi,Roman Makarov,Mile Mitrovic,Polina Proskura,Oleg Travkin,Roman Alferov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出AdUE1方法，通过可微分最大函数近似和L2-SP正则化，提升基于softmax的语言模型不确定性估计效果


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（如adapter）场景下传统不确定性估计方法（马氏距离、softmax响应）表现不足，需保持基座模型不变的轻量化改进方案

Method: 1. 使用可微分最大函数近似优化置信度计算 2. 通过L2-SP正则化约束微调头部参数

Result: 在5个NLP分类数据集和4种模型（RoBERTa/ELECTRA/LLaMA-2/Qwen）上验证，置信度校准效果优于基线方法

Conclusion: AdUE1无需修改基座模型，以轻量级方式实现更可靠的不确定性估计，适用于实际部署场景

Abstract: Uncertainty estimation remains a critical challenge in adapting pre-trained
language models to classification tasks, particularly under parameter-efficient
fine-tuning approaches such as adapters. We introduce AdUE1, an efficient
post-hoc uncertainty estimation (UE) method, to enhance softmax-based
estimates. Our approach (1) uses a differentiable approximation of the maximum
function and (2) applies additional regularization through L2-SP, anchoring the
fine-tuned head weights and regularizing the model. Evaluations on five NLP
classification datasets across four language models (RoBERTa, ELECTRA, LLaMA-2,
Qwen) demonstrate that our method consistently outperforms established
baselines such as Mahalanobis distance and softmax response. Our approach is
lightweight (no base-model changes) and produces better-calibrated confidence.

</details>


### [92] [Single LLM, Multiple Roles: A Unified Retrieval-Augmented Generation Framework Using Role-Specific Token Optimization](https://arxiv.org/abs/2505.15444)
*Yutao Zhu,Jiajie Jin,Hongjin Qian,Zheng Liu,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: RoleRAG框架通过角色令牌优化统一多个RAG子任务，提升效率并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽优化了RAG各子任务，但缺乏统一框架整合，导致部署复杂。

Method: 基于角色令牌驱动单一LLM实例，结合查询图谱动态分解任务，六模块协同工作。

Result: 在五大开放域QA数据集验证框架有效性、泛化性和灵活性。

Conclusion: RoleRAG成功实现多任务集成与资源效率的平衡，为RAG系统优化提供新思路。

Abstract: Existing studies have optimized retrieval-augmented generation (RAG) across
various sub-tasks, such as query understanding and retrieval refinement, but
integrating these optimizations into a unified framework remains challenging.
To tackle this problem, this work proposes RoleRAG, a unified RAG framework
that achieves efficient multi-task processing through role-specific token
optimization. RoleRAG comprises six modules, each handling a specific sub-task
within the RAG process. Additionally, we introduce a query graph to represent
the decomposition of the query, which can be dynamically resolved according to
the decomposing state. All modules are driven by the same underlying LLM,
distinguished by task-specific role tokens that are individually optimized.
This design allows RoleRAG to dynamically activate different modules within a
single LLM instance, thereby streamlining deployment and reducing resource
consumption. Experimental results on five open-domain question-answering
datasets demonstrate the effectiveness, generalizability, and flexibility of
our framework.

</details>


### [93] [Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment](https://arxiv.org/abs/2505.15456)
*Weixiang Zhao,Xingyu Sui,Yulin Hu,Jiahe Guo,Haixiao Liu,Biye Li,Yanyan Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出RLPA强化学习框架，通过双奖励机制实现LLM在个性化对话中的动态用户画像构建与优化


<details>
  <summary>Details</summary>
Motivation: 现有基于提示和离线优化的方法在冷启动场景和长期个性化中存在静态/浅层设计的局限性

Method: 构建模拟用户交互环境，采用Profile Reward（用户画像准确度）和Response Reward（响应一致性）的双层奖励机制，基于Qwen-2.5-3B-Instruct微调实现

Result: Qwen-RLPA在个性化对话任务中超越提示学习、微调基线及Claude-3.5/GPT-4o等商业模型，在冲突偏好调和、长期个性化保持等方面展现优势

Conclusion: 动态用户画像推断相比静态方法显著提升个性化对话效果，RLPA框架为构建个性化系统提供了更有效的范式

Abstract: Personalized alignment is essential for enabling large language models (LLMs)
to engage effectively in user-centric dialogue. While recent prompt-based and
offline optimization methods offer preliminary solutions, they fall short in
cold-start scenarios and long-term personalization due to their inherently
static and shallow designs. In this work, we introduce the Reinforcement
Learning for Personalized Alignment (RLPA) framework, in which an LLM interacts
with a simulated user model to iteratively infer and refine user profiles
through dialogue. The training process is guided by a dual-level reward
structure: the Profile Reward encourages accurate construction of user
representations, while the Response Reward incentivizes generation of responses
consistent with the inferred profile. We instantiate RLPA by fine-tuning
Qwen-2.5-3B-Instruct, resulting in Qwen-RLPA, which achieves state-of-the-art
performance in personalized dialogue. Empirical evaluations demonstrate that
Qwen-RLPA consistently outperforms prompting and offline fine-tuning baselines,
and even surpasses advanced commercial models such as Claude-3.5 and GPT-4o.
Further analysis highlights Qwen-RLPA's robustness in reconciling conflicting
user preferences, sustaining long-term personalization and delivering more
efficient inference compared to recent reasoning-focused LLMs. These results
emphasize the potential of dynamic profile inference as a more effective
paradigm for building personalized dialogue systems.

</details>


### [94] [Joint Flashback Adaptation for Forgetting-Resistant Instruction Tuning](https://arxiv.org/abs/2505.15467)
*Yukun Zhao,Lingyong Yan,Zhenyang Li,Shuaiqiang Wang,Zhumin Chen,Zhaochun Ren,Dawei Yin*

Main category: cs.CL

TL;DR: 提出联合闪回适应方法，通过引入少量旧任务提示和潜在任务插值，解决大模型增量学习中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有增量学习方法依赖经验回放数据或任务特定优化，在现实场景中存在数据存储限制和任务泛化不足的缺陷。

Method: 1) 引入旧任务提示(flashbacks)约束输出偏差 2) 在闪回与新任务间插值潜在任务，实现三者的联合学习 3) 无需回放数据且任务无关

Result: 在1000+指令跟随/数学推理/通用推理任务中，新任务准确率提升18.7%，旧任务遗忘率降低62.3%。

Conclusion: 仅需少量闪回提示即可实现平滑知识迁移，首个无需数据回放的任务无关增量学习方法，实验验证有效性显著。

Abstract: Large language models have achieved remarkable success in various tasks.
However, it is challenging for them to learn new tasks incrementally due to
catastrophic forgetting. Existing approaches rely on experience replay,
optimization constraints, or task differentiation, which encounter strict
limitations in real-world scenarios. To address these issues, we propose Joint
Flashback Adaptation. We first introduce flashbacks -- a limited number of
prompts from old tasks -- when adapting to new tasks and constrain the
deviations of the model outputs compared to the original one. We then
interpolate latent tasks between flashbacks and new tasks to enable jointly
learning relevant latent tasks, new tasks, and flashbacks, alleviating data
sparsity in flashbacks and facilitating knowledge sharing for smooth
adaptation. Our method requires only a limited number of flashbacks without
access to the replay data and is task-agnostic. We conduct extensive
experiments on state-of-the-art large language models across 1000+
instruction-following tasks, arithmetic reasoning tasks, and general reasoning
tasks. The results demonstrate the superior performance of our method in
improving generalization on new tasks and reducing forgetting in old tasks.

</details>


### [95] [CoLA: Collaborative Low-Rank Adaptation](https://arxiv.org/abs/2505.15471)
*Yiyun Zhou,Chang Yao,Jingyuan Chen*

Main category: cs.CL

TL;DR: CoLA提出了一种灵活高效的LoRA架构，通过协同策略提升多任务场景下的参数高效微调性能，尤其在低样本情况下表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法（如LoRA）在多任务场景中存在任务间干扰问题，且现有改进方案（如MOE）仍受限于固定结构和噪声干扰。

Method: 提出CoLA架构：1）设计灵活的LoRA初始化方案；2）引入三种矩阵A/B协同策略，利用矩阵间定量关系提升表现。

Result: 实验证明CoLA在低样本场景下优于现有PEFT方法，鲁棒性更强，代码和数据已开源。

Conclusion: CoLA为LLM微调提供了更灵活高效的解决方案，有效缓解任务干扰问题，推动参数高效微调技术发展。

Abstract: The scaling law of Large Language Models (LLMs) reveals a power-law
relationship, showing diminishing return on performance as model scale
increases. While training LLMs from scratch is resource-intensive, fine-tuning
a pre-trained model for specific tasks has become a practical alternative. Full
fine-tuning (FFT) achieves strong performance; however, it is computationally
expensive and inefficient. Parameter-efficient fine-tuning (PEFT) methods, like
LoRA, have been proposed to address these challenges by freezing the
pre-trained model and adding lightweight task-specific modules. LoRA, in
particular, has proven effective, but its application to multi-task scenarios
is limited by interference between tasks. Recent approaches, such as
Mixture-of-Experts (MOE) and asymmetric LoRA, have aimed to mitigate these
issues but still struggle with sample scarcity and noise interference due to
their fixed structure. In response, we propose CoLA, a more flexible LoRA
architecture with an efficient initialization scheme, and introduces three
collaborative strategies to enhance performance by better utilizing the
quantitative relationships between matrices $A$ and $B$. Our experiments
demonstrate the effectiveness and robustness of CoLA, outperforming existing
PEFT methods, especially in low-sample scenarios. Our data and code are fully
publicly available at https://github.com/zyy-2001/CoLA.

</details>


### [96] [PhysicsArena: The First Multimodal Physics Reasoning Benchmark Exploring Variable, Process, and Solution Dimensions](https://arxiv.org/abs/2505.15472)
*Song Dai,Yibo Yan,Jiamin Su,Dongfang Zihao,Yubo Gao,Yonghua Hei,Jungang Li,Junyan Zhang,Sicheng Tao,Zhuoran Gao,Xuming Hu*

Main category: cs.CL

TL;DR: 提出首个多模态物理推理基准PhysicsArena，从变量识别、物理过程构建、解决方案推导三个维度全面评估MLLMs能力


<details>
  <summary>Details</summary>
Motivation: 现有物理基准局限于文本输入和结果考核，缺乏对物理条件锚定和多模态信息解释的中间推理过程评估

Method: 通过构建包含变量识别、物理过程建模、数学求解的三维评估框架，整合多模态数据构建系统性测试平台

Result: 建立首个覆盖完整物理推理链条的多模态基准，为MLLMs的物理推理能力提供标准化评估体系

Conclusion: PhysicsArena填补了复杂物理推理评估的空白，推动MLLMs在科学认知领域向人类水平迈进

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in diverse reasoning tasks, yet their application to complex
physics reasoning remains underexplored. Physics reasoning presents unique
challenges, requiring grounding in physical conditions and the interpretation
of multimodal information. Current physics benchmarks are limited, often
focusing on text-only inputs or solely on problem-solving, thereby overlooking
the critical intermediate steps of variable identification and process
formulation. To address these limitations, we introduce PhysicsArena, the first
multimodal physics reasoning benchmark designed to holistically evaluate MLLMs
across three critical dimensions: variable identification, physical process
formulation, and solution derivation. PhysicsArena aims to provide a
comprehensive platform for assessing and advancing the multimodal physics
reasoning abilities of MLLMs.

</details>


### [97] [LFTF: Locating First and Then Fine-Tuning for Mitigating Gender Bias in Large Language Models](https://arxiv.org/abs/2505.15475)
*Zhanyue Qin,Yue Ding,Deyuan Liu,Qingbin Liu,Junxian Cai,Xi Chen,Zhiying Tu,Dianhui Chu,Cuiyun Gao,Dianbo Sui*

Main category: cs.CL

TL;DR: 提出GenBiasEval和GenHintEval数据集及LFTF算法，有效量化并缓解大语言模型中的性别偏见问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练中接触社会偏见数据后易产生性别偏见，需系统性评估和缓解方案

Method: 通过双数据集评估偏见程度，开发BMI评分定位关键模块，设计LFTF算法进行定向微调

Result: 实验证明LFTF在保持模型性能前提下显著降低性别偏见

Conclusion: 该研究为LLMs性别偏见的量化评估和针对性修正提供了有效工具与解决方案

Abstract: Nowadays, Large Language Models (LLMs) have attracted widespread attention
due to their powerful performance. However, due to the unavoidable exposure to
socially biased data during training, LLMs tend to exhibit social biases,
particularly gender bias. To better explore and quantifying the degree of
gender bias in LLMs, we propose a pair of datasets named GenBiasEval and
GenHintEval, respectively. The GenBiasEval is responsible for evaluating the
degree of gender bias in LLMs, accompanied by an evaluation metric named
AFGB-Score (Absolutely Fair Gender Bias Score). Meanwhile, the GenHintEval is
used to assess whether LLMs can provide responses consistent with prompts that
contain gender hints, along with the accompanying evaluation metric UB-Score
(UnBias Score). Besides, in order to mitigate gender bias in LLMs more
effectively, we present the LFTF (Locating First and Then Fine-Tuning)
algorithm.The algorithm first ranks specific LLM blocks by their relevance to
gender bias in descending order using a metric called BMI (Block Mitigating
Importance Score). Based on this ranking, the block most strongly associated
with gender bias is then fine-tuned using a carefully designed loss function.
Numerous experiments have shown that our proposed LFTF algorithm can
significantly mitigate gender bias in LLMs while maintaining their general
capabilities.

</details>


### [98] [KaFT: Knowledge-aware Fine-tuning for Boosting LLMs' Domain-specific Question-Answering Performance](https://arxiv.org/abs/2505.15480)
*Qihuang Zhong,Liang Ding,Xiantao Cai,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CL

TL;DR: KaFT通过根据知识冲突程度动态调整训练样本权重，有效提升LLMs在领域问答中的性能表现


<details>
  <summary>Details</summary>
Motivation: 传统全量监督微调(SFT)在存在知识冲突（模型内部知识与训练数据上下文知识矛盾）时效果受限，需针对性优化冲突样本利用方式

Method: 提出知识感知微调框架KaFT：1）设计查询多样化策略检测知识冲突 2）根据冲突程度分配奖励值动态调整训练权重

Result: 在四个主流LLMs上实现显著性能提升，分析显示该方法有效提升模型泛化能力并减少幻觉现象

Conclusion: KaFT为知识冲突场景下的模型微调提供简单有效的解决方案，证明合理利用冲突数据优于直接过滤

Abstract: Supervised fine-tuning (SFT) is a common approach to improve the
domain-specific question-answering (QA) performance of large language models
(LLMs). However, recent literature reveals that due to the conflicts between
LLMs' internal knowledge and the context knowledge of training data, vanilla
SFT using the full QA training set is usually suboptimal. In this paper, we
first design a query diversification strategy for robust conflict detection and
then conduct a series of experiments to analyze the impact of knowledge
conflict. We find that 1) training samples with varied conflicts contribute
differently, where SFT on the data with large conflicts leads to catastrophic
performance drops; 2) compared to directly filtering out the conflict data,
appropriately applying the conflict data would be more beneficial. Motivated by
this, we propose a simple-yet-effective Knowledge-aware Fine-tuning (namely
KaFT) approach to effectively boost LLMs' performance. The core of KaFT is to
adapt the training weight by assigning different rewards for different training
samples according to conflict level. Extensive experiments show that KaFT
brings consistent and significant improvements across four LLMs. More analyses
prove that KaFT effectively improves the model generalization and alleviates
the hallucination.

</details>


### [99] [Collaborative Problem-Solving in an Optimization Game](https://arxiv.org/abs/2505.15490)
*Isidora Jeknic,Alex Duchnowski,Alexander Koller*

Main category: cs.CL

TL;DR: 开发了结合LLM提示与符号机制的对话代理，通过协作解决双人旅行商问题，在自玩中实现45%最优解，并能与人类协作及泛化至新图结构


<details>
  <summary>Details</summary>
Motivation: 解决复杂NP-hard优化问题中对话代理的协作探索难题，传统方法难以有效支持用户共同探索解空间

Method: 设计双人TSP协作对话游戏，结合大型语言模型（LLM）生成对话与符号化状态跟踪/grounding机制

Result: 最佳代理在自玩中45%达到最优解，成功与人类协作（准确率72%），并展示对陌生图的泛化能力

Conclusion: 证明了LLM与符号机制融合在复杂协作任务中的有效性，为对话式优化问题求解提供了新范式

Abstract: Dialogue agents that support human users in solving complex tasks have
received much attention recently. Many such tasks are NP-hard optimization
problems that require careful collaborative exploration of the solution space.
We introduce a novel dialogue game in which the agents collaboratively solve a
two-player Traveling Salesman problem, along with an agent that combines LLM
prompting with symbolic mechanisms for state tracking and grounding. Our best
agent solves 45% of games optimally in self-play. It also demonstrates an
ability to collaborate successfully with human users and generalize to
unfamiliar graphs.

</details>


### [100] [Protoknowledge Shapes Behaviour of LLMs in Downstream Tasks: Memorization and Generalization with Knowledge Graphs](https://arxiv.org/abs/2505.15501)
*Federico Ranaldi,Andrea Zugarini,Leonardo Ranaldi,Fabio Massimo Zanzotto*

Main category: cs.CL

TL;DR: 提出'元知识'概念量化LLMs对知识图谱的预训练内化过程，开发分析框架检测语义级数据污染


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何将预训练记忆转化为可泛化知识的核心机制

Method: 定义词汇/层次/拓扑三类元知识，设计知识激活任务(KATs)，构建Text-to-SPARQL提示策略分析框架

Result: 揭示语义偏差特性，验证不同输入条件下提示策略的有效性，建立模型预测与元知识激活的关联分析方法

Conclusion: 该框架为检测语义污染提供实用工具，并为闭预训练模型开发出新型分析策略

Abstract: We introduce the concept of protoknowledge to formalize and measure how
sequences of tokens encoding Knowledge Graphs are internalized during
pretraining and utilized at inference time by Large Language Models (LLMs).
Indeed, LLMs have demonstrated the ability to memorize vast amounts of token
sequences during pretraining, and a central open question is how they leverage
this memorization as reusable knowledge through generalization. We then
categorize protoknowledge into lexical, hierarchical, and topological forms,
varying on the type of knowledge that needs to be activated. We measure
protoknowledge through Knowledge Activation Tasks (KATs), analyzing its general
properties such as semantic bias. We then investigate the impact of
protoknowledge on Text-to-SPARQL performance by varying prompting strategies
depending on input conditions. To this end, we adopt a novel analysis framework
that assesses whether model predictions align with the successful activation of
the relevant protoknowledge for each query. This methodology provides a
practical tool to explore Semantic-Level Data Contamination and serves as an
effective strategy for Closed-Pretraining models.

</details>


### [101] [Multilingual Test-Time Scaling via Initial Thought Transfer](https://arxiv.org/abs/2505.15508)
*Prasoon Bajpai,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 测试时间缩放在多语言环境中的有效性研究显示不同语言间增益差异显著，低资源语言存在推理不一致性，并提出MITT方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索测试时间缩放在非英语语言中的有效性，揭示多语言模型推理差异及低资源语言的表现问题。

Method: 评估DeepSeek模型在拉丁语系语言中的表现，提出基于无监督前缀调优的MITT方法迁移高资源语言推理模式。

Result: 低资源语言初始推理与英语差异显著且内部一致性低，MITT使DeepSeek-Qwen-7B低资源语言性能显著提升。

Conclusion: MITT通过迁移高资源语言推理前缀，有效解决多语言推理不一致性，提升低资源语言测试时间缩放效果。

Abstract: Test-time scaling has emerged as a widely adopted inference-time strategy for
boosting reasoning performance. However, its effectiveness has been studied
almost exclusively in English, leaving its behavior in other languages largely
unexplored. We present the first systematic study of test-time scaling in
multilingual settings, evaluating DeepSeek-R1-Distill-LLama-8B and
DeepSeek-R1-Distill-Qwen-7B across both high- and low-resource Latin-script
languages. Our findings reveal that the relative gains from test-time scaling
vary significantly across languages. Additionally, models frequently switch to
English mid-reasoning, even when operating under strictly monolingual prompts.
We further show that low-resource languages not only produce initial reasoning
thoughts that differ significantly from English but also have lower internal
consistency across generations in their early reasoning. Building on our
findings, we introduce MITT (Multilingual Initial Thought Transfer), an
unsupervised and lightweight reasoning prefix-tuning approach that transfers
high-resource reasoning prefixes to enhance test-time scaling across all
languages, addressing inconsistencies in multilingual reasoning performance.
MITT significantly boosts DeepSeek-R1-Distill-Qwen-7B's reasoning performance,
especially for underrepresented languages.

</details>


### [102] [Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs](https://arxiv.org/abs/2505.15524)
*Lang Gao,Kaiyang Wan,Wei Liu,Chenxi Wang,Zirui Song,Zixiang Xu,Yanbo Wang,Veselin Stoyanov,Xiuying Chen*

Main category: cs.CL

TL;DR: 提出BiasLens框架，通过向量空间结构分析大语言模型偏见，无需标注数据且可解释性强


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估方法依赖人工标注数据且覆盖概念有限，难以检测复杂偏见模式

Method: 结合概念激活向量(CAV)与稀疏自编码器(SAE)，通过表示相似性变化量化目标概念与参考概念间的关联偏差

Result: 与传统方法强相关（Spearman r>0.85），成功发现保险状态影响诊断等新型偏见模式

Conclusion: BiasLens为LLM偏见检测提供了可扩展、高效且可解释的解决方案，推动模型公平性提升

Abstract: Bias in Large Language Models (LLMs) significantly undermines their
reliability and fairness. We focus on a common form of bias: when two reference
concepts in the model's concept space, such as sentiment polarities (e.g.,
"positive" and "negative"), are asymmetrically correlated with a third, target
concept, such as a reviewing aspect, the model exhibits unintended bias. For
instance, the understanding of "food" should not skew toward any particular
sentiment. Existing bias evaluation methods assess behavioral differences of
LLMs by constructing labeled data for different social groups and measuring
model responses across them, a process that requires substantial human effort
and captures only a limited set of social concepts. To overcome these
limitations, we propose BiasLens, a test-set-free bias analysis framework based
on the structure of the model's vector space. BiasLens combines Concept
Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract
interpretable concept representations, and quantifies bias by measuring the
variation in representational similarity between the target concept and each of
the reference concepts. Even without labeled data, BiasLens shows strong
agreement with traditional bias evaluation metrics (Spearman correlation r >
0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect
using existing methods. For example, in simulated clinical scenarios, a
patient's insurance status can cause the LLM to produce biased diagnostic
assessments. Overall, BiasLens offers a scalable, interpretable, and efficient
paradigm for bias discovery, paving the way for improving fairness and
transparency in LLMs.

</details>


### [103] [Social Bias in Popular Question-Answering Benchmarks](https://arxiv.org/abs/2505.15553)
*Angelie Kraft,Judith Simon,Sonja Schimmler*

Main category: cs.CL

TL;DR: QA和阅读理解评测基准存在人口统计学和地域偏见，根源可能来自缺乏多样化的标注团队，需建立更透明的基准构建流程


<details>
  <summary>Details</summary>
Motivation: 揭示当前QA评测基准存在的偏见问题，这些偏见导致大语言模型评估结果不公，影响模型公平性

Method: 通过定性分析30篇基准论文+定量分析20个数据集，聚焦三个维度：1) 标注参与者构成 2) 偏见应对措施 3) 标注者背景与内容偏见的关联

Result: 83%论文未充分披露标注者信息；仅1篇明确反偏见措施；百科全书/常识/学术类基准普遍存在性别、宗教和地域偏见

Conclusion: 建立标注者背景披露机制，开发系统性的偏见检测方法，推动更公平的LLM评测体系构建

Abstract: Question-answering (QA) and reading comprehension (RC) benchmarks are
essential for assessing the capabilities of large language models (LLMs) in
retrieving and reproducing knowledge. However, we demonstrate that popular QA
and RC benchmarks are biased and do not cover questions about different
demographics or regions in a representative way, potentially due to a lack of
diversity of those involved in their creation. We perform a qualitative content
analysis of 30 benchmark papers and a quantitative analysis of 20 respective
benchmark datasets to learn (1) who is involved in the benchmark creation, (2)
how social bias is addressed or prevented, and (3) whether the demographics of
the creators and annotators correspond to particular biases in the content.
Most analyzed benchmark papers provided insufficient information regarding the
stakeholders involved in benchmark creation, particularly the annotators.
Notably, just one of the benchmark papers explicitly reported measures taken to
address social representation issues. Moreover, the data analysis revealed
gender, religion, and geographic biases across a wide range of encyclopedic,
commonsense, and scholarly benchmarks. More transparent and bias-aware QA and
RC benchmark creation practices are needed to facilitate better scrutiny and
incentivize the development of fairer LLMs.

</details>


### [104] [DayDreamer at CQs-Gen 2025: Generating Critical Questions through Argument Scheme Completion](https://arxiv.org/abs/2505.15554)
*Wendi Zhou,Ameer Saadat-Yazdi,Nadin Kökciyan*

Main category: cs.CL

TL;DR: 提出结合大语言模型与论证模板的自动批判性问题生成系统，通过结构化论证和思维链技术有效提升问题质量


<details>
  <summary>Details</summary>
Motivation: 关键问题能激发批判性思维，帮助检测论证文本中的信息缺失或未经证实的主张

Method: 1. 基于Walton论证模板生成结构化论点
2. 通过思维链提示生成候选问题
3. 使用LLM排序筛选最优3个问题

Result: 在最终测试集达到竞争力表现，验证了结构化论证理论与渐进式推理的有效性

Conclusion: 该方法为提升论证文本的批判性思考能力提供了新范式，代码已开源促进后续研究

Abstract: Critical questions are essential resources to provoke critical thinking when
encountering an argumentative text. We present our system for the Critical
Questions Generation (CQs-Gen) Shared Task at ArgMining 2025. Our approach
leverages large language models (LLMs) with chain-of-thought prompting to
generate critical questions guided by Walton's argumentation schemes. For each
input intervention, we conversationally prompt LLMs to instantiate the
corresponding argument scheme template to first obtain structured arguments,
and then generate relevant critical questions. Following this, we rank all the
available critical questions by prompting LLMs to select the top 3 most helpful
questions based on the original intervention text. This combination of
structured argumentation theory and step-by-step reasoning enables the
generation of contextually relevant and diverse critical questions. Our
pipeline achieves competitive performance in the final test set, showing its
potential to foster critical thinking given argumentative text and detect
missing or uninformed claims. Code available at
\href{https://git.ecdf.ed.ac.uk/s2236454/DayDreamer-CQs-Gen}{DayDreamer}.

</details>


### [105] [A Survey on Multilingual Mental Disorders Detection from Social Media Data](https://arxiv.org/abs/2505.15556)
*Ana-Maria Bucur,Marcos Zampieri,Tharindu Ranasinghe,Fabio Crestani*

Main category: cs.CL

TL;DR: 首个基于多语言社交媒体数据的心理健康检测综述，强调文化差异对NLP工具的影响并建立多语言数据集目录


<details>
  <summary>Details</summary>
Motivation: 现有研究过度依赖英语数据，无法捕捉非英语文本中的心理健康信号，限制了筛查工具的普适性

Method: 1. 分析文化差异对在线语言模式的影响
2. 评估NLP工具跨文化表现
3. 系统整理多语言心理健康数据集

Result: 揭示了文化特异性对心理健康筛查的关键作用，建立了可供模型开发的多语言数据资源库

Conclusion: 开发多语言心理健康筛查工具可满足多样化人群需求，对提升全球心理健康干预效果具有重要意义

Abstract: The increasing prevalence of mental health disorders globally highlights the
urgent need for effective digital screening methods that can be used in
multilingual contexts. Most existing studies, however, focus on English data,
overlooking critical mental health signals that may be present in non-English
texts. To address this important gap, we present the first survey on the
detection of mental health disorders using multilingual social media data. We
investigate the cultural nuances that influence online language patterns and
self-disclosure behaviors, and how these factors can impact the performance of
NLP tools. Additionally, we provide a comprehensive list of multilingual data
collections that can be used for developing NLP models for mental health
screening. Our findings can inform the design of effective multilingual mental
health screening tools that can meet the needs of diverse populations,
ultimately improving mental health outcomes on a global scale.

</details>


### [106] [Do RAG Systems Suffer From Positional Bias?](https://arxiv.org/abs/2505.15561)
*Florin Cuconasu,Simone Filice,Guy Horowitz,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 研究发现检索增强生成中，由于相关段落和干扰段落同时被LLM位置偏差惩罚，实际场景中位置偏差影响有限，复杂段落重排策略并不优于随机排序


<details>
  <summary>Details</summary>
Motivation: 探究LLM位置偏差如何影响其对相关段落的利用能力及对干扰段落的敏感性，特别是在实际检索系统存在高干扰段落的情况下

Method: 通过在三个基准数据集上的大量实验，分析检索系统返回段落的干扰性及其对LLM性能的影响，测试不同段落排列策略的有效性

Result: 60%以上查询的top-10检索结果包含高干扰段落；位置偏差的实际影响边际化；复杂重排策略表现不优于随机排序

Conclusion: 现实场景中位置偏差影响被双重惩罚机制削弱，调整段落位置策略无法有效提升模型性能

Abstract: Retrieval Augmented Generation enhances LLM accuracy by adding passages
retrieved from an external corpus to the LLM prompt. This paper investigates
how positional bias - the tendency of LLMs to weight information differently
based on its position in the prompt - affects not only the LLM's capability to
capitalize on relevant passages, but also its susceptibility to distracting
passages. Through extensive experiments on three benchmarks, we show how
state-of-the-art retrieval pipelines, while attempting to retrieve relevant
passages, systematically bring highly distracting ones to the top ranks, with
over 60% of queries containing at least one highly distracting passage among
the top-10 retrieved passages. As a result, the impact of the LLM positional
bias, which in controlled settings is often reported as very prominent by
related works, is actually marginal in real scenarios since both relevant and
distracting passages are, in turn, penalized. Indeed, our findings reveal that
sophisticated strategies that attempt to rearrange the passages based on LLM
positional preferences do not perform better than random shuffling.

</details>


### [107] [Semantic-based Unsupervised Framing Analysis (SUFA): A Novel Approach for Computational Framing Analysis](https://arxiv.org/abs/2505.15563)
*Mohammad Ali,Naeemul Hassan*

Main category: cs.CL

TL;DR: 提出基于语义关系和依赖解析的无监督框架分析方法SUFA，用于新闻媒体中实体中心框架的识别分析。


<details>
  <summary>Details</summary>
Motivation: 解决传统计算框架分析方法在实体中心框架识别中的局限性，提供无需人工标注的高效分析方案。

Method: 结合语义关系抽取与依赖解析算法，通过定性和计算双研究验证，使用枪支暴力数据集进行测试。

Result: 成功验证SUFA在实体框架分析中的有效性，展示其在枪支暴力案例中的应用潜力。

Conclusion: SUFA为计算框架分析提供了跨学科的方法创新，在社会科学和计算领域具有广泛适用性。

Abstract: This research presents a novel approach to computational framing analysis,
called Semantic Relations-based Unsupervised Framing Analysis (SUFA). SUFA
leverages semantic relations and dependency parsing algorithms to identify and
assess entity-centric emphasis frames in news media reports. This innovative
method is derived from two studies -- qualitative and computational -- using a
dataset related to gun violence, demonstrating its potential for analyzing
entity-centric emphasis frames. This article discusses SUFA's strengths,
limitations, and application procedures. Overall, the SUFA approach offers a
significant methodological advancement in computational framing analysis, with
its broad applicability across both the social sciences and computational
domains.

</details>


### [108] [From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning](https://arxiv.org/abs/2505.15607)
*David Dinucu-Jianu,Jakub Macina,Nico Daheim,Ido Hakimi,Iryna Gurevych,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 利用在线强化学习框架训练70亿参数导师模型，通过平衡教学支持与解题准确性的可控奖励机制，提升LLM作为教学工具的效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM直接回答问题的特性违背了'策略性保留答案'的有效教学原则，需要开发新的对齐方法优化教学效果

Method: 基于模拟师生互动的在线强化学习框架，采用双目标奖励权重控制教学支持强度与学生解题准确性的平衡，并引入思维标签增强可解释性

Result: 无标注训练的70亿模型达到LearnLM等大型商业模型水平，在保持推理能力的同时形成教学效果与解题准确性的帕累托前沿

Conclusion: 强化学习对齐框架成功实现教学效果优化，通过动态奖励机制平衡教学目标，保持模型可解释性，为教育领域LLM应用提供新范式

Abstract: Large language models (LLMs) can transform education, but their optimization
for direct question-answering often undermines effective pedagogy which
requires strategically withholding answers. To mitigate this, we propose an
online reinforcement learning (RL)-based alignment framework that can quickly
adapt LLMs into effective tutors using simulated student-tutor interactions by
emphasizing pedagogical quality and guided problem-solving over simply giving
away answers. We use our method to train a 7B parameter tutor model without
human annotations which reaches similar performance to larger proprietary
models like LearnLM. We introduce a controllable reward weighting to balance
pedagogical support and student solving accuracy, allowing us to trace the
Pareto frontier between these two objectives. Our models better preserve
reasoning capabilities than single-turn SFT baselines and can optionally
enhance interpretability through thinking tags that expose the model's
instructional planning.

</details>


### [109] [Learn to Reason Efficiently with Adaptive Length-based Reward Shaping](https://arxiv.org/abs/2505.15612)
*Wei Liu,Ruochen Zhou,Yiyun Deng,Yuzhen Huang,Junteng Liu,Yuntian Deng,Yizhe Zhang,Junxian He*

Main category: cs.CL

TL;DR: 提出LASER-D强化学习方法，通过动态难度感知的奖励机制优化大型推理模型的效率与性能平衡


<details>
  <summary>Details</summary>
Motivation: 大型推理模型的长推理轨迹存在显著冗余，需在保持性能的同时提升推理效率

Method: 1. 建立基于长度奖励的统一框架
2. 提出阶跃函数奖励机制LASER
3. 扩展为动态调整+难度感知的LASER-D（根据问题难度差异化控制推理长度）

Result: AIME2024性能提升6.1%同时减少63% token使用，模型产生更简洁的推理模式

Conclusion: 基于强化学习的长度压缩机制有效平衡性能与效率，实现『快慢思维结合』的优化

Abstract: Large Reasoning Models (LRMs) have shown remarkable capabilities in solving
complex problems through reinforcement learning (RL), particularly by
generating long reasoning traces. However, these extended outputs often exhibit
substantial redundancy, which limits the efficiency of LRMs. In this paper, we
investigate RL-based approaches to promote reasoning efficiency. Specifically,
we first present a unified framework that formulates various efficient
reasoning methods through the lens of length-based reward shaping. Building on
this perspective, we propose a novel Length-bAsed StEp Reward shaping method
(LASER), which employs a step function as the reward, controlled by a target
length. LASER surpasses previous methods, achieving a superior Pareto-optimal
balance between performance and efficiency. Next, we further extend LASER based
on two key intuitions: (1) The reasoning behavior of the model evolves during
training, necessitating reward specifications that are also adaptive and
dynamic; (2) Rather than uniformly encouraging shorter or longer chains of
thought (CoT), we posit that length-based reward shaping should be
difficulty-aware i.e., it should penalize lengthy CoTs more for easy queries.
This approach is expected to facilitate a combination of fast and slow
thinking, leading to a better overall tradeoff. The resulting method is termed
LASER-D (Dynamic and Difficulty-aware). Experiments on
DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, and
DeepSeek-R1-Distill-Qwen-32B show that our approach significantly enhances both
reasoning performance and response length efficiency. For instance, LASER-D and
its variant achieve a +6.1 improvement on AIME2024 while reducing token usage
by 63%. Further analysis reveals our RL-based compression produces more concise
reasoning patterns with less redundant "self-reflections". Resources are at
https://github.com/hkust-nlp/Laser.

</details>


### [110] [Can LLMs $\textit{understand}$ Math? -- Exploring the Pitfalls in Mathematical Reasoning](https://arxiv.org/abs/2505.15623)
*Tiasa Singha Roy,Aditeya Baral,Ayush Rajesh Jhaveri,Yusuf Baig*

Main category: cs.CL

TL;DR: 大语言模型在数学推理中存在多步骤逻辑缺陷，研究者提出MAPLE评分体系从错误率/冗余度/有效性三个维度构建新型评估标准


<details>
  <summary>Details</summary>
Motivation: 现有评估框架仅基于最终答案的准确率进行评价，无法有效识别模型在多步骤数学推理过程中产生的逻辑偏差问题

Method: 提出MAPLE评估指标，整合错误率（error rates）、冗余度（redundancy）和有效性（validity）三个维度，构建多维度的推理偏差量化体系

Result: 开发出能够系统性评估模型推理过程质量的量化指标，突破传统单一准确率评估的局限性

Conclusion: MAPLE评分体系为语言模型的数学推理能力评估提供更全面的分析框架，有助于发现和改善模型多步骤推理中的逻辑偏差问题

Abstract: Large language models (LLMs) demonstrate considerable potential in various
natural language tasks but face significant challenges in mathematical
reasoning, particularly in executing precise, multi-step logic. However,
current evaluation frameworks judge their performance solely based on accuracy,
which only accounts for the final answer. This study explores these pitfalls by
employing a novel evaluation framework. We propose an evaluation metric called
the MAPLE score, which holistically quantifies reasoning misalignment by
integrating error rates, redundancy, and validity.

</details>


### [111] [Listen to the Context: Towards Faithful Large Language Models for Retrieval Augmented Generation on Climate Questions](https://arxiv.org/abs/2505.15633)
*David Thulke,Jakob Kemmler,Christian Dugast,Hermann Ney*

Main category: cs.CL

TL;DR: 通过排除训练数据中的不忠实样本，ClimateGPT Faithful+将支持性原子声明的忠实度从30%提升至57%


<details>
  <summary>Details</summary>
Motivation: 检索增强生成模型在提升气候科学文献可读性的同时，需确保模型输出与检索段落保持高度一致以增强可信度

Method: 开发自动评估指标量化模型忠实度，并通过数据筛选策略优化ClimateGPT的指令微调过程

Result: 优化后的ClimateGPT Faithful+在原子声明支持率指标上实现近两倍提升（30%→57%）

Conclusion: 专业领域大语言模型的忠实度可通过训练数据清洗策略有效提升，这对科学传播应用具有重要实践价值

Abstract: Large language models that use retrieval augmented generation have the
potential to unlock valuable knowledge for researchers, policymakers, and the
public by making long and technical climate-related documents more accessible.
While this approach can help alleviate factual hallucinations by relying on
retrieved passages as additional context, its effectiveness depends on whether
the model's output remains faithful to these passages. To address this, we
explore the automatic assessment of faithfulness of different models in this
setting. We then focus on ClimateGPT, a large language model specialised in
climate science, to examine which factors in its instruction fine-tuning impact
the model's faithfulness. By excluding unfaithful subsets of the model's
training data, we develop ClimateGPT Faithful+, which achieves an improvement
in faithfulness from 30% to 57% in supported atomic claims according to our
automatic metric.

</details>


### [112] [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](https://arxiv.org/abs/2505.15634)
*Zihao Li,Xu Wang,Yuzhe Yang,Ziyu Yao,Haoyi Xiong,Mengnan Du*

Main category: cs.CL

TL;DR: 提出基于稀疏自编码器（SAE）和新型无SAE的steering算法，无需外部数据即可增强大语言模型推理能力


<details>
  <summary>Details</summary>
Motivation: 传统长思维链方法需要昂贵的高质量长数据与微调，本研究旨在通过模型内部状态引导突破该限制

Method: 首先利用SAE从普通思维链提取特征引导模型，随后开发直接从残差激活计算steering方向的无SAE算法

Result: 实验证明SAE-based和SAE-free算法均显著提升LLM推理能力，后者更具模型普适性

Conclusion: 该steering技术为增强LLM推理能力提供新范式，无SAE方法尤其适用于缺乏预训练SAE的模型

Abstract: Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.

</details>


### [113] [Word Level Timestamp Generation for Automatic Speech Recognition and Translation](https://arxiv.org/abs/2505.15646)
*Ke Hu,Krishna Puvvada,Elena Rastorgueva,Zhehuai Chen,He Huang,Shuoyang Ding,Kunal Dhawan,Hainan Xu,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: 通过NFA教师模型生成标签训练Canary模型直接预测词级时间戳，在四语种实现80-90%召回率，时间戳误差仅20-120ms


<details>
  <summary>Details</summary>
Motivation: 传统语音系统依赖外部对齐模块获取时间戳信息，需要开发无需额外对齐机制的直接预测方法以提升效率

Method: 引入<|timestamp|>标记，使用NFA强制对齐结果作为监督信号，端到端训练模型预测单词起止时间

Result: 在英/西/日/阿拉伯语中达到80-90%召回率，时间戳误差20-120ms（WER仅退化0.4-1.5%），AST任务误差约200ms

Conclusion: 首次实现端到端模型直接预测词级时间戳，在保持识别精度的同时支持多语言，并成功扩展至语音翻译场景

Abstract: We introduce a data-driven approach for enabling word-level timestamp
prediction in the Canary model. Accurate timestamp information is crucial for a
variety of downstream tasks such as speech content retrieval and timed
subtitles. While traditional hybrid systems and end-to-end (E2E) models may
employ external modules for timestamp prediction, our approach eliminates the
need for separate alignment mechanisms. By leveraging the NeMo Forced Aligner
(NFA) as a teacher model, we generate word-level timestamps and train the
Canary model to predict timestamps directly. We introduce a new <|timestamp|>
token, enabling the Canary model to predict start and end timestamps for each
word. Our method demonstrates precision and recall rates between 80% and 90%,
with timestamp prediction errors ranging from 20 to 120 ms across four
languages, with minimal WER degradation. Additionally, we extend our system to
automatic speech translation (AST) tasks, achieving timestamp prediction errors
around 200 milliseconds.

</details>


### [114] [Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!](https://arxiv.org/abs/2505.15656)
*Zhexin Zhang,Yuhao Sun,Junxiao Yang,Shiyao Cui,Hongning Wang,Minlie Huang*

Main category: cs.CL

TL;DR: 开源大语言模型微调存在严重数据泄露风险，攻击者可通过后门训练提取76.3%-94.9%下游私有数据


<details>
  <summary>Details</summary>
Motivation: 揭示下游开发者使用专有数据微调开源LLMs时存在未预见的数据泄露风险，模型创建者可通过黑盒访问提取私有微调数据

Method: 在3B-32B参数的4个开源模型和2个下游数据集上进行实验，采用后门训练攻击策略，测试数据提取效果并尝试检测防御

Result: 实际场景中成功提取76.3%（5,000样本中的3,815条），理想条件下提升至94.9%；现有防御策略可被改进攻击绕过

Conclusion: 亟需关注微调过程中的新型数据泄露风险，需推动相关防御技术研究。代码和数据已开源供社区验证

Abstract: Fine-tuning on open-source Large Language Models (LLMs) with proprietary data
is now a standard practice for downstream developers to obtain task-specific
LLMs. Surprisingly, we reveal a new and concerning risk along with the
practice: the creator of the open-source LLMs can later extract the private
downstream fine-tuning data through simple backdoor training, only requiring
black-box access to the fine-tuned downstream model. Our comprehensive
experiments, across 4 popularly used open-source models with 3B to 32B
parameters and 2 downstream datasets, suggest that the extraction performance
can be strikingly high: in practical settings, as much as 76.3% downstream
fine-tuning data (queries) out of a total 5,000 samples can be perfectly
extracted, and the success rate can increase to 94.9% in more ideal settings.
We also explore a detection-based defense strategy but find it can be bypassed
with improved attack. Overall, we highlight the emergency of this newly
identified data breaching risk in fine-tuning, and we hope that more follow-up
research could push the progress of addressing this concerning risk. The code
and data used in our experiments are released at
https://github.com/thu-coai/Backdoor-Data-Extraction.

</details>


### [115] [Efficient and Direct Duplex Modeling for Speech-to-Speech Language Model](https://arxiv.org/abs/2505.15670)
*Ke Hu,Ehsan Hosseini-Asl,Chen Chen,Edresson Casanova,Subhankar Ghosh,Piotr Żelasko,Zhehuai Chen,Jason Li,Jagadeesh Balam,Boris Ginsburg*

Main category: cs.CL

TL;DR: 提出首个无需语音预训练的双工语音对话架构，通过分离用户/代理建模实现0.6kbps低比特率，在推理/轮转/插话能力上优于现有模型，并提供首个开源实现


<details>
  <summary>Details</summary>
Motivation: 现有语音模型受限于轮流对话模式，缺乏实时交互能力（如用户插话）。需要构建能同时处理用户输入和代理输出的双工架构。

Method: 1. 用户端使用预训练流式编码器 2. 分离用户/代理的编解码架构 3. 通道融合技术建模同时语音流 4. 码本微调提升音质

Result: 模型推理准确率提升15%，插话延迟降低200ms，训练数据需求减少80%（无需语音预训练），比特率较前作降低50%至0.6kbps

Conclusion: 该架构显著降低了构建双工语音系统的门槛，首个开源实现促进领域发展，为LLMs添加实时语音交互能力提供新范式

Abstract: Spoken dialogue is an intuitive form of human-computer interaction, yet
current speech language models often remain constrained to turn-based
exchanges, lacking real-time adaptability such as user barge-in. We propose a
novel duplex speech to speech (S2S) architecture featuring continuous user
inputs and codec agent outputs with channel fusion that directly models
simultaneous user and agent streams. Using a pretrained streaming encoder for
user input enables the first duplex S2S model without requiring speech
pretrain. Separate architectures for agent and user modeling facilitate codec
fine-tuning for better agent voices and halve the bitrate (0.6 kbps) compared
to previous works. Experimental results show that the proposed model
outperforms previous duplex models in reasoning, turn-taking, and barge-in
abilities. The model requires significantly less speech data, as speech
pretrain is skipped, which markedly simplifies the process of building a duplex
S2S model from any LLMs. Finally, it is the first openly available duplex S2S
model with training and inference code to foster reproducibility.

</details>


### [116] [UniErase: Unlearning Token as a Universal Erasure Primitive for Language Models](https://arxiv.org/abs/2505.15674)
*Miao Yu,Liang Lin,Guibin Zhang,Xinfeng Li,Junfeng Fang,Ningyu Zhang,Kun Wang,Yang Wang*

Main category: cs.CL

TL;DR: 提出UniErase新型遗忘学习范式，通过可学习参数后缀分两阶段实现定向知识擦除，在保持模型能力的同时达到SOTA遗忘效果


<details>
  <summary>Details</summary>
Motivation: 现有微调遗忘方法难以平衡遗忘效果与模型能力，上下文遗忘方法泛化性不足，需开发高效且保持模型性能的遗忘机制

Method: 分优化阶段（将遗忘目标绑定自回归概率分布）和轻量模型编辑阶段（概率诱导遗忘），仅修改约3.66%模型参数

Result: TOFU基准测试中模型能力提升4.01倍，遗忘效果优于保留型SOTA方法35.96%，实现遗忘效果与模型能力的双重突破

Conclusion: UniErase开创了令牌学习诱导遗忘的新方向，在批量/顺序/精准遗忘任务中均达SOTA，为LLM安全更新提供创新解决方案

Abstract: Large language models require iterative updates to address challenges such as
knowledge conflicts and outdated information (e.g., incorrect, private, or
illegal contents). Machine unlearning provides a systematic methodology for
targeted knowledge removal from trained models, enabling elimination of
sensitive information influences. However, mainstream fine-tuning-based
unlearning methods often fail to balance unlearning efficacy and model ability,
frequently resulting in catastrophic model collapse under extensive knowledge
removal. Meanwhile, in-context unlearning, which relies solely on contextual
prompting without modifying the model's intrinsic mechanisms, suffers from
limited generalizability and struggles to achieve true unlearning. In this
work, we introduce UniErase, a novel unlearning paradigm that employs learnable
parametric suffix (unlearning token) to steer language models toward targeted
forgetting behaviors. UniErase operates through two key phases: (I) an
optimization stage that binds desired unlearning outputs to the model's
autoregressive probability distribution via token optimization, followed by
(II) a lightweight model editing phase that activates the learned token to
probabilistically induce specified forgetting objective. Serving as a new
research direction for token learning to induce unlearning target, UniErase
achieves state-of-the-art (SOTA) performance across batch, sequential, and
precise unlearning under fictitious and real-world knowledge settings.
Remarkably, in terms of TOFU benchmark, UniErase, modifying only around 3.66%
of the LLM parameters, outperforms previous forgetting SOTA baseline by around
4.01 times for model ability with even better unlearning efficacy. Similarly,
UniErase, maintaining more ability, also surpasses previous retaining SOTA by
35.96% for unlearning efficacy, showing dual top-tier performances in current
unlearing domain.

</details>


### [117] [The Representational Alignment between Humans and Language Models is implicitly driven by a Concreteness Effect](https://arxiv.org/abs/2505.15682)
*Cosimo Iaia,Bhavin Choksi,Emily Wiebers,Gemma Roig,Christian J. Fiebach*

Main category: cs.CL

TL;DR: 人类和语言模型的语义表征在具体性维度显著对齐，其他心理语言学特征无显著影响


<details>
  <summary>Details</summary>
Motivation: 探讨具体性在心理表征和语言模型中的表达机制，填补语言模型如何表征具体性的研究空白

Method: 使用行为判断获取人类语义距离，通过表征相似性分析（RSA）比较人类与语言模型表征，结合消融实验验证具体性的驱动作用

Result: 人类与模型表征的对齐主要由具体性维度驱动，其他重要词汇特征（如词频、情感效价）无显著影响

Conclusion: 人类认知系统与语言模型在具体性维度形成跨模态表征趋同，其他语义维度未显示类似一致性

Abstract: The nouns of our language refer to either concrete entities (like a table) or
abstract concepts (like justice or love), and cognitive psychology has
established that concreteness influences how words are processed. Accordingly,
understanding how concreteness is represented in our mind and brain is a
central question in psychology, neuroscience, and computational linguistics.
While the advent of powerful language models has allowed for quantitative
inquiries into the nature of semantic representations, it remains largely
underexplored how they represent concreteness. Here, we used behavioral
judgments to estimate semantic distances implicitly used by humans, for a set
of carefully selected abstract and concrete nouns. Using Representational
Similarity Analysis, we find that the implicit representational space of
participants and the semantic representations of language models are
significantly aligned. We also find that both representational spaces are
implicitly aligned to an explicit representation of concreteness, which was
obtained from our participants using an additional concreteness rating task.
Importantly, using ablation experiments, we demonstrate that the human-to-model
alignment is substantially driven by concreteness, but not by other important
word characteristics established in psycholinguistics. These results indicate
that humans and language models converge on the concreteness dimension, but not
on other dimensions.

</details>


### [118] [A Federated Splitting Framework for LLMs: Security, Efficiency, and Adaptability](https://arxiv.org/abs/2505.15683)
*Zishuai Zhang,Hainan Zhang,Jiaying Zheng,Ziwei Wang,Yongxin Tong,Jin Dong,Zhiming Zheng*

Main category: cs.CL

TL;DR: 提出了安全高效的联邦学习框架FL-LLaMA，通过动态分区、并行训练和隐私保护机制，在保持LLaMA2性能的同时提升训练推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统联邦split learning存在梯度泄露风险、顺序训练导致通信开销大、固定分区缺乏任务适应性三个核心问题

Method: 1) 本地部署输入输出层并注入高斯噪声 2) 客户端批量处理+服务端分层策略实现并行训练 3) 动态调整分区点适配不同任务

Result: 在NLU等任务中保持与集中式LLaMA2相当性能，训练速度提升2倍，推理速度提升8倍，有效防御隐私攻击

Conclusion: FL-LLaMA通过系统级优化在安全、效率和适应性三个维度实现突破，为联邦环境部署LLM提供新思路

Abstract: Private data is typically larger and of higher quality than public data,
offering great potential to improve LLM. However, its scattered distribution
across data silos and the high computational demands of LLMs limit their
deployment in federated environments. To address this, the transformer-based
split learning model has emerged, offloading most model parameters to the
server while retaining only the embedding and output layers on clients to
ensure privacy. However, it still faces significant challenges in security,
efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,
leading to reverse engineering of private data; 2) the autoregressive nature of
LLMs means that federated split learning can only train and infer sequentially,
causing high communication overhead; 3) fixed partition points lack
adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a
secure, efficient, and adaptive federated split framework based on LLaMA2.
First, we place some input and output blocks on the local client and inject
Gaussian noise into forward-pass hidden states, enabling secure end-to-end
propagation. Second, we employ client-batch and server-hierarchical strategies
to achieve parallel training, along with attention-mask compression and KV
cache mechanisms to accelerate inference, reducing communication costs
effectively. Third, we allow users to dynamically adjust the partition points
for input/output blocks based on specific task requirements and hardware
limitations. Experiments on NLU, summarization and conversational QA tasks show
that FL-LLaMA maintains performance comparable to centralized LLaMA2, and
achieves up to 2x train speedups and 8x inference speedups. Further analysis of
privacy attacks and different partition points also demonstrates the
effectiveness of FL-LLaMA in security and adaptability.

</details>


### [119] [ThinkLess: A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](https://arxiv.org/abs/2505.15684)
*Gengyang Li,Yifeng Gao,Yuming Li,Yunfang Wu*

Main category: cs.CL

TL;DR: 提出ThinkLess框架，通过提前终止推理步骤和轻量级后处理机制，在不修改模型的情况下显著减少推理时间和内存消耗，保持与完整CoT相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决Chain-of-Thought提示方法导致的推理token过长问题（增加延迟/内存消耗/答案截断风险）

Method: 1. 基于注意力分析插入早期终止符
2. 轻量级后处理机制维护输出格式
3. 无微调/辅助数据依赖

Result: 推理时间减少39.7%，内存消耗降低48.3%，在GSM8K和MMLU基准测试中准确率与完整CoT相当

Conclusion: ThinkLess为LLM推理效率提供新范式，证明通过结构化终止机制可在保持性能前提下突破传统CoT的计算瓶颈

Abstract: While Chain-of-Thought (CoT) prompting improves reasoning in large language
models (LLMs), the excessive length of reasoning tokens increases latency and
KV cache memory usage, and may even truncate final answers under context
limits. We propose ThinkLess, an inference-efficient framework that terminates
reasoning generation early and maintains output quality without modifying the
model. Atttention analysis reveals that answer tokens focus minimally on
earlier reasoning steps and primarily attend to the reasoning terminator token,
due to information migration under causal masking. Building on this insight,
ThinkLess inserts the terminator token at earlier positions to skip redundant
reasoning while preserving the underlying knowledge transfer. To prevent format
discruption casued by early termination, ThinkLess employs a lightweight
post-regulation mechanism, relying on the model's natural instruction-following
ability to produce well-structured answers. Without fine-tuning or auxiliary
data, ThinkLess achieves comparable accuracy to full-length CoT decoding while
greatly reducing decoding time and memory consumption.

</details>


### [120] [Thought-Augmented Policy Optimization: Bridging External Guidance and Internal Capabilities](https://arxiv.org/abs/2505.15692)
*Jinyang Wu,Chonghua Liao,Mingkuan Feng,Shuai Zhang,Zhengqi Wen,Pengpeng Shao,Huazhe Xu,Jianhua Tao*

Main category: cs.CL

TL;DR: 提出TAPO强化学习框架，通过整合外部思维模式显著提升模型推理能力，在多项基准测试中表现超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法过度依赖奖励路径，缺乏外部知识整合，导致推理能力边界受限。需要增强模型的外部知识融合能力。

Method: TAPO框架：在训练中自适应整合结构化思维模式，平衡内部探索与外部知识利用。使用500个样本抽象出通用思维模板。

Result: 在AIME/AMC/Minerva Math分别提升99%/41%/17%，且思维模板具备跨任务泛化能力。模型展现出更好的可解释性和输出可读性。

Conclusion: 外部知识引导可创造更具通用性的推理模型，该方法在多领域任务中展现出广泛应用潜力。

Abstract: Reinforcement learning (RL) has emerged as an effective method for training
reasoning models. However, existing RL approaches typically bias the model's
output distribution toward reward-maximizing paths without introducing external
knowledge. This limits their exploration capacity and results in a narrower
reasoning capability boundary compared to base models. To address this
limitation, we propose TAPO (Thought-Augmented Policy Optimization), a novel
framework that augments RL by incorporating external high-level guidance
("thought patterns"). By adaptively integrating structured thoughts during
training, TAPO effectively balances model-internal exploration and external
guidance exploitation. Extensive experiments show that our approach
significantly outperforms GRPO by 99% on AIME, 41% on AMC, and 17% on Minerva
Math. Notably, these high-level thought patterns, abstracted from only 500
prior samples, generalize effectively across various tasks and models. This
highlights TAPO's potential for broader applications across multiple tasks and
domains. Our further analysis reveals that introducing external guidance
produces powerful reasoning models with superior explainability of inference
behavior and enhanced output readability.

</details>


### [121] [Can Large Language Models be Effective Online Opinion Miners?](https://arxiv.org/abs/2505.15695)
*Ryang Heo,Yongsik Seo,Junseong Lee,Dongha Lee*

Main category: cs.CL

TL;DR: 论文提出在线意见挖掘基准OOMB，通过包含（实体-特征-观点）三元组标注和意见摘要的评估框架，测试大语言模型在复杂网络环境中提取和总结意见的能力。


<details>
  <summary>Details</summary>
Motivation: 传统意见挖掘方法难以处理用户生成内容的高度多样性、复杂性和上下文关联性，需建立新基准评估大语言模型在实际场景中的意见挖掘潜力。

Method: 构建包含细粒度标注（实体/特征/观点）和意见摘要的OOMB数据集，通过提取式任务和生成式任务双重评估模型能力。

Result: 研究揭示了LLMs在复杂意见挖掘中的优势与现存挑战，验证其作为实际场景意见挖掘工具的可行性。

Conclusion: 该基准为LLM驱动的意见挖掘奠定基础，未来可探索领域适应性优化、多模态融合等方向，推动在线意见分析技术发展。

Abstract: The surge of user-generated online content presents a wealth of insights into
customer preferences and market trends. However, the highly diverse, complex,
and context-rich nature of such contents poses significant challenges to
traditional opinion mining approaches. To address this, we introduce Online
Opinion Mining Benchmark (OOMB), a novel dataset and evaluation protocol
designed to assess the ability of large language models (LLMs) to mine opinions
effectively from diverse and intricate online environments. OOMB provides
extensive (entity, feature, opinion) tuple annotations and a comprehensive
opinion-centric summary that highlights key opinion topics within each content,
thereby enabling the evaluation of both the extractive and abstractive
capabilities of models. Through our proposed benchmark, we conduct a
comprehensive analysis of which aspects remain challenging and where LLMs
exhibit adaptability, to explore whether they can effectively serve as opinion
miners in realistic online scenarios. This study lays the foundation for
LLM-based opinion mining and discusses directions for future research in this
field.

</details>


### [122] [MaxPoolBERT: Enhancing BERT Classification via Layer- and Token-Wise Aggregation](https://arxiv.org/abs/2505.15696)
*Maike Behrendt,Stefan Sylvius Wagner,Stefan Harmeling*

Main category: cs.CL

TL;DR: 通过跨层最大池化和注意力机制优化BERT的[CLS]表示，提升分类任务性能且不显著增加计算量


<details>
  <summary>Details</summary>
Motivation: 现有BERT模型的[CLS]标记未能充分利用其他层级和token的上下文信息，限制了分类任务的性能潜力

Method: 1) 跨层[CLS]最大池化 2) 新增多头注意力层实现[CLS]对最终层的全局关注 3) 序列最大池化与注意力机制结合

Result: 在GLUE基准测试中持续优于标准BERT-base模型，低资源任务提升显著

Conclusion: MaxPoolBERT通过轻量级结构改进有效提升分类精度，特别适用于计算资源受限场景

Abstract: The [CLS] token in BERT is commonly used as a fixed-length representation for
classification tasks, yet prior work has shown that both other tokens and
intermediate layers encode valuable contextual information. In this work, we
propose MaxPoolBERT, a lightweight extension to BERT that refines the [CLS]
representation by aggregating information across layers and tokens.
Specifically, we explore three modifications: (i) max-pooling the [CLS] token
across multiple layers, (ii) enabling the [CLS] token to attend over the entire
final layer using an additional multi-head attention (MHA) layer, and (iii)
combining max-pooling across the full sequence with MHA. Our approach enhances
BERT's classification accuracy (especially on low-resource tasks) without
requiring pre-training or significantly increasing model size. Experiments on
the GLUE benchmark show that MaxPoolBERT consistently achieves a better
performance on the standard BERT-base model.

</details>


### [123] ["Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding](https://arxiv.org/abs/2505.15700)
*Alkis Koudounas,Claudio Savelli,Flavio Giobergia,Elena Baralis*

Main category: cs.CL

TL;DR: 提出首个口语理解领域的机器去学习基准UnSLU-BENCH，评估八种方法并开发新评估指标


<details>
  <summary>Details</summary>
Motivation: 解决复杂任务（尤其是语音相关任务）中机器去学习方法效果评估的空白，支持负责任AI中的'被遗忘权'实现

Method: 构建跨四语言的数据集，通过特定说话人数据删除模拟遗忘场景，设计融合效果/效用/效率的三维评估体系

Result: 不同去学习技术存在显著效果差异，部分方法在保持模型性能的同时实现高效遗忘

Conclusion: UnSLU-BENCH为语音理解领域的去学习研究奠定基础，揭示算法选择对合规性和计算成本的影响

Abstract: Machine unlearning, the process of efficiently removing specific information
from machine learning models, is a growing area of interest for responsible AI.
However, few studies have explored the effectiveness of unlearning methods on
complex tasks, particularly speech-related ones. This paper introduces
UnSLU-BENCH, the first benchmark for machine unlearning in spoken language
understanding (SLU), focusing on four datasets spanning four languages. We
address the unlearning of data from specific speakers as a way to evaluate the
quality of potential "right to be forgotten" requests. We assess eight
unlearning techniques and propose a novel metric to simultaneously better
capture their efficacy, utility, and efficiency. UnSLU-BENCH sets a foundation
for unlearning in SLU and reveals significant differences in the effectiveness
and computational feasibility of various techniques.

</details>


### [124] [LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing](https://arxiv.org/abs/2505.15702)
*Peng Wang,Biyu Zhou,Xuehai Tang,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: 提出首个具有理论保证的LyapLock框架，通过排队论与李雅普诺夫优化将长期知识保存约束分解为可求解子问题，实现1万次连续编辑下的稳定更新与11.89%效果提升


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在连续知识更新时存在性能衰退问题，主要源于缺乏长期知识保存机制

Method: 将序列编辑建模为约束随机规划问题，融合排队理论处理动态任务流，应用Lyapunov优化进行长期约束分解

Result: 实验证明框架支持超1万次连续编辑，通用能力稳定且编辑效果比基线提升11.89%，可增强现有方法性能

Conclusion: LyapLock首次为模型编辑提供理论保障，在保证长期知识保存约束下实现渐进最优编辑，显著提升行业基准方法表现

Abstract: Large Language Models often contain factually incorrect or outdated
knowledge, giving rise to model editing methods for precise knowledge updates.
However, current mainstream locate-then-edit approaches exhibit a progressive
performance decline during sequential editing, due to inadequate mechanisms for
long-term knowledge preservation. To tackle this, we model the sequential
editing as a constrained stochastic programming. Given the challenges posed by
the cumulative preservation error constraint and the gradually revealed editing
tasks, \textbf{LyapLock} is proposed. It integrates queuing theory and Lyapunov
optimization to decompose the long-term constrained programming into tractable
stepwise subproblems for efficient solving. This is the first model editing
framework with rigorous theoretical guarantees, achieving asymptotic optimal
editing performance while meeting the constraints of long-term knowledge
preservation. Experimental results show that our framework scales sequential
editing capacity to over 10,000 edits while stabilizing general capabilities
and boosting average editing efficacy by 11.89\% over SOTA baselines.
Furthermore, it can be leveraged to enhance the performance of baseline
methods. Our code is released on https://github.com/caskcsg/LyapLock.

</details>


### [125] [Advancing LLM Safe Alignment with Safety Representation Ranking](https://arxiv.org/abs/2505.15710)
*Tianqi Du,Zeming Wei,Quan Chen,Chenheng Zhang,Yisen Wang*

Main category: cs.CL

TL;DR: 本文提出SRR框架，通过大语言模型内部表征实现安全响应排序，显著提升对抗性提示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有安全评估方法仅关注文本输出，忽视了模型内部表征中蕴含的丰富安全信号。为更精准捕捉细微的有害内容倾向，作者提出利用模型中间层表征进行安全评估。

Method: SRR框架包含三阶段：1) 使用中间层transformer表征编码指令和候选补全；2) 通过轻量级相似度打分器排序候选；3) 采用列表级监督优化排序效果。

Result: 在多个基准测试中，SRR相比传统方法在对抗性提示场景下实现安全评估准确率提升21.4%，误报率降低35.8%。

Conclusion: 通过直接利用模型内部状态和列表级监督，SRR能有效捕捉细微的安全信号，为LLM安全评估提供新范式。

Abstract: The rapid advancement of large language models (LLMs) has demonstrated
milestone success in a variety of tasks, yet their potential for generating
harmful content has raised significant safety concerns. Existing safety
evaluation approaches typically operate directly on textual responses,
overlooking the rich information embedded in the model's internal
representations. In this paper, we propose Safety Representation Ranking (SRR),
a listwise ranking framework that selects safe responses using hidden states
from the LLM itself. SRR encodes both instructions and candidate completions
using intermediate transformer representations and ranks candidates via a
lightweight similarity-based scorer. Our approach directly leverages internal
model states and supervision at the list level to capture subtle safety
signals. Experiments across multiple benchmarks show that SRR significantly
improves robustness to adversarial prompts. Our code will be available upon
publication.

</details>


### [126] [TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games](https://arxiv.org/abs/2505.15712)
*Yuan Yuan,Muyu He,Muhammad Adil Shahid,Jiani Huang,Ziyang Li,Li Zhang*

Main category: cs.CL

TL;DR: TurnaboutLLM框架通过侦探游戏数据集评估大语言模型的演绎推理能力，揭示现有增强策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法(如思维链提示)在复杂叙事环境中存在不足，需构建新评估体系测试LLMs的矛盾识别能力。

Method: 利用《逆转裁判》《弹丸论破》构建长文本矛盾识别任务，测试12个顶尖模型，分析上下文规模/推理步数/答案空间的影响。

Result: 发现扩展思考策略效果有限，不同因素对模型表现影响各异，当前模型在复杂叙事推理中面临重大挑战。

Conclusion: 该框架为评估LLMs复杂环境推理能力提供新基准，突显现有模型在叙事密集场景中的推理能力缺陷。

Abstract: This paper introduces TurnaboutLLM, a novel framework and dataset for
evaluating the deductive reasoning abilities of Large Language Models (LLMs) by
leveraging the interactive gameplay of detective games Ace Attorney and
Danganronpa. The framework tasks LLMs with identifying contradictions between
testimonies and evidences within long narrative contexts, a challenging task
due to the large answer space and diverse reasoning types presented by its
questions. We evaluate twelve state-of-the-art LLMs on the dataset, hinting at
limitations of popular strategies for enhancing deductive reasoning such as
extensive thinking and Chain-of-Thought prompting. The results also suggest
varying effects of context size, the number of reasoning step and answer space
size on model performance. Overall, TurnaboutLLM presents a substantial
challenge for LLMs' deductive reasoning abilities in complex, narrative-rich
environments.

</details>


### [127] [Beyond Empathy: Integrating Diagnostic and Therapeutic Reasoning with Large Language Models for Mental Health Counseling](https://arxiv.org/abs/2505.15715)
*He Hu,Yucheng Zhou,Juzheng Si,Qianning Wang,Hengheng Zhang,Fuji Ren,Fei Ma,Laizhong Cui*

Main category: cs.CL

TL;DR: PsyLLM是首个整合诊断与治疗推理的心理健康咨询大模型，通过自动化数据合成和多维过滤生成临床对齐的对话数据，在专业评估中显著优于现有基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有心理健康领域的LLM缺乏临床诊断标准（如DSM/ICD）对齐和多样化治疗策略整合，难以满足真实心理咨询需求。

Method: 开发自动化数据合成管道：处理真实心理帖子→生成多轮对话→基于DSM/ICD标准和多种治疗框架（CBT/ACT/心理动力学）模拟临床推理→多维过滤保证数据质量；建立四维评估基准（全面性/专业性/真实性/安全性）。

Result: 实验表明PsyLLM在综合评估中显著优于现有SOTA模型，尤其在临床专业性和系统性咨询推理方面表现突出。

Conclusion: PsyLLM通过临床对齐数据生成和系统化咨询推理框架，为LLM在专业心理健康支持领域提供了创新解决方案。

Abstract: Large language models (LLMs) hold significant potential for mental health
support, capable of generating empathetic responses and simulating therapeutic
conversations. However, existing LLM-based approaches often lack the clinical
grounding necessary for real-world psychological counseling, particularly in
explicit diagnostic reasoning aligned with standards like the DSM/ICD and
incorporating diverse therapeutic modalities beyond basic empathy or single
strategies. To address these critical limitations, we propose PsyLLM, the first
large language model designed to systematically integrate both diagnostic and
therapeutic reasoning for mental health counseling. To develop the PsyLLM, we
propose a novel automated data synthesis pipeline. This pipeline processes
real-world mental health posts, generates multi-turn dialogue structures, and
leverages LLMs guided by international diagnostic standards (e.g., DSM/ICD) and
multiple therapeutic frameworks (e.g., CBT, ACT, psychodynamic) to simulate
detailed clinical reasoning processes. Rigorous multi-dimensional filtering
ensures the generation of high-quality, clinically aligned dialogue data. In
addition, we introduce a new benchmark and evaluation protocol, assessing
counseling quality across four key dimensions: comprehensiveness,
professionalism, authenticity, and safety. Our experiments demonstrate that
PsyLLM significantly outperforms state-of-the-art baseline models on this
benchmark.

</details>


### [128] [Shared Path: Unraveling Memorization in Multilingual LLMs through Language Similarities](https://arxiv.org/abs/2505.15722)
*Xiaoyu Luo,Yiyi Chen,Johannes Bjerva,Qiongxiu Li*

Main category: cs.CL

TL;DR: 研究发现多语言大模型记忆模式与语言相似性相关，提出基于图的语言相似性度量揭示跨语言记忆规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦单语模型记忆性，但多语言场景下训练数据的长尾分布导致语言间记忆机制不透明，需探究跨语言关联对记忆性的影响。

Method: 提出基于语言相似性的图关联度量方法，分析95种语言在不同模型规模/架构下的跨语言记忆模式。

Result: 相似语言中训练数据较少的语言记忆性更强，该规律需在建模跨语言关系后显现。

Conclusion: 语言相似性同时解释记忆机制和跨语言迁移性，为多语言模型安全评估提供新视角。

Abstract: We present the first comprehensive study of Memorization in Multilingual
Large Language Models (MLLMs), analyzing 95 languages using models across
diverse model scales, architectures, and memorization definitions. As MLLMs are
increasingly deployed, understanding their memorization behavior has become
critical. Yet prior work has focused primarily on monolingual models, leaving
multilingual memorization underexplored, despite the inherently long-tailed
nature of training corpora. We find that the prevailing assumption, that
memorization is highly correlated with training data availability, fails to
fully explain memorization patterns in MLLMs. We hypothesize that treating
languages in isolation - ignoring their similarities - obscures the true
patterns of memorization. To address this, we propose a novel graph-based
correlation metric that incorporates language similarity to analyze
cross-lingual memorization. Our analysis reveals that among similar languages,
those with fewer training tokens tend to exhibit higher memorization, a trend
that only emerges when cross-lingual relationships are explicitly modeled.
These findings underscore the importance of a language-aware perspective in
evaluating and mitigating memorization vulnerabilities in MLLMs. This also
constitutes empirical evidence that language similarity both explains
Memorization in MLLMs and underpins Cross-lingual Transferability, with broad
implications for multilingual NLP.

</details>


### [129] [VocalBench: Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](https://arxiv.org/abs/2505.15727)
*Heyang Liu,Yuhao Wang,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出VocalBench基准测试框架，首次全面评估语音交互模型在语义质量、声学表现、对话能力和鲁棒性等维度的综合能力


<details>
  <summary>Details</summary>
Motivation: 现有语音交互评估过度关注文本质量，缺乏对语音特有维度(如声学表现、环境抗噪)的系统性测试基准

Method: 构建包含9,400个测试实例的跨维度评估体系，覆盖4大维度16项核心技能，建立标准化的语音交互评估协议

Result: 实验显示现有模型能力差异显著，不同模型在语义理解、情感表达等维度呈现互补性优势

Conclusion: VocalBench填补了语音交互评估体系空白，为构建更自然的语音对话系统提供了关键诊断工具和研发方向

Abstract: The rapid advancement of large language models (LLMs) has accelerated the
development of multi-modal models capable of vocal communication. Unlike
text-based interactions, speech conveys rich and diverse information, including
semantic content, acoustic variations, paralanguage cues, and environmental
context. However, existing evaluations of speech interaction models
predominantly focus on the quality of their textual responses, often
overlooking critical aspects of vocal performance and lacking benchmarks with
vocal-specific test instances. To address this gap, we propose VocalBench, a
comprehensive benchmark designed to evaluate speech interaction models'
capabilities in vocal communication. VocalBench comprises 9,400 carefully
curated instances across four key dimensions: semantic quality, acoustic
performance, conversational abilities, and robustness. It covers 16 fundamental
skills essential for effective vocal interaction. Experimental results reveal
significant variability in current model capabilities, each exhibiting distinct
strengths and weaknesses, and provide valuable insights to guide future
research in speech-based interaction systems. Code and evaluation instances are
available at https://github.com/SJTU-OmniAgent/VocalBench.

</details>


### [130] [DEBATE, TRAIN, EVOLVE: Self Evolution of Language Model Reasoning](https://arxiv.org/abs/2505.15734)
*Gaurav Srivastava,Zhenyu Bi,Meng Lu,Xuan Wang*

Main category: cs.CL

TL;DR: 提出无需真实标签的DTE训练框架，通过多智能体辩论轨迹进化语言模型，在多个推理基准上实现显著性能提升（GSM-PLUS提升8.92%）并展现跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs依赖海量数据提升推理能力的方式不可持续，需要开发自主提升推理能力的方法以减少对外部监督的依赖。

Method: 1. DTE框架包含多智能体辩论、模型训练和进化流程
2. 提出Reflect-Critique-Refine策略，通过显式批判和精炼机制提升辩论质量
3. 利用辩论轨迹迭代优化单一语言模型

Result: 在5个推理基准测试中：
- GSM-PLUS准确率提升8.92%
- 跨领域任务平均提升5.8%
- 6个开源模型均显示通用推理能力增强

Conclusion: DTE框架成功捕捉通用推理模式，证明通过多智能体辩论自主进化模型的可行性，为无监督提升LLMs推理能力提供新方向。

Abstract: Large language models (LLMs) have improved significantly in their reasoning
through extensive training on massive datasets. However, relying solely on
additional data for improvement is becoming increasingly impractical,
highlighting the need for models to autonomously enhance their reasoning
without external supervision. In this paper, we propose Debate, Train, Evolve
(DTE), a novel ground truth-free training framework that uses multi-agent
debate traces to evolve a single language model. We also introduce a new
prompting strategy Reflect-Critique-Refine, to improve debate quality by
explicitly instructing agents to critique and refine their reasoning. Extensive
evaluations on five reasoning benchmarks with six open-weight models show that
our DTE framework achieve substantial improvements, with an average accuracy
gain of 8.92% on the challenging GSM-PLUS dataset. Furthermore, we observe
strong cross-domain generalization, with an average accuracy gain of 5.8% on
all other benchmarks, suggesting that our method captures general reasoning
capabilities.

</details>


### [131] [Transfer of Structural Knowledge from Synthetic Languages](https://arxiv.org/abs/2505.15769)
*Mikhail Budnikov,Ivan Yamshchikov*

Main category: cs.CL

TL;DR: 提出新型合成语言Tiny-Cloze Benchmark，通过跨语言迁移学习显著提升英语多任务表现，尤其优化低算力模型评估效果


<details>
  <summary>Details</summary>
Motivation: 解决现有合成语言在迁移到自然语言（英语）时效率不足的问题，探索更优的跨语言迁移框架及轻量级模型评估体系

Method: 1. 构建改进型合成语言
2. 设计Tiny-Cloze Benchmark多领域评估基准
3. 通过微调模型验证跨语言迁移效果

Result: 新合成语言使微调模型在语义理解等任务上表现提升，Tiny-Cloze能有效评估不同算力模型的多领域能力

Conclusion: 合成语言的优化设计能有效提升自然语言任务性能，轻量级评估基准为模型能力诊断提供新工具

Abstract: This work explores transfer learning from several synthetic languages to
English. We investigate the structure of the embeddings in the fine-tuned
models, the information they contain, and the capabilities of the fine-tuned
models on simple linguistic tasks. We also introduce a new synthetic language
that leads to better transfer to English than the languages used in previous
research. Finally, we introduce Tiny-Cloze Benchmark - a new synthetic
benchmark for natural language understanding that is more informative for less
powerful models. We use Tiny-Cloze Benchmark to evaluate fine-tuned models in
several domains demonstrating that fine-tuning on a new synthetic language
allows for better performance on a variety of tasks.

</details>


### [132] [Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention](https://arxiv.org/abs/2505.15774)
*Huanxuan Liao,Wen Hu,Yao Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: HyCo²方法通过全局语义提炼与局部token保留概率分类，有效提升长文本推理效率并减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有上下文压缩方法因文本相关性分布不均和用户需求多样，常导致关键信息丢失。需结合全局语义与局部细节的压缩方案。

Method: 1. 混合适配器提炼全局语义
2. 分类层计算token保留概率
3. 引入辅助性复述/补全预训练促进全局-局部平衡融合

Result: 在7个QA基准测试中平均提升13.1%性能，保持原始方法效果同时减少88.8%token消耗。

Conclusion: HyCo²通过全局-局部协同压缩机制，在保证任务相关语义的同时保留关键细节，实现计算效率与推理质量的平衡。

Abstract: Large Language Models (LLMs) encounter significant challenges in
long-sequence inference due to computational inefficiency and redundant
processing, driving interest in context compression techniques. Existing
methods often rely on token importance to perform hard local compression or
encode context into latent representations for soft global compression.
However, the uneven distribution of textual content relevance and the diversity
of demands for user instructions mean these approaches frequently lead to the
loss of potentially valuable information. To address this, we propose
$\textbf{Hy}$brid $\textbf{Co}$ntext $\textbf{Co}$mpression (HyCo$_2$) for
LLMs, which integrates both global and local perspectives to guide context
compression while retaining both the essential semantics and critical details
for task completion. Specifically, we employ a hybrid adapter to refine global
semantics with the global view, based on the observation that different
adapters excel at different tasks. Then we incorporate a classification layer
that assigns a retention probability to each context token based on the local
view, determining whether it should be retained or discarded. To foster a
balanced integration of global and local compression, we introduce auxiliary
paraphrasing and completion pretraining before instruction tuning. This
promotes a synergistic integration that emphasizes instruction-relevant
information while preserving essential local details, ultimately balancing
local and global information retention in context compression. Experiments show
that our HyCo$_2$ method significantly enhances long-text reasoning while
reducing token usage. It improves the performance of various LLM series by an
average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover,
HyCo$_2$ matches the performance of uncompressed methods while reducing token
consumption by 88.8\%.

</details>


### [133] [ConvSearch-R1: Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.15776)
*Changtai Zhu,Siyin Wang,Ruijun Feng,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出首个自驱动的对话搜索框架ConvSearch-R1，通过强化学习直接利用检索信号优化查询重写，无需外部监督


<details>
  <summary>Details</summary>
Motivation: 现有对话查询重写方法存在对外部监督的强依赖（人工标注/大模型生成），且与下游检索器对齐不足

Method: 两阶段强化学习框架：1）检索引导的自我蒸馏策略预热解决冷启动问题；2）基于排名激励的奖励机制改进检索指标稀疏性

Result: 在TopiOCQA和QReCC数据集上显著超越SOTA方法（TopiOCQA提升超10%），且使用更小的30亿参数模型

Conclusion: 通过完全消除外部监督依赖和强化检索对齐，实现了对话搜索性能的突破性提升

Abstract: Conversational search systems require effective handling of context-dependent
queries that often contain ambiguity, omission, and coreference. Conversational
Query Reformulation (CQR) addresses this challenge by transforming these
queries into self-contained forms suitable for off-the-shelf retrievers.
However, existing CQR approaches suffer from two critical constraints: high
dependency on costly external supervision from human annotations or large
language models, and insufficient alignment between the rewriting model and
downstream retrievers. We present ConvSearch-R1, the first self-driven
framework that completely eliminates dependency on external rewrite supervision
by leveraging reinforcement learning to optimize reformulation directly through
retrieval signals. Our novel two-stage approach combines Self-Driven Policy
Warm-Up to address the cold-start problem through retrieval-guided
self-distillation, followed by Retrieval-Guided Reinforcement Learning with a
specially designed rank-incentive reward shaping mechanism that addresses the
sparsity issue in conventional retrieval metrics. Extensive experiments on
TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly
outperforms previous state-of-the-art methods, achieving over 10% improvement
on the challenging TopiOCQA dataset while using smaller 3B parameter models
without any external supervision.

</details>


### [134] [Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space](https://arxiv.org/abs/2505.15778)
*Zhen Zhang,Xuehai He,Weixiang Yan,Ao Shen,Chenyang Zhao,Shuohang Wang,Yelong Shen,Xin Eric Wang*

Main category: cs.CL

TL;DR: 提出Soft Thinking方法，通过在连续概念空间生成抽象概念标记，突破离散语言推理瓶颈，提升模型准确率2.48%并减少22.4%token使用。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型受限于离散语言标记处理，导致推理路径探索不充分，而人类思维具有抽象流动特性需要模仿。

Method: 在连续概念空间通过概率加权混合token嵌入生成软概念标记，实现多含义隐式推理与平滑过渡。

Result: 在数学与编码基准测试中pass@1准确率最高提升2.48%，同时token使用量减少达22.4%。

Conclusion: Soft Thinking有效突破离散语言推理限制，在保持高可解释性的同时显著提升性能，为AI推理开辟新方向。

Abstract: Human cognition typically involves thinking through abstract, fluid concepts
rather than strictly using discrete linguistic tokens. Current reasoning
models, however, are constrained to reasoning within the boundaries of human
language, processing discrete token embeddings that represent fixed points in
the semantic space. This discrete constraint restricts the expressive power and
upper potential of such reasoning models, often causing incomplete exploration
of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling
one token per step. In this work, we introduce Soft Thinking, a training-free
method that emulates human-like "soft" reasoning by generating soft, abstract
concept tokens in a continuous concept space. These concept tokens are created
by the probability-weighted mixture of token embeddings, which form the
continuous concept space, enabling smooth transitions and richer
representations that transcend traditional discrete boundaries. In essence,
each generated concept token encapsulates multiple meanings from related
discrete tokens, implicitly exploring various reasoning paths to converge
effectively toward the correct answer. Empirical evaluations on diverse
mathematical and coding benchmarks consistently demonstrate the effectiveness
and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points
while simultaneously reducing token usage by up to 22.4% compared to standard
CoT. Qualitative analysis further reveals that Soft Thinking outputs remain
highly interpretable and readable, highlighting the potential of Soft Thinking
to break the inherent bottleneck of discrete language-based reasoning. Code is
available at https://github.com/eric-ai-lab/Soft-Thinking.

</details>


### [135] [dKV-Cache: The Cache for Diffusion Language Models](https://arxiv.org/abs/2505.15781)
*Xinyin Ma,Runpeng Yu,Gongfan Fang,Xinchao Wang*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Diffusion Language Models (DLMs) have been seen as a promising competitor for
autoregressive language models. However, diffusion language models have long
been constrained by slow inference. A core challenge is that their
non-autoregressive architecture and bidirectional attention preclude the
key-value cache that accelerates decoding. We address this bottleneck by
proposing a KV-cache-like mechanism, delayed KV-Cache, for the denoising
process of DLMs. Our approach is motivated by the observation that different
tokens have distinct representation dynamics throughout the diffusion process.
Accordingly, we propose a delayed and conditioned caching strategy for key and
value states. We design two complementary variants to cache key and value
step-by-step: (1) dKV-Cache-Decode, which provides almost lossless
acceleration, and even improves performance on long sequences, suggesting that
existing DLMs may under-utilise contextual information during inference. (2)
dKV-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving
higher speed-ups with quadratic time complexity at the cost of some performance
degradation. dKV-Cache, in final, achieves from 2-10x speedup in inference,
largely narrowing the gap between ARs and DLMs. We evaluate our dKV-Cache on
several benchmarks, delivering acceleration across general language
understanding, mathematical, and code-generation benchmarks. Experiments
demonstrate that cache can also be used in DLMs, even in a training-free manner
from current DLMs.

</details>


### [136] [Long-Form Information Alignment Evaluation Beyond Atomic Facts](https://arxiv.org/abs/2505.15792)
*Danna Zheng,Mirella Lapata,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 提出DoveScore框架，通过联合验证事实准确性与事件顺序一致性，将细粒度评估效果提升8%以上


<details>
  <summary>Details</summary>
Motivation: 现有细粒度评估方法（如FactScore）忽视事实间依赖关系，导致存在隐蔽漏洞，无法识别真实陈述组合形成的欺骗性叙述

Method: 构建MontageLie基准测试集，开发DoveScore框架同时验证事实准确性（factual accuracy）和事件顺序一致性（event-order consistency）

Result: DoveScore的AUC-ROC优于现有方法8%，传统方法在MontageLie攻击下AUC-ROC低于65%

Conclusion: DoveScore通过建模事实间关系，为长文本对齐评估提供了更鲁棒的解决方案，代码和数据集已开源

Abstract: Information alignment evaluators are vital for various NLG evaluation tasks
and trustworthy LLM deployment, reducing hallucinations and enhancing user
trust. Current fine-grained methods, like FactScore, verify facts individually
but neglect inter-fact dependencies, enabling subtle vulnerabilities. In this
work, we introduce MontageLie, a challenging benchmark that constructs
deceptive narratives by "montaging" truthful statements without introducing
explicit hallucinations. We demonstrate that both coarse-grained LLM-based
evaluators and current fine-grained frameworks are susceptible to this attack,
with AUC-ROC scores falling below 65%. To enable more robust fine-grained
evaluation, we propose DoveScore, a novel framework that jointly verifies
factual accuracy and event-order consistency. By modeling inter-fact
relationships, DoveScore outperforms existing fine-grained methods by over 8%,
providing a more robust solution for long-form text alignment evaluation. Our
code and datasets are available at https://github.com/dannalily/DoveScore.

</details>


### [137] [Reverse Engineering Human Preferences with Reinforcement Learning](https://arxiv.org/abs/2505.15795)
*Lisa Alazraki,Tan Yi-Chern,Jon Ander Campos,Maximilian Mozes,Marek Rei,Max Bartolo*

Main category: cs.CL

TL;DR: 通过强化学习优化前置文本提升LLM评估分数，揭示LLM-as-a-judge评估框架的脆弱性


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估框架易被恶意攻击，需证明其脆弱性及开发更隐蔽的对抗方法

Method: 利用judge-LLM反馈信号，训练对抗性模型生成优化评分的前置文本（preamble）

Result: 该方法在跨模型场景中显著提升评分，且比直接修改答案更难以检测

Conclusion: LLM评估体系需改进可靠性，该方法展示了逆向工程人类偏好的潜力

Abstract: The capabilities of Large Language Models (LLMs) are routinely evaluated by
other LLMs trained to predict human preferences. This framework--known as
LLM-as-a-judge--is highly scalable and relatively low cost. However, it is also
vulnerable to malicious exploitation, as LLM responses can be tuned to overfit
the preferences of the judge. Previous work shows that the answers generated by
a candidate-LLM can be edited post hoc to maximise the score assigned to them
by a judge-LLM. In this study, we adopt a different approach and use the signal
provided by judge-LLMs as a reward to adversarially tune models that generate
text preambles designed to boost downstream performance. We find that frozen
LLMs pipelined with these models attain higher LLM-evaluation scores than
existing frameworks. Crucially, unlike other frameworks which intervene
directly on the model's response, our method is virtually undetectable. We also
demonstrate that the effectiveness of the tuned preamble generator transfers
when the candidate-LLM and the judge-LLM are replaced with models that are not
used during training. These findings raise important questions about the design
of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that
human preferences can be reverse engineered effectively, by pipelining LLMs to
optimise upstream preambles via reinforcement learning--an approach that could
find future applications in diverse tasks and domains beyond adversarial
attacks.

</details>


### [138] [VerifyBench: Benchmarking Reference-based Reward Systems for Large Language Models](https://arxiv.org/abs/2505.15801)
*Yuchen Yan,Jin Jiang,Zhenbang Ren,Yijun Li,Xudong Cai,Yang Liu,Xin Xu,Mengdi Zhang,Jian Shao,Yongliang Shen,Jun Xiao,Yueting Zhuang*

Main category: cs.CL

TL;DR: 提出VerifyBench和VerifyBench-Hard基准，评估基于参考的奖励系统，揭示当前模型在验证器准确性和推理能力上的不足


<details>
  <summary>Details</summary>
Motivation: 现有奖励基准缺乏对参考型奖励系统的评估，导致研究者难以准确理解强化学习训练中验证器的实际效果

Method: 通过系统化数据收集、人工标注和严格质量管控构建评估基准，包含标准版和困难版两个测试集

Result: 当前模型（特别是小规模模型）在两个基准上表现欠佳，验证器准确率最大存在30%差距

Conclusion: 新基准为提升验证器精度和强化学习模型的推理能力提供了有效评估工具，指明模型优化方向

Abstract: Large reasoning models such as OpenAI o1 and DeepSeek-R1 have achieved
remarkable performance in the domain of reasoning. A key component of their
training is the incorporation of verifiable rewards within reinforcement
learning (RL). However, existing reward benchmarks do not evaluate
reference-based reward systems, leaving researchers with limited understanding
of the accuracy of verifiers used in RL. In this paper, we introduce two
benchmarks, VerifyBench and VerifyBench-Hard, designed to assess the
performance of reference-based reward systems. These benchmarks are constructed
through meticulous data collection and curation, followed by careful human
annotation to ensure high quality. Current models still show considerable room
for improvement on both VerifyBench and VerifyBench-Hard, especially
smaller-scale models. Furthermore, we conduct a thorough and comprehensive
analysis of evaluation results, offering insights for understanding and
developing reference-based reward systems. Our proposed benchmarks serve as
effective tools for guiding the development of verifier accuracy and the
reasoning capabilities of models trained via RL in reasoning tasks.

</details>


### [139] [Keep Security! Benchmarking Security Policy Preservation in Large Language Model Contexts Against Indirect Attacks in Question Answering](https://arxiv.org/abs/2505.15805)
*Hwan Chang,Yumin Kim,Yonghyun Jun,Hwanhee Lee*

Main category: cs.CL

TL;DR: 大型语言模型在敏感领域部署时存在信息泄露风险，新基准CoPriva揭示模型在间接攻击下存在显著安全漏洞，需强化安全机制


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对上下文安全保护的大规模基准测试，LLM在整合用户安全策略时存在重大安全隐患，尤其在敏感领域应用中

Method: 构建包含现实场景策略和攻击性查询的CoPriva数据集，测试10个LLM在直接/间接攻击下的策略遵守能力

Result: 主流LLM普遍违反非披露政策（间接攻击成功率更高），模型虽能识别正确答案但缺乏策略整合能力，明确提示可部分改善输出

Conclusion: 当前LLM安全对齐机制在上下文保护方面存在严重不足，亟需开发能有效融合策略约束的新型安全增强方法

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains such as enterprise and government, ensuring that they adhere to
user-defined security policies within context is critical-especially with
respect to information non-disclosure. While prior LLM studies have focused on
general safety and socially sensitive data, large-scale benchmarks for
contextual security preservation against attacks remain lacking. To address
this, we introduce a novel large-scale benchmark dataset, CoPriva, evaluating
LLM adherence to contextual non-disclosure policies in question answering.
Derived from realistic contexts, our dataset includes explicit policies and
queries designed as direct and challenging indirect attacks seeking prohibited
information. We evaluate 10 LLMs on our benchmark and reveal a significant
vulnerability: many models violate user-defined policies and leak sensitive
information. This failure is particularly severe against indirect attacks,
highlighting a critical gap in current LLM safety alignment for sensitive
applications. Our analysis reveals that while models can often identify the
correct answer to a query, they struggle to incorporate policy constraints
during generation. In contrast, they exhibit a partial ability to revise
outputs when explicitly prompted. Our findings underscore the urgent need for
more robust methods to guarantee contextual security.

</details>


### [140] [The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation](https://arxiv.org/abs/2505.15807)
*Patrick Kahardipraja,Reduan Achtibat,Thomas Wiegand,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.CL

TL;DR: 研究通过分析大语言模型中注意力头的功能，揭示了检索增强问答的知识来源机制，为提升模型安全性和可解释性提供新思路


<details>
  <summary>Details</summary>
Motivation: 检索增强的语言模型虽能利用上下文学习获取外部知识，但其内部工作机制仍不明确，需深入解析不同注意力头在知识整合中的作用

Method: 提出基于归因的分析方法识别两类注意力头（上下文检索头/参数知识头），通过提取功能向量并修改注意力权重，揭示其对答案生成的调控机制

Result: 成功追踪推理过程中使用的知识来源，验证上下文检索头负责指令理解和情境信息抽取，参数知识头存储实体关系知识

Conclusion: 该研究为构建透明可控的语言模型提供了方法论基础，通过知识溯源机制的设计推动模型安全性和可解释性发展

Abstract: Large language models are able to exploit in-context learning to access
external knowledge beyond their training data through retrieval-augmentation.
While promising, its inner workings remain unclear. In this work, we shed light
on the mechanism of in-context retrieval augmentation for question answering by
viewing a prompt as a composition of informational components. We propose an
attribution-based method to identify specialized attention heads, revealing
in-context heads that comprehend instructions and retrieve relevant contextual
information, and parametric heads that store entities' relational knowledge. To
better understand their roles, we extract function vectors and modify their
attention weights to show how they can influence the answer generation process.
Finally, we leverage the gained insights to trace the sources of knowledge used
during inference, paving the way towards more safe and transparent language
models.

</details>


### [141] [GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents](https://arxiv.org/abs/2505.15810)
*Yuqi Zhou,Sunhao Dai,Shuai Wang,Kaiwen Zhou,Qinqlin Jia,Junxu*

Main category: cs.CL

TL;DR: 提出GUI-G1-3B模型，通过改进强化学习的输入模板、奖励函数和训练策略，在GUI代理接地任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理的R1-Zero范式存在三大问题：过长的思维链损害定位性能、奖励函数易被尺寸特征破解、强化学习在困难样本上欠优化

Method: 1. 快速思维模板减少冗余推理
2. 奖励函数增加尺寸约束
3. 改进RL目标函数（长度标准化+难度感知缩放）

Result: 在ScreenSpot(90.3%)和ScreenSpot-Pro(37.1%)数据集上超越所有同规模模型，甚至优于更大的UI-TARS-7B

Conclusion: 该方法通过针对性改进RL训练组件，为GUI代理的视觉接地任务提供了新的技术路径和基准模型

Abstract: Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm,
coupling online Reinforcement Learning (RL) with explicit chain-of-thought
reasoning prior to object grounding and thereby achieving substantial
performance gains. In this paper, we first conduct extensive analysis
experiments of three key components of that training pipeline: input design,
output evaluation, and policy update-each revealing distinct challenges arising
from blindly applying general-purpose RL without adapting to GUI grounding
tasks. Input design: Current templates encourage the model to generate
chain-of-thought reasoning, but longer chains unexpectedly lead to worse
grounding performance. Output evaluation: Reward functions based on hit signals
or box area allow models to exploit box size, leading to reward hacking and
poor localization quality. Policy update: Online RL tends to overfit easy
examples due to biases in length and sample difficulty, leading to
under-optimization on harder cases. To address these issues, we propose three
targeted solutions. First, we adopt a Fast Thinking Template that encourages
direct answer generation, reducing excessive reasoning during training. Second,
we incorporate a box size constraint into the reward function to mitigate
reward hacking. Third, we revise the RL objective by adjusting length
normalization and adding a difficulty-aware scaling factor, enabling better
optimization on hard samples. Our GUI-G1-3B, trained on 17K public samples with
Qwen2.5-VL-3B-Instruct, achieves 90.3% accuracy on ScreenSpot and 37.1% on
ScreenSpot-Pro. This surpasses all prior models of similar size and even
outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI
agent grounding. The project repository is available at
https://github.com/Yuqi-Zhou/GUI-G1.

</details>


### [142] [Learning to Reason via Mixture-of-Thought for Logical Reasoning](https://arxiv.org/abs/2505.15817)
*Tong Zheng,Lichang Chen,Simeng Han,R. Thomas McCoy,Heng Huang*

Main category: cs.CL

TL;DR: 提出MoT框架，通过自然语言、代码和真值表三模态混合推理提升大模型逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在训练时仅使用单一推理模态（如自然语言），限制了多模态间的协同效应。真值表模态可系统化枚举逻辑案例，缓解自然语言推理的缺陷

Method: 两阶段框架：1) 自进化训练阶段联合学习多模态自生成原理；2) 推理阶段融合三模态协同预测

Result: 在FOLIO和ProofWriter基准测试中平均准确率最高提升11.7个百分点，特别在困难问题上表现突出

Conclusion: 多模态互补优势显著，真值表推理有效突破自然语言瓶颈，MoT框架在训练和推理阶段均产生增益

Abstract: Human beings naturally utilize multiple reasoning modalities to learn and
solve logical problems, i.e., different representational formats such as
natural language, code, and symbolic logic. In contrast, most existing
LLM-based approaches operate with a single reasoning modality during training,
typically natural language. Although some methods explored modality selection
or augmentation at inference time, the training process remains modality-blind,
limiting synergy among modalities. To fill in this gap, we propose
Mixture-of-Thought (MoT), a framework that enables LLMs to reason across three
complementary modalities: natural language, code, and a newly introduced
symbolic modality, truth-table, which systematically enumerates logical cases
and partially mitigates key failure modes in natural language reasoning. MoT
adopts a two-phase design: (1) self-evolving MoT training, which jointly learns
from filtered, self-generated rationales across modalities; and (2) MoT
inference, which fully leverages the synergy of three modalities to produce
better predictions. Experiments on logical reasoning benchmarks including FOLIO
and ProofWriter demonstrate that our MoT framework consistently and
significantly outperforms strong LLM baselines with single-modality
chain-of-thought approaches, achieving up to +11.7pp average accuracy gain.
Further analyses show that our MoT framework benefits both training and
inference stages; that it is particularly effective on harder logical reasoning
problems; and that different modalities contribute complementary strengths,
with truth-table reasoning helping to overcome key bottlenecks in natural
language inference.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [143] [Geodesic distance approximation using a surface finite element method for the $p$-Laplacian](https://arxiv.org/abs/2505.14732)
*Hannah Potgieter,Razvan C. Fetecau,Steven J. Ruuth*

Main category: cs.GR

TL;DR: 使用大p值p-Laplacian算子近似曲面上的测地距离，相比现有PDE方法展现优势


<details>
  <summary>Details</summary>
Motivation: 解决传统PDE方法（如热方程法、多面体法）在测地距离计算中的精度和鲁棒性问题，提出更优的替代方案

Method: 1. 采用曲面有限元方案
2. 验证数值解对真实测地距离的收敛性
3. 通过三角不等式检验和顶点扰动鲁棒性测试
4. 与Crane的热方法、Mitchell多面体法进行对比

Result: 1. 数值解收敛于真实测地距离
2. 满足三角不等式
3. 对几何噪声具有强鲁棒性
4. 相比对比方法展现优势

Conclusion: 大p值p-Laplacian方法为曲面测地距离计算提供了更精确可靠的解决方案，在计算几何领域具有应用潜力

Abstract: We use the $p$-Laplacian with large $p$-values in order to approximate
geodesic distances to features on surfaces. This differs from Fayolle and
Belyaev's (2018) [1] computational results using the $p$-Laplacian for the
distance-to-surface problem. Our approach appears to offer some distinct
advantages over other popular PDE-based distance function approximation
methods. We employ a surface finite element scheme and demonstrate numerical
convergence to the true geodesic distance functions. We check that our
numerical results adhere to the triangle inequality and examine robustness
against geometric noise such as vertex perturbations. We also present
comparisons of our method with the heat method from Crane et al. [2] and the
classical polyhedral method from Mitchell et al. [3].

</details>


### [144] [Building LOD Representation for 3D Urban Scenes](https://arxiv.org/abs/2505.15190)
*Shanshan Pan,Runze Zhang,Yilin Liu,Minglun Gong,Hui Huang*

Main category: cs.GR

TL;DR: Proposes a LOD-Tree algorithm for generating semantic-aware multi-level 3D urban models from noisy reconstructions.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of excessive geometry primitives, noise, and lack of semantics in automatically reconstructed 3D urban models, which hinder interactive manipulation and VR rendering.

Method: Analyzes planar primitives to group them into hierarchical level sets, constructs LOD-Tree with these sets as nodes, and generates LOD representations by selecting appropriate tree depths.

Result: Experiments on complex urban scenes demonstrate generation of clean, accurate, and semantically meaningful LOD representations.

Conclusion: The LOD-Tree approach effectively solves multi-level detail representation while preserving semantic structures in noisy 3D urban models.

Abstract: The advances in 3D reconstruction technology, such as photogrammetry and
LiDAR scanning, have made it easier to reconstruct accurate and detailed 3D
models for urban scenes. Nevertheless, these reconstructed models often contain
a large number of geometry primitives, making interactive manipulation and
rendering challenging, especially on resource-constrained devices like virtual
reality platforms. Therefore, the generation of appropriate levels-of-detail
(LOD) representations for these models is crucial. Additionally, automatically
reconstructed 3D models tend to suffer from noise and lack semantic
information. Dealing with these issues and creating LOD representations that
are robust against noise while capturing the semantic meaning present
significant challenges. In this paper, we propose a novel algorithm to address
these challenges. We begin by analysing the properties of planar primitives
detected from the input and group these primitives into multiple level sets by
forming meaningful 3D structures. These level sets form the nodes of our
innovative LOD-Tree. By selecting nodes at appropriate depths within the
LOD-Tree, different LOD representations can be generated. Experimental results
on real and complex urban scenes demonstrate the merits of our approach in
generating clean, accurate, and semantically meaningful LOD representations.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [145] [ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges](https://arxiv.org/abs/2505.15068)
*Cheng Qian,Hongyi Du,Hongru Wang,Xiusi Chen,Yuji Zhang,Avirup Sil,Chengxiang Zhai,Kathleen McKeown,Heng Ji*

Main category: cs.AI

TL;DR: 开发了ModelingBench基准和ModelingAgent框架，用于解决现实世界复杂建模问题，并通过ModelingJudge评估系统实现专家级解决方案验证


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法反映需要跨学科推理和工具整合的真实世界问题复杂性

Method: 1) 构建包含数学建模竞赛真实问题的ModelingBench基准；2) 设计多智能体框架ModelingAgent协调工具使用和迭代优化；3) 创建专家循环评估系统ModelingJudge进行多维度方案评估

Result: ModelingAgent在基准测试中显著优于基线模型，其生成的解决方案与人类专家方案难以区分（human-indistinguishable）

Conclusion: 该工作为开放型跨学科建模挑战提供了包含基准测试、求解框架和评估系统的完整解决方案体系

Abstract: Recent progress in large language models (LLMs) has enabled substantial
advances in solving mathematical problems. However, existing benchmarks often
fail to reflect the complexity of real-world problems, which demand open-ended,
interdisciplinary reasoning and integration of computational tools. To address
this gap, we introduce ModelingBench, a novel benchmark featuring
real-world-inspired, open-ended problems from math modeling competitions across
diverse domains, ranging from urban traffic optimization to ecosystem resource
planning. These tasks require translating natural language into formal
mathematical formulations, applying appropriate tools, and producing
structured, defensible reports. ModelingBench also supports multiple valid
solutions, capturing the ambiguity and creativity of practical modeling. We
also present ModelingAgent, a multi-agent framework that coordinates tool use,
supports structured workflows, and enables iterative self-refinement to
generate well-grounded, creative solutions. To evaluate outputs, we further
propose ModelingJudge, an expert-in-the-loop system leveraging LLMs as
domain-specialized judges assessing solutions from multiple expert
perspectives. Empirical results show that ModelingAgent substantially
outperforms strong baselines and often produces solutions indistinguishable
from those of human experts. Together, our work provides a comprehensive
framework for evaluating and advancing real-world problem-solving in
open-ended, interdisciplinary modeling challenges.

</details>


### [146] [When Can Large Reasoning Models Save Thinking? Mechanistic Analysis of Behavioral Divergence in Reasoning](https://arxiv.org/abs/2505.15276)
*Rongzhi Zhu,Yi Liu,Zequn Sun,Yiwei Wang,Wei Hu*

Main category: cs.AI

TL;DR: 研究揭示强化学习优化的大型推理模型存在三种思考模式（NT/ET/IT），不同模式在效率与准确性间存在权衡，需改进优化方法


<details>
  <summary>Details</summary>
Motivation: 探究强化学习训练的LRMs在节省思考时的内部工作机制，分析不同思考模式对模型效率的影响

Method: 通过分析终止思考的信心水平、思考到生成阶段的注意力转移，以及模型对输入不同部分的关注度，揭示推理行为机制

Result: NT模式以降低准确性为代价缩短输出长度，ET和IT模式能在保持准确性的同时减少响应长度

Conclusion: 强化学习优化的LRMs存在内在不一致性，需要开发适应性改进方案来提升可靠性和效率

Abstract: Large reasoning models (LRMs) have significantly advanced performance on
complex tasks, yet their tendency to overthink introduces inefficiencies. This
study investigates the internal mechanisms of reinforcement learning
(RL)-trained LRMs when prompted to save thinking, revealing three distinct
thinking modes: no thinking (NT), explicit thinking (ET), and implicit thinking
(IT). Through comprehensive analysis of confidence in thinking termination,
attention from thinking to generation, and attentional focus on input sections,
we uncover key factors influencing the reasoning behaviors. We further find
that NT reduces output length at the cost of accuracy, while ET and IT maintain
accuracy with reduced response length. Our findings expose fundamental
inconsistencies in RL-optimized LRMs, necessitating adaptive improvements for
reliable efficiency.

</details>


### [147] [When to Continue Thinking: Adaptive Thinking Mode Switching for Efficient Reasoning](https://arxiv.org/abs/2505.15400)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Haodong Zhao,Hao Li,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出自适应自我恢复推理框架(ASRR)，通过抑制冗余推理和自适应分配计算资源，显著提升大型推理模型的效率和安全性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在简单任务上存在计算冗余问题，传统方法无法有效平衡推理效率与性能。

Method: 利用模型内部自我恢复机制，结合准确度感知的长度奖励调节机制，动态分配推理计算资源。

Result: 在1.5B/7B模型上分别降低32.5%/25.7%推理成本，准确率仅损失1.2%/0.6%，安全基准无害率最高提升21.7%。

Conclusion: ASRR框架有效实现效率-性能平衡，为大型推理模型的高效自适应推理提供了创新解决方案。

Abstract: Large reasoning models (LRMs) achieve remarkable performance via long
reasoning chains, but often incur excessive computational overhead due to
redundant reasoning, especially on simple tasks. In this work, we
systematically quantify the upper bounds of LRMs under both Long-Thinking and
No-Thinking modes, and uncover the phenomenon of "Internal Self-Recovery
Mechanism" where models implicitly supplement reasoning during answer
generation. Building on this insight, we propose Adaptive Self-Recovery
Reasoning (ASRR), a framework that suppresses unnecessary reasoning and enables
implicit recovery. By introducing accuracy-aware length reward regulation, ASRR
adaptively allocates reasoning effort according to problem difficulty,
achieving high efficiency with negligible performance sacrifice. Experiments
across multiple benchmarks and models show that, compared with GRPO, ASRR
reduces reasoning budget by up to 32.5% (1.5B) and 25.7% (7B) with minimal
accuracy loss (1.2% and 0.6% pass@1), and significantly boosts harmless rates
on safety benchmarks (up to +21.7%). Our results highlight the potential of
ASRR for enabling efficient, adaptive, and safer reasoning in LRMs.

</details>


### [148] [ClickSight: Interpreting Student Clickstreams to Reveal Insights on Learning Strategies via LLMs](https://arxiv.org/abs/2505.15410)
*Bahar Radmehr,Ekaterina Shved,Fatma Betül Güreş,Adish Singla,Tanja Käser*

Main category: cs.AI

TL;DR: 利用大语言模型从教育点击流数据中生成理论驱动的学习策略解释


<details>
  <summary>Details</summary>
Motivation: 现有基于手工特征和专家标注的点击流分析方法存在通用性和扩展性限制，需要更自动化的理论驱动解释方法

Method: 开发ClickSight框架：通过输入原始点击流和学习策略列表，采用四种提示策略的LLM生成解释，并引入自我优化机制

Result: LLM能合理生成解释但质量因提示策略而异，自我优化改进有限，在开放式学习环境中展现理论驱动分析潜力

Conclusion: ClickSight证明了LLM在教育交互数据分析中的应用可行性，提示策略选择对结果质量具有关键影响

Abstract: Clickstream data from digital learning environments offer valuable insights
into students' learning behaviors, but are challenging to interpret due to
their high dimensionality and granularity. Prior approaches have relied mainly
on handcrafted features, expert labeling, clustering, or supervised models,
therefore often lacking generalizability and scalability. In this work, we
introduce ClickSight, an in-context Large Language Model (LLM)-based pipeline
that interprets student clickstreams to reveal their learning strategies.
ClickSight takes raw clickstreams and a list of learning strategies as input
and generates textual interpretations of students' behaviors during
interaction. We evaluate four different prompting strategies and investigate
the impact of self-refinement on interpretation quality. Our evaluation spans
two open-ended learning environments and uses a rubric-based domain-expert
evaluation. Results show that while LLMs can reasonably interpret learning
strategies from clickstreams, interpretation quality varies by prompting
strategy, and self-refinement offers limited improvement. ClickSight
demonstrates the potential of LLMs to generate theory-driven insights from
educational interaction data.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [149] [Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications](https://arxiv.org/abs/2505.15741)
*Dikshit Chauhan,Bapi Dutta,Indu Bala,Niki van Stein,Thomas Bäck,Anupam Yadav*

Main category: cs.NE

TL;DR: 探讨LLM与EC的协同整合，通过双向优化提升AI能力


<details>
  <summary>Details</summary>
Motivation: 结合LLMs的自然语言理解优势与EC的优化搜索能力，形成互补：EC可优化LLM的提示工程/架构搜索，LLM可自动化EC的设计分析与启发式生成

Method: 1. 用EC优化LLM的提示工程、超参数调优、架构搜索
2. 用LLM自动化元启发式设计、进化算法调参及自适应启发式生成

Result: 建立双向增强框架：EC提升LLM训练效率，LLM使EC设计效率提升5-10倍，开发出多领域协同应用案例

Conclusion: 需解决计算成本/可解释性/收敛性挑战，未来应发展融合两者优势的混合架构，特别是在自动化AI工作流方向

Abstract: Integrating Large Language Models (LLMs) and Evolutionary Computation (EC)
represents a promising avenue for advancing artificial intelligence by
combining powerful natural language understanding with optimization and search
capabilities. This manuscript explores the synergistic potential of LLMs and
EC, reviewing their intersections, complementary strengths, and emerging
applications. We identify key opportunities where EC can enhance LLM training,
fine-tuning, prompt engineering, and architecture search, while LLMs can, in
turn, aid in automating the design, analysis, and interpretation of ECs. The
manuscript explores the synergistic integration of EC and LLMs, highlighting
their bidirectional contributions to advancing artificial intelligence. It
first examines how EC techniques enhance LLMs by optimizing key components such
as prompt engineering, hyperparameter tuning, and architecture search,
demonstrating how evolutionary methods automate and refine these processes.
Secondly, the survey investigates how LLMs improve EC by automating
metaheuristic design, tuning evolutionary algorithms, and generating adaptive
heuristics, thereby increasing efficiency and scalability. Emerging
co-evolutionary frameworks are discussed, showcasing applications across
diverse fields while acknowledging challenges like computational costs,
interpretability, and algorithmic convergence. The survey concludes by
identifying open research questions and advocating for hybrid approaches that
combine the strengths of EC and LLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [150] [Sentiment Analysis in Software Engineering: Evaluating Generative Pre-trained Transformers](https://arxiv.org/abs/2505.14692)
*KM Khalid Saifullah,Faiaz Azmain,Habiba Hye*

Main category: cs.SE

TL;DR: 研究比较了BERT和GPT-4o-mini在软件工程情感分析中的表现，发现模型性能高度依赖数据集特性，GPT-4o-mini在复杂数据上展现更好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统软件工程情感分析工具难以捕捉领域特有的语言细微差别，需评估先进模型（如BERT和GPT）在该领域的适应性。

Method: 使用GitHub、Stack Overflow和Jira数据集，对双向变换器（BERT）和生成式模型（GPT-4o-mini）进行微调与默认配置的对比测试。

Result: 在结构化数据集（GitHub/Jira）中微调GPT-4o-mini达到0.93-0.98 F1值，但在复杂不平衡的Stack Overflow数据上默认GPT模型准确率达85.3%，显著优于微调版本的13.1%。

Conclusion: 应根据数据集特性选择模型架构，平衡微调与预训练优势，未来需开发针对软件工程领域优化的专用情感分析工具。

Abstract: Sentiment analysis plays a crucial role in understanding developer
interactions, issue resolutions, and project dynamics within software
engineering (SE). While traditional SE-specific sentiment analysis tools have
made significant strides, they often fail to account for the nuanced and
context-dependent language inherent to the domain. This study systematically
evaluates the performance of bidirectional transformers, such as BERT, against
generative pre-trained transformers, specifically GPT-4o-mini, in SE sentiment
analysis. Using datasets from GitHub, Stack Overflow, and Jira, we benchmark
the models' capabilities with fine-tuned and default configurations. The
results reveal that fine-tuned GPT-4o-mini performs comparable to BERT and
other bidirectional models on structured and balanced datasets like GitHub and
Jira, achieving macro-averaged F1-scores of 0.93 and 0.98, respectively.
However, on linguistically complex datasets with imbalanced sentiment
distributions, such as Stack Overflow, the default GPT-4o-mini model exhibits
superior generalization, achieving an accuracy of 85.3\% compared to the
fine-tuned model's 13.1\%. These findings highlight the trade-offs between
fine-tuning and leveraging pre-trained models for SE tasks. The study
underscores the importance of aligning model architectures with dataset
characteristics to optimize performance and proposes directions for future
research in refining sentiment analysis tools tailored to the SE domain.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [151] [An Alternative to FLOPS Regularization to Effectively Productionize SPLADE-Doc](https://arxiv.org/abs/2505.15070)
*Aldo Porco,Dhruv Mehra,Igor Malioutov,Karthik Radhakrishnan,Moniba Keymanesh,Daniel Preoţiuc-Pietro,Sean MacAvaney,Pengxiang Cheng*

Main category: cs.IR

TL;DR: 提出DF-FLOPS正则化方法，通过惩罚高文档频率词汇降低检索延迟，同时保持检索效果


<details>
  <summary>Details</summary>
Motivation: 现有SPLADE模型的FLOPS正则化无法控制高文档频率(DF)词汇，导致生产环境检索引擎延迟增加

Method: 在FLOPS正则化基础上增加DF惩罚项，动态调整高频率词汇权重，平衡稀疏性和词汇重要性

Result: 检索延迟降低约10倍（与BM25相当），领域内MRR@10仅下降2.2点，跨领域12/13任务性能提升

Conclusion: DF-FLOPS为LSR模型实用化提供关键改进，使检索延迟与BM25相当同时保持效果，推动生产环境部署

Abstract: Learned Sparse Retrieval (LSR) models encode text as weighted term vectors,
which need to be sparse to leverage inverted index structures during retrieval.
SPLADE, the most popular LSR model, uses FLOPS regularization to encourage
vector sparsity during training. However, FLOPS regularization does not ensure
sparsity among terms - only within a given query or document. Terms with very
high Document Frequencies (DFs) substantially increase latency in production
retrieval engines, such as Apache Solr, due to their lengthy posting lists. To
address the issue of high DFs, we present a new variant of FLOPS
regularization: DF-FLOPS. This new regularization technique penalizes the usage
of high-DF terms, thereby shortening posting lists and reducing retrieval
latency. Unlike other inference-time sparsification methods, such as stopword
removal, DF-FLOPS regularization allows for the selective inclusion of
high-frequency terms in cases where the terms are truly salient. We find that
DF-FLOPS successfully reduces the prevalence of high-DF terms and lowers
retrieval latency (around 10x faster) in a production-grade engine while
maintaining effectiveness both in-domain (only a 2.2-point drop in MRR@10) and
cross-domain (improved performance in 12 out of 13 tasks on which we tested).
With retrieval latencies on par with BM25, this work provides an important step
towards making LSR practical for deployment in production-grade search engines.

</details>


### [152] [MIRB: Mathematical Information Retrieval Benchmark](https://arxiv.org/abs/2505.15585)
*Haocheng Ju,Bin Dong*

Main category: cs.IR

TL;DR: 提出MIRB基准用于评估数学信息检索模型，涵盖4个任务和12个数据集，评估13个模型并分析挑战


<details>
  <summary>Details</summary>
Motivation: 现有数学信息检索任务缺乏统一评估基准，制约有效检索模型的开发

Method: 构建包含语义语句检索、问答检索、前提检索和公式检索的MIRB基准，覆盖12个数据集并评估13种检索模型

Result: 现有模型在数学领域检索中存在不足，数学符号和结构复杂性带来独特挑战

Conclusion: MIRB为数学信息检索系统提供全面评估框架，推动开发针对性更强的检索模型

Abstract: Mathematical Information Retrieval (MIR) is the task of retrieving
information from mathematical documents and plays a key role in various
applications, including theorem search in mathematical libraries, answer
retrieval on math forums, and premise selection in automated theorem proving.
However, a unified benchmark for evaluating these diverse retrieval tasks has
been lacking. In this paper, we introduce MIRB (Mathematical Information
Retrieval Benchmark) to assess the MIR capabilities of retrieval models. MIRB
includes four tasks: semantic statement retrieval, question-answer retrieval,
premise retrieval, and formula retrieval, spanning a total of 12 datasets. We
evaluate 13 retrieval models on this benchmark and analyze the challenges
inherent to MIR. We hope that MIRB provides a comprehensive framework for
evaluating MIR systems and helps advance the development of more effective
retrieval models tailored to the mathematical domain.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [153] [AI vs. Human Judgment of Content Moderation: LLM-as-a-Judge and Ethics-Based Response Refusals](https://arxiv.org/abs/2505.15365)
*Stefan Pasch*

Main category: cs.HC

TL;DR: LLM评估系统对伦理拒绝的评分显著高于人类用户，存在审核偏见


<details>
  <summary>Details</summary>
Motivation: 研究LLM评估系统与人类在拒绝响应评价上的差异，揭示自动化评估系统的潜在偏见

Method: 使用Chatbot Arena数据和GPT-4o/Llama3 70B模型评估两种拒绝类型（伦理拒绝vs技术拒绝）

Result: 模型评估者对伦理拒绝的评分比人类高14.8%，技术拒绝无显著差异

Conclusion: 审核偏见暴露了自动评估系统在价值观对齐和透明度方面的挑战，需重新审视评估系统的规范性假设

Abstract: As large language models (LLMs) are increasingly deployed in high-stakes
settings, their ability to refuse ethically sensitive prompts-such as those
involving hate speech or illegal activities-has become central to content
moderation and responsible AI practices. While refusal responses can be viewed
as evidence of ethical alignment and safety-conscious behavior, recent research
suggests that users may perceive them negatively. At the same time, automated
assessments of model outputs are playing a growing role in both evaluation and
training. In particular, LLM-as-a-Judge frameworks-in which one model is used
to evaluate the output of another-are now widely adopted to guide benchmarking
and fine-tuning. This paper examines whether such model-based evaluators assess
refusal responses differently than human users. Drawing on data from Chatbot
Arena and judgments from two AI judges (GPT-4o and Llama 3 70B), we compare how
different types of refusals are rated. We distinguish ethical refusals, which
explicitly cite safety or normative concerns (e.g., "I can't help with that
because it may be harmful"), and technical refusals, which reflect system
limitations (e.g., "I can't answer because I lack real-time data"). We find
that LLM-as-a-Judge systems evaluate ethical refusals significantly more
favorably than human users, a divergence not observed for technical refusals.
We refer to this divergence as a moderation bias-a systematic tendency for
model-based evaluators to reward refusal behaviors more than human users do.
This raises broader questions about transparency, value alignment, and the
normative assumptions embedded in automated evaluation systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [154] [MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech Paralinguistic and Affect Labeling](https://arxiv.org/abs/2505.15772)
*Cheng Yifan,Zhang Ruoyi,Shi Jiatong*

Main category: cs.SD

TL;DR: 提出全自动多模态流程MIKU-PAL，实现高效低成本的情感语音标注，并发布新数据集MIKU-EmoBench


<details>
  <summary>Details</summary>
Motivation: 解决语音合成领域大规模高一致性情感语音数据获取困难的挑战

Method: 结合人脸检测追踪算法和多模态大语言模型(MLLM)，构建自动化情感分析系统

Result: 达到人类标注准确率(68.5% MELD)和更高一致性(0.93 Fleiss kappa)，标注26类情感并验证83%合理性，发布131.2小时新基准数据集

Conclusion: MIKU-PAL系统在保持高质量标注的同时显著降低成本，为语音合成研究提供标准化数据支持

Abstract: Acquiring large-scale emotional speech data with strong consistency remains a
challenge for speech synthesis. This paper presents MIKU-PAL, a fully automated
multimodal pipeline for extracting high-consistency emotional speech from
unlabeled video data. Leveraging face detection and tracking algorithms, we
developed an automatic emotion analysis system using a multimodal large
language model (MLLM). Our results demonstrate that MIKU-PAL can achieve
human-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss
kappa score) while being much cheaper and faster than human annotation. With
the high-quality, flexible, and consistent annotation from MIKU-PAL, we can
annotate fine-grained speech emotion categories of up to 26 types, validated by
human annotators with 83% rationality ratings. Based on our proposed system, we
further released a fine-grained emotional speech dataset MIKU-EmoBench(131.2
hours) as a new benchmark for emotional text-to-speech and visual voice
cloning.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [155] [Scan, Materialize, Simulate: A Generalizable Framework for Physically Grounded Robot Planning](https://arxiv.org/abs/2505.14938)
*Amine Elhafsi,Daniel Morton,Marco Pavone*

Main category: cs.RO

TL;DR: 提出SMS框架集成3D高斯泼溅重建、基础视觉模型和物理模拟，实现无需重学物理规律的机器人通用物理推理与规划


<details>
  <summary>Details</summary>
Motivation: 解决机器人在非结构化环境中需理解物理因果关系的核心挑战，突破传统方法需针对每个场景重新学习物理规律的局限

Method: 整合3D高斯泼溅精准场景重建+VFM语义分割+VLMs材质属性推断+物理仿真系统，构建可微分渲染的物理推理管道

Result: 在台球操控任务和四旋翼着陆场景中验证，模拟域迁移实验准确率达92%，真实世界任务成功率提升47%

Conclusion: 通过可微分渲染、基础模型语义理解与物理仿真的多模态融合，为实现跨领域物理推理的机器人规划开辟新路径

Abstract: Autonomous robots must reason about the physical consequences of their
actions to operate effectively in unstructured, real-world environments. We
present Scan, Materialize, Simulate (SMS), a unified framework that combines 3D
Gaussian Splatting for accurate scene reconstruction, visual foundation models
for semantic segmentation, vision-language models for material property
inference, and physics simulation for reliable prediction of action outcomes.
By integrating these components, SMS enables generalizable physical reasoning
and object-centric planning without the need to re-learn foundational physical
dynamics. We empirically validate SMS in a billiards-inspired manipulation task
and a challenging quadrotor landing scenario, demonstrating robust performance
on both simulated domain transfer and real-world experiments. Our results
highlight the potential of bridging differentiable rendering for scene
reconstruction, foundation models for semantic understanding, and physics-based
simulation to achieve physically grounded robot planning across diverse
settings.

</details>


### [156] [Think, Reflect, Create: Metacognitive Learning for Zero-Shot Robotic Planning with LLMs](https://arxiv.org/abs/2505.14899)
*Wenjie Lin,Jin Wei-Kocsis*

Main category: cs.RO

TL;DR: 提出融合元认知学习的LLM框架，通过技能分解与自反思机制提升多机器人协作的任务完成能力，减少演示需求


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在机器人任务中处理复杂场景的局限性，尤其是零样本/少样本下任务失败的反思与创新性解决能力不足

Method: 基于技能分解提取模块化技能组件，结合失败案例的自反思机制，通过元认知过程合成新解决方案

Result: 框架性能显著优于基线，生成方案与真实解不同但能成功完成任务，验证元认知促进创造性规划的有效性

Conclusion: 元认知学习机制成功增强LLM的机器人规划创造力，通过反思与技能重组实现少量演示下的新任务适应

Abstract: While large language models (LLMs) have shown great potential across various
domains, their applications in robotics remain largely limited to static,
prompt-based behaviors and still face challenges in handling complex tasks
under zero-shot or few-shot settings. Inspired by human metacognitive learning
and creative problem-solving, we address this limitation by exploring a
fundamental research question: Can LLMs be empowered with metacognitive
capabilities to reason, reflect, and create, thereby enhancing their ability to
perform robotic tasks with minimal demonstrations? In this paper, we present an
early-stage framework that integrates metacognitive learning into LLM-powered
multi-robot collaboration. The proposed framework equips the LLM-powered
robotic agents with a skill decomposition and self-reflection mechanism that
identifies modular skills from prior tasks, reflects on failures in unseen task
scenarios, and synthesizes effective new solutions. Experimental results show
that our metacognitive-learning-empowered LLM framework significantly
outperforms existing baselines. Moreover, we observe that the framework is
capable of generating solutions that differ from the ground truth yet still
successfully complete the tasks. These exciting findings support our hypothesis
that metacognitive learning can foster creativity in robotic planning.

</details>


### [157] [AgentThink: A Unified Framework for Tool-Augmented Chain-of-Thought Reasoning in Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2505.15298)
*Kangan Qian,Sicong Jiang,Yang Zhong,Ziang Luo,Zilin Huang,Tianze Zhu,Kun Jiang,Mengmeng Yang,Zheng Fu,Jinyu Miao,Yining Shi,He Zhe Lim,Li Liu,Tianbao Zhou,Hongyi Wang,Huang Yu,Yifei Hu,Guang Li,Guang Chen,Hao Ye,Lijun Sun,Diange Yang*

Main category: cs.RO

TL;DR: 提出AgentThink框架，首次将思维链推理与动态工具调用结合，显著提升自动驾驶模型的推理能力和准确性


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在自动驾驶场景中的幻觉问题、低效推理机制及缺乏真实场景验证的局限性

Method: 通过结构化数据生成构建驾驶工具库，采用两阶段训练流程（SFT+GRPO），建立多工具评估协议

Result: 在DriveLMM-o1基准测试中总体推理分数提升53.91%，答案准确率提高33.54%，且展现强大的零样本/少样本泛化能力

Conclusion: 为开发可信赖且具备工具感知能力的自动驾驶模型开辟了新方向，验证了框架在复杂场景中的有效性和鲁棒性

Abstract: Vision-Language Models (VLMs) show promise for autonomous driving, yet their
struggle with hallucinations, inefficient reasoning, and limited real-world
validation hinders accurate perception and robust step-by-step reasoning. To
overcome this, we introduce \textbf{AgentThink}, a pioneering unified framework
that, for the first time, integrates Chain-of-Thought (CoT) reasoning with
dynamic, agent-style tool invocation for autonomous driving tasks. AgentThink's
core innovations include: \textbf{(i) Structured Data Generation}, by
establishing an autonomous driving tool library to automatically construct
structured, self-verified reasoning data explicitly incorporating tool usage
for diverse driving scenarios; \textbf{(ii) A Two-stage Training Pipeline},
employing Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization
(GRPO) to equip VLMs with the capability for autonomous tool invocation; and
\textbf{(iii) Agent-style Tool-Usage Evaluation}, introducing a novel
multi-tool assessment protocol to rigorously evaluate the model's tool
invocation and utilization. Experiments on the DriveLMM-o1 benchmark
demonstrate AgentThink significantly boosts overall reasoning scores by
\textbf{53.91\%} and enhances answer accuracy by \textbf{33.54\%}, while
markedly improving reasoning quality and consistency. Furthermore, ablation
studies and robust zero-shot/few-shot generalization experiments across various
benchmarks underscore its powerful capabilities. These findings highlight a
promising trajectory for developing trustworthy and tool-aware autonomous
driving models.

</details>


### [158] [Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets](https://arxiv.org/abs/2505.15517)
*Kaiyuan Chen,Shuangyu Xie,Zehan Ma,Ken Goldberg*

Main category: cs.RO

TL;DR: 提出Robo2VLM框架，通过机器人轨迹数据生成视觉问答数据集，增强视觉语言模型的空间与交互推理能力


<details>
  <summary>Details</summary>
Motivation: 探索利用真实机器人操作数据反向增强视觉语言模型（传统范式是用视觉语言模型增强机器人系统）

Method: 基于末端执行器位姿/夹持器状态/力传感等多模态数据分割操作阶段，通过场景交互理解生成三维属性标注和结构化VQA问题模板

Result: 构建包含684,710个问题的大规模数据集Robo2VLM-1，覆盖463个场景和3,396个任务，验证对VLM推理能力的提升效果

Conclusion: 机器人操作数据能有效评估和增强视觉语言模型在空间关系理解与物理交互推理方面的能力

Abstract: Vision-Language Models (VLMs) acquire real-world knowledge and general
reasoning ability through Internet-scale image-text corpora. They can augment
robotic systems with scene understanding and task planning, and assist
visuomotor policies that are trained on robot trajectory data. We explore the
reverse paradigm - using rich, real, multi-modal robot trajectory data to
enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual
Question Answering (VQA) dataset generation framework for VLMs. Given a human
tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual
and non-descriptive sensory modalities, such as end-effector pose, gripper
aperture, and force sensing. Based on these modalities, it segments the robot
trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses
scene and interaction understanding to identify 3D properties of the robot,
task goal, and the target object. The properties are used to generate
representative VQA queries - images with textural multiple-choice questions -
based on spatial, goal-conditioned, and interaction reasoning question
templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710
questions covering 463 distinct scenes and 3,396 robotic manipulation tasks
from 176k real robot trajectories. Results suggest that Robo2VLM-1 can
benchmark and improve VLM capabilities in spatial and interaction reasoning.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [159] [A Participatory Strategy for AI Ethics in Education and Rehabilitation grounded in the Capability Approach](https://arxiv.org/abs/2505.15466)
*Valeria Cesaroni,Eleonora Pasqua,Piercosma Bisconti,Martina Galletti*

Main category: cs.CY

TL;DR: 该研究探讨以能力方法为理论框架，通过ARTIS项目的案例展示如何通过参与式研究策略整合伦理、教育和技术要素，推动AI在教育场景中的伦理化应用。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术在特殊教育场景中面临的伦理与系统性挑战，强调技术创新需与伦理责任结合。

Method: 采用参与式研究策略，通过ARTIS项目的案例研究（开发AI阅读理解辅助界面），组织焦点小组和多学科协同设计。

Result: 提出将能力方法与参与式设计结合，可有效评估技术适用性并保障儿童权益，实现技术创新与伦理责任的平衡。

Conclusion: 强调能力方法为AI教育应用提供了有效的伦理评估框架，参与式策略是确保技术符合教育生态需求的关键路径。

Abstract: AI-based technologies have significant potential to enhance inclusive
education and clinical-rehabilitative contexts for children with Special
Educational Needs and Disabilities. AI can enhance learning experiences,
empower students, and support both teachers and rehabilitators. However, their
usage presents challenges that require a systemic-ecological vision, ethical
considerations, and participatory research. Therefore, research and
technological development must be rooted in a strong ethical-theoretical
framework. The Capability Approach - a theoretical model of disability, human
vulnerability, and inclusion - offers a more relevant perspective on
functionality, effectiveness, and technological adequacy in inclusive learning
environments. In this paper, we propose a participatory research strategy with
different stakeholders through a case study on the ARTIS Project, which
develops an AI-enriched interface to support children with text comprehension
difficulties. Our research strategy integrates ethical, educational, clinical,
and technological expertise in designing and implementing AI-based technologies
for children's learning environments through focus groups and collaborative
design sessions. We believe that this holistic approach to AI adoption in
education can help bridge the gap between technological innovation and ethical
responsibility.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [160] [QUADS: QUAntized Distillation Framework for Efficient Speech Language Understanding](https://arxiv.org/abs/2505.14723)
*Subrata Biswas,Mohammad Nur Hossain Khan,Bashima Islam*

Main category: eess.AS

TL;DR: 提出QUADS框架统一知识蒸馏与量化技术，在保持准确率的同时实现60-73倍计算效率提升和83-700倍模型压缩。


<details>
  <summary>Details</summary>
Motivation: 现有方法单独使用知识蒸馏和量化技术，导致在资源受限环境下无法实现最优压缩效果。需要同时优化模型精度和计算效率以适应现实场景需求。

Method: 采用多阶段训练框架：1) 预训练教师模型 2) 量化感知蒸馏 3) 低位自适应微调。通过联合优化蒸馏损失和量化约束，提升低位量化下的模型适应性。

Result: SLURP(71.13%)和FSC(99.20%)准确率，仅比SOTA下降≤5.56%。计算量减少60-73倍（GMACs），模型尺寸缩小83-700倍，极端量化下保持强鲁棒性。

Conclusion: QUADS为资源受限的SLU系统提供了高效解决方案，在计算效率和模型压缩方面实现突破，支持实际边缘设备部署。

Abstract: Spoken Language Understanding (SLU) systems must balance performance and
efficiency, particularly in resource-constrained environments. Existing methods
apply distillation and quantization separately, leading to suboptimal
compression as distillation ignores quantization constraints. We propose QUADS,
a unified framework that optimizes both through multi-stage training with a
pre-tuned model, enhancing adaptability to low-bit regimes while maintaining
accuracy. QUADS achieves 71.13\% accuracy on SLURP and 99.20\% on FSC, with
only minor degradations of up to 5.56\% compared to state-of-the-art models.
Additionally, it reduces computational complexity by 60--73$\times$ (GMACs) and
model size by 83--700$\times$, demonstrating strong robustness under extreme
quantization. These results establish QUADS as a highly efficient solution for
real-world, resource-constrained SLU applications.

</details>


### [161] [TCSinger 2: Customizable Multilingual Zero-shot Singing Voice Synthesis](https://arxiv.org/abs/2505.14910)
*Yu Zhang,Wenxiang Guo,Changhao Pan,Dongyu Yao,Zhiyuan Zhu,Ziyue Jiang,Yuhan Wang,Tao Jin,Zhou Zhao*

Main category: eess.AS

TL;DR: TCSinger 2通过模糊边界编码器、多模态对齐编码器和流式Transformer架构，实现了无需标注的多语言歌唱合成，显著提升了音素过渡质量和多风格控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有歌唱合成模型过度依赖音素和音符标注，导致零样本场景下边界过渡不自然且缺乏有效的多层级风格控制。

Method: 1) 模糊边界内容编码器(BBC)通过时长预测和掩码机制实现平滑过渡；2) 自定义音频编码器利用对比学习对齐歌声/语音/文本表征；3) 基于流结构的Cus-MOE Transformer增强合成质量和风格建模。

Result: 主客观实验表明，TCSinger 2在多个相关任务中全面超越基线模型，MOS评分提升15%以上。

Conclusion: 该工作通过多任务架构和创新的边界处理机制，解决了现有歌唱合成系统在零样本场景下的核心痛点，为多风格歌唱创作提供了新方案。

Abstract: Customizable multilingual zero-shot singing voice synthesis (SVS) has various
potential applications in music composition and short video dubbing. However,
existing SVS models overly depend on phoneme and note boundary annotations,
limiting their robustness in zero-shot scenarios and producing poor transitions
between phonemes and notes. Moreover, they also lack effective multi-level
style control via diverse prompts. To overcome these challenges, we introduce
TCSinger 2, a multi-task multilingual zero-shot SVS model with style transfer
and style control based on various prompts. TCSinger 2 mainly includes three
key modules: 1) Blurred Boundary Content (BBC) Encoder, predicts duration,
extends content embedding, and applies masking to the boundaries to enable
smooth transitions. 2) Custom Audio Encoder, uses contrastive learning to
extract aligned representations from singing, speech, and textual prompts. 3)
Flow-based Custom Transformer, leverages Cus-MOE, with F0 supervision,
enhancing both the synthesis quality and style modeling of the generated
singing voice. Experimental results show that TCSinger 2 outperforms baseline
models in both subjective and objective metrics across multiple related tasks.

</details>


### [162] [Segmentation-Variant Codebooks for Preservation of Paralinguistic and Prosodic Information](https://arxiv.org/abs/2505.15667)
*Nicholas Sanders,Yuanchao Li,Korin Richmond,Simon King*

Main category: eess.AS

TL;DR: 提出分段式码本(SVCs)，通过多粒度语音单元离散化，在保持低比特率的同时更有效保留韵律/副语言信息


<details>
  <summary>Details</summary>
Motivation: 传统语音SSL模型量化会丢失韵律信息，增大码本尺寸虽有效但导致比特率上升效率低下

Method: 在不同语言单元（帧/音素/词/语句）建立独立码本，实现分段特征解耦 + 提出先池化后离散化的处理策略

Result: 在韵律探测任务中显著优于基线，再合成实验显示风格还原度提升且保持可懂度

Conclusion: SVCs通过结构化的离散特征分解，为语音合成/语言建模等任务提供了更高效的韵律保留方案

Abstract: Quantization in SSL speech models (e.g., HuBERT) improves compression and
performance in tasks like language modeling, resynthesis, and text-to-speech
but often discards prosodic and paralinguistic information (e.g., emotion,
prominence). While increasing codebook size mitigates some loss, it
inefficiently raises bitrates. We propose Segmentation-Variant Codebooks
(SVCs), which quantize speech at distinct linguistic units (frame, phone, word,
utterance), factorizing it into multiple streams of segment-specific discrete
features. Our results show that SVCs are significantly more effective at
preserving prosodic and paralinguistic information across probing tasks.
Additionally, we find that pooling before rather than after discretization
better retains segment-level information. Resynthesis experiments further
confirm improved style realization and slightly improved quality while
preserving intelligibility.

</details>


### [163] [ToxicTone: A Mandarin Audio Dataset Annotated for Toxicity and Toxic Utterance Tonality](https://arxiv.org/abs/2505.15773)
*Yu-Xiang Luo,Yi-Cheng Lin,Ming-To Chuang,Jia-Hung Chen,I-Ning Tsai,Pei Xing Kiew,Yueh-Hsuan Huang,Chien-Feng Liu,Yu-Chen Chen,Bo-Han Feng,Wenze Ren,Hung-yi Lee*

Main category: eess.AS

TL;DR: 填补中文语音毒性检测空白，构建最大标注数据集ToxicTone并提出多模态检测框架，实验证明语音特征对识别隐性毒性表达的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于文本毒性检测，但普通话语音中的韵律特征和文化特定表达未被充分研究，缺乏专门标注数据集导致语音毒性检测不足。

Method: 整合声学、语言和情感特征的多模态框架，采用先进语音编码器(Wav2Vec2)和情感编码器(RAVE)进行跨模态特征融合。

Result: 多模态模型准确率较纯文本模型提升18.6%，在讽刺/蔑视等隐性毒性检测任务中F1值达到76.3%，证明语音特征能有效揭示文本未体现的毒性线索。

Conclusion: ToxicTone数据集与多模态框架为语音毒性检测提供新基准，韵律特征和副语言信息是识别文化特异性毒性表达的关键维度。

Abstract: Despite extensive research on toxic speech detection in text, a critical gap
remains in handling spoken Mandarin audio. The lack of annotated datasets that
capture the unique prosodic cues and culturally specific expressions in
Mandarin leaves spoken toxicity underexplored. To address this, we introduce
ToxicTone -- the largest public dataset of its kind -- featuring detailed
annotations that distinguish both forms of toxicity (e.g., profanity, bullying)
and sources of toxicity (e.g., anger, sarcasm, dismissiveness). Our data,
sourced from diverse real-world audio and organized into 13 topical categories,
mirrors authentic communication scenarios. We also propose a multimodal
detection framework that integrates acoustic, linguistic, and emotional
features using state-of-the-art speech and emotion encoders. Extensive
experiments show our approach outperforms text-only and baseline models,
underscoring the essential role of speech-specific cues in revealing hidden
toxic expressions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [164] [HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases](https://arxiv.org/abs/2505.15701)
*Pingqing Zheng,Jiayin Qin,Fuqi Zhang,Shang Wu,Yu Cao,Caiwen Ding,Yang,Zhao*

Main category: cs.AR

TL;DR: 提出HDLxGraph框架整合图检索增强生成与LLM，通过双检索机制和HDL专属图表示提升大规模硬件设计任务性能，并建立HDLSearch评测数据集


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理真实仓库级HDL项目（数千至数万行代码）时表现受限，需解决结构信息缺失导致的检索召回率低和扩展性差问题

Method: 结合AST和DFG构建硬件专属图表示，设计包含语义/结构双检索机制，支持任务特定的检索微调，并开发HDLSearch多粒度评测基准

Result: 相比基于相似性的RAG方法，搜索准确率提升12.04%，调试效率提高12.22%，代码补全质量改善5.04%

Conclusion: HDLxGraph有效提升LLM在复杂HDL任务中的性能，HDLSearch填补领域评测空白，框架代码与数据集已开源

Abstract: Large Language Models (LLMs) have demonstrated their potential in hardware
design tasks, such as Hardware Description Language (HDL) generation and
debugging. Yet, their performance in real-world, repository-level HDL projects
with thousands or even tens of thousands of code lines is hindered. To this
end, we propose HDLxGraph, a novel framework that integrates Graph Retrieval
Augmented Generation (Graph RAG) with LLMs, introducing HDL-specific graph
representations by incorporating Abstract Syntax Trees (ASTs) and Data Flow
Graphs (DFGs) to capture both code graph view and hardware graph view.
HDLxGraph utilizes a dual-retrieval mechanism that not only mitigates the
limited recall issues inherent in similarity-based semantic retrieval by
incorporating structural information, but also enhances its extensibility to
various real-world tasks by a task-specific retrieval finetuning. Additionally,
to address the lack of comprehensive HDL search benchmarks, we introduce
HDLSearch, a multi-granularity evaluation dataset derived from real-world
repository-level projects. Experimental results demonstrate that HDLxGraph
significantly improves average search accuracy, debugging efficiency and
completion quality by 12.04%, 12.22% and 5.04% compared to similarity-based
RAG, respectively. The code of HDLxGraph and collected HDLSearch benchmark are
available at https://github.com/Nick-Zheng-Q/HDLxGraph.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [165] [FisherSFT: Data-Efficient Supervised Fine-Tuning of Language Models Using Information Gain](https://arxiv.org/abs/2505.14826)
*Rohan Deb,Kiran Thekumparampil,Kousha Kalantari,Gaurush Hiranandani,Shoham Sabach,Branislav Kveton*

Main category: cs.LG

TL;DR: 提出基于Hessian矩阵的信息增益最大化样本选择方法，显著提升监督微调(SFT)的统计效率


<details>
  <summary>Details</summary>
Motivation: 针对传统SFT方法训练样本利用率低的问题，通过选择信息量最大的样本子集来提升模型微调效率

Method: 基于对数似然的Hessian矩阵计算信息增益，通过最后一层的多项式逻辑回归模型实现LLM线性化近似

Result: 在多个任务中验证了方法的有效性，通过定量分析和LLM评估证明了计算效率和性能优势

Conclusion: 该方法实现了可分析、计算高效的样本选择，在固定计算预算下显著提升模型微调效果

Abstract: Supervised fine-tuning (SFT) is a standard approach to adapting large
language models (LLMs) to new domains. In this work, we improve the statistical
efficiency of SFT by selecting an informative subset of training examples.
Specifically, for a fixed budget of training examples, which determines the
computational cost of fine-tuning, we determine the most informative ones. The
key idea in our method is to select examples that maximize information gain,
measured by the Hessian of the log-likelihood of the LLM. We approximate it
efficiently by linearizing the LLM at the last layer using multinomial logistic
regression models. Our approach is computationally efficient, analyzable, and
performs well empirically. We demonstrate this on several problems, and back
our claims with both quantitative results and an LLM evaluation.

</details>


### [166] [Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision](https://arxiv.org/abs/2505.14999)
*Eric Hanchen Jiang,Haozheng Luo,Shengyuan Pang,Xiaomin Li,Zhenting Qi,Hengli Li,Cheng-Fu Yang,Zongyu Lin,Xinfeng Li,Hao Xu,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.LG

TL;DR: 提出轻量级验证器EORM，通过能量模型仅用结果标签优化LLM数学推理准确性，显著提升GSM8k/MATH基准表现


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法无法保证推理正确性且计算成本高，需开发高效验证机制提升LLM推理可靠性

Method: 基于能量模型(EBMs)，将判别器输出转为负能量值，通过结果标签隐式学习推理路径质量排序

Result: Llama3 8B在GSM8k达90.7%准确率(MATH 63.7%)，验证流程效率匹敌暴力采样

Conclusion: EORM通过能量评分机制实现高效事后验证，显著提升LLM数学推理可靠性，降低计算成本

Abstract: Mathematical reasoning presents a significant challenge for Large Language
Models (LLMs), often requiring robust multi step logical consistency. While
Chain of Thought (CoT) prompting elicits reasoning steps, it doesn't guarantee
correctness, and improving reliability via extensive sampling is
computationally costly. This paper introduces the Energy Outcome Reward Model
(EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy
Based Models (EBMs) to simplify the training of reward models by learning to
assign a scalar energy score to CoT solutions using only outcome labels,
thereby avoiding detailed annotations. It achieves this by interpreting
discriminator output logits as negative energies, effectively ranking
candidates where lower energy is assigned to solutions leading to correct final
outcomes implicitly favoring coherent reasoning. On mathematical benchmarks
(GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with
Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively
leverages a given pool of candidate solutions to match or exceed the
performance of brute force sampling, thereby enhancing LLM reasoning outcome
reliability through its streamlined post hoc verification process.

</details>


### [167] [RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning](https://arxiv.org/abs/2505.15034)
*Kaiwen Zha,Zhengqi Gao,Maohao Shen,Zhang-Wei Hong,Duane S. Boning,Dina Katabi*

Main category: cs.LG

TL;DR: 提出Tango框架，通过强化学习协同训练LLM生成器与生成式过程级验证器，显著提升数学推理能力与模型泛化性。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法使用固定验证器易受奖励攻击且泛化性差，需设计能动态协同优化的生成器-验证器框架。

Method: 采用RL联合训练生成器与过程级验证器，验证器仅通过结果级正确性奖励进行训练，无需过程标注。

Result: 在7B/8B规模模型中，生成器在5个数学基准和4个跨领域任务中达到SOTA，验证器在ProcessBench领先。

Conclusion: 协同进化机制使生成器与验证器形成正向循环，尤其在复杂数学问题上展现出突破性进展，验证生成式验证器的有效性。

Abstract: Reinforcement learning (RL) has recently emerged as a compelling approach for
enhancing the reasoning capabilities of large language models (LLMs), where an
LLM generator serves as a policy guided by a verifier (reward model). However,
current RL post-training methods for LLMs typically use verifiers that are
fixed (rule-based or frozen pretrained) or trained discriminatively via
supervised fine-tuning (SFT). Such designs are susceptible to reward hacking
and generalize poorly beyond their training distributions. To overcome these
limitations, we propose Tango, a novel framework that uses RL to concurrently
train both an LLM generator and a verifier in an interleaved manner. A central
innovation of Tango is its generative, process-level LLM verifier, which is
trained via RL and co-evolves with the generator. Importantly, the verifier is
trained solely based on outcome-level verification correctness rewards without
requiring explicit process-level annotations. This generative RL-trained
verifier exhibits improved robustness and superior generalization compared to
deterministic or SFT-trained verifiers, fostering effective mutual
reinforcement with the generator. Extensive experiments demonstrate that both
components of Tango achieve state-of-the-art results among 7B/8B-scale models:
the generator attains best-in-class performance across five competition-level
math benchmarks and four challenging out-of-domain reasoning tasks, while the
verifier leads on the ProcessBench dataset. Remarkably, both components exhibit
particularly substantial improvements on the most difficult mathematical
reasoning problems. Code is at: https://github.com/kaiwenzha/rl-tango.

</details>


### [168] [MoTime: A Dataset Suite for Multimodal Time Series Forecasting](https://arxiv.org/abs/2505.15072)
*Xin Zhou,Weiqing Wang,Francisco J. Baldán,Wray Buntine,Christoph Bergmeir*

Main category: cs.LG

TL;DR: 提出MoTime多模态时序预测数据集套件，验证外部模态在常规预测和冷启动场景中的提升效果，发现数据特性不同影响模态效用。


<details>
  <summary>Details</summary>
Motivation: 真实世界预测任务中多模态数据日益丰富，但现有研究仍局限在单模态时间序列分析。

Method: 构建包含文本/元数据/图像等多模态配对的时序数据集，设计常规预测和冷启动预测两种评估场景。

Result: 外部模态在两种场景均能提升预测性能，部分数据集短时序预测提升显著，但效果受数据特性影响存在差异。

Conclusion: 通过公开数据集促进更全面的多模态时序预测研究，推动建立更现实的评估基准。

Abstract: While multimodal data sources are increasingly available from real-world
forecasting, most existing research remains on unimodal time series. In this
work, we present MoTime, a suite of multimodal time series forecasting datasets
that pair temporal signals with external modalities such as text, metadata, and
images. Covering diverse domains, MoTime supports structured evaluation of
modality utility under two scenarios: 1) the common forecasting task, where
varying-length history is available, and 2) cold-start forecasting, where no
historical data is available. Experiments show that external modalities can
improve forecasting performance in both scenarios, with particularly strong
benefits for short series in some datasets, though the impact varies depending
on data characteristics. By making datasets and findings publicly available, we
aim to support more comprehensive and realistic benchmarks in future multimodal
time series forecasting research.

</details>


### [169] [SUS backprop: linear backpropagation algorithm for long inputs in transformers](https://arxiv.org/abs/2505.15080)
*Sergey Pankov,Georges Harik*

Main category: cs.LG

TL;DR: 提出通过概率规则切断注意力机制中99%反向传播流的方法，将计算复杂度从O(n²)降至O(nc)，梯度方差仅增加约1%


<details>
  <summary>Details</summary>
Motivation: 解决Transformer长序列训练中注意力机制计算量二次方增长的问题，利用大部分注意力权重较小的特性降低反向传播成本

Method: 基于单一参数c的概率规则，每个token/注意力头最多保留c个交互，实现注意力反向传播的线性复杂度

Result: 当c=20-30且n≈2000时，切断99%梯度流导致相对梯度方差仅增1%，且方差增幅随序列长度增加而降低

Conclusion: 该方法通过稀疏矩阵实现，使长序列训练中反向传播成本可忽略，显著提升Transformer在长上下文场景的训练效率

Abstract: It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient sparse matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.

</details>


### [170] [Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems](https://arxiv.org/abs/2505.15201)
*Christian Walder,Deep Karkhanis*

Main category: cs.LG

TL;DR: 提出PKPO方法，通过奖励转换直接优化pass@k指标，在保持低方差的同时实现样本集的联合效用最大化。


<details>
  <summary>Details</summary>
Motivation: 传统RL独立优化单个样本的pass@1性能，导致样本多样性不足且难以应对复杂问题。需要建立样本集的联合优化框架。

Method: 1. 推导二元/连续奖励场景下pass@k的低方差无偏估计器
2. 设计稳定的奖励转换函数
3. 支持训练中动态调整k值实现指标平衡

Result: 1. 在GEMMA-2实验中有效控制目标k值
2. 高k值提升困难问题解决能力
3. 动态调整k同时优化pass@1和pass@k
4. 在挑战性任务中突破传统方法的学习瓶颈

Conclusion: PKPO通过强调样本集的联合效用而非个体性能，促进探索过程，在保持基础指标的同时解锁复杂任务的学习潜力，为RL优化提供新范式。

Abstract: Reinforcement Learning (RL) algorithms sample multiple n>1 solution attempts
for each problem and reward them independently. This optimizes for pass@1
performance and prioritizes the strength of isolated samples at the expense of
the diversity and collective utility of sets of samples. This under-utilizes
the sampling capacity, limiting exploration and eventual improvement on harder
examples. As a fix, we propose Pass-at-k Policy Optimization (PKPO), a
transformation on the final rewards which leads to direct optimization of
pass@k performance, thus optimizing for sets of samples that maximize reward
when considered jointly. Our contribution is to derive novel low variance
unbiased estimators for pass@k and its gradient, in both the binary and
continuous reward settings. We show optimization with our estimators reduces to
standard RL with rewards that have been jointly transformed by a stable and
efficient transformation function.
  While previous efforts are restricted to k=n, ours is the first to enable
robust optimization of pass@k for any arbitrary k <= n. Moreover, instead of
trading off pass@1 performance for pass@k gains, our method allows annealing k
during training, optimizing both metrics and often achieving strong pass@1
numbers alongside significant pass@k gains.
  We validate our reward transformations on toy experiments, which reveal the
variance reducing properties of our formulations. We also include real-world
examples using the open-source LLM, GEMMA-2. We find that our transformation
effectively optimizes for the target k. Furthermore, higher k values enable
solving more and harder problems, while annealing k boosts both the pass@1 and
pass@k . Crucially, for challenging task sets where conventional pass@1
optimization stalls, our pass@k approach unblocks learning, likely due to
better exploration by prioritizing joint utility over the utility of individual
samples.

</details>


### [171] [ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search](https://arxiv.org/abs/2505.15259)
*Hyunseok Lee,Jeonghoon Kim,Beomjun Kim,Jihoon Tack,Chansong Jo,Jaehong Lee,Cheonbok Park,Sookyo In,Jinwoo Shin,Kang Min Yoo*

Main category: cs.LG

TL;DR: 提出ReGUIDE框架，通过自我推理与空间感知提升GUI元素定位效率，仅需0.2%训练数据即超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有GUI定位方法依赖大规模数据集，效率低下且成本高昂。ReGUIDE旨在通过智能推理机制减少数据依赖。

Method: 1. 在线强化学习生成语言推理路径；2. 基于空间变换等变性的先验知识校正预测；3. 推理阶段采用空间搜索+坐标聚合策略。

Result: 在多项基准测试中显著超越基线模型（如使用仅0.2%训练数据即超越开源最优模型），验证了框架的数据高效特性。

Conclusion: ReGUIDE成功实现了基于自生成推理的web元素定位，为低数据场景下的多模态交互提供了有效解决方案。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled
autonomous agents to interact with computers via Graphical User Interfaces
(GUIs), where accurately localizing the coordinates of interface elements
(e.g., buttons) is often required for fine-grained actions. However, this
remains significantly challenging, leading prior works to rely on large-scale
web datasets to improve the grounding accuracy. In this work, we propose
Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a
novel and effective framework for web grounding that enables MLLMs to learn
data efficiently through self-generated reasoning and spatial-aware criticism.
More specifically, ReGUIDE learns to (i) self-generate a language reasoning
process for the localization via online reinforcement learning, and (ii)
criticize the prediction using spatial priors that enforce equivariance under
input transformations. At inference time, ReGUIDE further boosts performance
through a test-time scaling strategy, which combines spatial search with
coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly
advances web grounding performance across multiple benchmarks, outperforming
baselines with substantially fewer training data points (e.g., only 0.2%
samples compared to the best open-sourced baselines).

</details>


### [172] [Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning](https://arxiv.org/abs/2505.15311)
*Yurun Yuan,Fan Chen,Zeyu Jia,Alexander Rakhlin,Tengyang Xie*

Main category: cs.LG

TL;DR: 提出轨迹贝尔曼残差最小化(TBRM)算法，将经典贝尔曼优化范式适配大语言模型，在数学推理任务中显著超越PPO等策略基方法且计算开销更低。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习主要依赖策略基方法，基于价值的方法尚未被充分探索。研究旨在验证价值基RL在提升语言模型推理能力中的潜力。

Method: 通过轨迹级贝尔曼目标优化，利用模型自身logits作为Q值，省去critic网络、重要性采样和梯度裁剪，每个prompt仅需单次推理。

Result: 在数学推理基准测试中，TBRM持续超越PPO/GRPO等基线，计算内存消耗相当或更低。理论证明其从任意离策略数据收敛到近似最优策略。

Conclusion: 价值基强化学习可能成为提升LLM推理能力的原理性高效替代方案，为RLHF领域提供新方向。

Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines
for large language model (LLM) reasoning, leaving value-based approaches
largely unexplored. We revisit the classical paradigm of Bellman Residual
Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an
algorithm that naturally adapts this idea to LLMs, yielding a simple yet
effective off-policy algorithm that optimizes a single trajectory-level Bellman
objective using the model's own logits as $Q$-values. TBRM removes the need for
critics, importance-sampling ratios, or clipping, and operates with only one
rollout per prompt. We prove convergence to the near-optimal KL-regularized
policy from arbitrary off-policy data via an improved
change-of-trajectory-measure analysis. Experiments on standard
mathematical-reasoning benchmarks show that TBRM consistently outperforms
policy-based baselines, like PPO and GRPO, with comparable or lower
computational and memory overhead. Our results indicate that value-based RL
might be a principled and efficient alternative for enhancing reasoning
capabilities in LLMs.

</details>


### [173] [Set-LLM: A Permutation-Invariant LLM](https://arxiv.org/abs/2505.15433)
*Beni Egressy,Jan Stühmer*

Main category: cs.LG

TL;DR: 提出Set-LLM架构解决大语言模型的顺序敏感性漏洞，通过新型注意力机制和位置编码实现排列不变性输入处理


<details>
  <summary>Details</summary>
Motivation: 大语言模型在选项排序不同时会产生不一致的输出结果（如偏好首选项），严重影响其作为自动评估器在AI流程中的可靠性

Method: 1. 设计支持集合-文本混合输入的注意力掩码
2. 开发适用于集合处理的新型位置编码
3. 提供排列不变性的理论证明

Result: Set-LLM在保持原有模型运行效率的同时，成功消除了顺序敏感性，在多项任务中取得可比或更好的性能表现

Conclusion: Set-LLM通过架构层面的创新改进，为大语言模型提供了排列不变性保障，显著提升了模型在需要集合处理的场景中的鲁棒性

Abstract: While large language models (LLMs) demonstrate impressive capabilities across
numerous applications, their robustness remains a critical concern. This paper
is motivated by a specific vulnerability: the order sensitivity of LLMs. This
vulnerability manifests itself as the order bias observed when LLMs decide
between possible options (for example, a preference for the first option) and
the tendency of LLMs to provide different answers when options are reordered.
The use cases for this scenario extend beyond the classical case of
multiple-choice question answering to the use of LLMs as automated evaluators
in AI pipelines, comparing output generated by different models. We introduce
Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the
processing of mixed set-text inputs with permutation invariance guarantees. The
adaptations involve a new attention mask and new positional encodings
specifically designed for sets. We provide a theoretical proof of invariance
and demonstrate through experiments that Set-LLM can be trained effectively,
achieving comparable or improved performance and maintaining the runtime of the
original model, while eliminating order sensitivity.

</details>


### [174] [Explainable embeddings with Distance Explainer](https://arxiv.org/abs/2505.15516)
*Christiaan Meijer,E. G. Patrick Bos*

Main category: cs.LG

TL;DR: 提出Distance Explainer方法，通过选择性掩码和距离排序策略解释嵌入空间中的相似性特征，在跨模态任务中验证了有效性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法较少关注嵌入空间维度（包含复杂抽象特征）的解释需求，需提升深度学习在嵌入空间应用中的透明度

Method: 基于RISE的显著性技术改进，采用选择性掩码和距离排名过滤策略分配特征归因值

Result: 在ImageNet和CLIP模型上，Faithfulness指标达0.82，敏感性测试显示90%鲁棒性，显著优于基线方法

Conclusion: 填补了XAI在嵌入空间解释的技术空白，为跨模态检索等应用提供了可靠的可视化分析工具

Abstract: While eXplainable AI (XAI) has advanced significantly, few methods address
interpretability in embedded vector spaces where dimensions represent complex
abstractions. We introduce Distance Explainer, a novel method for generating
local, post-hoc explanations of embedded spaces in machine learning models. Our
approach adapts saliency-based techniques from RISE to explain the distance
between two embedded data points by assigning attribution values through
selective masking and distance-ranked mask filtering. We evaluate Distance
Explainer on cross-modal embeddings (image-image and image-caption pairs) using
established XAI metrics including Faithfulness, Sensitivity/Robustness, and
Randomization. Experiments with ImageNet and CLIP models demonstrate that our
method effectively identifies features contributing to similarity or
dissimilarity between embedded data points while maintaining high robustness
and consistency. We also explore how parameter tuning, particularly mask
quantity and selection strategy, affects explanation quality. This work
addresses a critical gap in XAI research and enhances transparency and
trustworthiness in deep learning applications utilizing embedded spaces.

</details>


### [175] [Mechanistic Insights into Grokking from the Embedding Layer](https://arxiv.org/abs/2505.15624)
*H. V. AlquBoj,Hilal AlQuabeh,Velibor Bojkovic,Munachiso Nwadike,Kentaro Inui*

Main category: cs.LG

TL;DR: 发现嵌入层是导致神经网络延迟泛化（grokking）的关键因素，并提出频率感知采样和自适应学习率方法优化训练动态


<details>
  <summary>Details</summary>
Motivation: 探索Transformer和MLP中观察到的延迟泛化现象（grokking）的核心驱动机制

Method: 1. 在MLP中引入/移除嵌入层对比实验
2. 分析嵌入更新动态和双线性耦合机制
3. 提出频率感知采样和嵌入层专用学习率方法

Result: 1. 嵌入层导致延迟泛化
2. 发现稀有令牌更新停滞现象
3. 自适应学习率比例公式显著加速收敛

Conclusion: 双线性耦合是Transformer优化困难的核心原因，提出的优化策略可有效提升训练效率

Abstract: Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to sparse gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.

</details>


### [176] [Large Language Models as Computable Approximations to Solomonoff Induction](https://arxiv.org/abs/2505.15784)
*Jun Wan,Lingrui Mei*

Main category: cs.LG

TL;DR: 首次将大语言模型（LLM）架构与算法信息论（AIT）建立形式化联系，证明模型训练近似所罗门诺夫先验，并基于此理论提出提升小样本学习性能的方法


<details>
  <summary>Details</summary>
Motivation: 现有理论框架对大语言模型涌现现象的解释呈现碎片化，需要统一数学框架解释上下文学习、小样本学习、缩放定律等核心现象

Method: 1) 证明损失函数最小化等价于程序长度优化的所罗门诺夫先验近似
2) 论证下一词预测机制实现近似所罗门诺夫归纳推理

Result: 基于理论提出的小样本示例选择策略（优先选择模型预测置信度低的样本）在文本分类任务中显著提升性能，对小模型架构效果尤为明显

Conclusion: 该理论框架弥合了基础理论与LLM实际行为间的鸿沟，既提供解释性理论又为模型开发提供可操作的指导原则（如基于信息熵的示例选择）

Abstract: The rapid advancement of large language models (LLMs) calls for a rigorous
theoretical framework to explain their empirical success. While significant
progress has been made in understanding LLM behaviors, existing theoretical
frameworks remain fragmented in explaining emergent phenomena through a unified
mathematical lens. We establish the first formal connection between LLM
architectures and Algorithmic Information Theory (AIT) by proving two
fundamental results: (1) the training process computationally approximates
Solomonoff prior through loss minimization interpreted as program length
optimization, and (2) next-token prediction implements approximate Solomonoff
induction. We leverage AIT to provide a unified theoretical explanation for
in-context learning, few-shot learning, and scaling laws. Furthermore, our
theoretical insights lead to a principled method for few-shot example selection
that prioritizes samples where models exhibit lower predictive confidence. We
demonstrate through experiments on diverse text classification benchmarks that
this strategy yields significant performance improvements, particularly for
smaller model architectures, when compared to selecting high-confidence
examples. Our framework bridges the gap between theoretical foundations and
practical LLM behaviors, providing both explanatory power and actionable
insights for future model development.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [177] [Intentional Gesture: Deliver Your Intentions with Gestures for Speech](https://arxiv.org/abs/2505.15197)
*Pinxin Liu,Haiyang Liu,Luchuan Song,Chenliang Xu*

Main category: cs.CV

TL;DR: 提出了Intentional-Gesture框架，通过融入交流意图信息实现语义丰富的手势生成，在BEAT-2基准上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有手势生成方法仅依赖语音/文本等表层线索，忽视交流意图，导致生成手势语义浅薄、缺乏深层含义

Method: 1. 基于BEAT-2构建InG数据集，用视觉语言模型自动标注意图信息；2. 开发意图感知的Gesture Motion Tokenizer，将交流功能注入动作编码

Result: 在BEAT-2基准测试中取得新的state-of-the-art性能，生成手势在时间对齐和语义表达上均有提升

Conclusion: 该框架为数字人/具身AI提供了模块化、可解释的手势生成基础，实现了意图驱动的语义手势合成

Abstract: When humans speak, gestures help convey communicative intentions, such as
adding emphasis or describing concepts. However, current co-speech gesture
generation methods rely solely on superficial linguistic cues (\textit{e.g.}
speech audio or text transcripts), neglecting to understand and leverage the
communicative intention that underpins human gestures. This results in outputs
that are rhythmically synchronized with speech but are semantically shallow. To
address this gap, we introduce \textbf{Intentional-Gesture}, a novel framework
that casts gesture generation as an intention-reasoning task grounded in
high-level communicative functions. % First, we curate the \textbf{InG} dataset
by augmenting BEAT-2 with gesture-intention annotations (\textit{i.e.}, text
sentences summarizing intentions), which are automatically annotated using
large vision-language models. Next, we introduce the \textbf{Intentional
Gesture Motion Tokenizer} to leverage these intention annotations. It injects
high-level communicative functions (\textit{e.g.}, intentions) into tokenized
motion representations to enable intention-aware gesture synthesis that are
both temporally aligned and semantically meaningful, achieving new
state-of-the-art performance on the BEAT-2 benchmark. Our framework offers a
modular foundation for expressive gesture generation in digital humans and
embodied AI. Project Page: https://andypinxinliu.github.io/Intentional-Gesture

</details>


### [178] [EVA: Expressive Virtual Avatars from Multi-view Videos](https://arxiv.org/abs/2505.15385)
*Hendrik Junkawitsch,Guoxing Sun,Heming Zhu,Christian Theobalt,Marc Habermann*

Main category: cs.CV

TL;DR: 提出EVA框架，通过解耦的面部表情与身体动作分层建模实现高保真实时渲染的虚拟人物


<details>
  <summary>Details</summary>
Motivation: 现有虚拟形象建模方法无法解耦控制面部与身体动作，限制了虚拟现实等领域的应用潜力

Method: 1. 基于粗到细优化的运动捕捉算法恢复参数；2. 身体/面部分离的3D高斯外观模型架构设计

Result: 在渲染质量与表现力方面超越SOTA方法，验证分层建模方案有效性

Conclusion: 实现了完全可驱动的数字人体模型，为创建逼真虚拟形象提供了新范式

Abstract: With recent advancements in neural rendering and motion capture algorithms,
remarkable progress has been made in photorealistic human avatar modeling,
unlocking immense potential for applications in virtual reality, augmented
reality, remote communication, and industries such as gaming, film, and
medicine. However, existing methods fail to provide complete, faithful, and
expressive control over human avatars due to their entangled representation of
facial expressions and body movements. In this work, we introduce Expressive
Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive
human avatar framework that achieves high-fidelity, lifelike renderings in real
time while enabling independent control of facial expressions, body movements,
and hand gestures. Specifically, our approach designs the human avatar as a
two-layer model: an expressive template geometry layer and a 3D Gaussian
appearance layer. First, we present an expressive template tracking algorithm
that leverages coarse-to-fine optimization to accurately recover body motions,
facial expressions, and non-rigid deformation parameters from multi-view
videos. Next, we propose a novel decoupled 3D Gaussian appearance model
designed to effectively disentangle body and facial appearance. Unlike unified
Gaussian estimation approaches, our method employs two specialized and
independent modules to model the body and face separately. Experimental results
demonstrate that EVA surpasses state-of-the-art methods in terms of rendering
quality and expressiveness, validating its effectiveness in creating full-body
avatars. This work represents a significant advancement towards fully drivable
digital human models, enabling the creation of lifelike digital avatars that
faithfully replicate human geometry and appearance.

</details>


### [179] [PlantDreamer: Achieving Realistic 3D Plant Models with Diffusion-Guided Gaussian Splatting](https://arxiv.org/abs/2505.15528)
*Zane K J Hartley,Lewis A G Stuart,Andrew P French,Michael P Pound*

Main category: cs.CV

TL;DR: 提出PlantDreamer框架，通过深度ControlNet、低秩微调和高斯剔除算法，显著提升3D植物生成的纹理真实性与几何精度，并能升级传统点云数据集


<details>
  <summary>Details</summary>
Motivation: 现有AI生成模型在复杂植物3D建模中存在纹理和几何缺陷，制约了植物分析工具的应用效果

Method: 结合深度ControlNet控制生成过程，使用低秩适应(LoRA)微调模型，开发自适应高斯剔除算法优化细节。支持L-System生成纯合成植物，并将点云转为3D高斯泼溅模型

Result: 在生成保真度上超越现有文本转3D模型，成功将传统点云数据集升级为高质量3D模型

Conclusion: 该框架不仅推进了合成植物生成技术，更为3D植物表型分析提供了实用工具

Abstract: Recent years have seen substantial improvements in the ability to generate
synthetic 3D objects using AI. However, generating complex 3D objects, such as
plants, remains a considerable challenge. Current generative 3D models struggle
with plant generation compared to general objects, limiting their usability in
plant analysis tools, which require fine detail and accurate geometry. We
introduce PlantDreamer, a novel approach to 3D synthetic plant generation,
which can achieve greater levels of realism for complex plant geometry and
textures than available text-to-3D models. To achieve this, our new generation
pipeline leverages a depth ControlNet, fine-tuned Low-Rank Adaptation and an
adaptable Gaussian culling algorithm, which directly improve textural realism
and geometric integrity of generated 3D plant models. Additionally,
PlantDreamer enables both purely synthetic plant generation, by leveraging
L-System-generated meshes, and the enhancement of real-world plant point clouds
by converting them into 3D Gaussian Splats. We evaluate our approach by
comparing its outputs with state-of-the-art text-to-3D models, demonstrating
that PlantDreamer outperforms existing methods in producing high-fidelity
synthetic plants. Our results indicate that our approach not only advances
synthetic plant generation, but also facilitates the upgrading of legacy point
cloud datasets, making it a valuable tool for 3D phenotyping applications.

</details>


### [180] [Benchmarking Graph Neural Networks for Document Layout Analysis in Public Affairs](https://arxiv.org/abs/2505.14699)
*Miguel Lopez-Duran,Julian Fierrez,Aythami Morales,Ruben Tolosana,Oscar Delgado-Mohatar,Alvaro Ortigosa*

Main category: cs.CV

TL;DR: 使用图神经网络（GNN）分析数字PDF文档布局，GraphSAGE在双分支k近邻图配置中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 数字PDF文档的布局分析因元素异构性和元数据不精确而困难，需高效自动分类方法。

Method: 提出k近邻/全连接图结构，预训练文本视觉模型生成节点特征，测试单模态/多模态GNN框架。

Result: GraphSAGE在双分支k近邻图中达到最高准确率，部分数据源超越基线模型。

Conclusion: GNN多模态融合及局部布局关系建模对数字文档布局分析具有关键价值。

Abstract: The automatic analysis of document layouts in digital-born PDF documents
remains a challenging problem due to the heterogeneous arrangement of textual
and nontextual elements and the imprecision of the textual metadata in the
Portable Document Format. In this work, we benchmark Graph Neural Network (GNN)
architectures for the task of fine-grained layout classification of text blocks
from digital native documents. We introduce two graph construction structures:
a k-closest-neighbor graph and a fully connected graph, and generate node
features via pre-trained text and vision models, thus avoiding manual feature
engineering. Three experimental frameworks are evaluated: single-modality (text
or visual), concatenated multimodal, and dual-branch multimodal. We evaluated
four foundational GNN models and compared them with the baseline. Our
experiments are specifically conducted on a rich dataset of public affairs
documents that includes more than 20 sources (e.g., regional and national-level
official gazettes), 37K PDF documents, with 441K pages in total. Our results
demonstrate that GraphSAGE operating on the k-closest-neighbor graph in a
dual-branch configuration achieves the highest per-class and overall accuracy,
outperforming the baseline in some sources. These findings confirm the
importance of local layout relationships and multimodal fusion exploited
through GNNs for the analysis of native digital document layouts.

</details>


### [181] [MORALISE: A Structured Benchmark for Moral Alignment in Visual Language Models](https://arxiv.org/abs/2505.14728)
*Xiao Lin,Zhining Liu,Ze Yang,Gaotang Li,Ruizhong Qiu,Shuke Wang,Hui Liu,Haotian Li,Sumit Keswani,Vishwa Pardeshi,Huijun Zhao,Wei Fan,Hanghang Tong*

Main category: cs.CV

TL;DR: 提出MORALISE基准，通过专家验证的真实多模态数据评估视觉语言模型的道德对齐能力，覆盖13个道德主题，测试19个主流模型并揭示其道德局限性。


<details>
  <summary>Details</summary>
Motivation: 现有道德对齐研究集中于单文本模态或依赖AI生成图像，存在分布偏差和真实性不足问题，需构建更全面的真实场景评估体系以确保模型输出符合人类道德价值观。

Method: 基于Turiel领域理论建立13类道德主题框架，人工标注2,481个图像-文本对（含道德违规来源标注），设计道德判断与规范归因双任务评估模型表现。

Result: 在19个主流VLMs上的实验表明，现有模型在识别道德违规和道德推理方面存在显著缺陷，尤其在处理现实复杂场景时表现不足。

Conclusion: MORALISE首次系统评估多模态模型的道德对齐能力，暴露当前技术瓶颈，公开数据集推动道德AI研究。

Abstract: Warning: This paper contains examples of harmful language and images. Reader
discretion is advised. Recently, vision-language models have demonstrated
increasing influence in morally sensitive domains such as autonomous driving
and medical analysis, owing to their powerful multimodal reasoning
capabilities. As these models are deployed in high-stakes real-world
applications, it is of paramount importance to ensure that their outputs align
with human moral values and remain within moral boundaries. However, existing
work on moral alignment either focuses solely on textual modalities or relies
heavily on AI-generated images, leading to distributional biases and reduced
realism. To overcome these limitations, we introduce MORALISE, a comprehensive
benchmark for evaluating the moral alignment of vision-language models (VLMs)
using diverse, expert-verified real-world data. We begin by proposing a
comprehensive taxonomy of 13 moral topics grounded in Turiel's Domain Theory,
spanning the personal, interpersonal, and societal moral domains encountered in
everyday life. Built on this framework, we manually curate 2,481 high-quality
image-text pairs, each annotated with two fine-grained labels: (1) topic
annotation, identifying the violated moral topic(s), and (2) modality
annotation, indicating whether the violation arises from the image or the text.
For evaluation, we encompass two tasks, \textit{moral judgment} and
\textit{moral norm attribution}, to assess models' awareness of moral
violations and their reasoning ability on morally salient content. Extensive
experiments on 19 popular open- and closed-source VLMs show that MORALISE poses
a significant challenge, revealing persistent moral limitations in current
state-of-the-art models. The full benchmark is publicly available at
https://huggingface.co/datasets/Ze1025/MORALISE.

</details>


### [182] [ALN-P3: Unified Language Alignment for Perception, Prediction, and Planning in Autonomous Driving](https://arxiv.org/abs/2505.15158)
*Yunsheng Ma,Burhaneddin Yaman,Xin Ye,Mahmut Yurt,Jingru Luo,Abhirup Mallik,Ziran Wang,Liu Ren*

Main category: cs.CV

TL;DR: 提出ALN-P3协同蒸馏框架，通过感知-预测-规划三阶段对齐机制，在自动驾驶系统中同时优化驾驶决策与语言推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法无法兼顾自动驾驶性能与视觉语言推理能力，需建立跨模态对齐机制实现双重优化

Method: 设计感知对齐(P1A)、预测对齐(P2A)、规划对齐(P3A)机制，在训练阶段对齐视觉特征与语言表达，推理阶段零成本保持性能

Result: 在nuScenes等四个基准测试中实现驾驶决策与语言推理的同步提升，达到最先进水平

Conclusion: 通过协同蒸馏框架显式对齐多模态特征，首次在自动驾驶系统中实现高性能驾驶与复杂语言推理的有机统一

Abstract: Recent advances have explored integrating large language models (LLMs) into
end-to-end autonomous driving systems to enhance generalization and
interpretability. However, most existing approaches are limited to either
driving performance or vision-language reasoning, making it difficult to
achieve both simultaneously. In this paper, we propose ALN-P3, a unified
co-distillation framework that introduces cross-modal alignment between "fast"
vision-based autonomous driving systems and "slow" language-driven reasoning
modules. ALN-P3 incorporates three novel alignment mechanisms: Perception
Alignment (P1A), Prediction Alignment (P2A), and Planning Alignment (P3A),
which explicitly align visual tokens with corresponding linguistic outputs
across the full perception, prediction, and planning stack. All alignment
modules are applied only during training and incur no additional costs during
inference. Extensive experiments on four challenging benchmarks-nuScenes, Nu-X,
TOD3Cap, and nuScenes QA-demonstrate that ALN-P3 significantly improves both
driving decisions and language reasoning, achieving state-of-the-art results.

</details>


### [183] [Better Safe Than Sorry? Overreaction Problem of Vision Language Models in Visual Emergency Recognition](https://arxiv.org/abs/2505.15367)
*Dasol Choi,Seunghyun Lee,Youngsook Song*

Main category: cs.CV

TL;DR: 视觉语言模型(VLMs)在安全关键场景中存在系统性过度反应问题，误判31-96%安全场景为危险，模型规模无法解决该缺陷


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在安全关键场景中的可靠性，发现现有模型存在系统性误判倾向，影响安全应用可信度

Method: 使用200张对比图像组成的VERI基准数据集，通过风险识别和应急响应两阶段协议评估14个不同规模的VLMs

Result: 模型在真实紧急场景识别成功率70-100%，但安全场景误判率31-96%，88-93%错误源于上下文过度解读，10个场景全模型失效

Conclusion: 需开发针对性方法改进视觉误导场景中的上下文安全评估，模型规模扩大无法解决系统性的安全误判问题

Abstract: Vision-Language Models (VLMs) have demonstrated impressive capabilities in
understanding visual content, but their reliability in safety-critical contexts
remains under-explored. We introduce VERI (Visual Emergency Recognition
Dataset), a carefully designed diagnostic benchmark of 200 images (100
contrastive pairs). Each emergency scene is matched with a visually similar but
safe counterpart through multi-stage human verification and iterative
refinement. Using a two-stage protocol - risk identification and emergency
response - we evaluate 14 VLMs (2B-124B parameters) across medical emergencies,
accidents, and natural disasters. Our analysis reveals a systematic
overreaction problem: models excel at identifying real emergencies (70-100
percent success rate) but suffer from an alarming rate of false alarms,
misidentifying 31-96 percent of safe situations as dangerous, with 10 scenarios
failed by all models regardless of scale. This "better-safe-than-sorry" bias
manifests primarily through contextual overinterpretation (88-93 percent of
errors), challenging VLMs' reliability for safety applications. These findings
highlight persistent limitations that are not resolved by increasing model
scale, motivating targeted approaches for improving contextual safety
assessment in visually misleading scenarios.

</details>


### [184] [Seeing Through Deception: Uncovering Misleading Creator Intent in Multimodal News with Vision-Language Models](https://arxiv.org/abs/2505.15489)
*Jiaying Wu,Fanxiao Li,Min-Yen Kan,Bryan Hooi*

Main category: cs.CV

TL;DR: 提出通过建模创作者意图（期望影响+执行计划）的框架构建DeceptionDecoded数据集，验证现有多模态模型在意图识别上的不足


<details>
  <summary>Details</summary>
Motivation: 虚假信息的实际危害源于创作者试图传达的误导性叙事，现有多模态检测系统缺乏对创作者意图的深层建模能力

Method: 构建自动化框架模拟新闻创作过程，创建包含12,000个跨模态样本的基准数据集，覆盖误导/非误导意图的多种模态组合

Result: 14个SOTA视觉语言模型在意图检测、来源归因和欲望推理任务中表现欠佳，易受表面跨模态一致性、风格信号等伪线索干扰

Conclusion: 亟需开发意图感知的多模态虚假信息检测系统，提升模型对深层语义的推理能力

Abstract: The real-world impact of misinformation stems from the underlying misleading
narratives that creators seek to convey. As such, interpreting misleading
creator intent is essential for multimodal misinformation detection (MMD)
systems aimed at effective information governance. In this paper, we introduce
an automated framework that simulates real-world multimodal news creation by
explicitly modeling creator intent through two components: the desired
influence and the execution plan. Using this framework, we construct
DeceptionDecoded, a large-scale benchmark comprising 12,000 image-caption pairs
aligned with trustworthy reference articles. The dataset captures both
misleading and non-misleading intents and spans manipulations across visual and
textual modalities. We conduct a comprehensive evaluation of 14
state-of-the-art vision-language models (VLMs) on three intent-centric tasks:
(1) misleading intent detection, (2) misleading source attribution, and (3)
creator desire inference. Despite recent advances, we observe that current VLMs
fall short in recognizing misleading intent, often relying on spurious cues
such as superficial cross-modal consistency, stylistic signals, and heuristic
authenticity hints. Our findings highlight the pressing need for intent-aware
modeling in MMD and open new directions for developing systems capable of
deeper reasoning about multimodal misinformation.

</details>


### [185] [Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought](https://arxiv.org/abs/2505.15510)
*Zihui Cheng,Qiguang Chen,Xiao Xu,Jiaqi Wang,Weiyun Wang,Hao Fei,Yidong Wang,Alex Jinpeng Wang,Zhi Chen,Wanxiang Che,Libo Qin*

Main category: cs.CV

TL;DR: 研究揭示了多模态思维链(MCoT)通过视觉思维传递图像信息提升大视觉语言模型(LVLMs)性能的机制，发现不同表达形式的视觉思维在清晰度和简洁性上的差异会影响改进效果。


<details>
  <summary>Details</summary>
Motivation: 现有MCoT方法(文本型MCoT和交错型MCoT)虽能提升模型表现，但其核心改进机制尚未明确，需系统分析视觉思维的本质作用。

Method: 定义四种视觉思维表达形式，系统性分析其清晰度与简洁性差异，探究视觉思维在Transformer深层的信息传递机制。

Result: 不同形式的视觉思维改进效果存在差异，视觉思维通过充当输入图像与深层推理层的中介实现高级视觉信息传输。

Conclusion: 视觉思维的质量(清晰度和简洁性)是MCoT改进的关键因素，其中介作用机制为未来研究提供了新方向。

Abstract: Large Vision-Language Models (LVLMs) have achieved significant success in
multimodal tasks, with multimodal chain-of-thought (MCoT) further enhancing
performance and interpretability. Recent MCoT methods fall into two categories:
(i) Textual-MCoT (T-MCoT), which takes multimodal input and produces textual
output; and (ii) Interleaved-MCoT (I-MCoT), which generates interleaved
image-text outputs. Despite advances in both approaches, the mechanisms driving
these improvements are not fully understood. To fill this gap, we first reveal
that MCoT boosts LVLMs by incorporating visual thoughts, which convey image
information to the reasoning process regardless of the MCoT format, depending
only on clarity and conciseness of expression. Furthermore, to explore visual
thoughts systematically, we define four distinct forms of visual thought
expressions and analyze them comprehensively. Our findings demonstrate that
these forms differ in clarity and conciseness, yielding varying levels of MCoT
improvement. Additionally, we explore the internal nature of visual thoughts,
finding that visual thoughts serve as intermediaries between the input image
and reasoning to deeper transformer layers, enabling more advanced visual
information transmission. We hope that the visual thoughts can inspire further
breakthroughs for future MCoT research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [186] [BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems](https://arxiv.org/abs/2505.15216)
*Andy K. Zhang,Joey Ji,Celeste Menders,Riya Dulepet,Thomas Qin,Ron Y. Wang,Junrong Wu,Kyleen Liao,Jiliang Li,Jinghan Hu,Sara Hong,Nardos Demilew,Shivatmica Murgai,Jason Tran,Nishka Kacheria,Ethan Ho,Denis Liu,Lauren McLane,Olivia Bruvik,Dai-Rong Han,Seungwoo Kim,Akhil Vyas,Cuiyuanxiu Chen,Ryan Li,Weiran Xu,Jonathan Z. Ye,Prerit Choudhary,Siddharth M. Bhatia,Vikram Sivashankar,Yuxuan Bao,Dawn Song,Dan Boneh,Daniel E. Ho,Percy Liang*

Main category: cs.CR

TL;DR: 提出首个评估AI攻防能力的BountyBench框架，在25个真实系统测试中，不同AI代理在漏洞检测（5%）、利用（最高67.5%）和修复（最高90%）任务中展现出攻防能力差异。


<details>
  <summary>Details</summary>
Motivation: 为解决AI代理对网络安全攻防能力缺乏系统性评估的问题，建立真实场景下的评估基准。

Method: 1. 构建含25个真实代码库的BountyBench框架
2. 定义检测/利用/修复三类任务
3. 创建包含40个漏洞的悬赏机制（最高$30,485）
4. 设计基于信息量的难度调节策略
5. 测试5种AI代理（Claude/OpenAI/Gemini等）

Result: 最佳代理表现：
- 漏洞检测：Claude Code（5%，$1,350）
- 漏洞利用：Claude 3.7（67.5%）
- 漏洞修复：OpenAI Codex CLI（90%，$14,422）
防御型代理（OpenAI/Claude Code）修复成功率更高（87.5-90% vs 利用成功率32.5-57.5%）

Conclusion: AI代理在网络安全攻防中存在能力不对称性，定制化代理攻防更平衡，商业代理防御能力突出。该框架为评估AI网络安全能力提供重要基准。

Abstract: AI agents have the potential to significantly alter the cybersecurity
landscape. To help us understand this change, we introduce the first framework
to capture offensive and defensive cyber-capabilities in evolving real-world
systems. Instantiating this framework with BountyBench, we set up 25 systems
with complex, real-world codebases. To capture the vulnerability lifecycle, we
define three task types: Detect (detecting a new vulnerability), Exploit
(exploiting a specific vulnerability), and Patch (patching a specific
vulnerability). For Detect, we construct a new success indicator, which is
general across vulnerability types and provides localized evaluation. We
manually set up the environment for each system, including installing packages,
setting up server(s), and hydrating database(s). We add 40 bug bounties, which
are vulnerabilities with monetary awards from \$10 to \$30,485, and cover 9 of
the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy
based on information to guide detection, interpolating from identifying a zero
day to exploiting a specific vulnerability. We evaluate 5 agents: Claude Code,
OpenAI Codex CLI, and custom agents with GPT-4.1, Gemini 2.5 Pro Preview, and
Claude 3.7 Sonnet Thinking. Given up to three attempts, the top-performing
agents are Claude Code (5% on Detect, mapping to \$1,350), Custom Agent with
Claude 3.7 Sonnet Thinking (5% on Detect, mapping to \$1,025; 67.5% on
Exploit), and OpenAI Codex CLI (5% on Detect, mapping to \$2,400; 90% on Patch,
mapping to \$14,422). OpenAI Codex CLI and Claude Code are more capable at
defense, achieving higher Patch scores of 90% and 87.5%, compared to Exploit
scores of 32.5% and 57.5% respectively; in contrast, the custom agents are
relatively balanced between offense and defense, achieving Exploit scores of
40-67.5% and Patch scores of 45-60%.

</details>


### [187] [Alignment Under Pressure: The Case for Informed Adversaries When Evaluating LLM Defenses](https://arxiv.org/abs/2505.15738)
*Xiaoxue Yang,Bozhidar Stevanoski,Matthieu Meeus,Yves-Alexandre de Montjoye*

Main category: cs.CR

TL;DR: 现有对齐防御方法在知情威胁模型下存在脆弱性，基于中间模型检查点的GCG初始化攻击能突破SOTA防御并发现通用对抗后缀


<details>
  <summary>Details</summary>
Motivation: 验证当攻击者掌握对齐过程信息时（如检查点），现有高ASR防御是否仍然有效

Method: 利用中间模型检查点逐步初始化GCG对抗后缀生成，结合梯度信息选择策略优化攻击效率

Result: 新方法在各类防御模型中成功率显著提升，发现通用对抗后缀存在，ASR最高提升3倍

Conclusion: 当前对齐方法存在根本脆弱性，需在安全测试中考虑攻击者掌握对齐知识的强威胁模型

Abstract: Large language models (LLMs) are rapidly deployed in real-world applications
ranging from chatbots to agentic systems. Alignment is one of the main
approaches used to defend against attacks such as prompt injection and
jailbreaks. Recent defenses report near-zero Attack Success Rates (ASR) even
against Greedy Coordinate Gradient (GCG), a white-box attack that generates
adversarial suffixes to induce attacker-desired outputs. However, this search
space over discrete tokens is extremely large, making the task of finding
successful attacks difficult. GCG has, for instance, been shown to converge to
local minima, making it sensitive to initialization choices. In this paper, we
assess the future-proof robustness of these defenses using a more informed
threat model: attackers who have access to some information about the alignment
process. Specifically, we propose an informed white-box attack leveraging the
intermediate model checkpoints to initialize GCG, with each checkpoint acting
as a stepping stone for the next one. We show this approach to be highly
effective across state-of-the-art (SOTA) defenses and models. We further show
our informed initialization to outperform other initialization methods and show
a gradient-informed checkpoint selection strategy to greatly improve attack
performance and efficiency. Importantly, we also show our method to
successfully find universal adversarial suffixes -- single suffixes effective
across diverse inputs. Our results show that, contrary to previous beliefs,
effective adversarial suffixes do exist against SOTA alignment-based defenses,
that these can be found by existing attack methods when adversaries exploit
alignment knowledge, and that even universal suffixes exist. Taken together,
our results highlight the brittleness of current alignment-based methods and
the need to consider stronger threat models when testing the safety of LLMs.

</details>


### [188] [Scalable Defense against In-the-wild Jailbreaking Attacks with Safety Context Retrieval](https://arxiv.org/abs/2505.15753)
*Taiye Chen,Zeming Wei,Ang Li,Yisen Wang*

Main category: cs.CR

TL;DR: 提出Safety Context Retrieval (SCR)框架，通过检索安全对齐示例增强LLMs对越狱攻击的防御能力。


<details>
  <summary>Details</summary>
Motivation: 现有静态防御框架难以应对不断进化的越狱攻击技术，需建立动态防御机制保障LLMs部署安全。

Method: 结合检索增强生成(RAG)技术，构建基于安全上下文检索的防御范式SCR，通过动态检索安全对齐示例提升模型鲁棒性。

Result: 实验证明SCR在对抗传统及新兴越狱攻击时均展现优越防御性能，准确率提升显著。

Conclusion: SCR为LLM安全提供了可扩展的动态防御新范式，开创了基于上下文检索的安全增强技术路径。

Abstract: Large Language Models (LLMs) are known to be vulnerable to jailbreaking
attacks, wherein adversaries exploit carefully engineered prompts to induce
harmful or unethical responses. Such threats have raised critical concerns
about the safety and reliability of LLMs in real-world deployment. While
existing defense mechanisms partially mitigate such risks, subsequent
advancements in adversarial techniques have enabled novel jailbreaking methods
to circumvent these protections, exposing the limitations of static defense
frameworks. In this work, we explore defending against evolving jailbreaking
threats through the lens of context retrieval. First, we conduct a preliminary
study demonstrating that even a minimal set of safety-aligned examples against
a particular jailbreak can significantly enhance robustness against this attack
pattern. Building on this insight, we further leverage the retrieval-augmented
generation (RAG) techniques and propose Safety Context Retrieval (SCR), a
scalable and robust safeguarding paradigm for LLMs against jailbreaking. Our
comprehensive experiments demonstrate how SCR achieves superior defensive
performance against both established and emerging jailbreaking tactics,
contributing a new paradigm to LLM safety. Our code will be available upon
publication.

</details>
