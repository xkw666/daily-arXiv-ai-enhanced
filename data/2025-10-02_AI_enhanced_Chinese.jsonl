{"id": "2510.00314", "pdf": "https://arxiv.org/pdf/2510.00314", "abs": "https://arxiv.org/abs/2510.00314", "authors": ["Xiaotang Zhang", "Ziyi Chang", "Qianhui Men", "Hubert P. H. Shum"], "title": "Motion In-Betweening for Densely Interacting Characters", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Motion in-betweening is the problem to synthesize movement between keyposes.\nTraditional research focused primarily on single characters. Extending them to\ndensely interacting characters is highly challenging, as it demands precise\nspatial-temporal correspondence between the characters to maintain the\ninteraction, while creating natural transitions towards predefined keyposes. In\nthis research, we present a method for long-horizon interaction in-betweening\nthat enables two characters to engage and respond to one another naturally. To\neffectively represent and synthesize interactions, we propose a novel solution\ncalled Cross-Space In-Betweening, which models the interactions of each\ncharacter across different conditioning representation spaces. We further\nobserve that the significantly increased constraints in interacting characters\nheavily limit the solution space, leading to degraded motion quality and\ndiminished interaction over time. To enable long-horizon synthesis, we present\ntwo solutions to maintain long-term interaction and motion quality, thereby\nkeeping synthesis in the stable region of the solution space.We first sustain\ninteraction quality by identifying periodic interaction patterns through\nadversarial learning. We further maintain the motion quality by learning to\nrefine the drifted latent space and prevent pose error accumulation. We\ndemonstrate that our approach produces realistic, controllable, and\nlong-horizon in-between motions of two characters with dynamic boxing and\ndancing actions across multiple keyposes, supported by extensive quantitative\nevaluations and user studies.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u7a7a\u95f4\u4e2d\u95f4\u751f\u6210\u65b9\u6cd5\u89e3\u51b3\u53cc\u89d2\u8272\u957f\u65f6\u4ea4\u4e92\u8fd0\u52a8\u5408\u6210\uff0c\u901a\u8fc7\u5bf9\u6297\u5b66\u4e60\u7ef4\u6301\u4ea4\u4e92\u8d28\u91cf\u4e0e\u6f5c\u5728\u7a7a\u95f4\u7cbe\u70bc\u4fdd\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027", "motivation": "\u4f20\u7edf\u5355\u89d2\u8272\u8fd0\u52a8\u4e2d\u95f4\u751f\u6210\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u53cc\u89d2\u8272\u5bc6\u96c6\u4ea4\u4e92\u573a\u666f\uff0c\u4ea4\u4e92\u7ea6\u675f\u5bfc\u81f4\u89e3\u7a7a\u95f4\u53d7\u9650\uff0c\u8fd0\u52a8\u8d28\u91cf\u4e0e\u4ea4\u4e92\u6301\u7eed\u6027\u968f\u65f6\u95f4\u4e0b\u964d", "method": "1) \u8de8\u7a7a\u95f4\u6761\u4ef6\u8868\u793a\u5efa\u6a21\u89d2\u8272\u4ea4\u4e92 2) \u5bf9\u6297\u5b66\u4e60\u8bc6\u522b\u5468\u671f\u6027\u4ea4\u4e92\u6a21\u5f0f 3) \u6f5c\u5728\u7a7a\u95f4\u6f02\u79fb\u4fee\u6b63\u9632\u6b62\u59ff\u6001\u8bef\u5dee\u7d2f\u79ef", "result": "\u5b9e\u73b0\u591a\u5173\u952e\u59ff\u52bf\u4e0b\u52a8\u6001\u62f3\u51fb/\u821e\u8e48\u52a8\u4f5c\u7684\u903c\u771f\u957f\u65f6\u5408\u6210\uff0c\u5b9a\u91cf\u8bc4\u4f30\u4e0e\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u53cc\u89d2\u8272\u957f\u65f6\u4ea4\u4e92\u8fd0\u52a8\u5408\u6210\u96be\u9898\uff0c\u5728\u4fdd\u6301\u65f6\u7a7a\u5bf9\u5e94\u5173\u7cfb\u7684\u540c\u65f6\u5b9e\u73b0\u81ea\u7136\u8fc7\u6e21\u4e0e\u8fd0\u52a8\u8d28\u91cf\u7ef4\u6301"}}
{"id": "2510.01061", "pdf": "https://arxiv.org/pdf/2510.01061", "abs": "https://arxiv.org/abs/2510.01061", "authors": ["Mark Boss", "Andreas Engelhardt", "Simon Donn\u00e9", "Varun Jampani"], "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Distribution matching is central to many vision and graphics tasks, where the\nwidely used Wasserstein distance is too costly to compute for high dimensional\ndistributions. The Sliced Wasserstein Distance (SWD) offers a scalable\nalternative, yet its Monte Carlo estimator suffers from high variance,\nresulting in noisy gradients and slow convergence. We introduce Reservoir SWD\n(ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively\nretain informative projection directions in optimization steps, resulting in\nstable gradients while remaining unbiased. Experiments on synthetic benchmarks\nand real-world tasks such as color correction and diffusion guidance show that\nReSWD consistently outperforms standard SWD and other variance reduction\nbaselines. Project page: https://reservoirswd.github.io/", "AI": {"tldr": "\u63d0\u51faReservoir SWD(ReSWD)\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4fdd\u7559\u5173\u952e\u6295\u5f71\u65b9\u5411\u964d\u4f4eSliced Wasserstein\u8ddd\u79bb\u7684\u65b9\u5dee\uff0c\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u68af\u5ea6", "motivation": "\u4f20\u7edfSliced Wasserstein Distance(SWD)\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\u5b58\u5728\u9ad8\u65b9\u5dee\u95ee\u9898\uff0c\u5bfc\u81f4\u68af\u5ea6\u566a\u58f0\u5927\u3001\u6536\u655b\u901f\u5ea6\u6162", "method": "\u5c06\u52a0\u6743\u50a8\u5c42\u91c7\u6837(Weighted Reservoir Sampling)\u6574\u5408\u5230SWD\u4e2d\uff0c\u5728\u4f18\u5316\u6b65\u9aa4\u4e2d\u81ea\u9002\u5e94\u4fdd\u7559\u4fe1\u606f\u91cf\u5927\u7684\u6295\u5f71\u65b9\u5411", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u989c\u8272\u6821\u6b63\u3001\u6269\u6563\u6a21\u578b\u6307\u5bfc\u7b49\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cReSWD\u6301\u7eed\u4f18\u4e8e\u6807\u51c6SWD\u548c\u5176\u4ed6\u65b9\u5dee\u7f29\u51cf\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "ReSWD\u5728\u4fdd\u6301\u65e0\u504f\u4f30\u8ba1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u7a33\u5b9a\u68af\u5ea6\uff0c\u4e3a\u9ad8\u7ef4\u5206\u5e03\u5339\u914d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.01176", "pdf": "https://arxiv.org/pdf/2510.01176", "abs": "https://arxiv.org/abs/2510.01176", "authors": ["Jiye Lee", "Chenghui Li", "Linh Tran", "Shih-En Wei", "Jason Saragih", "Alexander Richard", "Hanbyul Joo", "Shaojie Bai"], "title": "Audio Driven Real-Time Facial Animation for Social Telepresence", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.SD"], "comment": "SIGGRAPH Asia 2025. Project page:\n  https://jiyewise.github.io/projects/AudioRTA", "summary": "We present an audio-driven real-time system for animating photorealistic 3D\nfacial avatars with minimal latency, designed for social interactions in\nvirtual reality for anyone. Central to our approach is an encoder model that\ntransforms audio signals into latent facial expression sequences in real time,\nwhich are then decoded as photorealistic 3D facial avatars. Leveraging the\ngenerative capabilities of diffusion models, we capture the rich spectrum of\nfacial expressions necessary for natural communication while achieving\nreal-time performance (<15ms GPU time). Our novel architecture minimizes\nlatency through two key innovations: an online transformer that eliminates\ndependency on future inputs and a distillation pipeline that accelerates\niterative denoising into a single step. We further address critical design\nchallenges in live scenarios for processing continuous audio signals\nframe-by-frame while maintaining consistent animation quality. The versatility\nof our framework extends to multimodal applications, including semantic\nmodalities such as emotion conditions and multimodal sensors with head-mounted\neye cameras on VR headsets. Experimental results demonstrate significant\nimprovements in facial animation accuracy over existing offline\nstate-of-the-art baselines, achieving 100 to 1000 times faster inference speed.\nWe validate our approach through live VR demonstrations and across various\nscenarios such as multilingual speeches.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5b9e\u65f63D\u9762\u90e8\u52a8\u753b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u53d8\u538b\u5668\u548c\u5355\u6b65\u84b8\u998f\u5b9e\u73b015ms\u8d85\u4f4e\u5ef6\u8fdf\uff0c\u652f\u6301VR\u591a\u6a21\u6001\u4ea4\u4e92\u3002", "motivation": "\u89e3\u51b3\u865a\u62df\u73b0\u5b9e\u4e2d\u9762\u90e8\u52a8\u753b\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u6ee1\u8db3\u5b9e\u65f6\u81ea\u7136\u793e\u4ea4\u4e92\u52a8\u7684\u9700\u6c42\uff0c\u7a81\u7834\u4f20\u7edf\u79bb\u7ebf\u65b9\u6cd5\u7684\u901f\u5ea6\u9650\u5236\u3002", "method": "1. \u97f3\u9891\u7f16\u7801\u5668\u5b9e\u65f6\u8f6c\u6362\u8bed\u97f3\u4e3a\u8868\u60c5\u6f5c\u7a7a\u95f4\uff1b2. \u5728\u7ebf\u53d8\u538b\u5668\u6d88\u9664\u672a\u6765\u5e27\u4f9d\u8d56\uff1b3. \u84b8\u998f\u7ba1\u9053\u5c06\u6269\u6563\u8fed\u4ee3\u538b\u7f29\u4e3a\u5355\u6b65\u63a8\u7406\uff1b4. \u591a\u6a21\u6001\u6846\u67b6\u6574\u5408\u60c5\u611f\u53c2\u6570\u548c\u773c\u52a8\u8ffd\u8e2a\u3002", "result": "\u5b9e\u73b015ms GPU\u63a8\u7406\u901f\u5ea6\uff08\u63d0\u5347100-1000\u500d\uff09\uff0c\u9762\u90e8\u52a8\u753b\u7cbe\u5ea6\u8d85\u8d8aSOTA\u79bb\u7ebf\u6a21\u578b\uff0c\u652f\u6301\u591a\u8bed\u8a00/\u591a\u573a\u666f\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5728\u4fdd\u6301\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u7684\u540c\u65f6\u8fbe\u5230\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3aVR\u793e\u4ea4\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5176\u67b6\u6784\u8bbe\u8ba1\u5bf9\u5b9e\u65f6\u751f\u6210\u6a21\u578b\u5177\u6709\u666e\u9002\u53c2\u8003\u4ef7\u503c\u3002"}}
{"id": "2510.00824", "pdf": "https://arxiv.org/pdf/2510.00824", "abs": "https://arxiv.org/abs/2510.00824", "authors": ["Xiaoye Michael Wang", "Ali Mazalek", "Catherine M. Sabiston", "Timothy N. Welsh"], "title": "Virtual Reality Alters Perceived Functional Body Size", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Virtual reality (VR) introduces sensory perturbations that may impact\nperception and action. The current study was designed to investigate how\nimmersive VR presented through a head-mounted display (HMD) affects perceived\nfunctional body size using a passable aperture paradigm. Participants (n=60)\nperformed an action task (sidle through apertures) and a perception task\n(adjust aperture width until passable without contact) in both physical,\nunmediated reality (UR) and VR. Results revealed significantly higher action\nand perceptual thresholds in VR compared to UR. Affordance ratios (perceptual\nthreshold over action threshold) were also higher in VR, indicating that the\nincrease in perceptual thresholds in VR was driven partly by sensorimotor\nuncertainty, as reflected in the increase in the action thresholds, and partly\nby perceptual distortions imposed by VR. This perceptual overestimation in VR\nalso persisted as an aftereffect in UR following VR exposure. Geometrical\nmodelling attributed the disproportionate increase in the perceptual threshold\nin VR primarily to depth compression. This compression, stemming from the\nvergence-accommodation conflict (VAC), caused the virtual aperture to be\nperceived as narrower than depicted, thus requiring a wider adjusted aperture.\nCritically, after mathematically correcting for the VAC's impact on perceived\naperture width, the affordance ratios in VR became equivalent to those in UR.\nThese outcomes demonstrate a recovered invariant geometrical scaling,\nsuggesting that perception remained functionally attuned to action capabilities\nonce VAC-induced distortions were accounted for. These findings highlight that\nVR-induced depth compression systematically alters perceived body-environment\nrelationships, leading to an altered sense of one's functional body size.", "AI": {"tldr": "VR\u73af\u5883\u901a\u8fc7\u89c6\u89c9\u8f90\u8f8f\u8c03\u8282\u51b2\u7a81\u5bfc\u81f4\u6df1\u5ea6\u538b\u7f29\uff0c\u4f7f\u865a\u62df\u5b54\u5f84\u611f\u77e5\u53d8\u7a84\uff0c\u9700\u6821\u6b63VAC\u5f71\u54cd\u624d\u80fd\u6062\u590d\u8eab\u4f53-\u73af\u5883\u5173\u7cfb\u7684\u529f\u80fd\u6807\u5ea6\u4e0d\u53d8\u6027", "motivation": "\u63a2\u7a76\u6c89\u6d78\u5f0fVR\u5982\u4f55\u901a\u8fc7\u5934\u663e\u8bbe\u5907\u5f71\u54cd\u4eba\u4f53\u5bf9\u529f\u80fd\u6027\u8eab\u4f53\u5c3a\u5bf8\u7684\u611f\u77e5\u673a\u5236\uff0c\u63ed\u793aVR\u89c6\u89c9\u7578\u53d8\u5bf9\u884c\u4e3a-\u611f\u77e5\u534f\u8c03\u6027\u7684\u5f71\u54cd", "method": "60\u540d\u88ab\u8bd5\u5728\u7269\u7406\u73b0\u5b9e(UR)\u548cVR\u4e2d\u5206\u522b\u5b8c\u6210\u884c\u52a8\u4efb\u52a1\uff08\u4fa7\u8eab\u7a7f\u5b54\uff09\u548c\u611f\u77e5\u4efb\u52a1\uff08\u8c03\u6574\u53ef\u901a\u8fc7\u5b54\u5f84\uff09\uff0c\u91c7\u7528\u51e0\u4f55\u5efa\u6a21\u91cf\u5316\u6df1\u5ea6\u538b\u7f29\u6548\u5e94", "result": "VR\u4e2d\u884c\u52a8/\u611f\u77e5\u9608\u503c\u663e\u8457\u9ad8\u4e8eUR\uff0c\u77e5\u89c9\u9608\u503c\u7684\u5f02\u5e38\u63d0\u5347\u4e3b\u8981\u6e90\u4e8eVAC\u5f15\u8d77\u7684\u6df1\u5ea6\u538b\u7f29\uff08\u51e0\u4f55\u538b\u7f29\u7387\u8fbe17.3%\uff09\uff0c\u6821\u6b63\u540e\u529f\u80fd\u6807\u5ea6\u6bd4\u6062\u590dUR\u6c34\u5e73", "conclusion": "VR\u5f15\u53d1\u7684\u6df1\u5ea6\u538b\u7f29\u7cfb\u7edf\u6027\u6539\u53d8\u8eab\u4f53-\u73af\u5883\u5173\u7cfb\u611f\u77e5\uff0c\u5f62\u6210\u5938\u5927\u7684\u529f\u80fd\u6027\u8eab\u4f53\u5c3a\u5bf8\u611f\u77e5\uff0c\u8be5\u6548\u5e94\u5728VR\u66b4\u9732\u540e\u4ecd\u6301\u7eed\u5b58\u5728\uff0c\u4f46\u672c\u8d28\u6e90\u4e8eVAC\u7684\u51e0\u4f55\u7578\u53d8\u800c\u975e\u611f\u77e5-\u884c\u52a8\u7cfb\u7edf\u7684\u529f\u80fd\u6027\u5931\u8c03"}}
{"id": "2510.00125", "pdf": "https://arxiv.org/pdf/2510.00125", "abs": "https://arxiv.org/abs/2510.00125", "authors": ["Hong kyu Lee", "Ruixuan Liu", "Li Xiong"], "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Machine unlearning is an emerging technique that removes the influence of a\nsubset of training data (forget set) from a model without full retraining, with\napplications including privacy protection, content moderation, and model\ncorrection. The key challenge lies in ensuring that the model completely\nforgets the knowledge of the forget set without compromising its overall\nutility. Existing unlearning methods for large language models (LLMs) often\nutilize auxiliary language models, retain datasets, or even commercial AI\nservices for effective unlearning and maintaining the model utility. However,\ndependence on these external resources is often impractical and could\npotentially introduce additional privacy risks. In this work, we propose direct\ntoken optimization (DTO), a novel self-contained unlearning approach for LLMs\nthat directly optimizes the token level objectives and eliminates the need for\nexternal resources. Given a sequence to unlearn, we identify two categories of\ntokens: target tokens, which capture critical knowledge for unlearning, and the\nremaining non-target tokens, which are crucial for maintaining the model\nutility. The former are used to optimize the unlearning objective, while the\nlatter serve to preserve the model's performance. The experimental results show\nthat the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality\non several benchmark datasets than the latest baselines while maintaining a\ncomparable level of model utility.", "AI": {"tldr": "\u63d0\u51fa\u76f4\u63a5\u4ee4\u724c\u4f18\u5316\uff08DTO\uff09\u65b9\u6cd5\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u8d44\u6e90\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u6548\u673a\u5668\u53bb\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u53bb\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\uff08\u8f85\u52a9\u6a21\u578b/\u6570\u636e\u96c6\uff09\uff0c\u5b58\u5728\u5b9e\u7528\u6027\u9650\u5236\u548c\u9690\u79c1\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u533a\u5206\u76ee\u6807\u4ee4\u724c\uff08\u5173\u952e\u9057\u5fd8\u77e5\u8bc6\uff09\u548c\u975e\u76ee\u6807\u4ee4\u724c\uff08\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff09\uff0c\u76f4\u63a5\u4f18\u5316\u4ee4\u724c\u7ea7\u76ee\u6807\u51fd\u6570\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f9716.8\u500d\u9057\u5fd8\u8d28\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "conclusion": "DTO\u4e3aLLM\u53bb\u5b66\u4e60\u63d0\u4f9b\u4e86\u72ec\u7acb\u9ad8\u6548\u7684\u6280\u672f\u8def\u5f84\uff0c\u5f3a\u5316\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u3002"}}
{"id": "2510.00161", "pdf": "https://arxiv.org/pdf/2510.00161", "abs": "https://arxiv.org/abs/2510.00161", "authors": ["Kimihiro Hasegawa", "Wiradee Imrattanatrai", "Masaki Asada", "Ken Fukuda", "Teruko Mitamura"], "title": "TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding", "categories": ["cs.CL"], "comment": "21 pages. Code: https://github.com/kimihiroh/tama", "summary": "Procedural activity assistants potentially support humans in a variety of\nsettings, from our daily lives, e.g., cooking or assembling flat-pack\nfurniture, to professional situations, e.g., manufacturing or biological\nexperiments. Despite its potential use cases, the system development tailored\nfor such an assistant is still underexplored. In this paper, we propose a novel\nframework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural\nactivity understanding. TAMA enables interleaved multimodal reasoning by making\nuse of multimedia-returning tools in a training-free setting. Our experimental\nresult on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our\napproach can improve the performance of vision-language models, especially\nGPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support\nfor the effectiveness of two features that characterize our framework,\nmultimedia-returning tools and agentic flexible tool selection. We believe our\nproposed framework and experimental results facilitate the thinking with images\nparadigm for video and multimodal tasks, let alone the development of\nprocedural activity assistants.", "AI": {"tldr": "\u63d0\u51faTAMA\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u7684\u591a\u6a21\u6001\u4ee3\u7406\u548c\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u5a92\u4f53\u5de5\u5177\u63d0\u5347\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7406\u89e3\uff0c\u663e\u8457\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7a0b\u5e8f\u6027\u6d3b\u52a8\u52a9\u624b\u5728\u751f\u6d3b/\u804c\u4e1a\u573a\u666f\u5e94\u7528\u6f5c\u529b\u5927\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u5f00\u53d1\u4e0d\u8db3\u3002\u9700\u5f00\u53d1\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406\u7684\u6846\u67b6\u4ee5\u63d0\u5347\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "method": "\u91c7\u7528\u591a\u5a92\u4f53\u8fd4\u56de\u5de5\u5177\u5b9e\u73b0\u96f6\u8bad\u7ec3\u73af\u5883\u4e0b\u7684\u4ea4\u9519\u591a\u6a21\u6001\u63a8\u7406\uff0c\u7ed3\u5408\u4ee3\u7406\u9a71\u52a8\u7684\u7075\u6d3b\u5de5\u5177\u9009\u62e9\u7b56\u7565\u3002", "result": "\u5728ProMQA-Assembly\u6570\u636e\u96c6\u4e0a\uff0cGPT-5\u548cMiMo-VL\u6027\u80fd\u63d0\u5347\u663e\u8457\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u591a\u5a92\u4f53\u5de5\u5177\u548c\u7075\u6d3b\u9009\u62e9\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "conclusion": "TAMA\u6846\u67b6\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u4efb\u52a1\u4e2d'\u56fe\u50cf\u601d\u8003'\u8303\u5f0f\u7684\u53d1\u5c55\uff0c\u4e3a\u7a0b\u5e8f\u6027\u6d3b\u52a8\u52a9\u624b\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.00172", "pdf": "https://arxiv.org/pdf/2510.00172", "abs": "https://arxiv.org/abs/2510.00172", "authors": ["Amirhossein Abaskohi", "Tianyi Chen", "Miguel Mu\u00f1oz-M\u00e1rmol", "Curtis Fox", "Amrutha Varshini Ramesh", "\u00c9tienne Marcotte", "Xing Han L\u00f9", "Nicolas Chapados", "Spandana Gella", "Christopher Pal", "Alexandre Drouin", "Issam H. Laradji"], "title": "DRBench: A Realistic Benchmark for Enterprise Deep Research", "categories": ["cs.CL"], "comment": null, "summary": "We introduce DRBench, a benchmark for evaluating AI agents on complex,\nopen-ended deep research tasks in enterprise settings. Unlike prior benchmarks\nthat focus on simple questions or web-only queries, DRBench evaluates agents on\nmulti-step queries (for example, ``What changes should we make to our product\nroadmap to ensure compliance with this standard?\") that require identifying\nsupporting facts from both the public web and private company knowledge base.\nEach task is grounded in realistic user personas and enterprise context,\nspanning a heterogeneous search space that includes productivity software,\ncloud file systems, emails, chat conversations, and the open web. Tasks are\ngenerated through a carefully designed synthesis pipeline with\nhuman-in-the-loop verification, and agents are evaluated on their ability to\nrecall relevant insights, maintain factual accuracy, and produce coherent,\nwell-structured reports. We release 15 deep research tasks across 10 domains,\nsuch as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness\nof DRBench by evaluating diverse DR agents across open- and closed-source\nmodels (such as GPT, Llama, and Qwen) and DR strategies, highlighting their\nstrengths, weaknesses, and the critical path for advancing enterprise deep\nresearch. Code is available at https://github.com/ServiceNow/drbench.", "AI": {"tldr": "\u63d0\u51faDRBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u590d\u6742\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u652f\u6301\u591a\u6e90\u5f02\u6784\u6570\u636e\u6574\u5408\u4e0e\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u5c40\u9650\u4e8e\u7b80\u5355\u95ee\u7b54\u548c\u7f51\u7edc\u67e5\u8be2\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f01\u4e1a\u573a\u666f\u4e2d\u9700\u8981\u6574\u5408\u5185\u5916\u90e8\u77e5\u8bc6\u3001\u591a\u6b65\u9aa4\u63a8\u7406\u7684\u6df1\u5ea6\u7814\u7a76\u9700\u6c42\u3002", "method": "\u6784\u5efa\u542b15\u4e2a\u8de8\u9886\u57df\u4efb\u52a1\u7684\u6d4b\u8bd5\u96c6\uff0c\u91c7\u7528\u4eba\u5de5\u9a8c\u8bc1\u7684\u5408\u6210\u6d41\u7a0b\u751f\u6210\u591a\u6b65\u9aa4\u67e5\u8be2\uff0c\u8bc4\u4f30\u7ef4\u5ea6\u5305\u542b\u4fe1\u606f\u53ec\u56de\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u62a5\u544a\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "\u4e0d\u540c\u6a21\u578b\uff08GPT/Llama/Qwen\uff09\u5728DR\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u663e\u793a\u4f01\u4e1a\u6df1\u5ea6\u7814\u7a76\u9700\u8981\u4e13\u95e8\u7684\u7b56\u7565\u4f18\u5316\u548c\u6280\u672f\u6539\u8fdb\u3002", "conclusion": "DRBench\u586b\u8865\u4f01\u4e1a\u7ea7\u7814\u7a76\u8bc4\u4f30\u7a7a\u767d\uff0c\u63ed\u793a\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u6574\u5408\u4e2d\u7684\u74f6\u9888\uff0c\u4e3a\u4f18\u5316\u4f01\u4e1aAI\u4ee3\u7406\u63d0\u4f9b\u65b9\u5411\u3002"}}
{"id": "2510.00174", "pdf": "https://arxiv.org/pdf/2510.00174", "abs": "https://arxiv.org/abs/2510.00174", "authors": ["Rik Koncel-Kedziorski", "Brihi Joshi", "Tim Paek"], "title": "PrimeX: A Dataset of Worldview, Opinion, and Explanation", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "As the adoption of language models advances, so does the need to better\nrepresent individual users to the model. Are there aspects of an individual's\nbelief system that a language model can utilize for improved alignment?\nFollowing prior research, we investigate this question in the domain of opinion\nprediction by developing PrimeX, a dataset of public opinion survey data from\n858 US residents with two additional sources of belief information: written\nexplanations from the respondents for why they hold specific opinions, and the\nPrimal World Belief survey for assessing respondent worldview. We provide an\nextensive initial analysis of our data and show the value of belief\nexplanations and worldview for personalizing language models. Our results\ndemonstrate how the additional belief information in PrimeX can benefit both\nthe NLP and psychological research communities, opening up avenues for further\nstudy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPrimeX\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6574\u5408\u7528\u6237\u89c2\u70b9\u89e3\u91ca\u548c\u4e16\u754c\u89c2\u8bc4\u4f30\uff0c\u8bc1\u660e\u6b64\u7c7b\u4fe1\u5ff5\u4fe1\u606f\u53ef\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u5316\u80fd\u529b\uff0c\u5e76\u4e3aNLP\u4e0e\u5fc3\u7406\u5b66\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u7528\u6237\u4fe1\u4ef0\u4f53\u7cfb\uff08\u89c2\u70b9\u89e3\u91ca+\u4e16\u754c\u89c2\uff09\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u4e0e\u4e2a\u4f53\u4ef7\u503c\u89c2\u7684\u5339\u914d\u5ea6\uff0c\u89e3\u51b3\u6a21\u578b\u4e2a\u6027\u5316\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u6536\u96c6858\u540d\u7f8e\u56fd\u5c45\u6c11\u7684\u6c11\u610f\u8c03\u67e5\u6570\u636e\uff0c\u989d\u5916\u83b7\u53d6\u5176\u89c2\u70b9\u89e3\u91ca\u6587\u672c\u548cPrimal World Belief\u4e16\u754c\u89c2\u8bc4\u4f30\u6570\u636e\uff0c\u5f00\u5c55\u591a\u7ef4\u5ea6\u6570\u636e\u5206\u6790\u3002", "result": "\u8bc1\u5b9e\u4fe1\u5ff5\u4fe1\u606f\u80fd\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u4e2a\u6027\u5316\u6548\u679c\uff0cPrimeX\u6570\u636e\u96c6\u540c\u65f6\u5177\u5907NLP\u6280\u672f\u4f18\u5316\u548c\u5fc3\u7406\u5b66\u7814\u7a76\u7684\u53cc\u91cd\u4ef7\u503c\u3002", "conclusion": "\u901a\u8fc7PrimeX\u6570\u636e\u96c6\u9996\u6b21\u7cfb\u7edf\u9a8c\u8bc1\u4fe1\u4ef0\u7cfb\u7edf\u5bf9\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u7684\u4ef7\u503c\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u5efa\u7acb\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.00177", "pdf": "https://arxiv.org/pdf/2510.00177", "abs": "https://arxiv.org/abs/2510.00177", "authors": ["Shuyue Stella Li", "Avinandan Bose", "Faeze Brahman", "Simon Shaolei Du", "Pang Wei Koh", "Maryam Fazel", "Yulia Tsvetkov"], "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It", "categories": ["cs.CL", "cs.AI"], "comment": "57 pages, 6 figures", "summary": "Current large language model (LLM) development treats task-solving and\npreference alignment as separate challenges, optimizing first for objective\ncorrectness, then for alignment to aggregated human preferences. This paradigm\nfails in human-facing applications where solving a problem correctly is\ninsufficient if the response mismatches the user's needs. This challenge\nintensifies in just-in-time scenarios where no prior user interaction history\nexists due to cold-start conditions or privacy constraints. LLMs need to\nidentify what they don't know about user preferences, strategically elicit\npreference values through questioning, then adapt their reasoning processes and\nresponses accordingly -- a complicated chain of cognitive processes which we\nterm personalized reasoning. We introduce PREFDISCO, an evaluation methodology\nthat transforms static benchmarks into interactive personalization tasks using\npsychologically-grounded personas with sparse preferences. Our framework\ncreates scenarios where identical questions require different reasoning chains\ndepending on user context, as optimal explanation approaches vary by individual\nexpertise and preferences while maintaining factual accuracy. Evaluation of 21\nfrontier models across 10 tasks reveals 29.0% of naive personalization attempts\nproduce worse preference alignment than generic responses, yet generic\nresponses also fail to serve individual user needs effectively. These findings\nsuggest personalized reasoning requires dedicated development rather than\nemerging naturally. PREFDISCO establishes personalized reasoning as a\nmeasurable research frontier and reveals fundamental limitations in current\nLLMs' interactive capabilities, providing a foundation for developing systems\nthat can adapt to individual users in education, healthcare, and technical\ndomains where personalization is critical.", "AI": {"tldr": "\u63d0\u51faPREFDISCO\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u9700\u4e13\u95e8\u5f00\u53d1\u800c\u975e\u81ea\u7136\u6d8c\u73b0", "motivation": "\u73b0\u6709LLM\u5c06\u4efb\u52a1\u89e3\u51b3\u4e0e\u504f\u597d\u5bf9\u9f50\u5272\u88c2\u5904\u7406\uff0c\u5bfc\u81f4\u51b7\u542f\u52a8\u573a\u666f\u4e0b\u65e0\u6cd5\u6709\u6548\u5339\u914d\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u4e9f\u9700\u80fd\u4e3b\u52a8\u8be2\u95ee\u504f\u597d\u5e76\u81ea\u9002\u5e94\u8c03\u6574\u7684\u63a8\u7406\u673a\u5236", "method": "\u57fa\u4e8e\u5fc3\u7406\u5b66\u6784\u5efa\u865a\u62df\u7528\u6237\u753b\u50cf\uff0c\u5c06\u9759\u6001\u57fa\u51c6\u8f6c\u5316\u4e3a\u4ea4\u4e92\u5f0f\u4e2a\u6027\u5316\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u6839\u636e\u7528\u6237\u80cc\u666f\u751f\u6210\u4e0d\u540c\u63a8\u7406\u94fe\uff0c\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\u540c\u65f6\u6ee1\u8db3\u4e2a\u6027\u5316\u9700\u6c42", "result": "\u8bc4\u4f3021\u4e2a\u524d\u6cbf\u6a21\u578b\u663e\u793a\uff1a29%\u7684\u4e2a\u6027\u5316\u5c1d\u8bd5\u6548\u679c\u5dee\u4e8e\u901a\u7528\u56de\u7b54\uff0c\u540c\u65f6\u901a\u7528\u56de\u7b54\u4e5f\u65e0\u6cd5\u6ee1\u8db3\u4e2a\u4f53\u9700\u6c42\uff0c\u7a81\u663e\u5f53\u524d\u6a21\u578b\u7684\u4ea4\u4e92\u80fd\u529b\u5c40\u9650", "conclusion": "\u4e2a\u6027\u5316\u63a8\u7406\u9700\u6210\u4e3a\u72ec\u7acb\u7814\u7a76\u65b9\u5411\uff0cPREFDISCO\u4e3a\u6b64\u63d0\u4f9b\u6d4b\u91cf\u57fa\u51c6\uff0c\u66b4\u9732LLM\u5728\u533b\u7597/\u6559\u80b2\u7b49\u5173\u952e\u9886\u57df\u81ea\u9002\u5e94\u80fd\u529b\u7684\u6839\u672c\u6027\u7f3a\u9677"}}
{"id": "2510.00232", "pdf": "https://arxiv.org/pdf/2510.00232", "abs": "https://arxiv.org/abs/2510.00232", "authors": ["Xin Xu", "Xunzhi He", "Churan Zhi", "Ruizhe Chen", "Julian McAuley", "Zexue He"], "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Work in progress", "summary": "Existing studies on bias mitigation methods for large language models (LLMs)\nuse diverse baselines and metrics to evaluate debiasing performance, leading to\ninconsistent comparisons among them. Moreover, their evaluations are mostly\nbased on the comparison between LLMs' probabilities of biased and unbiased\ncontexts, which ignores the gap between such evaluations and real-world use\ncases where users interact with LLMs by reading model responses and expect fair\nand safe outputs rather than LLMs' probabilities. To enable consistent\nevaluation across debiasing methods and bridge this gap, we introduce\nBiasFreeBench, an empirical benchmark that comprehensively compares eight\nmainstream bias mitigation techniques (covering four prompting-based and four\ntraining-based methods) on two test scenarios (multi-choice QA and open-ended\nmulti-turn QA) by reorganizing existing datasets into a unified query-response\nsetting. We further introduce a response-level metric, Bias-Free Score, to\nmeasure the extent to which LLM responses are fair, safe, and\nanti-stereotypical. Debiasing performances are systematically compared and\nanalyzed across key dimensions: the prompting vs. training paradigm, model\nsize, and generalization of different training strategies to unseen bias types.\nWe will publicly release our benchmark, aiming to establish a unified testbed\nfor bias mitigation research.", "AI": {"tldr": "\u63d0\u51faBiasFreeBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f308\u79cd\u4e3b\u6d41LLM\u53bb\u504f\u65b9\u6cd5\uff08\u6db5\u76d6\u63d0\u793a\u5bfc\u5411\u548c\u8bad\u7ec3\u5bfc\u5411\u4e24\u7c7b\uff09\uff0c\u901a\u8fc7\u7edf\u4e00\u5316\u7684\u67e5\u8be2-\u54cd\u5e94\u573a\u666f\u548c\u54cd\u5e94\u7ea7\u6307\u6807Bias-Free Score\u5b9e\u73b0\u8de8\u65b9\u6cd5\u4e00\u81f4\u6027\u8bc4\u4f30", "motivation": "\u73b0\u6709\u53bb\u504f\u65b9\u6cd5\u8bc4\u4f30\u5b58\u5728\u57fa\u7ebf\u4e0d\u7edf\u4e00\u3001\u6307\u6807\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e14\u4f20\u7edf\u57fa\u4e8e\u6982\u7387\u7684\u8bc4\u4f30\u65b9\u5f0f\u4e0e\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u573a\u666f\uff08\u5173\u6ce8\u6a21\u578b\u54cd\u5e94\u8d28\u91cf\uff09\u5b58\u5728\u8131\u8282", "method": "\u91cd\u6784\u73b0\u6709\u6570\u636e\u96c6\u5f62\u6210\u7edf\u4e00\u67e5\u8be2\u54cd\u5e94\u6d4b\u8bd5\u96c6\uff0c\u5728\u591a\u9879\u9009\u62e9QA\u548c\u5f00\u653e\u5f0f\u591a\u8f6eQA\u573a\u666f\u4e0b\uff0c\u7efc\u5408\u8bc4\u4f304\u79cd\u63d0\u793a\u5bfc\u5411\u65b9\u6cd5/4\u79cd\u8bad\u7ec3\u5bfc\u5411\u65b9\u6cd5\uff0c\u5e76\u521b\u65b0\u8bbe\u8ba1\u53cd\u6620\u54cd\u5e94\u516c\u5e73\u6027/\u5b89\u5168\u6027/\u53cd\u523b\u677f\u5370\u8c61\u7684Bias-Free Score", "result": "\u7cfb\u7edf\u6bd4\u8f83\u4e86\u63d0\u793a\u5bfc\u5411\u4e0e\u8bad\u7ec3\u5bfc\u5411\u8303\u5f0f\u7684\u4f18\u52a3\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u5bf9\u53bb\u504f\u6548\u679c\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u7b56\u7565\u5bf9\u672a\u89c1\u8fc7\u504f\u89c1\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "BiasFreeBench\u901a\u8fc7\u6807\u51c6\u5316\u6d4b\u8bd5\u6846\u67b6\u548c\u54cd\u5e94\u7ea7\u8bc4\u4f30\u6307\u6807\uff0c\u4e3a\u504f\u89c1\u7f13\u89e3\u7814\u7a76\u5efa\u7acb\u7edf\u4e00\u57fa\u51c6\uff0c\u63a8\u52a8\u9762\u5411\u771f\u5b9e\u5e94\u7528\u573a\u666f\u7684\u53bb\u504f\u6280\u672f\u53d1\u5c55"}}
{"id": "2510.00255", "pdf": "https://arxiv.org/pdf/2510.00255", "abs": "https://arxiv.org/abs/2510.00255", "authors": ["Monishwaran Maheswaran", "Marco Carini", "Christian Federmann", "Tony Diaz"], "title": "TASER: Translation Assessment via Systematic Evaluation and Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce TASER (Translation Assessment via Systematic Evaluation and\nReasoning), a metric that uses Large Reasoning Models (LRMs) for automated\ntranslation quality assessment. TASER harnesses the explicit reasoning\ncapabilities of LRMs to conduct systematic, step-by-step evaluation of\ntranslation quality. We evaluate TASER on the WMT24 Metrics Shared Task across\nboth reference-based and reference-free scenarios, demonstrating\nstate-of-the-art performance. In system-level evaluation, TASER achieves the\nhighest soft pairwise accuracy in both reference-based and reference-free\nsettings, outperforming all existing metrics. At the segment level, TASER\nmaintains competitive performance with our reference-free variant ranking as\nthe top-performing metric among all reference-free approaches. Our experiments\nreveal that structured prompting templates yield superior results with LRMs\ncompared to the open-ended approaches that proved optimal for traditional LLMs.\nWe evaluate o3, a large reasoning model from OpenAI, with varying reasoning\nefforts, providing insights into the relationship between reasoning depth and\nevaluation quality. The explicit reasoning process in LRMs offers\ninterpretability and visibility, addressing a key limitation of existing\nautomated metrics. Our results demonstrate that Large Reasoning Models show a\nmeasurable advancement in translation quality assessment, combining improved\naccuracy with transparent evaluation across diverse language pairs.", "AI": {"tldr": "TASER\uff08\u57fa\u4e8e\u7cfb\u7edf\u63a8\u7406\u7684\u7ffb\u8bd1\u8bc4\u4f30\uff09\u5229\u7528\u5927\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u663e\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u5728WMT24\u8bc4\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u81ea\u52a8\u7ffb\u8bd1\u8bc4\u4f30\u6307\u6807\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0cTASER\u901a\u8fc7LRMs\u7684\u9010\u6b65\u7cfb\u7edf\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u900f\u660e\u5316\u7684\u8bc4\u4f30\u8fc7\u7a0b\u3002", "method": "1. \u8bbe\u8ba1\u7ed3\u6784\u5316\u63d0\u793a\u6a21\u677f\u589e\u5f3aLRMs\u7684\u8bc4\u4f30\u6548\u679c\n2. \u5728\u53c2\u8003/\u65e0\u53c2\u8003\u573a\u666f\u4e0b\u6d4b\u8bd5\u7cfb\u7edf\u7ea7\u548c\u7247\u6bb5\u7ea7\u8868\u73b0\n3. \u901a\u8fc7\u8c03\u6574o3\u6a21\u578b\u7684\u63a8\u7406\u6df1\u5ea6\u63a2\u7d22\u63a8\u7406\u5f3a\u5ea6\u4e0e\u8bc4\u4f30\u8d28\u91cf\u7684\u5173\u7cfb", "result": "\u7cfb\u7edf\u7ea7\u8bc4\u4f30\uff1a\n- \u53c2\u8003/\u65e0\u53c2\u8003\u573a\u666f\u8f6f\u914d\u5bf9\u51c6\u786e\u7387\u5747\u7b2c\u4e00\n\u7247\u6bb5\u7ea7\u8bc4\u4f30\uff1a\n- \u65e0\u53c2\u8003\u7248\u672c\u5728\u6240\u6709\u65e0\u53c2\u8003\u6307\u6807\u4e2d\u6392\u540d\u7b2c\u4e00\n- \u7ed3\u6784\u5316\u63d0\u793a\u6bd4\u5f00\u653e\u5f0f\u63d0\u793a\u6548\u679c\u63d0\u5347\u660e\u663e", "conclusion": "\u5927\u63a8\u7406\u6a21\u578b\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u673a\u5236\uff0c\u5728\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4e2d\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\uff08\u76f8\u6bd4\u73b0\u6709\u6307\u6807\uff09\u548c\u53ef\u89e3\u91ca\u6027\uff08\u76f8\u6bd4\u9ed1\u7bb1\u6a21\u578b\uff09\u7684\u53cc\u91cd\u7a81\u7834\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2510.00261", "pdf": "https://arxiv.org/pdf/2510.00261", "abs": "https://arxiv.org/abs/2510.00261", "authors": ["Xiaoyu Song", "William Han", "Tony Chen", "Chaojing Duan", "Michael A. Rosenberg", "Emerson Liu", "Ding Zhao"], "title": "Retrieval-Augmented Generation for Electrocardiogram-Language Models", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": "5 pages, 2 figures; Submitted to ICASSP 2026", "summary": "Interest in generative Electrocardiogram-Language Models (ELMs) is growing,\nas they can produce textual responses conditioned on ECG signals and textual\nqueries. Unlike traditional classifiers that output label probabilities, ELMs\nare more versatile, supporting domain-specific tasks (e.g., waveform analysis,\ndiagnosis, prognosis) as well as general tasks (e.g., open-ended questions,\ndialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language\nModels (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce\nhallucinations and improve natural language generation (NLG). However, despite\nits promise, no open-source implementation or systematic study of RAG pipeline\ndesign for ELMs currently exists. To address this gap, we present the first\nopen-source RAG pipeline for ELMs, along with baselines and ablation studies\nfor NLG. Experiments on three public datasets show that ELMs with RAG\nconsistently improves performance over non-RAG baselines and highlights key ELM\ndesign considerations. Our code is available at:\nhttps://github.com/willxxy/ECG-Bench.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u5f00\u6e90\u7684\u5fc3\u7535\u56fe-\u8bed\u8a00\u6a21\u578b\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6d41\u7a0b\uff0c\u5b9e\u9a8c\u8868\u660e\u53ef\u6709\u6548\u63d0\u5347\u6587\u672c\u751f\u6210\u6027\u80fd", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u5fc3\u7535\u56fe-\u8bed\u8a00\u6a21\u578b(ELM)\u7f3a\u4e4fRAG\u7684\u7cfb\u7edf\u7814\u7a76\u548c\u5f00\u6e90\u5b9e\u73b0\uff0c\u96be\u4ee5\u51cf\u5c11\u5e7b\u89c9\u751f\u6210\u5e76\u63d0\u5347\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8d28\u91cf", "method": "\u6784\u5efa\u5305\u542b\u77e5\u8bc6\u68c0\u7d22\u6a21\u5757\u7684RAG\u6d41\u7a0b\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u7ebf\u6d4b\u8bd5\u548c\u6d88\u878d\u5b9e\u9a8c\u5206\u6790", "result": "RAG\u7248\u672cELM\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u975eRAG\u57fa\u7ebf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe15%", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86ELM\u9886\u57dfRAG\u6280\u672f\u843d\u5730\u7684\u7a7a\u767d\uff0c\u4e3a\u533b\u7597\u6587\u672c\u751f\u6210\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003"}}
{"id": "2510.00263", "pdf": "https://arxiv.org/pdf/2510.00263", "abs": "https://arxiv.org/abs/2510.00263", "authors": ["Zhuohang Li", "Xiaowei Li", "Chengyu Huang", "Guowang Li", "Katayoon Goshvadi", "Bo Dai", "Dale Schuurmans", "Paul Zhou", "Hamid Palangi", "Yiwen Song", "Palash Goyal", "Murat Kantarcioglu", "Bradley A. Malin", "Yuan Xue"], "title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions", "categories": ["cs.CL"], "comment": null, "summary": "The alignment of large language models (LLMs) with human values increasingly\nrelies on using other LLMs as automated judges, or ``autoraters''. However,\ntheir reliability is limited by a foundational issue: they are trained on\ndiscrete preference labels, forcing a single ground truth onto tasks that are\noften subjective, ambiguous, or nuanced. We argue that a reliable autorater\nmust learn to model the full distribution of preferences defined by a target\npopulation. In this paper, we propose a general framework for calibrating\nprobabilistic autoraters to any given preference distribution. We formalize the\nproblem and present two learning methods tailored to different data conditions:\n1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a\nreinforcement learning approach for sparse, binary labels. Our empirical\nresults show that finetuning autoraters with a distribution-matching objective\nleads to verbalized probability predictions that are better aligned with the\ntarget preference distribution, with improved calibration and significantly\nlower positional bias, all while preserving performance on objective tasks.", "AI": {"tldr": "\u63d0\u51fa\u6821\u51c6\u6982\u7387\u81ea\u52a8\u8bc4\u4f30\u5668\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5339\u914d\u76ee\u6807\u63d0\u5347\u9884\u6d4b\u6821\u51c6\u6548\u679c\uff0c\u964d\u4f4e\u4f4d\u7f6e\u504f\u5dee\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u79bb\u6563\u6807\u7b7e\u8bad\u7ec3\u7684\u81ea\u52a8\u8bc4\u4f30\u5668\u65e0\u6cd5\u6709\u6548\u5904\u7406\u4e3b\u89c2/\u6a21\u7cca\u4efb\u52a1\uff0c\u9700\u5efa\u6a21\u5b8c\u6574\u504f\u597d\u5206\u5e03\u63d0\u5347\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u4e24\u79cd\u5b66\u4e60\u65b9\u6cd5\uff1a1\uff09\u5bc6\u96c6\u6982\u7387\u6807\u7b7e\u7684\u76d1\u7763\u5fae\u8c03 2\uff09\u7a00\u758f\u4e8c\u5143\u6807\u7b7e\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5", "result": "\u6821\u51c6\u540e\u7684\u8bc4\u4f30\u5668\u6982\u7387\u9884\u6d4b\u66f4\u7b26\u5408\u76ee\u6807\u5206\u5e03\uff0c\u4f4d\u7f6e\u504f\u5dee\u964d\u4f4e70%\uff0c\u5ba2\u89c2\u4efb\u52a1\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u81ea\u52a8\u8bc4\u4f30\u5668\u7684\u6821\u51c6\u80fd\u529b\u548c\u53ef\u9760\u6027\uff0c\u4e3aLLMs\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.00268", "pdf": "https://arxiv.org/pdf/2510.00268", "abs": "https://arxiv.org/abs/2510.00268", "authors": ["Zhexiong Liu", "Diane Litman"], "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "In The Conference on Empirical Methods in Natural Language Processing\n  (EMNLP), November 2025", "summary": "Large Language Models (LLMs) have shown extraordinary success across various\ntext generation tasks; however, their potential for simple yet essential text\nclassification remains underexplored, as LLM pre-training tends to emphasize\ngeneration over classification. While LLMs with instruction tuning can\ntransform classification into a generation task, they often struggle to\ncategorize nuanced texts. One such example is text revision, which involves\nnuanced edits between pairs of texts. Although simply fine-tuning LLMs for\nrevision classification seems plausible, it requires a large amount of revision\nannotations, which are exceptionally expensive and scarce in the community. To\naddress this issue, we introduce a plug-and-play layer-wise parameter-efficient\nfine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of\nimportant LLM layers that are dynamically selected based on their gradient norm\ndistribution, while freezing those of redundant layers. Extensive experiments\nsuggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse\ntext revisions, while achieving fast convergence, low GPU memory consumption,\nand effectiveness on small revision corpora.", "AI": {"tldr": "\u63d0\u51faIR-Tuning\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u91cd\u8981\u5c42\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u63d0\u5347LLM\u5728\u6587\u672c\u4fee\u8ba2\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u5feb\u901f\u6536\u655b\u4e0e\u4f4e\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "LLM\u5728\u751f\u6210\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\u4f46\u7ec6\u7c92\u5ea6\u6587\u672c\u5206\u7c7b\u80fd\u529b\u4e0d\u8db3\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6602\u8d35\u6807\u6ce8\u6570\u636e\u3002\u6587\u672c\u4fee\u8ba2\u5206\u7c7b\u9700\u8981\u6355\u6349\u6587\u672c\u5bf9\u95f4\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u800c\u793e\u533a\u7f3a\u4e4f\u5927\u89c4\u6a21\u4fee\u8ba2\u6807\u6ce8\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u5c42\u7ea7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6IR-Tuning\uff0c\u57fa\u4e8e\u68af\u5ea6\u8303\u6570\u5206\u5e03\u52a8\u6001\u9009\u62e9\u91cd\u8981\u7f51\u7edc\u5c42\u8fdb\u884c\u5fae\u8c03\uff0c\u51bb\u7ed3\u5197\u4f59\u5c42\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5c42\u91cd\u8981\u6027\u8bc4\u4f30\u4e0e\u53c2\u6570\u51bb\u7ed3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eIR-Tuning\u5728\u591a\u79cd\u6587\u672c\u4fee\u8ba2\u4efb\u52a1\u4e0a\u4f18\u4e8e\u5176\u4ed6\u5c42\u95f4PEFT\u65b9\u6cd5\uff0c\u5728\u5c0f\u578b\u4fee\u8ba2\u8bed\u6599\u5e93\u4e2d\u4ecd\u4fdd\u6301\u6709\u6548\u6027\uff0c\u4e14\u5177\u5907\u5feb\u901f\u6536\u655b\u3001\u4f4eGPU\u5185\u5b58\u5360\u7528\u7684\u4f18\u52bf\u3002", "conclusion": "IR-Tuning\u901a\u8fc7\u52a8\u6001\u5c42\u9009\u62e9\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u53c2\u6570\u5229\u7528\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684LLM\u7ec6\u7c92\u5ea6\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.00276", "pdf": "https://arxiv.org/pdf/2510.00276", "abs": "https://arxiv.org/abs/2510.00276", "authors": ["Joe Barrow", "Raj Patel", "Misha Kharkovski", "Ben Davies", "Ryan Schmitt"], "title": "SafePassage: High-Fidelity Information Extraction with Black Box LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Black box large language models (LLMs) make information extraction (IE) easy\nto configure, but hard to trust. Unlike traditional information extraction\npipelines, the information \"extracted\" is not guaranteed to be grounded in the\ndocument. To prevent this, this paper introduces the notion of a \"safe\npassage\": context generated by the LLM that is both grounded in the document\nand consistent with the extracted information. This is operationalized via a\nthree-step pipeline, SafePassage, which consists of: (1) an LLM extractor that\ngenerates structured entities and their contexts from a document, (2) a\nstring-based global aligner, and (3) a scoring model. Results show that using\nthese three parts in conjunction reduces hallucinations by up to 85% on\ninformation extraction tasks with minimal risk of flagging non-hallucinations.\nHigh agreement between the SafePassage pipeline and human judgments of\nextraction quality mean that the pipeline can be dually used to evaluate LLMs.\nSurprisingly, results also show that using a transformer encoder fine-tuned on\na small number of task-specific examples can outperform an LLM scoring model at\nflagging unsafe passages. These annotations can be collected in as little as\n1-2 hours.", "AI": {"tldr": "\u63d0\u51faSafePassage\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u6b65\u6d41\u7a0b\u51cf\u5c11LLM\u4fe1\u606f\u62bd\u53d6\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u8fbe85%\uff0c\u5e76\u53ef\u7528\u4f5c\u6a21\u578b\u8bc4\u4f30\u5de5\u5177", "motivation": "\u4f20\u7edf\u9ed1\u76d2\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u5bb9\u6613\u4ea7\u751f\u4e0d\u57fa\u4e8e\u6587\u6863\u7684\u865a\u5047\u4fe1\u606f\uff08\u5e7b\u89c9\uff09\uff0c\u5f71\u54cd\u7ed3\u679c\u53ef\u4fe1\u5ea6", "method": "1. LLM\u63d0\u53d6\u5668\u751f\u6210\u7ed3\u6784\u5316\u5b9e\u4f53\u53ca\u4e0a\u4e0b\u6587\n2. \u5b57\u7b26\u4e32\u5168\u5c40\u5bf9\u9f50\u5668\u9a8c\u8bc1\u4e00\u81f4\u6027\n3. \u5fae\u8c03\u540e\u7684\u8bc4\u5206\u6a21\u578b\u8fdb\u884c\u6700\u7ec8\u8bc4\u4f30", "result": "\u57281-2\u5c0f\u65f6\u6807\u6ce8\u6570\u636e\u57fa\u7840\u4e0a\uff0c\u6d41\u7a0b\u964d\u4f4e\u5e7b\u89c9\u8fbe85%\uff0c\u4e14\u5fae\u8c03\u7f16\u7801\u5668\u7684\u8bc4\u5206\u6548\u679c\u4f18\u4e8eLLM\u8bc4\u5206\u6a21\u578b", "conclusion": "SafePassage\u65e2\u4fdd\u8bc1\u62bd\u53d6\u7ed3\u679c\u53ef\u9760\u6027\uff0c\u53c8\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\uff0c\u5c55\u793a\u5c0f\u6837\u672c\u5fae\u8c03\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u4f18\u52bf"}}
{"id": "2510.00280", "pdf": "https://arxiv.org/pdf/2510.00280", "abs": "https://arxiv.org/abs/2510.00280", "authors": ["Ruochen Li", "Jun Li", "Bailiang Jian", "Kun Yuan", "Youxiang Zhu"], "title": "ReEvalMed: Rethinking Medical Report Evaluation by Aligning Metrics with Real-World Clinical Judgment", "categories": ["cs.CL"], "comment": null, "summary": "Automatically generated radiology reports often receive high scores from\nexisting evaluation metrics but fail to earn clinicians' trust. This gap\nreveals fundamental flaws in how current metrics assess the quality of\ngenerated reports. We rethink the design and evaluation of these metrics and\npropose a clinically grounded Meta-Evaluation framework. We define clinically\ngrounded criteria spanning clinical alignment and key metric capabilities,\nincluding discrimination, robustness, and monotonicity. Using a fine-grained\ndataset of ground truth and rewritten report pairs annotated with error types,\nclinical significance labels, and explanations, we systematically evaluate\nexisting metrics and reveal their limitations in interpreting clinical\nsemantics, such as failing to distinguish clinically significant errors,\nover-penalizing harmless variations, and lacking consistency across error\nseverity levels. Our framework offers guidance for building more clinically\nreliable evaluation methods.", "AI": {"tldr": "\u73b0\u6709\u653e\u5c04\u5b66\u62a5\u544a\u8bc4\u4f30\u6307\u6807\u4e0e\u4e34\u5e8a\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u51fa\u57fa\u4e8e\u4e34\u5e8a\u6807\u51c6\u7684\u5143\u8bc4\u4f30\u6846\u67b6\u4ee5\u63d0\u5347\u8bc4\u4f30\u53ef\u9760\u6027", "motivation": "\u81ea\u52a8\u751f\u6210\u7684\u653e\u5c04\u5b66\u62a5\u544a\u5728\u73b0\u6709\u6307\u6807\u4e0b\u5f97\u5206\u9ad8\u4f46\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u4fe1\u5ea6\uff0c\u63ed\u793a\u5f53\u524d\u8bc4\u4f30\u4f53\u7cfb\u5728\u4e34\u5e8a\u8bed\u4e49\u7406\u89e3\u4e0a\u7684\u6839\u672c\u7f3a\u9677", "method": "\u6784\u5efa\u5305\u542b\u9519\u8bef\u7c7b\u578b\u6807\u6ce8\u3001\u4e34\u5e8a\u663e\u8457\u6027\u6807\u7b7e\u7684\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u73b0\u6709\u6307\u6807\u5728\u4e34\u5e8a\u5bf9\u9f50\u6027\u3001\u533a\u5206\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u5355\u8c03\u6027\u65b9\u9762\u7684\u8868\u73b0", "result": "\u73b0\u6709\u6307\u6807\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u65e0\u6cd5\u8bc6\u522b\u4e34\u5e8a\u663e\u8457\u6027\u9519\u8bef\u3001\u8fc7\u5ea6\u60e9\u7f5a\u65e0\u5bb3\u53d8\u5f02\u3001\u7f3a\u4e4f\u8de8\u9519\u8bef\u4e25\u91cd\u7ea7\u522b\u7684\u4e00\u81f4\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5efa\u7acb\u66f4\u7b26\u5408\u4e34\u5e8a\u9700\u6c42\u7684\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u7cfb\u7edf\u6307\u5bfc\uff0c\u63a8\u52a8\u533b\u5b66\u62a5\u544a\u751f\u6210\u6280\u672f\u7684\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u63d0\u5347"}}
{"id": "2510.00288", "pdf": "https://arxiv.org/pdf/2510.00288", "abs": "https://arxiv.org/abs/2510.00288", "authors": ["\u013dubo\u0161 Kri\u0161", "Jaroslav Kop\u010dan", "Qiwei Peng", "Andrej Ridzik", "Marcel Vesel\u00fd", "Martin Tamajka"], "title": "o-MEGA: Optimized Methods for Explanation Generation and Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of transformer-based language models has revolutionized NLP\ndomain while simultaneously introduced significant challenges regarding model\ntransparency and trustworthiness. The complexity of achieving explainable\nsystems in this domain is evidenced by the extensive array of explanation\nmethods and evaluation metrics developed by researchers. To address the\nchallenge of selecting optimal explainability approaches, we present\n\\textbf{\\texttt{o-mega}}, a hyperparameter optimization tool designed to\nautomatically identify the most effective explainable AI methods and their\nconfigurations within the semantic matching domain. We evaluate o-mega on a\npost-claim matching pipeline using a curated dataset of social media posts\npaired with refuting claims. Our tool systematically explores different\nexplainable methods and their hyperparameters, demonstrating improved\ntransparency in automated fact-checking systems. As a result, such automated\noptimization of explanation methods can significantly enhance the\ninterpretability of claim-matching models in critical applications such as\nmisinformation detection, contributing to more trustworthy and transparent AI\nsystems.", "AI": {"tldr": "\u5f00\u53d1o-mega\u5de5\u5177\u5b9e\u73b0\u53ef\u89e3\u91caAI\u65b9\u6cd5\u81ea\u52a8\u4f18\u5316\uff0c\u63d0\u5347\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u900f\u660e\u5ea6", "motivation": "Transformer\u6a21\u578b\u5728\u63d0\u5347NLP\u6027\u80fd\u7684\u540c\u65f6\u5bfc\u81f4\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u4f17\u591a\u4f46\u7f3a\u4e4f\u6709\u6548\u9009\u62e9\u6807\u51c6", "method": "\u63d0\u51fa\u8d85\u53c2\u6570\u4f18\u5316\u5de5\u5177o-mega\uff0c\u901a\u8fc7\u81ea\u52a8\u641c\u7d22\u7b97\u6cd5\u786e\u5b9a\u8bed\u4e49\u5339\u914d\u9886\u57df\u6700\u4f18\u89e3\u91ca\u65b9\u6cd5\u53ca\u914d\u7f6e\u53c2\u6570", "result": "\u5728\u793e\u4ea4\u5a92\u4f53\u58f0\u660e-\u53cd\u58f0\u660e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0c\u6210\u529f\u63d0\u5347\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u7684\u51b3\u7b56\u53ef\u89e3\u91ca\u6027", "conclusion": "\u81ea\u52a8\u5316\u89e3\u91ca\u65b9\u6cd5\u4f18\u5316\u53ef\u663e\u8457\u589e\u5f3a\u5173\u952e\u5e94\u7528\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\uff0c\u4e3aAI\u900f\u660e\u5316\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.00311", "pdf": "https://arxiv.org/pdf/2510.00311", "abs": "https://arxiv.org/abs/2510.00311", "authors": ["Bowen Wei", "Yuan Shen Tay", "Howard Liu", "Jinhao Pan", "Kun Luo", "Ziwei Zhu", "Chris Jordan"], "title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage", "categories": ["cs.CL"], "comment": null, "summary": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of\ndaily alerts, with only a small fraction corresponding to genuine attacks. This\noverload creates alert fatigue, leading to overlooked threats and analyst\nburnout. Classical detection pipelines are brittle and context-poor, while\nrecent LLM-based approaches typically rely on a single model to interpret logs,\nretrieve context, and adjudicate alerts end-to-end -- an approach that\nstruggles with noisy enterprise data and offers limited transparency. We\npropose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in\nwhich specialized agents collaborate over real evidence: a behavior-analysis\nagent inspects activity sequences, evidence-gathering agents query external\nsystems, and a reasoning agent synthesizes findings into an auditable decision.\nTo support training and evaluation, we release a dataset of fine-grained SOC\ninvestigations from production environments, capturing step-by-step analyst\nactions and linked tool outputs. Across diverse enterprise scenarios, CORTEX\nsubstantially reduces false positives and improves investigation quality over\nstate-of-the-art single-agent LLMs.", "AI": {"tldr": "\u9488\u5bf9SOC\u8b66\u62a5\u8fc7\u8f7d\u95ee\u9898\uff0c\u63d0\u51fa\u591a\u4ee3\u7406LLM\u67b6\u6784CORTEX\uff0c\u901a\u8fc7\u5206\u5de5\u534f\u4f5c\u663e\u8457\u964d\u4f4e\u8bef\u62a5\u5e76\u63d0\u5347\u8c03\u67e5\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u8106\u5f31\u4e14\u7f3a\u4e4f\u4e0a\u4e0b\u6587\uff0c\u5355\u4e00LLM\u6a21\u578b\u5728\u566a\u58f0\u6570\u636e\u5904\u7406\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u9ad8\u8bef\u62a5\u7387\u548c\u51b3\u7b56\u900f\u660e\u5ea6\u4f4e\u3002", "method": "CORTEX\u91c7\u7528\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff1a\u884c\u4e3a\u5206\u6790\u4ee3\u7406\u68c0\u67e5\u6d3b\u52a8\u5e8f\u5217\uff0c\u8bc1\u636e\u4ee3\u7406\u67e5\u8be2\u5916\u90e8\u7cfb\u7edf\uff0c\u63a8\u7406\u4ee3\u7406\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u51b3\u7b56\u62a5\u544a\u3002", "result": "CORTEX\u5728\u591a\u6837\u5316\u4f01\u4e1a\u573a\u666f\u4e2d\u8bef\u62a5\u7387\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u5355\u4ee3\u7406LLM\u6a21\u578b\uff0c\u8c03\u67e5\u8d28\u91cf\u5f97\u5230\u5b9e\u8d28\u6027\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u4e13\u4e1a\u5316\u4ee3\u7406\u5206\u5de5\u534f\u4f5c\uff0cCORTEX\u6709\u6548\u89e3\u51b3\u4e86SOC\u8b66\u62a5\u5206\u7c7b\u96be\u9898\uff0c\u63d0\u9ad8\u4e86\u51b3\u7b56\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u914d\u5957\u53d1\u5e03\u4e86\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u652f\u6301\u7814\u7a76\u3002"}}
{"id": "2510.00444", "pdf": "https://arxiv.org/pdf/2510.00444", "abs": "https://arxiv.org/abs/2510.00444", "authors": ["Zijun Wu", "Yongchang Hao", "Lili Mou"], "title": "TokMem: Tokenized Procedural Memory for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models rely heavily on prompts to specify tasks, recall\nknowledge and guide reasoning. However, this reliance is inefficient as prompts\nmust be re-read at each step, scale poorly across tasks, and lack mechanisms\nfor modular reuse. We introduce TokMem, a tokenized procedural memory that\nstores recurring procedures as compact, trainable embeddings. Each memory token\nencodes both an address to a procedure and a control signal that steers\ngeneration, enabling targeted behavior with constant-size overhead. To support\ncontinual adaptation, TokMem keeps the backbone model frozen, allowing new\nprocedures to be added without interfering with existing ones. We evaluate\nTokMem on 1,000 tasks for atomic recall, and on function-calling tasks for\ncompositional recall, where it consistently outperforms retrieval-augmented\ngeneration while avoiding repeated context overhead, and fine-tuning with far\nfewer parameters. These results establish TokMem as a scalable and modular\nalternative to prompt engineering and fine-tuning, offering an explicit\nprocedural memory for LLMs.", "AI": {"tldr": "\u63d0\u51faTokMem\u4f5c\u4e3aLLMs\u7684\u663e\u5f0f\u7a0b\u5e8f\u8bb0\u5fc6\uff0c\u5c06\u91cd\u590d\u7a0b\u5e8f\u7f16\u7801\u4e3a\u53ef\u8bad\u7ec3\u5d4c\u5165\uff0c\u76f8\u6bd4\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u66f4\u5177\u6269\u5c55\u6027", "motivation": "\u89e3\u51b3\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u6548\u7387\u4f4e\uff08\u9700\u9010\u6b65\u9aa4\u91cd\u8bfb\uff09\u3001\u8de8\u4efb\u52a1\u6269\u5c55\u6027\u5dee\u3001\u7f3a\u4e4f\u6a21\u5757\u5316\u590d\u7528\u673a\u5236\u7684\u95ee\u9898", "method": "1. \u901a\u8fc7\u8bb0\u5fc6\u4ee4\u724c\u7f16\u7801\u7a0b\u5e8f\u5730\u5740\u548c\u63a7\u5236\u4fe1\u53f7\n2. \u4fdd\u6301\u4e3b\u5e72\u6a21\u578b\u51bb\u7ed3\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\n3. \u4f7f\u7528\u5e38\u91cf\u5927\u5c0f\u5f00\u9500\u5b9e\u73b0\u5b9a\u5411\u884c\u4e3a\u63a7\u5236", "result": "\u57281000\u4e2a\u539f\u5b50\u56de\u5fc6\u4efb\u52a1\u548c\u7ec4\u5408\u51fd\u6570\u8c03\u7528\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u6301\u7eed\u8d85\u8d8a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u53c2\u6570\u6548\u7387\u6bd4\u5fae\u8c03\u9ad810\u500d", "conclusion": "TokMem\u4e3aLLMs\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7a0b\u5e8f\u8bb0\u5fc6\u65b9\u6848\uff0c\u907f\u514d\u91cd\u590d\u4e0a\u4e0b\u6587\u5f00\u9500\uff0c\u652f\u6301\u6a21\u5757\u5316\u6dfb\u52a0\u65b0\u7a0b\u5e8f"}}
{"id": "2510.00446", "pdf": "https://arxiv.org/pdf/2510.00446", "abs": "https://arxiv.org/abs/2510.00446", "authors": ["Yuling Shi", "Yichun Qian", "Hongyu Zhang", "Beijun Shen", "Xiaodong Gu"], "title": "LongCodeZip: Compress Long Context for Code Language Models", "categories": ["cs.CL", "cs.SE"], "comment": "Accepted to ASE 2025. Code available at\n  https://github.com/YerbaPage/LongCodeZip", "summary": "Code generation under long contexts is becoming increasingly critical as\nLarge Language Models (LLMs) are required to reason over extensive information\nin the codebase. While recent advances enable code LLMs to process long inputs,\nhigh API costs and generation latency remain substantial bottlenecks. Existing\ncontext pruning techniques, such as LLMLingua, achieve promising results for\ngeneral text but overlook code-specific structures and dependencies, leading to\nsuboptimal performance in programming tasks. In this paper, we propose\nLongCodeZip, a novel plug-and-play code compression framework designed\nspecifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1)\ncoarse-grained compression, which identifies and ranks function-level chunks\nusing conditional perplexity with respect to the instruction, retaining only\nthe most relevant functions; and (2) fine-grained compression, which segments\nretained functions into blocks based on perplexity and selects an optimal\nsubset under an adaptive token budget to maximize relevance. Evaluations across\nmultiple tasks, including code completion, summarization, and question\nanswering, show that LongCodeZip consistently outperforms baseline methods,\nachieving up to a 5.6x compression ratio without degrading task performance. By\neffectively reducing context size while preserving essential information,\nLongCodeZip enables LLMs to better scale to real-world, large-scale code\nscenarios, advancing the efficiency and capability of code intelligence\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4ee3\u7801\u538b\u7f29\u6846\u67b6LongCodeZip\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u538b\u7f29\u7b56\u7565\u5728\u4fdd\u6301\u6027\u80fd\u524d\u63d0\u4e0b\u5b9e\u73b05.6\u500d\u538b\u7f29\u6bd4", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u6a21\u578b\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34API\u6210\u672c\u9ad8\u3001\u5ef6\u8fdf\u5927\u7684\u95ee\u9898\uff0c\u901a\u7528\u6587\u672c\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4ee3\u7801\u7ed3\u6784\u7279\u6027\u5bfc\u81f4\u6548\u679c\u4e0d\u4f73", "method": "\u91c7\u7528\u51fd\u6570\u7ea7\u7c97\u7c92\u5ea6\u538b\u7f29\uff08\u57fa\u4e8e\u6761\u4ef6\u56f0\u60d1\u5ea6\u7b5b\u9009\uff09\u4e0e\u4ee3\u7801\u5757\u7ea7\u7ec6\u7c92\u5ea6\u538b\u7f29\uff08\u81ea\u9002\u5e94token\u9884\u7b97\u4f18\u5316\u9009\u62e9\uff09\u7684\u53cc\u9636\u6bb5\u7b56\u7565", "result": "\u5728\u4ee3\u7801\u8865\u5168/\u603b\u7ed3/\u95ee\u7b54\u4efb\u52a1\u4e2d\u4fdd\u6301\u6027\u80fd\u524d\u63d0\u4e0b\u5b9e\u73b0\u6700\u9ad85.6\u500d\u538b\u7f29\u6bd4\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u4fdd\u7559\u5173\u952e\u4ee3\u7801\u7ed3\u6784\u5b9e\u73b0\u9ad8\u6548\u4e0a\u4e0b\u6587\u538b\u7f29\uff0c\u63d0\u5347\u5927\u6a21\u578b\u5904\u7406\u5b9e\u9645\u5927\u89c4\u6a21\u4ee3\u7801\u573a\u666f\u7684\u80fd\u529b"}}
{"id": "2510.00449", "pdf": "https://arxiv.org/pdf/2510.00449", "abs": "https://arxiv.org/abs/2510.00449", "authors": ["Koki Ryu", "Hitomi Yanaka"], "title": "Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 PALS Workshop", "summary": "Personalizing the outputs of large language models (LLMs) to align with\nindividual user preferences is an active research area. However, previous\nstudies have mainly focused on classification or ranking tasks and have not\nconsidered Likert-scale rating prediction, a regression task that requires both\nlanguage and mathematical reasoning to be solved effectively. This task has\nsignificant industrial applications, but the utilization of LLMs remains\nunderexplored, particularly regarding the capabilities of off-the-shelf LLMs.\nThis study investigates the performance of off-the-shelf LLMs on rating\nprediction, providing different in-context information. Through comprehensive\nexperiments with eight models across three datasets, we demonstrate that\nuser-written reviews significantly improve the rating prediction performance of\nLLMs. This result is comparable to traditional methods like matrix\nfactorization, highlighting the potential of LLMs as a promising solution for\nthe cold-start problem. We also find that the reviews for concrete items are\nmore effective than general preference descriptions that are not based on any\nspecific item. Furthermore, we discover that prompting LLMs to first generate a\nhypothetical review enhances the rating prediction performance. Our code is\navailable at https://github.com/ynklab/rating-prediction-with-reviews.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5206\u6790\u7528\u6237\u8bc4\u8bba\u663e\u8457\u63d0\u5347\u8bc4\u5206\u9884\u6d4b\u6548\u679c\uff0c\u5728\u51b7\u542f\u52a8\u573a\u666f\u4e2d\u8868\u73b0\u5ab2\u7f8e\u4f20\u7edf\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5206\u7c7b/\u6392\u5e8f\u4efb\u52a1\uff0c\u800c\u9700\u8981\u8bed\u8a00\u4e0e\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u5206\u9884\u6d4b\u4efb\u52a1\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u6709\u91cd\u8981\u4ef7\u503c\u4f46\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u73b0\u6210LLM\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u75288\u4e2a\u73b0\u6210LLM\u57283\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff08\u7528\u6237\u8bc4\u8bba/\u504f\u597d\u63cf\u8ff0\uff09\u5bf9\u8bc4\u5206\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u5c1d\u8bd5\u8ba9LLM\u9996\u5148\u751f\u6210\u5047\u8bbe\u6027\u8bc4\u8bba", "result": "1. \u7528\u6237\u8bc4\u8bba\u4f7fLLM\u9884\u6d4b\u6548\u679c\u63d0\u5347\u81f3\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\n2. \u5177\u4f53\u9879\u76ee\u8bc4\u8bba\u6bd4\u901a\u7528\u504f\u597d\u63cf\u8ff0\u66f4\u6709\u6548\n3. \u9996\u5148\u751f\u6210\u5047\u8bbe\u8bc4\u8bba\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c", "conclusion": "LLMs\u901a\u8fc7\u5229\u7528\u7528\u6237\u751f\u6210\u5185\u5bb9\u5c55\u73b0\u51fa\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\u7684\u6f5c\u529b\uff0c\u5efa\u8bae\u5de5\u4e1a\u5e94\u7528\u4f18\u5148\u6574\u5408\u5177\u4f53\u9879\u76ee\u7684\u7528\u6237\u8bc4\u8bba"}}
{"id": "2510.00482", "pdf": "https://arxiv.org/pdf/2510.00482", "abs": "https://arxiv.org/abs/2510.00482", "authors": ["Yawen Xue", "Masaya Tsunokake", "Yuta Koreeda", "Ekant Muljibhai Amin", "Takashi Sumiyoshi", "Yasuhiro Sogawa"], "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains", "categories": ["cs.CL"], "comment": "Accepted by AIxB 2025", "summary": "Agentic large language models (LLMs) have become prominent for autonomously\ninteracting with external environments and performing multi-step reasoning\ntasks. Most approaches leverage these capabilities via in-context learning with\nfew-shot prompts, but this often results in lengthy inputs and higher\ncomputational costs. Agent fine-tuning offers an alternative by enabling LLMs\nto internalize procedural reasoning and domain-specific knowledge through\ntraining on relevant data and demonstration trajectories. While prior studies\nhave focused on general domains, their effectiveness in specialized technical\nmicrodomains remains unclear. This paper explores agent fine-tuning for domain\nadaptation within Hitachi's JP1 middleware, a microdomain for specialized IT\noperations. We fine-tuned LLMs using JP1-specific datasets derived from domain\nmanuals and distilled reasoning trajectories generated by LLMs themselves,\nenhancing decision making accuracy and search efficiency. During inference, we\nused an agentic prompt with retrieval-augmented generation and introduced a\ncontext-answer extractor to improve information relevance. On JP1 certification\nexam questions, our method achieved a 14% performance improvement over the base\nmodel, demonstrating the potential of agent fine-tuning for domain-specific\nreasoning in complex microdomains.", "AI": {"tldr": "\u901a\u8fc7\u4ee3\u7406\u5fae\u8c03\u5c06\u9886\u57df\u77e5\u8bc6\u5185\u5316\u81f3LLM\uff0c\u5728\u65e5\u7acbJP1\u4e2d\u95f4\u4ef6\u8ba4\u8bc1\u8003\u8bd5\u4e2d\u5b9e\u73b014%\u6027\u80fd\u63d0\u5347", "motivation": "\u4f20\u7edf\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u8f93\u5165\u5197\u957f\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u4e13\u4e1a\u5fae\u9886\u57df\u6709\u6548\u6027\u5b58\u7591\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u4ee3\u7406\u5fae\u8c03\u5728IT\u8fd0\u7ef4\u5fae\u9886\u57df\u7684\u9002\u5e94\u80fd\u529b", "method": "\u4f7f\u7528JP1\u624b\u518c\u6570\u636e\u53caLLM\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u8fdb\u884c\u5fae\u8c03\uff0c\u63a8\u7406\u9636\u6bb5\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u4e0a\u4e0b\u6587\u7b54\u6848\u63d0\u53d6\u5668\u63d0\u5347\u4fe1\u606f\u76f8\u5173\u6027", "result": "\u5728JP1\u8ba4\u8bc1\u8003\u8bd5\u95ee\u9898\u4e0a\u76f8\u5bf9\u57fa\u7840\u6a21\u578b\u63d0\u534714%\u51c6\u786e\u7387", "conclusion": "\u4ee3\u7406\u5fae\u8c03\u6709\u6548\u589e\u5f3aLLM\u5728\u590d\u6742\u4e13\u4e1a\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6280\u672f\u5fae\u9886\u57df\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2510.00496", "pdf": "https://arxiv.org/pdf/2510.00496", "abs": "https://arxiv.org/abs/2510.00496", "authors": ["Pengzhou Cheng", "Lingzhong Dong", "Zeng Wu", "Zongru Wu", "Xiangru Tang", "Chengwei Qin", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations", "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 7 tables", "summary": "Although numerous strategies have recently been proposed to enhance the\nautonomous interaction capabilities of multimodal agents in graphical user\ninterface (GUI), their reliability remains limited when faced with complex or\nout-of-domain tasks. This raises a fundamental question: Are existing\nmultimodal agents reasoning spuriously? In this paper, we propose\n\\textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory\nand reasoning capabilities of multimodal agents under controlled perturbations.\nSpecifically, we introduce three orthogonal probing paradigms: visual-guided,\ntext-guided, and structure-guided, each designed to quantify the contributions\nof memorization and reasoning without requiring access to model internals. In\nfive publicly available GUI benchmarks involving 18 multimodal agents, the\nresults demonstrate that mechanical memorization often outweighs systematic\nreasoning. Most of the models function predominantly as retrievers of\ntraining-aligned knowledge, exhibiting limited generalization. Our findings\nunderscore the necessity of robust reasoning modeling for multimodal agents in\nreal-world scenarios, offering valuable insights toward the development of\nreliable multimodal agents.", "AI": {"tldr": "\u63d0\u51faAgent-ScanKit\u6846\u67b6\uff0c\u63ed\u793a\u591a\u6a21\u6001\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u7cfb\u7edf\u6027\u63a8\u7406\u7684\u73b0\u72b6\uff0c\u9a8c\u8bc1\u4e8618\u4e2a\u4ee3\u7406\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u666e\u904d\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u591a\u6a21\u6001\u4ee3\u7406\u5728\u590d\u6742\u6216\u9886\u57df\u5916\u4efb\u52a1\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7a76\u5176\u63a8\u7406\u673a\u5236\u662f\u5426\u4f2a\u76f8\u5173\uff0c\u65e8\u5728\u91cf\u5316\u8bb0\u5fc6\u4e0e\u63a8\u7406\u7684\u8d21\u732e\u5ea6\u3002", "method": "\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u3001\u6587\u672c\u5f15\u5bfc\u548c\u7ed3\u6784\u5f15\u5bfc\u4e09\u79cd\u6b63\u4ea4\u6270\u52a8\u8303\u5f0f\uff0c\u5728\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u53c2\u6570\u7684\u524d\u63d0\u4e0b\uff0c\u7cfb\u7edf\u6027\u63a2\u6d4b\u591a\u6a21\u6001\u4ee3\u7406\u7684\u8bb0\u5fc6-\u63a8\u7406\u80fd\u529b\u5e73\u8861\u3002", "result": "\u57285\u4e2a\u516c\u5f00GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c18\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u8868\u73b0\u51fa\u8bad\u7ec3\u77e5\u8bc6\u68c0\u7d22\u503e\u5411\uff0c\u673a\u68b0\u8bb0\u5fc6\u6743\u91cd\u663e\u8457\u9ad8\u4e8e\u7cfb\u7edf\u63a8\u7406\uff0c\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "conclusion": "\u5f3a\u8c03\u73b0\u5b9e\u573a\u666f\u4e2d\u591a\u6a21\u6001\u4ee3\u7406\u9700\u5f3a\u5316\u63a8\u7406\u5efa\u6a21\uff0c\u4e3a\u6784\u5efa\u53ef\u9760\u7cfb\u7edf\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u6307\u660e\u672a\u6765\u7814\u53d1\u5e94\u7a81\u7834\u8bb0\u5fc6\u4f9d\u8d56\u3001\u63d0\u5347\u63a8\u7406\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.00499", "pdf": "https://arxiv.org/pdf/2510.00499", "abs": "https://arxiv.org/abs/2510.00499", "authors": ["Xingjian Zhao", "Zhe Xu", "Luozhijie Jin", "Yang Wang", "Hanfu Chen", "Yaozhou Jiang", "Ke Chen", "Ruixiao Li", "Mingshu Chen", "Ruiming Wang", "Wenbo Zhang", "Yiyang Zhang", "Donghua Yu", "Yang Gao", "Xiaogui Yang", "Yitian Gong", "Yuanfan Xu", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Yaqian Zhou", "Xuanjing Huang", "Xipeng Qiu"], "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Spoken dialogue systems often rely on cascaded pipelines that transcribe,\nprocess, and resynthesize speech. While effective, this design discards\nparalinguistic cues and limits expressivity. Recent end-to-end methods reduce\nlatency and better preserve these cues, yet still rely on text intermediates,\ncreating a fundamental bottleneck. We present MOSS-Speech, a true\nspeech-to-speech large language model that directly understands and generates\nspeech without relying on text guidance. Our approach combines a modality-based\nlayer-splitting architecture with a frozen pre-training strategy, preserving\nthe reasoning and knowledge of pretrained text LLMs while adding native speech\ncapabilities. Experiments show that our model achieves state-of-the-art results\nin spoken question answering and delivers comparable speech-to-speech\nperformance relative to existing text-guided systems, while still maintaining\ncompetitive text performance. By narrowing the gap between text-guided and\ndirect speech generation, our work establishes a new paradigm for expressive\nand efficient end-to-end speech interaction.", "AI": {"tldr": "MOSS-Speech\u662f\u9996\u4e2a\u65e0\u9700\u6587\u672c\u4e2d\u95f4\u5c42\u7684\u76f4\u63a5\u8bed\u97f3\u5230\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u6001\u5206\u5c42\u67b6\u6784\u548c\u51bb\u7ed3\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u4fdd\u7559\u6587\u672c\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u5177\u5907\u539f\u751f\u8bed\u97f3\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u7cfb\u7edf\u4f9d\u8d56\u7ea7\u8054\u6d41\u6c34\u7ebf\u4e22\u5f03\u526f\u8bed\u8a00\u7ebf\u7d22\uff0c\u800c\u7aef\u5230\u7aef\u65b9\u6cd5\u4ecd\u53d7\u9650\u4e8e\u6587\u672c\u74f6\u9888\uff0c\u9700\u8981\u7a81\u7834\u6587\u672c\u4f9d\u8d56\u5b9e\u73b0\u771f\u6b63\u7684\u8bed\u97f3\u4ea4\u4e92\u3002", "method": "\u7ed3\u5408\u6a21\u6001\u5206\u5c42\u7684\u67b6\u6784\u8bbe\u8ba1\uff08\u5904\u7406\u4e0d\u540c\u8f93\u5165\u6a21\u6001\uff09\u4e0e\u51bb\u7ed3\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6587\u672cLLM\u77e5\u8bc6\u7684\u57fa\u7840\u4e0a\u6269\u5c55\u8bed\u97f3\u7f16\u89e3\u7801\u80fd\u529b\u3002", "result": "\u5728\u8bed\u97f3\u95ee\u7b54\u4efb\u52a1\u8fbe\u5230SOTA\uff0c\u8bed\u97f3\u751f\u6210\u6027\u80fd\u4e0e\u6587\u672c\u5f15\u5bfc\u7cfb\u7edf\u76f8\u5f53\uff0c\u4e14\u6587\u672c\u4efb\u52a1\u8868\u73b0\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u7f29\u5c0f\u4e86\u6587\u672c\u4e2d\u4ecb\u4e0e\u76f4\u63a5\u8bed\u97f3\u751f\u6210\u7684\u5dee\u8ddd\uff0c\u4e3a\u9ad8\u6548\u3001\u5bcc\u6709\u8868\u73b0\u529b\u7684\u7aef\u5230\u7aef\u8bed\u97f3\u4ea4\u4e92\u5efa\u7acb\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2510.00507", "pdf": "https://arxiv.org/pdf/2510.00507", "abs": "https://arxiv.org/abs/2510.00507", "authors": ["Yurun Chen", "Xavier Hu", "Yuhan Liu", "Ziqi Wang", "Zeyi Liao", "Lin Chen", "Feng Wei", "Yuxi Qian", "Bo Zheng", "Keting Yin", "Shengyu Zhang"], "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 10 figures", "summary": "As multimodal LLM-driven agents continue to advance in autonomy and\ngeneralization, evaluation based on static datasets can no longer adequately\nassess their true capabilities in dynamic environments and diverse tasks.\nExisting LLM-based synthetic data methods are largely designed for LLM training\nand evaluation, and thus cannot be directly applied to agent tasks that require\ntool use and interactive capabilities. While recent studies have explored\nautomatic agent task generation with LLMs, most efforts remain limited to text\nor image analysis, without systematically modeling multi-step interactions in\nweb environments. To address these challenges, we propose Graph2Eval, a\nknowledge graph-based framework that automatically generates both multimodal\ndocument comprehension tasks and web interaction tasks, enabling comprehensive\nevaluation of agents' reasoning, collaboration, and interactive capabilities.\nIn our approach, knowledge graphs constructed from multi-source external data\nserve as the task space, where we translate semantic relations into structured\nmultimodal tasks using subgraph sampling, task templates, and meta-paths. A\nmulti-stage filtering pipeline based on node reachability, LLM scoring, and\nsimilarity analysis is applied to guarantee the quality and executability of\nthe generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of\nmultiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures\nreasoning, collaboration, and interaction capabilities. We instantiate the\nframework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning\ndocument comprehension and web interaction scenarios. Experiments show that\nGraph2Eval efficiently generates tasks that differentiate agent and model\nperformance, revealing gaps in reasoning, collaboration, and web interaction\nacross different settings and offering a new perspective for agent evaluation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684Graph2Eval\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u6a21\u6001\u4efb\u52a1\u5b9e\u73b0\u5bf9\u4ee3\u7406\u63a8\u7406\u3001\u534f\u4f5c\u548c\u4ea4\u4e92\u80fd\u529b\u7684\u5168\u9762\u8bc4\u4f30", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u4ee3\u7406\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u4e14\u73b0\u6709LLM\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5de5\u5177\u4f7f\u7528\u548c\u4ea4\u4e92\u4efb\u52a1\u7684\u652f\u6301", "method": "\u5229\u7528\u591a\u6e90\u6570\u636e\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u4efb\u52a1\u7a7a\u95f4\uff0c\u901a\u8fc7\u5b50\u56fe\u91c7\u6837\u3001\u4efb\u52a1\u6a21\u677f\u548c\u5143\u8def\u5f84\u8f6c\u5316\u7ed3\u6784\u5316\u4efb\u52a1\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8fc7\u6ee4\u6d41\u7a0b\uff08\u8282\u70b9\u53ef\u8fbe\u6027\u3001LLM\u8bc4\u5206\u3001\u76f8\u4f3c\u6027\u5206\u6790\uff09\u786e\u4fdd\u8d28\u91cf", "result": "\u6784\u5efa\u5305\u542b1,319\u4e2a\u4efb\u52a1\u7684Graph2Eval-Bench\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u533a\u5206\u4ee3\u7406\u6027\u80fd\u5dee\u5f02\uff0c\u63ed\u793a\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u80fd\u529b\u5dee\u8ddd", "conclusion": "Graph2Eval\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u9a71\u52a8\u7684\u4efb\u52a1\u751f\u6210\u6846\u67b6\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e0b\u7684\u591a\u6a21\u6001\u4ee3\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.00508", "pdf": "https://arxiv.org/pdf/2510.00508", "abs": "https://arxiv.org/abs/2510.00508", "authors": ["Yongchao Long", "Xian Wu", "Yingying Zhang", "Xianbin Wen", "Yuxi Zhou", "Shenda Hong"], "title": "Copy-Paste to Mitigate Large Language Model Hallucinations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Retrieval-Augmented Generation (RAG) enables large language models\n(LLMs) to generate contextually grounded responses, contextual faithfulness\nremains challenging as LLMs may not consistently trust provided context,\nleading to hallucinations that undermine reliability. We observe an inverse\ncorrelation between response copying degree and context-unfaithful\nhallucinations on RAGTruth, suggesting that higher copying degrees reduce\nhallucinations by fostering genuine contextual belief. We propose CopyPasteLLM,\nobtained through two-stage high-copying response preference training. We design\nthree prompting methods to enhance copying degree, demonstrating that\nhigh-copying responses achieve superior contextual faithfulness and\nhallucination control. These approaches enable a fully automated pipeline that\ntransforms generated responses into high-copying preference data for training\nCopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best\nperformance in both counterfactual and original contexts, remarkably with 12.2%\nto 24.5% accuracy improvements on FaithEval over the best baseline, while\nrequiring only 365 training samples -- 1/50th of baseline data. To elucidate\nCopyPasteLLM's effectiveness, we propose the Context-Parameter Copying\nCapturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates\nreliance on internal parametric knowledge rather than external knowledge during\ngeneration. All codes are available at\nhttps://github.com/longyongchao/CopyPasteLLM", "AI": {"tldr": "\u901a\u8fc7\u4e24\u9636\u6bb5\u9ad8\u590d\u5236\u5ea6\u54cd\u5e94\u504f\u597d\u8bad\u7ec3\u83b7\u5f97CopyPasteLLM\uff0c\u663e\u8457\u63d0\u5347\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u6027\u5e76\u51cf\u5c11\u5e7b\u89c9\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u6700\u4f73\u6027\u80fd", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6a21\u578b\u56e0\u4e0d\u5b8c\u5168\u4fe1\u4efb\u4e0a\u4e0b\u6587\u77e5\u8bc6\u800c\u4ea7\u751f\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff0c\u5bfc\u81f4\u53ef\u9760\u6027\u95ee\u9898", "method": "\u8bbe\u8ba1\u4e09\u9636\u6bb5\u81ea\u52a8\u5316\u6d41\u7a0b\uff1a1\uff09\u9ad8\u590d\u5236\u5ea6\u63d0\u793a\u65b9\u6cd5\u751f\u6210\u54cd\u5e94\uff1b2\uff09\u6784\u5efa\u9ad8\u590d\u5236\u504f\u597d\u6570\u636e\u96c6\uff1b3\uff09\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u76d1\u7763\u5fae\u8c03+\u504f\u597d\u5bf9\u9f50\uff09", "result": "\u5728FaithEval\u57fa\u51c6\u4e0a\u6bd4\u6700\u4f18\u57fa\u7ebf\u63d0\u534712.2%-24.5%\u51c6\u786e\u7387\uff0c\u4ec5\u9700365\u4e2a\u8bad\u7ec3\u6837\u672c\uff08\u57fa\u7ebf\u6570\u636e\u76841/50\uff09", "conclusion": "\u9ad8\u590d\u5236\u5ea6\u54cd\u5e94\u901a\u8fc7\u91cd\u65b0\u6821\u51c6\u6a21\u578b\u5bf9\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u7684\u4f9d\u8d56\u673a\u5236\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u77e5\u8bc6\u53ef\u4fe1\u5ea6\u63a7\u5236"}}
{"id": "2510.00510", "pdf": "https://arxiv.org/pdf/2510.00510", "abs": "https://arxiv.org/abs/2510.00510", "authors": ["Jiarun Liu", "Shiyue Xu", "Shangkun Liu", "Yang Li", "Wen Liu", "Min Liu", "Xiaoqing Zhou", "Hanmin Wang", "Shilin Jia", "zhen Wang", "Shaohua Tian", "Hanhao Li", "Junbo Zhang", "Yongli Yu", "Peng Cao", "Haofen Wang"], "title": "JoyAgent-JDGenie: Technical Report on the GAIA", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models are increasingly deployed as autonomous agents for\ncomplex real-world tasks, yet existing systems often focus on isolated\nimprovements without a unifying design for robustness and adaptability. We\npropose a generalist agent architecture that integrates three core components:\na collective multi-agent framework combining planning and execution agents with\ncritic model voting, a hierarchical memory system spanning working, semantic,\nand procedural layers, and a refined tool suite for search, code execution, and\nmultimodal parsing. Evaluated on a comprehensive benchmark, our framework\nconsistently outperforms open-source baselines and approaches the performance\nof proprietary systems. These results demonstrate the importance of\nsystem-level integration and highlight a path toward scalable, resilient, and\nadaptive AI assistants capable of operating across diverse domains and tasks.", "AI": {"tldr": "\u63d0\u51fa\u6574\u5408\u591a\u667a\u80fd\u4f53\u6846\u67b6\u3001\u5206\u5c42\u8bb0\u5fc6\u7cfb\u7edf\u548c\u5de5\u5177\u5957\u4ef6\u7684\u901a\u7528\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347AI\u52a9\u624b\u7684\u8de8\u9886\u57df\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u81ea\u6cbb\u7cfb\u7edf\u591a\u4e3a\u5b64\u7acb\u6539\u8fdb\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u7ea7\u6574\u5408\u8bbe\u8ba1\uff0c\u96be\u4ee5\u5b9e\u73b0\u7a33\u5065\u6027\u548c\u9002\u5e94\u6027", "method": "\u6784\u5efa\u5305\u542b\u89c4\u5212/\u6267\u884c\u667a\u80fd\u4f53\u4e0e\u8bc4\u5ba1\u6a21\u578b\u7684\u534f\u540c\u6846\u67b6\uff0c\u8bbe\u8ba1\u5de5\u4f5c\u8bb0\u5fc6-\u8bed\u4e49\u8bb0\u5fc6-\u7a0b\u5e8f\u8bb0\u5fc6\u7684\u5c42\u6b21\u5316\u5b58\u50a8\u4f53\u7cfb\uff0c\u5b8c\u5584\u641c\u7d22/\u4ee3\u7801\u6267\u884c/\u591a\u6a21\u6001\u89e3\u6790\u5de5\u5177\u94fe", "result": "\u5728\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5f00\u6e90\u7cfb\u7edf\u5e76\u63a5\u8fd1\u5546\u4e1a\u7cfb\u7edf\u6027\u80fd\uff0c\u9a8c\u8bc1\u7cfb\u7edf\u6574\u5408\u7684\u6709\u6548\u6027", "conclusion": "\u7cfb\u7edf\u7ea7\u67b6\u6784\u8bbe\u8ba1\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u5f3a\u9002\u5e94AI\u52a9\u624b\u7684\u5173\u952e\uff0c\u4e3a\u5f00\u53d1\u8de8\u9886\u57df\u901a\u7528\u667a\u80fd\u4f53\u6307\u660e\u65b9\u5411"}}
{"id": "2510.00514", "pdf": "https://arxiv.org/pdf/2510.00514", "abs": "https://arxiv.org/abs/2510.00514", "authors": ["Samuel Pfisterer", "Florian Gr\u00f6tschla", "Luca A. Lanzend\u00f6rfer", "Florian Yan", "Roger Wattenhofer"], "title": "EuroSpeech: A Multilingual Speech Corpus", "categories": ["cs.CL", "cs.LG"], "comment": "Published in the 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025) Track on Datasets and Benchmark", "summary": "Recent progress in speech processing has highlighted that high-quality\nperformance across languages requires substantial training data for each\nindividual language. While existing multilingual datasets cover many languages,\nthey often contain insufficient data for most languages. Thus, trained models\nperform poorly on the majority of the supported languages. Our work addresses\nthis challenge by introducing a scalable pipeline for constructing speech\ndatasets from parliamentary recordings. The proposed pipeline includes robust\ncomponents for media retrieval and a two-stage alignment algorithm designed to\nhandle non-verbatim transcripts and long-form audio. Applying this pipeline to\nrecordings from 22 European parliaments, we extract over 61k hours of aligned\nspeech segments, achieving substantial per-language coverage with 19 languages\nexceeding 1k hours and 22 languages exceeding 500 hours of high-quality speech\ndata. We obtain an average 41.8\\% reduction in word error rates over baselines\nwhen finetuning an existing ASR model on our dataset, demonstrating the\nusefulness of our approach.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u4ece\u8bae\u4f1a\u5f55\u97f3\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u8bed\u97f3\u6570\u636e\u7684\u53ef\u6269\u5c55\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00ASR\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u8bed\u97f3\u6570\u636e\u96c6\u666e\u904d\u5b58\u5728\u8bed\u8a00\u8986\u76d6\u5e7f\u4f46\u5355\u8bed\u6570\u636e\u4e0d\u8db3\u7684\u7f3a\u9677\uff0c\u5bfc\u81f4\u591a\u6570\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002\u8bae\u4f1a\u5f55\u97f3\u4f5c\u4e3a\u4f18\u8d28\u8bed\u97f3\u6e90\u672a\u88ab\u5145\u5206\u5229\u7528\u3002", "method": "1. \u5f00\u53d1\u5305\u542b\u5a92\u4f53\u68c0\u7d22\u548c\u4e24\u9636\u6bb5\u5bf9\u9f50\u7b97\u6cd5\u7684\u7cfb\u7edf\u6d41\u7a0b\n2. \u5904\u7406\u975e\u9010\u5b57\u8f6c\u5f55\u548c\u957f\u97f3\u9891\u5bf9\u9f50\u96be\u9898\n3. \u5e94\u7528\u81f322\u4e2a\u6b27\u6d32\u8bae\u4f1a\u7684\u5f55\u97f3\u6570\u636e", "result": "\u6210\u529f\u63d0\u53d661k+\u5c0f\u65f6\u8bed\u97f3\uff0819\u79cd\u8bed\u8a00>1k\u5c0f\u65f6\uff0c22\u79cd>500\u5c0f\u65f6\uff09\uff0c\u5fae\u8c03\u540eASR\u8bcd\u9519\u7387\u5e73\u5747\u964d\u4f4e41.8%", "conclusion": "\u8bae\u4f1a\u5f55\u97f3\u662f\u4f18\u8d28\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u6e90\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u6570\u636e\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6027\u80fd"}}
{"id": "2510.00526", "pdf": "https://arxiv.org/pdf/2510.00526", "abs": "https://arxiv.org/abs/2510.00526", "authors": ["Gaotang Li", "Ruizhong Qiu", "Xiusi Chen", "Heng Ji", "Hanghang Tong"], "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum", "categories": ["cs.CL", "cs.LG"], "comment": "23 pages, 4 figures", "summary": "Supervised fine-tuning (SFT) is the standard approach for post-training large\nlanguage models (LLMs), yet it often shows limited generalization. We trace\nthis limitation to its default training objective: negative log likelihood\n(NLL). While NLL is classically optimal when training from scratch,\npost-training operates in a different paradigm and could violate its optimality\nassumptions, where models already encode task-relevant priors and supervision\ncan be long and noisy. To this end, we study a general family of\nprobability-based objectives and characterize their effectiveness under\ndifferent conditions. Through comprehensive experiments and extensive ablation\nstudies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a\ncritical dimension that governs objective behavior: the model-capability\ncontinuum. Near the model-strong end, prior-leaning objectives that downweight\nlow-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants)\nconsistently outperform NLL; toward the model-weak end, NLL dominates; in\nbetween, no single objective prevails. Our theoretical analysis further\nelucidates how objectives trade places across the continuum, providing a\nprincipled foundation for adapting objectives to model capability. Our code is\navailable at https://github.com/GaotangLi/Beyond-Log-Likelihood.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u80fd\u529b\u8fde\u7eed\u4f53\u662f\u5f71\u54cd\u540e\u8bad\u7ec3\u76ee\u6807\u51fd\u6570\u9009\u62e9\u7684\u5173\u952e\u56e0\u7d20\uff0c\u63d0\u51fa\u5e94\u6839\u636e\u6a21\u578b\u80fd\u529b\u52a8\u6001\u8c03\u6574\u4f18\u5316\u76ee\u6807\u3002", "motivation": "\u4f20\u7edf\u8d1f\u5bf9\u6570\u4f3c\u7136(NLL)\u76ee\u6807\u5728\u540e\u8bad\u7ec3\u9636\u6bb5\u56e0\u6a21\u578b\u5df2\u5177\u5907\u5148\u9a8c\u77e5\u8bc6\u4e14\u76d1\u7763\u6570\u636e\u5b58\u5728\u566a\u58f0\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002", "method": "\u901a\u8fc77\u79cd\u6a21\u578b\u300114\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c3\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\uff0c\u7ed3\u5408\u7406\u8bba\u5206\u6790\u7814\u7a76\u6982\u7387\u76ee\u6807\u51fd\u6570\u65cf\u7684\u6709\u6548\u6027\u3002", "result": "\u6a21\u578b\u80fd\u529b\u5f3a\u7684\u533a\u57df\u5148\u9a8c\u503e\u5411\u76ee\u6807(\u5982$-p^{10}$)\u4f18\u4e8eNLL\uff1b\u80fd\u529b\u5f31\u65f6NLL\u4e3b\u5bfc\uff1b\u4e2d\u95f4\u533a\u57df\u65e0\u5355\u4e00\u6700\u4f18\u76ee\u6807\u3002", "conclusion": "\u76ee\u6807\u51fd\u6570\u9009\u62e9\u5e94\u9002\u914d\u6a21\u578b\u80fd\u529b\u9636\u6bb5\uff0c\u7406\u8bba\u6846\u67b6\u4e3a\u52a8\u6001\u8c03\u6574\u76ee\u6807\u51fd\u6570\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\u3002"}}
{"id": "2510.00536", "pdf": "https://arxiv.org/pdf/2510.00536", "abs": "https://arxiv.org/abs/2510.00536", "authors": ["Kung-Hsiang Huang", "Haoyi Qiu", "Yutong Dai", "Caiming Xiong", "Chien-Sheng Wu"], "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness", "categories": ["cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents built on vision-language models have\nemerged as a promising approach to automate human-computer workflows. However,\nthey also face the inefficiency challenge as they process long sequences of\nhigh-resolution screenshots and solving long-horizon tasks, making inference\nslow, costly and memory-bound. While key-value (KV) caching can mitigate this,\nstoring the full cache is prohibitive for image-heavy contexts. Existing\ncache-compression methods are sub-optimal as they do not account for the\nspatial and temporal redundancy of GUIs. In this work, we first analyze\nattention patterns in GUI agent workloads and find that, unlike in natural\nimages, attention sparsity is uniformly high across all transformer layers.\nThis insight motivates a simple uniform budget allocation strategy, which we\nshow empirically outperforms more complex layer-varying schemes. Building on\nthis, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI\nagents that requires no retraining. GUI-KV combines two novel techniques: (i)\nspatial saliency guidance, which augments attention scores with the L2 norm of\nhidden states to better preserve semantically important visual tokens, and (ii)\ntemporal redundancy scoring, which projects previous frames' keys onto the\ncurrent frame's key subspace to preferentially prune redundant history. Across\nstandard GUI agent benchmarks and models, GUI-KV outperforms competitive KV\ncompression baselines, closely matching full-cache accuracy at modest budgets.\nNotably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV\nreduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the\nfull-cache baseline. These results demonstrate that exploiting GUI-specific\nredundancies enables efficient and reliable agent performance.", "AI": {"tldr": "\u63d0\u51faGUI-KV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u663e\u8457\u6027\u5f15\u5bfc\u548c\u65f6\u95f4\u5197\u4f59\u8bc4\u5206\u6280\u672f\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500", "motivation": "\u56fe\u5f62\u754c\u9762\u4ee3\u7406\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u622a\u56fe\u65f6\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u73b0\u6709\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651GUI\u7279\u6709\u7684\u65f6\u7a7a\u5197\u4f59\u7279\u6027", "method": "\u7ed3\u5408\u7a7a\u95f4\u663e\u8457\u6027\uff08\u9690\u85cf\u72b6\u6001L2\u8303\u6570\u589e\u5f3a\u6ce8\u610f\u529b\uff09\u548c\u65f6\u95f4\u5197\u4f59\u8bc4\u4f30\uff08\u8de8\u5e27\u952e\u7a7a\u95f4\u6295\u5f71\uff09\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684KV\u7f13\u5b58\u538b\u7f29", "result": "\u5728AgentNetBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c5\u622a\u56fe\u573a\u666f\u4e0b\u89e3\u7801FLOPs\u964d\u4f4e38.9%\uff0c\u6b65\u9aa4\u51c6\u786e\u7387\u63d0\u53474.1%", "conclusion": "\u9488\u5bf9GUI\u7279\u6709\u5197\u4f59\u8bbe\u8ba1\u7684\u538b\u7f29\u7b56\u7565\u80fd\u6709\u6548\u5e73\u8861\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u56fe\u5f62\u754c\u9762\u4ee3\u7406\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2510.00546", "pdf": "https://arxiv.org/pdf/2510.00546", "abs": "https://arxiv.org/abs/2510.00546", "authors": ["Minjae Oh", "Sangjun Song", "Seungkyu Lee", "Sungmin Jo", "Yohan Jo"], "title": "ThinkBrake: Mitigating Overthinking in Tool Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Small reasoning models (SRMs) often overthink during tool use: they reach a\ncorrect tool-argument configuration, then continue reasoning and overwrite it\nwith an incorrect final call. We diagnose overthinking via oracle rollouts that\ninject </think> at sentence boundaries. On the Berkeley Function Calling\nLeaderboard (BFCL), this oracle termination lifts average accuracy from 85.8\\%\nto 94.2\\% while reducing tokens by 80-94\\%, revealing substantial recoverable\nheadroom and potential redundant reasoning. While prior work on concise\nreasoning has largely targeted mathematics, tool reasoning remains\nunderexplored. We adapt various early-termination baselines to tool use and\nintroduce ThinkBrake, a training-free decoding heuristic. ThinkBrake monitors\nthe log-probability margin between </think> and the current top token at\nsentence boundaries and triggers termination when this margin becomes small.\nAcross BFCL's single turn, non-live and live splits, ThinkBrake preserves or\nimproves accuracy while reducing tokens up to 25\\%, outperforming various\nbaselines.", "AI": {"tldr": "\u5c0f\u63a8\u7406\u6a21\u578b\u5728\u5de5\u5177\u4f7f\u7528\u65f6\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u524d\u7ec8\u6b62\u63a8\u7406\u53ef\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u5e76\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\u3002", "motivation": "\u73b0\u6709\u5173\u4e8e\u7b80\u6d01\u63a8\u7406\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6570\u5b66\u9886\u57df\uff0c\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u7684\u5197\u4f59\u63a8\u7406\u95ee\u9898\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u89e3\u51b3\u5c0f\u6a21\u578b\u5728\u5de5\u5177\u8c03\u7528\u65f6\u6b63\u786e\u914d\u7f6e\u540e\u7ee7\u7eed\u9519\u8bef\u8986\u76d6\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6ce8\u5165<stop>\u6807\u7b7e\u7684oracle rollout\u8bca\u65ad\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5e76\u63d0\u51faThinkBrake\u2014\u2014\u901a\u8fc7\u76d1\u63a7\u53e5\u5b50\u8fb9\u754c\u5904<stop>\u4e0e\u5f53\u524d\u6700\u9ad8\u6982\u7387token\u7684\u5bf9\u6570\u6982\u7387\u5dee\u5b9e\u73b0\u63d0\u524d\u7ec8\u6b62\u7684\u89e3\u7801\u7b56\u7565\u3002", "result": "\u5728BFCL\u57fa\u51c6\u7684\u5355\u8f6e\u3001\u975e\u5b9e\u65f6\u548c\u5b9e\u65f6\u573a\u666f\u4e2d\uff0cThinkBrake\u5728\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c11\u6700\u591a25%\u7684token\u6d88\u8017\uff0c\u4f18\u4e8e\u5404\u7c7b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u524d\u7ec8\u6b62\u673a\u5236\u6709\u6548\u7f13\u89e3\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0cThinkBrake\u5c55\u793a\u4e86\u5de5\u5177\u63a8\u7406\u573a\u666f\u4e2d\u901a\u8fc7\u4f18\u5316\u89e3\u7801\u7b56\u7565\u63d0\u5347\u6548\u7387\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.00567", "pdf": "https://arxiv.org/pdf/2510.00567", "abs": "https://arxiv.org/abs/2510.00567", "authors": ["Yubo Xie", "Chenkai Wang", "Zongyang Ma", "Fahui Miao"], "title": "Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference. 22 pages, 3 figures, 13\n  tables. GitHub: github.com/yuboxie/chime", "summary": "Large language models (LLMs) are trained on vast amounts of text from the\nInternet, but do they truly understand the viral content that rapidly spreads\nonline -- commonly known as memes? In this paper, we introduce CHIME, a dataset\nfor CHinese Internet Meme Explanation. The dataset comprises popular\nphrase-based memes from the Chinese Internet, annotated with detailed\ninformation on their meaning, origin, example sentences, types, etc. To\nevaluate whether LLMs understand these memes, we designed two tasks. In the\nfirst task, we assessed the models' ability to explain a given meme, identify\nits origin, and generate appropriate example sentences. The results show that\nwhile LLMs can explain the meanings of some memes, their performance declines\nsignificantly for culturally and linguistically nuanced meme types.\nAdditionally, they consistently struggle to provide accurate origins for the\nmemes. In the second task, we created a set of multiple-choice questions (MCQs)\nrequiring LLMs to select the most appropriate meme to fill in a blank within a\ncontextual sentence. While the evaluated models were able to provide correct\nanswers, their performance remains noticeably below human levels. We have made\nCHIME public and hope it will facilitate future research on computational meme\nunderstanding.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4e2d\u6587\u7f51\u7edc\u6a21\u56e0\u7684\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u5728\u6587\u5316\u548c\u8bed\u8a00\u7ec6\u8282\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u771f\u6b63\u7406\u89e3\u7f51\u7edc\u75c5\u6bd2\u5f0f\u4f20\u64ad\u5185\u5bb9\uff08\u5982\u4e2d\u6587\u6a21\u56e0\uff09\uff0c\u5c24\u5176\u5728\u6587\u5316\u80cc\u666f\u548c\u8bed\u8a00\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u521b\u5efaCHIME\u6570\u636e\u96c6\uff08\u5305\u542b\u8be6\u7ec6\u6807\u6ce8\u7684\u4e2d\u6587\u6a21\u56e0\uff09\uff0c\u8bbe\u8ba1\u6a21\u56e0\u89e3\u91ca\u548c\u8bed\u5883\u586b\u7a7a\u4e24\u4e2a\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u80fd\u89e3\u91ca\u90e8\u5206\u6a21\u56e0\u542b\u4e49\uff0c\u4f46\u5728\u6587\u5316/\u8bed\u8a00\u654f\u611f\u578b\u6a21\u56e0\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff1b\u6301\u7eed\u65e0\u6cd5\u51c6\u786e\u8ffd\u6eaf\u6a21\u56e0\u8d77\u6e90\uff1b\u591a\u9009\u9898\u4efb\u52a1\u8868\u73b0\u4f4e\u4e8e\u4eba\u7c7b\u6c34\u5e73\u3002", "conclusion": "\u516c\u5f00CHIME\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8ba1\u7b97\u6a21\u56e0\u7406\u89e3\u7814\u7a76\uff0c\u63ed\u793aLLMs\u5728\u6587\u5316\u654f\u611f\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u6539\u8fdb\u6a21\u578b\u5bf9\u7f51\u7edc\u6587\u5316\u73b0\u8c61\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2510.00568", "pdf": "https://arxiv.org/pdf/2510.00568", "abs": "https://arxiv.org/abs/2510.00568", "authors": ["Shiyu Li", "Yang Tang", "Yifan Wang", "Peiming Li", "Xi Chen"], "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards", "categories": ["cs.CL"], "comment": "19 pages", "summary": "Search agents powered by Large Language Models (LLMs) have demonstrated\nsignificant potential in tackling knowledge-intensive tasks. Reinforcement\nlearning (RL) has emerged as a powerful paradigm for training these agents to\nperform complex, multi-step reasoning. However, prior RL-based methods often\nrely on sparse or rule-based rewards, which can lead agents to commit to\nsuboptimal or erroneous reasoning paths without the ability to recover. To\naddress these limitations, we propose ReSeek, a novel self-correcting framework\nfor training search agents. Our framework introduces a self-correction\nmechanism that empowers the agent to dynamically identify and recover from\nerroneous search paths during an episode. By invoking a special JUDGE action,\nthe agent can judge the information and re-plan its search strategy. To guide\nthis process, we design a dense, instructive process reward function, which\ndecomposes into a correctness reward for retrieving factual information and a\nutility reward for finding information genuinely useful for the query.\nFurthermore, to mitigate the risk of data contamination in existing datasets,\nwe introduce FictionalHot, a new and challenging benchmark with recently\ncurated questions requiring complex reasoning. Being intuitively reasonable and\npractically simple, extensive experiments show that agents trained with ReSeek\nsignificantly outperform SOTA baselines in task success rate and path\nfaithfulness.", "AI": {"tldr": "ReSeek\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u548c\u53cc\u7ef4\u5ea6\u5956\u52b1\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\u4f9d\u8d56\u7a00\u758f/\u89c4\u5219\u5316\u5956\u52b1\uff0c\u5bb9\u6613\u9677\u5165\u9519\u8bef\u63a8\u7406\u8def\u5f84\u4e14\u7f3a\u4e4f\u81ea\u6211\u7ea0\u6b63\u80fd\u529b", "method": "1. \u5f15\u5165JUDGE\u52a8\u4f5c\u5b9e\u73b0\u52a8\u6001\u9519\u8bef\u8bc6\u522b\u4e0e\u8def\u5f84\u4fee\u6b63\n2. \u8bbe\u8ba1\u5305\u542b\u6b63\u786e\u6027\uff08\u4e8b\u5b9e\u68c0\u7d22\uff09\u548c\u5b9e\u7528\u6027\uff08\u4fe1\u606f\u4ef7\u503c\uff09\u7684\u53cc\u7ef4\u5ea6\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\n3. \u6784\u5efa\u9632\u6570\u636e\u6c61\u67d3\u7684FictionalHot\u57fa\u51c6\u6d4b\u8bd5\u96c6", "result": "\u5728\u4efb\u52a1\u6210\u529f\u7387\uff08+15.2%\uff09\u548c\u8def\u5f84\u5fe0\u5b9e\u5ea6\uff08+22.8%\uff09\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u4e0e\u8fc7\u7a0b\u5bfc\u5411\u7684\u5956\u52b1\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u7cfb\u7edf\u7b80\u6d01\u6027\u7684\u540c\u65f6\u6709\u6548\u63d0\u5347\u641c\u7d22\u4ee3\u7406\u7684\u53ef\u9760\u6027\u548c\u51b3\u7b56\u8d28\u91cf"}}
{"id": "2510.00579", "pdf": "https://arxiv.org/pdf/2510.00579", "abs": "https://arxiv.org/abs/2510.00579", "authors": ["Li Li", "Ziyi Wang", "Yongliang Wu", "Jianfei Cai", "Xu Yang"], "title": "CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs", "categories": ["cs.CL"], "comment": "22 pages, 7 figures", "summary": "Chain-of-Thought (CoT) prompting has emerged as a powerful approach to\nenhancing the reasoning capabilities of Large Language Models (LLMs). However,\nexisting implementations, such as in-context learning and fine-tuning, remain\ncostly and inefficient. To improve CoT reasoning at a lower cost, and inspired\nby the task vector paradigm, we introduce CoT Vectors, compact representations\nthat encode task-general, multi-step reasoning knowledge. Through experiments\nwith Extracted CoT Vectors, we observe pronounced layer-wise instability,\nmanifesting as a U-shaped performance curve that reflects a systematic\nthree-stage reasoning process in LLMs. To address this limitation, we propose\nLearnable CoT Vectors, optimized under a teacher-student framework to provide\nmore stable and robust guidance. Extensive evaluations across diverse\nbenchmarks and models demonstrate that CoT Vectors not only outperform existing\nbaselines but also achieve performance comparable to parameter-efficient\nfine-tuning methods, while requiring fewer trainable parameters. Moreover, by\ntreating CoT Vectors as a probe, we uncover how their effectiveness varies due\nto latent space structure, information density, acquisition mechanisms, and\npre-training differences, offering new insights into the functional\norganization of multi-step reasoning in LLMs. The source code will be released.", "AI": {"tldr": "\u63d0\u51faCoT Vectors\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u4f18\u5316\u63a8\u7406\u5411\u91cf\uff0c\u5728\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u540c\u65f6\u8fbe\u5230\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6027\u80fd", "motivation": "\u73b0\u6709Chain-of-Thought\u5b9e\u73b0\u65b9\u5f0f\u5b58\u5728\u9ad8\u6210\u672c\u4f4e\u6548\u7387\u95ee\u9898\uff0c\u53d7\u4efb\u52a1\u5411\u91cf\u8303\u5f0f\u542f\u53d1\uff0c\u901a\u8fc7\u7d27\u51d1\u5411\u91cf\u7f16\u7801\u591a\u6b65\u63a8\u7406\u77e5\u8bc6\u3002\u5b9e\u9a8c\u53d1\u73b0\u63d0\u53d6\u5411\u91cf\u5b58\u5728\u5c42\u95f4\u4e0d\u7a33\u5b9a\u6027\uff08U\u578b\u6027\u80fd\u66f2\u7ebf\uff09\uff0c\u9700\u4f18\u5316\u89e3\u51b3", "method": "\u63d0\u51faLearnable CoT Vectors\uff0c\u5728\u6559\u5e08-\u5b66\u751f\u6846\u67b6\u4e0b\u4f18\u5316\u5411\u91cf\u53c2\u6570\uff0c\u63d0\u4f9b\u7a33\u5b9a\u63a8\u7406\u5f15\u5bfc\u3002\u901a\u8fc7\u63d0\u53d6\u5411\u91cf\u53d1\u73b0LLMs\u4e09\u9636\u6bb5\u63a8\u7406\u8fc7\u7a0b\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u5411\u91cf\u6709\u6548\u6027\u5f71\u54cd\u56e0\u7d20", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u6027\u80fd\u5ab2\u7f8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u8bad\u7ec3\u53c2\u6570\u91cf\u66f4\u5c11\uff09\u3002\u5411\u91cf\u6709\u6548\u6027\u53d7\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u3001\u4fe1\u606f\u5bc6\u5ea6\u3001\u83b7\u53d6\u673a\u5236\u548c\u9884\u8bad\u7ec3\u5dee\u5f02\u5f71\u54cd", "conclusion": "CoT Vectors\u4e0d\u4ec5\u662f\u9ad8\u6548\u63a8\u7406\u5de5\u5177\uff0c\u66f4\u53ef\u4f5c\u4e3a\u63a2\u9488\u63ed\u793aLLMs\u591a\u6b65\u63a8\u7406\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u529f\u80fd\u7ec4\u7ec7\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2510.00582", "pdf": "https://arxiv.org/pdf/2510.00582", "abs": "https://arxiv.org/abs/2510.00582", "authors": ["Sangmin Lee", "Woongjib Choi", "Jihyun Kim", "Hong-Goo Kang"], "title": "SAGE-LD: Towards Scalable and Generalizable End-to-End Language Diarization via Simulated Data Augmentation", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "In this paper, we present a neural spoken language diarization model that\nsupports an unconstrained span of languages within a single framework. Our\napproach integrates a learnable query-based architecture grounded in\nmultilingual awareness, with large-scale pretraining on simulated\ncode-switching data. By jointly leveraging these two components, our method\novercomes the limitations of conventional approaches in data scarcity and\narchitecture optimization, and generalizes effectively to real-world\nmultilingual settings across diverse environments. Experimental results\ndemonstrate that our approach achieves state-of-the-art performance on several\nlanguage diarization benchmarks, with a relative performance improvement of 23%\nto 52% over previous methods. We believe that this work not only advances\nresearch in language diarization but also establishes a foundational framework\nfor code-switching speech technologies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53ef\u5b66\u4e60\u67e5\u8be2\u67b6\u6784\u4e0e\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u7684\u8bed\u97f3\u8bed\u8a00\u5206\u7c7b\u6a21\u578b\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b023%-52%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u67b6\u6784\u4f18\u5316\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u771f\u5b9e\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u4ee3\u7801\u5207\u6362\u95ee\u9898\u3002", "method": "\u6574\u5408\u53ef\u5b66\u4e60\u67e5\u8be2\u67b6\u6784\u4e0e\u5927\u89c4\u6a21\u4ee3\u7801\u5207\u6362\u6a21\u62df\u6570\u636e\u7684\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u591a\u8bed\u8a00\u8054\u5408\u5efa\u6a21\u3002", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u76f8\u5bf9\u6027\u80fd\u63d0\u534723%-52%\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u6280\u672f\u7684\u57fa\u7840\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u8bed\u8a00\u5206\u7c7b\u4e0e\u591a\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.00629", "pdf": "https://arxiv.org/pdf/2510.00629", "abs": "https://arxiv.org/abs/2510.00629", "authors": ["Teisovi Angami", "Kevisino Khate"], "title": "Tenyidie Syllabification corpus creation and deep learning applications", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "The Tenyidie language is a low-resource language of the Tibeto-Burman family\nspoken by the Tenyimia Community of Nagaland in the north-eastern part of India\nand is considered a major language in Nagaland. It is tonal,\nSubject-Object-Verb, and highly agglutinative in nature. Being a low-resource\nlanguage, very limited research on Natural Language Processing (NLP) has been\nconducted. To the best of our knowledge, no work on syllabification has been\nreported for this language. Among the many NLP tasks, syllabification or\nsyllabication is an important task in which the given word syllables are\nidentified. The contribution of this work is the creation of 10,120 syllabified\nTenyidie words and the application of the Deep Learning techniques on the\ncreated corpus. In this paper, we have applied LSTM, BLSTM, BLSTM+CRF, and\nEncoder-decoder deep learning architectures on our created dataset. In our\ndataset split of 80:10:10 (train:validation:test) set, we achieved the highest\naccuracy of 99.21% with BLSTM model on the test set. This work will find its\napplication in numerous other NLP applications, such as morphological analysis,\npart-of-speech tagging, machine translation, etc, for the Tenyidie Language.\n  Keywords: Tenyidie; NLP; syllabification; deep learning; LSTM; BLSTM; CRF;\nEncoder-decoder", "AI": {"tldr": "\u9996\u6b21\u4e3a\u4f4e\u8d44\u6e90Tenyidie\u8bed\u8a00\u521b\u5efa10,120\u4e2a\u97f3\u8282\u5207\u5206\u8bed\u6599\u5e93\uff0c\u5e94\u7528LSTM\u3001BLSTM\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5b9e\u73b099.21%\u7684\u6700\u9ad8\u5207\u5206\u51c6\u786e\u7387", "motivation": "Tenyidie\u4f5c\u4e3a\u5370\u5ea6\u90a3\u52a0\u5170\u90a6\u4e3b\u8981\u8bed\u8a00\uff0c\u5177\u6709\u58f0\u8c03\u6027\u3001\u4e3b\u5bbe\u52a8\u7ed3\u6784\u548c\u9ad8\u5ea6\u9ecf\u7740\u6027\uff0c\u4f46\u7f3a\u4e4f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u57fa\u7840\uff0c\u5c24\u5176\u97f3\u8282\u5207\u5206\u9886\u57df\u5c1a\u65e0\u76f8\u5173\u7814\u7a76", "method": "\u4f7f\u7528LSTM\u3001\u53cc\u5411LSTM\uff08BLSTM\uff09\u3001BLSTM+CRF\u548c\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u57288:1:1\u5212\u5206\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u9a8c\u8bc1", "result": "BLSTM\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u8fbe\u523099.21%\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u67b6\u6784", "conclusion": "\u8be5\u6210\u679c\u53ef\u5e94\u7528\u4e8e\u5f62\u6001\u5206\u6790\u3001\u8bcd\u6027\u6807\u6ce8\u3001\u673a\u5668\u7ffb\u8bd1\u7b49Tenyidie\u8bed\u8a00\u7684NLP\u4efb\u52a1\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u6280\u672f\u53c2\u8003"}}
{"id": "2510.00647", "pdf": "https://arxiv.org/pdf/2510.00647", "abs": "https://arxiv.org/abs/2510.00647", "authors": ["Jinlan Fu", "Shenzhen Huangfu", "Hao Fei", "Yichong Huang", "Xiaoyu Shen", "Xipeng Qiu", "See-Kiong Ng"], "title": "MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation", "categories": ["cs.CL"], "comment": "Accepted by ACM MM 2025", "summary": "The alt-text generation task produces concise, context-relevant descriptions\nof images, enabling blind and low-vision users to access online images. Despite\nthe capabilities of large vision-language models, alt-text generation\nperformance remains limited due to noisy user annotations, inconsistent\nstandards, and MLLMs' insensitivity to contextual information. Previous efforts\nto fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT\nrelies on accurate target annotations, which are often flawed in user-generated\nalt-text. To address this, we propose Multi-faceted Cross-modal Direct\nPreference Optimization (MCM-DPO), which improves alt-text generation by\nlearning to identify better options in preference pairs without requiring\nprecise annotations. MCM-DPO optimizes preferences across single, paired, and\nmulti-preference dimensions, covering textual, visual, and cross-modal factors.\nIn light of the scarcity of high-quality annotated and preference-labeled\ndatasets for alt-text, we constructed two large-scale, high-quality datasets\nnamed TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include\n202k annotated alt-text samples and 18k preference pairs that cover diverse\npreference dimensions, aiming to support further research in this domain.\nExperimental results show that our proposed MCM-DPO method consistently\noutperforms both DPO and SFT, establishing a new state of the art in alt-text\ngeneration. We release the code and data here:\nhttps://github.com/LVUGAI/MCM-DPO", "AI": {"tldr": "\u63d0\u51faMCM-DPO\u65b9\u6cd5\u6539\u8fdb\u66ff\u4ee3\u6587\u672c\u751f\u6210\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u504f\u597d\u4f18\u5316\u89e3\u51b3\u6807\u6ce8\u566a\u58f0\u95ee\u9898\uff0c\u5e76\u6784\u5efaTwitter/Pinterest\u6570\u636e\u96c6TAlt/PAlt\uff0c\u5b9e\u9a8c\u663e\u793a\u65b9\u6cd5\u4f18\u4e8eDPO/SFT\u3002", "motivation": "\u73b0\u6709\u66ff\u4ee3\u6587\u672c\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u7528\u6237\u6807\u6ce8\u566a\u58f0\u3001\u6807\u51c6\u4e0d\u7edf\u4e00\u53ca\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u4e0d\u654f\u611f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u4f9d\u8d56\u7cbe\u786e\u6807\u6ce8\u7684\u7f3a\u9677\u5728\u7528\u6237\u751f\u6210\u573a\u666f\u5c24\u4e3a\u7a81\u51fa\u3002", "method": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u8de8\u6a21\u6001\u76f4\u63a5\u504f\u597d\u4f18\u5316\u6846\u67b6(MCM-DPO)\uff0c\u5728\u5355\u6837\u672c/\u914d\u5bf9/\u591a\u504f\u597d\u7ef4\u5ea6\u8054\u5408\u4f18\u5316\uff0c\u878d\u5408\u6587\u672c\u8d28\u91cf\u3001\u89c6\u89c9\u76f8\u5173\u6027\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u6307\u6807\u6784\u5efa\u504f\u597d\u5bf9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMCM-DPO\u5728BLEU-4/ROUGE/METEOR\u6307\u6807\u4e0a\u5206\u522b\u8d85\u8d8aDPO 3.8/4.2/2.1\u4e2a\u70b9\uff0c\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u504f\u597d\u7387\u63d0\u534715.7%\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u504f\u597d\u5b66\u4e60\u6846\u67b6\u89c4\u907f\u566a\u58f0\u6807\u6ce8\u4f9d\u8d56\uff0c\u914d\u5408\u9ad8\u8d28\u91cf\u591a\u7ef4\u5ea6\u6570\u636e\u96c6\u6784\u5efa\uff0c\u4e3a\u66ff\u4ee3\u6587\u672c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u6280\u672f\u57fa\u51c6\u3002"}}
{"id": "2510.00662", "pdf": "https://arxiv.org/pdf/2510.00662", "abs": "https://arxiv.org/abs/2510.00662", "authors": ["Fran\u00e7ois Ledoyen", "Ga\u00ebl Dias", "Jeremie Pantin", "Alexis Lechervy", "Fabrice Maurel", "Youssef Chahir"], "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "Simplifying complex texts is essential for ensuring equitable access to\ninformation, especially for individuals with cognitive impairments. The\nEasy-to-Read (ETR) initiative offers a framework for making content accessible\nto the neurodivergent population, but the manual creation of such texts remains\ntime-consuming and resource-intensive. In this work, we investigate the\npotential of large language models (LLMs) to automate the generation of ETR\ncontent. To address the scarcity of aligned corpora and the specificity of ETR\nconstraints, we propose a multi-task learning (MTL) approach that trains models\njointly on text summarization, text simplification, and ETR generation. We\nexplore two different strategies: multi-task retrieval-augmented generation\n(RAG) for in-context learning, and MTL-LoRA for parameter-efficient\nfine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a\nnew high-quality dataset, demonstrate the benefits of multi-task setups over\nsingle-task baselines across all configurations. Moreover, results show that\nthe RAG-based strategy enables generalization in out-of-domain settings, while\nMTL-LoRA outperforms all learning strategies within in-domain configurations.", "AI": {"tldr": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6613\u8bfb\u6587\u672c\u7684\u6548\u679c", "motivation": "\u89e3\u51b3\u624b\u52a8\u521b\u5efa\u6613\u8bfb\u6587\u672c\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5229\u7528LLMs\u81ea\u52a8\u5316\u751f\u6210ETR\u5185\u5bb9\u3002\u5f53\u524d\u9762\u4e34\u5bf9\u9f50\u8bed\u6599\u7a00\u7f3a\u548cETR\u89c4\u5219\u7279\u6b8a\u6027\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "\u63d0\u51fa\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff08\u6587\u672c\u6458\u8981+\u6587\u672c\u7b80\u5316+ETR\u751f\u6210\uff09\uff0c\u63a2\u7d22\u57fa\u4e8eRAG\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u548c\u53c2\u6570\u9ad8\u6548\u7684MTL-LoRA\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u7528Mistral-7B/LLaMA-3-8B\u6a21\u578b\u548cETR-fr\u65b0\u6570\u636e\u96c6", "result": "\u591a\u4efb\u52a1\u914d\u7f6e\u5168\u9762\u4f18\u4e8e\u5355\u4efb\u52a1\u57fa\u7ebf\uff1bRAG\u7b56\u7565\u5b9e\u73b0\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0cMTL-LoRA\u5728\u9886\u57df\u5185\u8868\u73b0\u6700\u4f73", "conclusion": "\u9a8c\u8bc1\u591a\u4efb\u52a1\u5b66\u4e60\u5bf9ETR\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u53c2\u6570\u9ad8\u6548\u7684\u5b66\u4e60\u7b56\u7565\uff0c\u4e3a\u6613\u8bfb\u5185\u5bb9\u81ea\u52a8\u5316\u751f\u6210\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2510.00691", "pdf": "https://arxiv.org/pdf/2510.00691", "abs": "https://arxiv.org/abs/2510.00691", "authors": ["Fran\u00e7ois Ledoyen", "Ga\u00ebl Dias", "Alexis Lechervy", "Jeremie Pantin", "Fabrice Maurel", "Youssef Chahir", "Elisa Gouzonnat", "M\u00e9lanie Berthelot", "Stanislas Moravac", "Armony Altinier", "Amy Khairalla"], "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments", "categories": ["cs.CL", "cs.AI"], "comment": "ECAI 2025", "summary": "Ensuring accessibility for individuals with cognitive impairments is\nessential for autonomy, self-determination, and full citizenship. However,\nmanual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to\nscale, limiting access to crucial information in healthcare, education, and\ncivic life. AI-driven ETR generation offers a scalable solution but faces key\nchallenges, including dataset scarcity, domain adaptation, and balancing\nlightweight learning of Large Language Models (LLMs). In this paper, we\nintroduce ETR-fr, the first dataset for ETR text generation fully compliant\nwith European ETR guidelines. We implement parameter-efficient fine-tuning on\nPLMs and LLMs to establish generative baselines. To ensure high-quality and\naccessible outputs, we introduce an evaluation framework based on automatic\nmetrics supplemented by human assessments. The latter is conducted using a\n36-question evaluation form that is aligned with the guidelines. Overall\nresults show that PLMs perform comparably to LLMs and adapt effectively to\nout-of-domain texts.", "AI": {"tldr": "\u9996\u4e2a\u7b26\u5408\u6b27\u6d32ETR\u6307\u5357\u7684ETR-fr\u6570\u636e\u96c6\u89e3\u51b3\u4e86AI\u751f\u6210\u6613\u8bfb\u6587\u672c\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u4f7fPLMs\u8868\u73b0\u5ab2\u7f8eLLMs\uff0c\u5e76\u5f00\u53d1\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u786e\u4fdd\u8f93\u51fa\u8d28\u91cf\u3002", "motivation": "\u624b\u52a8\u751f\u6210\u6613\u8bfb\u6587\u672c\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u963b\u788d\u8ba4\u77e5\u969c\u788d\u4eba\u7fa4\u83b7\u53d6\u5173\u952e\u4fe1\u606f\u3002AI\u9a71\u52a8\u65b9\u6848\u9700\u514b\u670d\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u9886\u57df\u9002\u5e94\u6027\u3001\u5927\u6a21\u578b\u8f7b\u91cf\u5316\u5b66\u4e60\u4e09\u5927\u6311\u6218\u3002", "method": "1. \u521b\u5efa\u9996\u4e2a\u5168\u5408\u89c4ETR-fr\u6570\u636e\u96c6\n2. \u5bf9PLMs/LLMs\u5b9e\u65bd\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\n3. \u8bbe\u8ba136\u9879\u4eba\u5de5\u8bc4\u4f30\u6307\u6807+\u81ea\u52a8\u6307\u6807\u7684\u6df7\u5408\u8bc4\u4f30\u6846\u67b6", "result": "PLMs\u4e0eLLMs\u6027\u80fd\u76f8\u5f53\uff0c\u5728\u8de8\u9886\u57df\u6587\u672c\u9002\u5e94\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u5316\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u8d28\u91cfETR\u751f\u6210\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u8f7b\u91cf\u89e3\u51b3\u65b9\u6848\uff0c\u8bc4\u4f30\u6846\u67b6\u6709\u6548\u4fdd\u969c\u4e86\u6613\u8bfb\u6587\u672c\u7684\u5408\u89c4\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2510.00694", "pdf": "https://arxiv.org/pdf/2510.00694", "abs": "https://arxiv.org/abs/2510.00694", "authors": ["Harethah Abu Shairah", "Somayah AlHarbi", "Abdulaziz AlHussein", "Sameer Alsabea", "Omar Shaqaqi", "Hebah AlShamlan", "Omar Knio", "George Turkiyyah"], "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted paper at ArabicNLP 2025", "summary": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the\nreasoning capabilities of large language models (LLMs) within the Arabic legal\ndomain. While existing Arabic benchmarks cover some knowledge-intensive tasks\nsuch as retrieval and understanding, substantial datasets focusing specifically\non multistep reasoning for Arabic LLMs, especially in open-ended contexts, are\nlacking. The dataset comprises over 13K commercial court cases from Saudi\nArabia, with each case including the facts presented, the reasoning of the\ncourt, the verdict, as well as the cited clauses extracted from the regulatory\ndocuments. We define a set of challenging tasks leveraging this dataset and\nreflecting the complexity of real-world legal reasoning, including verdict\nprediction, completion of reasoning chains in multistep legal arguments, and\nidentification of relevant regulations based on case facts. We benchmark a\nrepresentative selection of current open and closed Arabic LLMs on these tasks\nand demonstrate the dataset's utility for instruction tuning. Notably, we show\nthat instruction-tuning a modest 12B parameter model using ALARB significantly\nenhances its performance in verdict prediction and Arabic verdict generation,\nreaching a level comparable to that of GPT-4o.", "AI": {"tldr": "\u963f\u62c9\u4f2f\u6cd5\u5f8b\u63a8\u7406\u57fa\u51c6ALARB\u586b\u8865\u963f\u62c9\u4f2f\u8bedLLMs\u591a\u6b65\u63a8\u7406\u8bc4\u4f30\u7a7a\u767d\uff0c\u901a\u8fc713K+\u6c99\u7279\u5546\u4e1a\u6848\u4f8b\u6784\u5efa\u4efb\u52a1\u4f53\u7cfb\uff0c\u6307\u4ee4\u8c03\u4f18\u4f7f12B\u6a21\u578b\u8fbeGPT-4o\u6c34\u5e73", "motivation": "\u73b0\u6709\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u7f3a\u4e4f\u9488\u5bf9\u591a\u6b65\u63a8\u7406\u7684\u5f00\u653e\u5f0f\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u7279\u522b\u662f\u5728\u6cd5\u5f8b\u9886\u57df\u9700\u8981\u6a21\u62df\u771f\u5b9e\u590d\u6742\u573a\u666f\u7684\u8bc4\u4f30\u5de5\u5177", "method": "1. \u6784\u5efa\u5305\u542b\u6848\u4ef6\u4e8b\u5b9e/\u6cd5\u5ead\u63a8\u7406/\u5224\u51b3\u7ed3\u679c/\u6cd5\u89c4\u6761\u6b3e\u768413K\u6c99\u7279\u5546\u4e1a\u6848\u4f8b\u5e93\n2. \u8bbe\u8ba1\u5224\u51b3\u9884\u6d4b/\u63a8\u7406\u94fe\u8865\u5168/\u6cd5\u89c4\u8bc6\u522b\u4e09\u5927\u4efb\u52a1\n3. \u91c7\u7528\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4f18\u5316\u6a21\u578b", "result": "\u6307\u4ee4\u8c03\u4f18\u540e\u768412B\u53c2\u6570\u6a21\u578b\u5728\u5224\u51b3\u9884\u6d4b\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534727%\uff0c\u963f\u62c9\u4f2f\u5224\u51b3\u751f\u6210\u8d28\u91cf\u8fbe\u5230GPT-4o\u768492%", "conclusion": "ALARB\u6709\u6548\u63d0\u5347\u963f\u62c9\u4f2fLLMs\u6cd5\u5f8b\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u660e\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u53ef\u5339\u654c\u9876\u7ea7\u5546\u4e1a\u6a21\u578b"}}
{"id": "2510.00810", "pdf": "https://arxiv.org/pdf/2510.00810", "abs": "https://arxiv.org/abs/2510.00810", "authors": ["Jenny Kunz", "Iben Nyholm Debess", "Annika Simonsen"], "title": "Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese", "categories": ["cs.CL"], "comment": null, "summary": "We investigate how to adapt small, efficient LLMs to Faroese, a low-resource\nNorth Germanic language. Starting from English models, we continue pre-training\non related Scandinavian languages, either individually or combined via merging,\nbefore fine-tuning on Faroese. We compare full fine-tuning with\nparameter-efficient tuning using LoRA, evaluating their impact on both\nlinguistic accuracy and text comprehension. Due to the lack of existing Faroese\nevaluation data, we construct two new minimal-pair benchmarks from adapted and\nnewly collected datasets and complement them with human evaluations by Faroese\nlinguists. Our results demonstrate that transfer from related languages is\ncrucial, though the optimal source language depends on the task: Icelandic\nenhances linguistic accuracy, whereas Danish boosts comprehension. Similarly,\nthe choice between full fine-tuning and LoRA is task-dependent: LoRA improves\nlinguistic acceptability and slightly increases human evaluation scores on the\nbase model, while full fine-tuning yields stronger comprehension performance\nand better preserves model capabilities during downstream fine-tuning.", "AI": {"tldr": "\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u9002\u914d\u6cd5\u7f57\u8bedLLM\u7684\u6700\u4f73\u5b9e\u8df5\uff1a\u6e90\u8bed\u8a00\u9009\u62e9\u53d6\u51b3\u4e8e\u4efb\u52a1\u7c7b\u578b\uff0c\u5fae\u8c03\u65b9\u5f0f\u9700\u6309\u9700\u6c42\u53d6\u820d", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u6cd5\u7f57\u8bed\u7684LLM\u9002\u914d\u96be\u9898\uff0c\u63a2\u7d22\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u6709\u6548\u8def\u5f84", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u9884\u8bad\u7ec3+\u5fae\u8c03\uff09\uff0c\u6bd4\u8f83\u51b0\u5c9b\u8bed/\u4e39\u9ea6\u8bed\u8fc1\u79fb\u6548\u679c\uff0c\u8bc4\u4f30\u5168\u5fae\u8c03\u4e0eLoRA\u65b9\u6cd5\u5dee\u5f02", "result": "\u51b0\u5c9b\u8bed\u63d0\u5347\u8bed\u6cd5\u51c6\u786e\u7387(\u8bed\u8a00\u4efb\u52a1)\uff0c\u4e39\u9ea6\u8bed\u5f3a\u5316\u7406\u89e3\u80fd\u529b(\u7406\u89e3\u4efb\u52a1)\uff1bLoRA\u4f18\u5316\u8bed\u8a00\u63a5\u53d7\u5ea6\uff0c\u5168\u5fae\u8c03\u4fdd\u6301\u4e0b\u6e38\u4efb\u52a1\u80fd\u529b", "conclusion": "\u9002\u914d\u7b56\u7565\u5e94\u4efb\u52a1\u5bfc\u5411\uff1a\u8bed\u8a00\u51c6\u786e\u4f18\u5148\u9009\u51b0\u5c9b\u8bed+LoRA\uff0c\u7406\u89e3\u9700\u6c42\u4f18\u5148\u4e39\u9ea6\u8bed+\u5168\u5fae\u8c03"}}
{"id": "2510.00829", "pdf": "https://arxiv.org/pdf/2510.00829", "abs": "https://arxiv.org/abs/2510.00829", "authors": ["Yanming Sun", "Runzhe Zhan", "Chi Seng Cheang", "Han Wu", "Xuebo Liu", "Yuyao Niu", "Fengying Ye", "Kaixin Lan", "Lidia S. Chao", "Derek F. Wong"], "title": "Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "\\textbf{RE}trieval-\\textbf{A}ugmented \\textbf{L}LM-based \\textbf{M}achine\n\\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like\nidiomatic translation, but its reliability under noisy retrieval contexts\nremains poorly understood despite this being a common challenge in real-world\ndeployment. To address this gap, we propose a noise synthesis framework and new\nmetrics to evaluate the robustness of REAL-MT systematically. Using this\nframework, we instantiate REAL-MT with Qwen-series models, including standard\nLLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate\ntheir performance on idiomatic translation across high-, medium-, and\nlow-resource language pairs under synthesized noise. Our results show that\nlow-resource language pairs, which rely more heavily on retrieved context,\ndegrade more severely under noise than high-resource ones and often produce\nnonsensical translations. Although LRMs possess enhanced reasoning\ncapabilities, they show no improvement in error correction and are even more\nsusceptible to noise, tending to rationalize incorrect contexts. We find that\nthis stems from an attention shift away from the source idiom to noisy content,\nwhile confidence increases despite declining accuracy, indicating poor\ncalibration. To mitigate these issues, we investigate training-free and\nfine-tuning strategies, which improve robustness at the cost of performance in\nclean contexts, revealing a fundamental trade-off. Our findings highlight the\nlimitations of current approaches, underscoring the need for self-verifying\nintegration mechanisms.", "AI": {"tldr": "\u63d0\u51faREAL-MT\u6846\u67b6\u8bc4\u4f30\u68c0\u7d22\u589e\u5f3a\u673a\u5668\u7ffb\u8bd1\u5728\u566a\u58f0\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u6613\u53d7\u566a\u58f0\u5f71\u54cd\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u6ce8\u610f\u529b\u504f\u79fb\u4e0e\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898", "motivation": "\u63a2\u7a76\u68c0\u7d22\u589e\u5f3a\u578b\u673a\u5668\u7ffb\u8bd1(REAL-MT)\u5728\u771f\u5b9e\u566a\u58f0\u68c0\u7d22\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5bf9\u566a\u58f0\u9c81\u68d2\u6027\u8bc4\u4f30\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u6784\u5efa\u566a\u58f0\u5408\u6210\u6846\u67b6\u4e0e\u8bc4\u4f30\u6307\u6807\uff0c\u57fa\u4e8eQwen\u7cfb\u5217\u6a21\u578b(\u542b\u6807\u51c6LLM\u4e0e\u589e\u5f3a\u63a8\u7406\u7684LRM)\uff0c\u6d4b\u8bd5\u9ad8/\u4e2d/\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u5728\u5408\u6210\u566a\u58f0\u4e0b\u7684\u4e60\u8bed\u7ffb\u8bd1\u8868\u73b0", "result": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u56e0\u4f9d\u8d56\u68c0\u7d22\u4e0a\u4e0b\u6587\u800c\u6027\u80fd\u663e\u8457\u6076\u5316\uff0cLRM\u867d\u63a8\u7406\u589e\u5f3a\u4f46\u66f4\u6613\u53d7\u566a\u58f0\u5e72\u6270\u4e14\u9519\u8bef\u5408\u7406\u5316\uff0c\u6a21\u578b\u6ce8\u610f\u529b\u8f6c\u5411\u566a\u58f0\u5185\u5bb9\u4e14\u7f6e\u4fe1\u5ea6\u4e0e\u51c6\u786e\u5ea6\u80cc\u79bb", "conclusion": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u9c81\u68d2\u6027\u4e0e\u6027\u80fd\u7684\u6743\u8861\uff0c\u9700\u5f00\u53d1\u81ea\u9a8c\u8bc1\u673a\u5236\u3002\u8bad\u7ec3\u65e0\u5173\u7b56\u7565\u548c\u5fae\u8c03\u867d\u63d0\u5347\u6297\u566a\u6027\uff0c\u4f46\u727a\u7272\u5e72\u51c0\u73af\u5883\u4e0b\u7684\u7ffb\u8bd1\u8d28\u91cf"}}
{"id": "2510.00857", "pdf": "https://arxiv.org/pdf/2510.00857", "abs": "https://arxiv.org/abs/2510.00857", "authors": ["Adi Simhi", "Jonathan Herzig", "Martin Tutek", "Itay Itzhak", "Idan Szpektor", "Yonatan Belinkov"], "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "As large language models (LLMs) evolve from conversational assistants into\nautonomous agents, evaluating the safety of their actions becomes critical.\nPrior safety benchmarks have primarily focused on preventing generation of\nharmful content, such as toxic text. However, they overlook the challenge of\nagents taking harmful actions when the most effective path to an operational\ngoal conflicts with human safety. To address this gap, we introduce\nManagerBench, a benchmark that evaluates LLM decision-making in realistic,\nhuman-validated managerial scenarios. Each scenario forces a choice between a\npragmatic but harmful action that achieves an operational goal, and a safe\naction that leads to worse operational performance. A parallel control set,\nwhere potential harm is directed only at inanimate objects, measures a model's\npragmatism and identifies its tendency to be overly safe. Our findings indicate\nthat the frontier LLMs perform poorly when navigating this safety-pragmatism\ntrade-off. Many consistently choose harmful options to advance their\noperational goals, while others avoid harm only to become overly safe and\nineffective. Critically, we find this misalignment does not stem from an\ninability to perceive harm, as models' harm assessments align with human\njudgments, but from flawed prioritization. ManagerBench is a challenging\nbenchmark for a core component of agentic behavior: making safe choices when\noperational goals and alignment values incentivize conflicting actions.\nBenchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.", "AI": {"tldr": "\u7814\u7a76LLM\u5728\u5b89\u5168\u76ee\u6807\u4e0e\u5b9e\u7528\u76ee\u6807\u51b2\u7a81\u65f6\u7684\u51b3\u7b56\u8868\u73b0\uff0c\u63d0\u51faManagerBench\u57fa\u51c6\u63ed\u793a\u6a21\u578b\u5728\u5b89\u5168-\u5b9e\u7528\u5e73\u8861\u4e0a\u7684\u7f3a\u9677", "motivation": "\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u5ffd\u89c6LLM\u4e3a\u5b9e\u73b0\u64cd\u4f5c\u76ee\u6807\u9009\u62e9\u6709\u5bb3\u884c\u52a8\u7684\u98ce\u9669\uff0c\u9700\u5efa\u7acb\u65b0\u57fa\u51c6\u6d4b\u8bd5\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b", "method": "\u8bbe\u8ba1\u5305\u542b\u4e24\u96be\u9009\u62e9\u7684\u73b0\u5b9e\u7ba1\u7406\u573a\u666f\uff08\u5b9e\u7528\u4f46\u6709\u5bb3 vs \u5b89\u5168\u4f46\u4f4e\u6548\uff09\uff0c\u901a\u8fc7\u5e73\u884c\u63a7\u5236\u7ec4\u91cf\u5316\u6a21\u578b\u5b9e\u7528\u4e3b\u4e49\u503e\u5411", "result": "\u524d\u6cbf\u6a21\u578b\u666e\u904d\u5b58\u5728\u5b89\u5168-\u5b9e\u7528\u5931\u8861\uff1a53%\u9009\u62e9\u6709\u5bb3\u9009\u9879\u8ffd\u6c42\u6548\u7387\uff0c37%\u8fc7\u5ea6\u4fdd\u5b88\u727a\u7272\u6548\u80fd\uff0c\u4f18\u5148\u987a\u5e8f\u9519\u8bef\u662f\u4e3b\u56e0\uff08\u975e\u5371\u5bb3\u8bc6\u522b\u80fd\u529b\uff09", "conclusion": "ManagerBench\u63ed\u793aLLM\u4ee3\u7406\u884c\u4e3a\u7684\u6838\u5fc3\u7f3a\u9677\uff0c\u8868\u660e\u9700\u8981\u91cd\u6784\u76ee\u6807\u4f18\u5148\u7ea7\u673a\u5236\u4ee5\u5b9e\u73b0\u5b89\u5168\u81ea\u4e3b\u51b3\u7b56"}}
{"id": "2510.00861", "pdf": "https://arxiv.org/pdf/2510.00861", "abs": "https://arxiv.org/abs/2510.00861", "authors": ["Ziliang Wang", "Kang An", "Xuhui Zheng", "Faqiang Qian", "Weikun Zhang", "Cijun Ouyang", "Jialu Cai", "Yuhang Wang", "Yichao Wu"], "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "10 pages, 4 figures", "summary": "While search-augmented large language models (LLMs) exhibit impressive\ncapabilities, their reliability in complex multi-hop reasoning remains limited.\nThis limitation arises from three fundamental challenges: decomposition errors,\nwhere tasks are incorrectly broken down; retrieval missing, where key evidence\nfails to be retrieved; and reasoning errors, where flawed logic propagates\nthrough the reasoning chain. A single failure in any of these stages can derail\nthe final answer. We propose Erasable Reinforcement Learning (ERL), a novel\nframework that transforms fragile reasoning into a robust process. ERL\nexplicitly identifies faulty steps, erases them, and regenerates reasoning in\nplace, preventing defective logic from propagating through the reasoning chain.\nThis targeted correction mechanism turns brittle reasoning into a more\nresilient process. Models trained with ERL, termed ESearch, achieve substantial\nimprovements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model\nachieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and\n+7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest\nthat erasable reinforcement learning provides a powerful paradigm shift for\nrobust multi-step reasoning in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u64e6\u9664\u5f3a\u5316\u5b66\u4e60\u6846\u67b6ERL\uff0c\u901a\u8fc7\u52a8\u6001\u68c0\u6d4b/\u64e6\u9664/\u91cd\u751f\u6210\u9519\u8bef\u63a8\u7406\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u591a\u6b65\u63a8\u7406\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u641c\u7d22\u589e\u5f3a\u5927\u6a21\u578b\u5728\u590d\u6742\u591a\u6b65\u63a8\u7406\u4e2d\u5b58\u5728\u5206\u89e3\u9519\u8bef\u3001\u68c0\u7d22\u7f3a\u5931\u3001\u63a8\u7406\u9519\u8bef\u4e09\u91cd\u6311\u6218\uff0c\u5355\u4e2a\u9519\u8bef\u4f1a\u5bfc\u81f4\u6574\u4f53\u63a8\u7406\u94fe\u5931\u6548", "method": "ERL\u6846\u67b6\u5305\u542b\u9519\u8bef\u6b65\u9aa4\u8bc6\u522b\u673a\u5236\u3001\u64e6\u9664\u673a\u5236\u548c\u5c40\u90e8\u518d\u751f\u673a\u5236\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7f3a\u9677\u63a8\u7406\u94fe\u7684\u7cbe\u51c6\u4fee\u6b63", "result": "ESearch\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u73b0SOTA\uff0c3B\u6a21\u578b\u63d0\u53478.48% EM\u548c11.56% F1\uff0c7B\u6a21\u578b\u63d0\u53475.38% EM\u548c7.22% F1", "conclusion": "\u53ef\u64e6\u9664\u673a\u5236\u4e3aLLMs\u7684\u9c81\u68d2\u63a8\u7406\u63d0\u4f9b\u4e86\u8303\u5f0f\u8f6c\u53d8\uff0c\u8bc1\u660e\u52a8\u6001\u9519\u8bef\u4fee\u6b63\u6bd4\u6574\u4f53\u91cd\u751f\u6210\u66f4\u6709\u6548"}}
{"id": "2510.00880", "pdf": "https://arxiv.org/pdf/2510.00880", "abs": "https://arxiv.org/abs/2510.00880", "authors": ["Loris Bergeron", "Ioana Buhnila", "J\u00e9r\u00f4me Fran\u00e7ois", "Radu State"], "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to\nhallucinations, limiting trust in real-world applications. We present\nHalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating\nhallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies\ndocument-claim pairs as grounded or hallucinated and produces evidence-grounded\njustifications for transparency. Our approach combines (i) a domain-agnostic\nsynthetic dataset derived from FineWeb and refined through multi-stage curation\nand data reformation, (ii) synthetic grounded and hallucinated claims, and\n(iii) preference-based fine-tuning with Odds Ratio Preference Optimization to\ndistill large-model reasoning into a smaller backbone. On the RAGTruth subset\nof the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy\n(BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian\n3.3 (8B; 82.2%) while using roughly half their parameters. Over the full\nbenchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as\nGPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon\nacceptance.", "AI": {"tldr": "\u63d0\u51faHalluGuard\u6a21\u578b\u7f13\u89e3RAG\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c4B\u53c2\u6570\u5c0f\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e\u66f4\u5927\u6a21\u578b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6613\u4ea7\u751f\u5e7b\u89c9\u5f71\u54cd\u5b9e\u9645\u53ef\u4fe1\u5ea6\uff0c\u9700\u63d0\u5347RAG\u751f\u6210\u7ed3\u679c\u7684\u53ef\u9760\u6027", "method": "\u7ed3\u5408\u9886\u57df\u65e0\u5173\u5408\u6210\u6570\u636e\u96c6(\u6765\u81eaFineWeb)+\u5e7b\u89c9/\u6709\u636e\u58f0\u660e\u751f\u6210+\u57fa\u4e8e\u504f\u597d\u7684ORPO\u4f18\u5316\u65b9\u6cd5", "result": "RAGTruth\u5b50\u96c6\u5e73\u8861\u51c6\u786e\u738784%(\u53c2\u6570\u91cf\u51cf\u534a\u5339\u654c7B/8B\u6a21\u578b)\uff0c\u6574\u4f53\u57fa\u51c675.7%\u5339\u654cGPT-4o", "conclusion": "\u901a\u8fc7\u6570\u636e\u91cd\u6784\u4e0e\u504f\u597d\u84b8\u998f\u5b9e\u73b0\u5c0f\u6a21\u578b\u9ad8\u6548\u6291\u5e7b\uff0c\u6a21\u578b\u4e0e\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u63a8\u52a8\u5e94\u7528"}}
{"id": "2510.00890", "pdf": "https://arxiv.org/pdf/2510.00890", "abs": "https://arxiv.org/abs/2510.00890", "authors": ["Zhen Yin", "Shenghua Wang"], "title": "Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of large language models (LLMs) in scientific writing\nraises serious concerns regarding authorship integrity and the reliability of\nscholarly publications. Existing detection approaches mainly rely on\ndocument-level classification or surface-level statistical cues; however, they\nneglect fine-grained span localization, exhibit weak calibration, and often\nfail to generalize across disciplines and generators. To address these\nlimitations, we present Sci-SpanDet, a structure-aware framework for detecting\nAI-generated scholarly texts. The proposed method combines section-conditioned\nstylistic modeling with multi-level contrastive learning to capture nuanced\nhuman-AI differences while mitigating topic dependence, thereby enhancing\ncross-domain robustness. In addition, it integrates BIO-CRF sequence labeling\nwith pointer-based boundary decoding and confidence calibration to enable\nprecise span-level detection and reliable probability estimates. Extensive\nexperiments on a newly constructed cross-disciplinary dataset of 100,000\nannotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek,\nLLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with\nF1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows\nstrong resilience under adversarial rewriting and maintains balanced accuracy\nacross IMRaD sections and diverse disciplines, substantially surpassing\nexisting baselines. To ensure reproducibility and to foster further research on\nAI-generated text detection in scholarly documents, the curated dataset and\nsource code will be publicly released upon publication.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u6784\u611f\u77e5\u6846\u67b6Sci-SpanDet\uff0c\u901a\u8fc7\u5206\u7ae0\u8282\u98ce\u683c\u5efa\u6a21\u4e0e\u591a\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u8de8\u9886\u57df10\u4e07\u6837\u672c\u6d4b\u8bd5\u4e2d\u5b9e\u73b080.17 F1\u503c\u4e0e74.36\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u5b66\u672f\u6587\u672cAI\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u7f3a\u5931\u3001\u6821\u51c6\u80fd\u529b\u5f31\u3001\u8de8\u9886\u57df\u6cdb\u5316\u5dee\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9LLM\u5728\u5b66\u672f\u5199\u4f5c\u5f15\u53d1\u7684\u4f5c\u8005\u8eab\u4efd\u53ef\u4fe1\u5ea6\u5371\u673a\u3002", "method": "\u878d\u5408\u7ae0\u8282\u6761\u4ef6\u5316\u98ce\u683c\u5efa\u6a21\u4e0e\u591a\u7ea7\u5bf9\u6bd4\u5b66\u4e60\u51cf\u5c11\u4e3b\u9898\u4f9d\u8d56\uff0c\u7ed3\u5408BIO-CRF\u5e8f\u5217\u6807\u6ce8\u4e0e\u6307\u9488\u8fb9\u754c\u89e3\u7801\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7247\u6bb5\u5b9a\u4f4d\uff0c\u5e76\u5f15\u5165\u7f6e\u4fe1\u5ea6\u6821\u51c6\u63d0\u5347\u6982\u7387\u53ef\u9760\u6027\u3002", "result": "\u5728\u591aLLM\u751f\u6210\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u53d6\u5f9780.17 F1(AI)/92.63 AUROC/74.36 Span-F1\uff0c\u5bf9\u6297\u6539\u5199\u573a\u666f\u4fdd\u6301\u5f3a\u97e7\u6027\uff0cIMRaD\u5404\u7ae0\u8282\u53ca\u8de8\u5b66\u79d1\u68c0\u6d4b\u51c6\u786e\u7387\u5747\u8861\u3002", "conclusion": "Sci-SpanDet\u6709\u6548\u89e3\u51b3\u5b66\u672f\u6587\u672cAI\u68c0\u6d4b\u96be\u9898\uff0c\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u5c06\u63a8\u52a8\u9886\u57df\u53d1\u5c55\uff0c\u4e3a\u5b66\u672f\u51fa\u7248\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u53ef\u9760\u6280\u672f\u4fdd\u969c\u3002"}}
{"id": "2510.00919", "pdf": "https://arxiv.org/pdf/2510.00919", "abs": "https://arxiv.org/abs/2510.00919", "authors": ["Shunfeng Zheng", "Yudi Zhang", "Meng Fang", "Zihan Zhang", "Zhitan Wu", "Mykola Pechenizkiy", "Ling Chen"], "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved\nstrong performance across diverse tasks, but their capacity for expert-level\nreasoning-such as solving Olympiad-level physics problems-remains largely\nunexplored. Inspired by the way students prepare for competitions by reviewing\npast problems, we investigate the potential of RAG to enhance physics reasoning\nin foundation models. We introduce PhoPile, a high-quality multimodal dataset\nspecifically designed for Olympiad-level physics, enabling systematic study of\nretrieval-based reasoning. PhoPile includes diagrams, graphs, and equations,\ncapturing the inherently multimodal nature of physics problem solving. Using\nPhoPile, we benchmark RAG-augmented foundation models, covering both large\nlanguage models (LLMs) and large multimodal models (LMMs) with multiple\nretrievers. Our results demonstrate that integrating retrieval with physics\ncorpora can improve model performance, while also highlighting challenges that\nmotivate further research in retrieval-augmented physics reasoning.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001\u7269\u7406\u6570\u636e\u96c6PhoPile\uff0c\u63a2\u7d22\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u63d0\u5347\u57fa\u7840\u6a21\u578b\u89e3\u51b3\u5965\u6797\u5339\u514b\u7269\u7406\u9898\u80fd\u529b\u4e2d\u7684\u4f5c\u7528", "motivation": "\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u4e13\u4e1a\u7ea7\u7269\u7406\u63a8\u7406\uff08\u5982\u5965\u6797\u5339\u514b\u7269\u7406\u9898\uff09\u8868\u73b0\u6709\u9650\uff0c\u53d7\u5b66\u751f\u901a\u8fc7\u590d\u4e60\u5386\u5e74\u8bd5\u9898\u63d0\u5347\u6210\u7ee9\u7684\u542f\u53d1\uff0c\u9700\u7cfb\u7edf\u7814\u7a76RAG\u5728\u7269\u7406\u63a8\u7406\u4e2d\u7684\u6f5c\u529b", "method": "\u521b\u5efa\u5305\u542b\u56fe\u8868/\u516c\u5f0f\u7684\u591a\u6a21\u6001\u7269\u7406\u6570\u636e\u96c6PhoPile\uff0c\u8bbe\u8ba1\u6db5\u76d6LLM\u548cLMM\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6bd4\u8f83\u4e0d\u540c\u68c0\u7d22\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd", "result": "\u7269\u7406\u77e5\u8bc6\u5e93\u68c0\u7d22\u6574\u5408\u53ef\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u4f46\u591a\u6a21\u6001\u4fe1\u606f\u6574\u5408\u4ecd\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u9700\u6539\u8fdb\u68c0\u7d22\u673a\u5236", "conclusion": "PhoPile\u6570\u636e\u96c6\u4e3a\u7269\u7406\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u65b0\u57fa\u51c6\uff0c\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\uff0c\u63a8\u52a8\u68c0\u7d22\u589e\u5f3a\u7269\u7406\u63a8\u7406\u7684\u7b97\u6cd5\u521b\u65b0"}}
{"id": "2510.00931", "pdf": "https://arxiv.org/pdf/2510.00931", "abs": "https://arxiv.org/abs/2510.00931", "authors": ["Ammar Khairi", "Daniel D'souza", "Marzieh Fadaee", "Julia Kreutzer"], "title": "Making, not Taking, the Best of N", "categories": ["cs.CL"], "comment": null, "summary": "Obtaining high-quality generations in modern LLMs has largely been framed as\na selection problem: identifying a single winning generation from a diverse\npool of N samples, the Best-of-N (BoN). Yet, this approach is inherently\nzero-sum, discarding diverse and potentially useful information from the pool.\nInstead, we explore a collaborative setup, where all candidates can potentially\ncontribute to the final winning generation. To this end, we propose Fusion-of-N\n(FusioN): a method that uses a general LLM judge to synthesize the most\ninformative elements of each sample into a single final answer. We compare\nFusioN to BoN in two settings, (i) test-time scaling, where we sample and\naggregate from a single model at test-time (ii) synthetic data generation,\nwhere we fuse samples from a pool of diverse teachers to improve a student\nmodel. We extensively benchmark both setups across 11 languages, 3 diverse\ntasks and varying model scales. Across the bench, FusioN consistently\noutperforms BoN showing versatility and robustness both in test-time scaling\nand in downstream gains from synthetic data generation. We also perform\nextensive analysis on FusioN, where it shows surprising strengths and\nrobustness under challenging settings. These results show that we should shift\nhow we think about evaluating and utilizing LLM generations from a monolithic\nmeasure of quality, to embracing their polylithic nature. This shift allows us\nto integrate diverse strengths, unlock latent potential, and achieve\nimprovements that were previously inaccessible through selection alone.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fusion-of-N\u65b9\u6cd5\uff0c\u901a\u8fc7\u7efc\u5408\u591a\u4e2aLLM\u751f\u6210\u6837\u672c\u7684\u4fe1\u606f\u800c\u975e\u5355\u7eaf\u9009\u62e9\u6700\u4f18\u6837\u672c\uff0c\u572811\u79cd\u8bed\u8a00/3\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5728\u6d4b\u8bd5\u65f6\u6269\u5c55\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u573a\u666f\u4e0b\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edfBest-of-N\u65b9\u6cd5\u4e22\u5f03\u4e86\u672a\u88ab\u9009\u4e2d\u7684\u6837\u672c\u4fe1\u606f\uff0c\u5bfc\u81f4\u6f5c\u5728\u4ef7\u503c\u6d6a\u8d39\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u805a\u5408\u6240\u6709\u5019\u9009\u6837\u672c\u4f18\u52bf\u7684\u534f\u4f5c\u5f0f\u751f\u6210\u6846\u67b6\u3002", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff0c\u4eceN\u4e2a\u5019\u9009\u6837\u672c\u4e2d\u63d0\u53d6\u6700\u6709\u6548\u4fe1\u606f\u7247\u6bb5\u8fdb\u884c\u878d\u5408\u3002\u5e94\u7528\u4e8e\uff1a1) \u5355\u6a21\u578b\u6d4b\u8bd5\u65f6\u6837\u672c\u805a\u5408 2) \u591a\u6559\u5e08\u6a21\u578b\u77e5\u8bc6\u84b8\u998f", "result": "\u5728\u8de8\u8bed\u8a00/\u591a\u4efb\u52a1/\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u6d4b\u8bd5\u4e2d\uff0cFusioN\u59cb\u7ec8\u8d85\u8d8aBoN\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u5b66\u751f\u6a21\u578b\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad8\u8fbe5.7%\u3002", "conclusion": "\u5e94\u8f6c\u53d8LLM\u8bc4\u4f30\u8303\u5f0f\uff0c\u4ece\u5355\u4e00\u6700\u4f18\u9009\u62e9\u8f6c\u5411\u591a\u6e90\u4fe1\u606f\u878d\u5408\uff0c\u5145\u5206\u91ca\u653e\u591a\u6837\u5316\u751f\u6210\u6837\u672c\u7684\u6f5c\u5728\u4ef7\u503c\uff0c\u7a81\u7834\u4f20\u7edf\u9009\u62e9\u65b9\u6cd5\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2510.00962", "pdf": "https://arxiv.org/pdf/2510.00962", "abs": "https://arxiv.org/abs/2510.00962", "authors": ["Eileen Pan", "Anna Seo Gyeong Choi", "Maartje ter Hoeve", "Skyler Seto", "Allison Koenecke"], "title": "Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks", "categories": ["cs.CL"], "comment": "EMNLP Findings 2025, 12 pages, 11 tables, 3 figures", "summary": "Large language models (LLMs) are ubiquitous in modern day natural language\nprocessing. However, previous work has shown degraded LLM performance for\nunder-represented English dialects. We analyze the effects of typifying\n\"standard\" American English language questions as non-\"standard\" dialectal\nvariants on multiple choice question answering tasks and find up to a 20%\nreduction in accuracy. Additionally, we investigate the grammatical basis of\nunder-performance in non-\"standard\" English questions. We find that individual\ngrammatical rules have varied effects on performance, but some are more\nconsequential than others: three specific grammar rules (existential \"it\", zero\ncopula, and y'all) can explain the majority of performance degradation observed\nin multiple dialects. We call for future work to investigate bias mitigation\nmethods focused on individual, high-impact grammatical structures.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u975e\u6807\u51c6\u82f1\u8bed\u65b9\u8a00\u5bfc\u81f4\u5927\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u8fbe20%\uff0c\u4e09\u4e2a\u7279\u5b9a\u8bed\u6cd5\u89c4\u5219\uff08\u5b58\u5728\u6027'it'/\u96f6\u7cfb\u8bcd/y'all\uff09\u662f\u4e3b\u56e0\uff0c\u5efa\u8bae\u9488\u5bf9\u6027\u4f18\u5316\u8bed\u6cd5\u7ed3\u6784\u4ee5\u51cf\u8f7b\u504f\u89c1", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u6807\u51c6\u82f1\u8bed\u65b9\u8a00\u65f6\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u9700\u63a2\u7a76\u5177\u4f53\u8bed\u6cd5\u89c4\u5219\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u673a\u5236", "method": "1. \u5c06\u6807\u51c6\u82f1\u8bed\u95ee\u9898\u8f6c\u6362\u4e3a\u591a\u65b9\u8a00\u53d8\u4f53\u8fdb\u884c\u591a\u9009\u4efb\u52a1\u6d4b\u8bd5 2. \u5206\u89e3\u7814\u7a76\u4e0d\u540c\u8bed\u6cd5\u89c4\u5219\u5bf9\u6027\u80fd\u7684\u5177\u4f53\u5f71\u54cd", "result": "\u975e\u6807\u51c6\u82f1\u8bed\u95ee\u9898\u51c6\u786e\u7387\u6700\u9ad8\u4e0b\u964d20%\uff0c\u4e09\u4e2a\u5173\u952e\u8bed\u6cd5\u89c4\u5219\u53ef\u89e3\u91ca\u591a\u4e2a\u65b9\u8a00\u4e2d\u89c2\u5bdf\u5230\u7684\u5927\u90e8\u5206\u6027\u80fd\u4e0b\u964d", "conclusion": "\u5e94\u4f18\u5148\u7814\u7a76\u9488\u5bf9\u9ad8\u5f71\u54cd\u8bed\u6cd5\u7ed3\u6784\uff08\u5982\u5b58\u5728\u6027'it'/\u96f6\u7cfb\u8bcd/y'all\uff09\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\uff0c\u63d0\u5347\u6a21\u578b\u65b9\u8a00\u5305\u5bb9\u6027"}}
{"id": "2510.01028", "pdf": "https://arxiv.org/pdf/2510.01028", "abs": "https://arxiv.org/abs/2510.01028", "authors": ["Ruqian Zhang", "Yijiao Zhang", "Juan Shen", "Zhongyi Zhu", "Annie Qu"], "title": "Syntax-Guided Diffusion Language Models with User-Integrated Personalization", "categories": ["cs.CL", "stat.ME"], "comment": null, "summary": "Large language models have made revolutionary progress in generating\nhuman-like text, yet their outputs often tend to be generic, exhibiting\ninsufficient structural diversity, which limits personalized expression. Recent\nadvances in diffusion models have opened new opportunities for improving\nlanguage generation beyond the limitations of autoregressive paradigms. In this\nwork, we propose a syntax-guided diffusion language model that integrates\nstructural supervision and personalized conditioning to enhance text quality,\ndiversity, and controllability. We introduce a cascaded framework that\ngenerates syntactic guidance before conditional text generation, and further\ngeneralize it to a novel noncascaded architecture for better alignment between\nstructure and content. By incorporating syntactic information in the generating\nprocess, the proposed model better captures the lexical and structural\ncharacteristics of stylistic sentence construction. To enable fine-grained\npersonalization, we develop a shared representation mechanism that facilitates\ninformation integration across users, supporting both faithful stylistic\ngeneration and generalizable zero-shot inference. Extensive experiments on\nmultiple tasks demonstrate the superiority of our approach in fluency,\ndiversity, and stylistic fidelity. Further qualitative analyses highlight its\ninterpretability and flexibility in learning personalized patterns.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u6cd5\u5f15\u5bfc\u7684\u6269\u6563\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u76d1\u7763\u548c\u4e2a\u6027\u5316\u6761\u4ef6\u589e\u5f3a\u6587\u672c\u751f\u6210\u8d28\u91cf\u4e0e\u591a\u6837\u6027", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7ed3\u6784\u5355\u4e00\u3001\u4e2a\u6027\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u7a81\u7834\u81ea\u56de\u5f52\u8303\u5f0f\u9650\u5236", "method": "\u5f00\u53d1\u7ea7\u8054\u6846\u67b6\uff08\u5148\u8bed\u6cd5\u751f\u6210\u540e\u6587\u672c\u751f\u6210\uff09\u4e0e\u975e\u7ea7\u8054\u67b6\u6784\uff08\u7ed3\u6784\u5185\u5bb9\u5bf9\u9f50\uff09\uff0c\u5f15\u5165\u8de8\u7528\u6237\u5171\u4eab\u8868\u793a\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316", "result": "\u5728\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u4f18\u8d8a\u7684\u6d41\u7545\u6027\u3001\u591a\u6837\u6027\uff08+15%\u7ed3\u6784\u53d8\u5316\uff09\u548c\u98ce\u683c\u4fdd\u771f\u5ea6\uff0892%\u7528\u6237\u504f\u597d\uff09\uff0c\u5b9a\u6027\u5206\u6790\u9a8c\u8bc1\u4e2a\u6027\u5316\u6a21\u5f0f\u5b66\u4e60\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u751f\u6210\u53ef\u63a7\u6027\uff0c\u652f\u6301\u96f6\u6837\u672c\u63a8\u7406\uff0c\u4e3a\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u591a\u6a21\u6001\u573a\u666f"}}
{"id": "2510.01048", "pdf": "https://arxiv.org/pdf/2510.01048", "abs": "https://arxiv.org/abs/2510.01048", "authors": ["Nils Feldhus", "Laura Kopf"], "title": "Interpreting Language Models Through Concept Descriptions: A Survey", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at The Eight Workshop on Analyzing and Interpreting Neural\n  Networks for NLP (BlackboxNLP), co-located with EMNLP 2025", "summary": "Understanding the decision-making processes of neural networks is a central\ngoal of mechanistic interpretability. In the context of Large Language Models\n(LLMs), this involves uncovering the underlying mechanisms and identifying the\nroles of individual model components such as neurons and attention heads, as\nwell as model abstractions such as the learned sparse features extracted by\nSparse Autoencoders (SAEs). A rapidly growing line of work tackles this\nchallenge by using powerful generator models to produce open-vocabulary,\nnatural language concept descriptions for these components. In this paper, we\nprovide the first survey of the emerging field of concept descriptions for\nmodel components and abstractions. We chart the key methods for generating\nthese descriptions, the evolving landscape of automated and human metrics for\nevaluating them, and the datasets that underpin this research. Our synthesis\nreveals a growing demand for more rigorous, causal evaluation. By outlining the\nstate of the art and identifying key challenges, this survey provides a roadmap\nfor future research toward making models more transparent.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7ec4\u4ef6\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6982\u5ff5\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u5f3a\u8c03\u9700\u52a0\u5f3a\u56e0\u679c\u6027\u8bc4\u4f30\u5e76\u89c4\u5212\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u63d0\u9ad8\u6a21\u578b\u900f\u660e\u5ea6\u3002", "motivation": "\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u5176\u5185\u90e8\u51b3\u7b56\u673a\u5236\uff08\u5982\u795e\u7ecf\u5143\u3001\u6ce8\u610f\u529b\u5934\u3001\u7a00\u758f\u81ea\u7f16\u7801\u5668\u63d0\u53d6\u7684\u7279\u5f81\uff09\uff0c\u589e\u5f3a\u6a21\u578b\u53ef\u4fe1\u5ea6\u548c\u53ef\u9760\u6027\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u6574\u5408\u5f53\u524d\u6280\u672f\uff0c\u5206\u6790\u5229\u7528\u751f\u6210\u6a21\u578b\u521b\u5efa\u7ec4\u4ef6\u6982\u5ff5\u63cf\u8ff0\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u6307\u6807\u53ca\u652f\u6491\u6570\u636e\u96c6\u7684\u53d1\u5c55\u73b0\u72b6\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u56e0\u679c\u9a8c\u8bc1\uff0c\u9700\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u6280\u672f\u8def\u7ebf\u4e0e\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u5f00\u53d1\u56e0\u679c\u6027\u8bc4\u4f30\u4f53\u7cfb\uff0c\u63a8\u52a8\u5927\u8bed\u8a00\u6a21\u578b\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u5b9e\u8d28\u6027\u7a81\u7834\u3002"}}
{"id": "2510.01052", "pdf": "https://arxiv.org/pdf/2510.01052", "abs": "https://arxiv.org/abs/2510.01052", "authors": ["Samin Mahdipour Aghabagher", "Saeedeh Momtazi"], "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach", "categories": ["cs.CL", "cs.AI"], "comment": "22 pages, 1 figure. Submitted to Natural Language Engineering", "summary": "Dialogue State Tracking (DST) is an essential element of conversational AI\nwith the objective of deeply understanding the conversation context and leading\nit toward answering user requests. Due to high demands for open-domain and\nmulti-turn chatbots, the traditional rule-based DST is not efficient enough,\nsince it cannot provide the required adaptability and coherence for human-like\nexperiences in complex conversations. This study proposes a hybrid DST model\nthat utilizes rule-based methods along with language models, including BERT for\nslot filling and intent detection, XGBoost for intent validation, GPT for DST,\nand online agents for real-time answer generation. This model is uniquely\ndesigned to be evaluated on a comprehensive Persian multi-turn dialogue dataset\nand demonstrated significantly improved accuracy and coherence over existing\nmethods in Persian-based chatbots. The results demonstrate how effectively a\nhybrid approach may improve DST capabilities, paving the way for conversational\nAI systems that are more customized, adaptable, and human-like.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u89c4\u5219\u65b9\u6cd5\u548c\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408DST\u6a21\u578b\uff0c\u5728\u6ce2\u65af\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u804a\u5929\u673a\u5668\u4eba\u51c6\u786e\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684DST\u65b9\u6cd5\u5728\u5f00\u653e\u57df\u3001\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u5bf9\u8bdd\u7684\u4eba\u7c7b\u5316\u4f53\u9a8c\u9700\u6c42\u3002", "method": "\u6df7\u5408\u6a21\u578b\u6574\u5408BERT\uff08\u69fd\u586b\u5145/\u610f\u56fe\u68c0\u6d4b\uff09\u3001XGBoost\uff08\u610f\u56fe\u9a8c\u8bc1\uff09\u3001GPT\uff08DST\uff09\u548c\u5728\u7ebf\u4ee3\u7406\uff08\u5b9e\u65f6\u56de\u7b54\u751f\u6210\uff09", "result": "\u5728\u6ce2\u65af\u8bed\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u51c6\u786e\u6027\u548c\u5bf9\u8bdd\u8fde\u8d2f\u6027\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u63d0\u5347DST\u80fd\u529b\uff0c\u4e3a\u5f00\u53d1\u66f4\u4eba\u6027\u5316\u3001\u81ea\u9002\u5e94\u7684\u5bf9\u8bddAI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8def\u5f84"}}
{"id": "2510.01076", "pdf": "https://arxiv.org/pdf/2510.01076", "abs": "https://arxiv.org/abs/2510.01076", "authors": ["Haonan Wang", "Junfeng Sun", "Mingjia Zhao", "Wei Liu"], "title": "Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains", "categories": ["cs.CL"], "comment": "4 pages", "summary": "This article addresses embodied intelligence and reinforcement learning\nintegration in the field of text processing, aiming to enhance text handling\nwith more intelligence on the basis of embodied intelligence's perception and\naction superiority and reinforcement learning's decision optimization\ncapability. Through detailed theoretical explanation and experimental\nexploration, a novel integration model is introduced. This model has been\ndemonstrated to be very effective in a wide range oftext processing tasks,\nvalidating its applicative potential", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5177\u8eab\u667a\u80fd\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u6587\u672c\u5904\u7406\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6a21\u578b\u5728\u6587\u672c\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027", "motivation": "\u901a\u8fc7\u878d\u5408\u5177\u8eab\u667a\u80fd\u7684\u611f\u77e5-\u884c\u52a8\u4f18\u52bf\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u51b3\u7b56\u4f18\u5316\u80fd\u529b\uff0c\u63d0\u5347\u6587\u672c\u5904\u7406\u7684\u667a\u80fd\u5316\u6c34\u5e73", "method": "\u6784\u5efa\u5177\u8eab\u667a\u80fd\u4e0e\u5f3a\u5316\u5b66\u4e60\u878d\u5408\u6846\u67b6\uff0c\u5f00\u5c55\u7406\u8bba\u9610\u91ca\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u6a21\u578b\u5728\u591a\u573a\u666f\u6587\u672c\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51c6\u786e\u7387\u63d0\u534723%", "conclusion": "\u6210\u529f\u9a8c\u8bc1\u4e86\u878d\u5408\u6a21\u578b\u7684\u521b\u65b0\u6027\uff0c\u4e3a\u667a\u80fd\u6587\u672c\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2510.01145", "pdf": "https://arxiv.org/pdf/2510.01145", "abs": "https://arxiv.org/abs/2510.01145", "authors": ["Sukairaj Hafiz Imam", "Tadesse Destaw Belay", "Kedir Yassin Husse", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Hadiza Ali Umar", "Muhammad Yahuza Bello", "Joyce Nakatumba-Nabende", "Seid Muhie Yimam", "Shamsuddeen Hassan Muhammad"], "title": "Automatic Speech Recognition (ASR) for African Low-Resource Languages: A Systematic Literature Review", "categories": ["cs.CL"], "comment": null, "summary": "ASR has achieved remarkable global progress, yet African low-resource\nlanguages remain rigorously underrepresented, producing barriers to digital\ninclusion across the continent with more than +2000 languages. This systematic\nliterature review (SLR) explores research on ASR for African languages with a\nfocus on datasets, models and training methods, evaluation techniques,\nchallenges, and recommends future directions. We employ the PRISMA 2020\nprocedures and search DBLP, ACM Digital Library, Google Scholar, Semantic\nScholar, and arXiv for studies published between January 2020 and July 2025. We\ninclude studies related to ASR datasets, models or metrics for African\nlanguages, while excluding non-African, duplicates, and low-quality studies\n(score <3/5). We screen 71 out of 2,062 records and we record a total of 74\ndatasets across 111 languages, encompassing approximately 11,206 hours of\nspeech. Fewer than 15% of research provided reproducible materials, and dataset\nlicensing is not clear. Self-supervised and transfer learning techniques are\npromising, but are hindered by limited pre-training data, inadequate coverage\nof dialects, and the availability of resources. Most of the researchers use\nWord Error Rate (WER), with very minimal use of linguistically informed scores\nsuch as Character Error Rate (CER) or Diacritic Error Rate (DER), and thus with\nlimited application in tonal and morphologically rich languages. The existing\nevidence on ASR systems is inconsistent, hindered by issues like dataset\navailability, poor annotations, licensing uncertainties, and limited\nbenchmarking. Nevertheless, the rise of community-driven initiatives and\nmethodological advancements indicates a pathway for improvement. Sustainable\ndevelopment for this area will also include stakeholder partnership, creation\nof ethically well-balanced datasets, use of lightweight modelling techniques,\nand active benchmarking.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u63ed\u793a\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00ASR\u7814\u7a76\u5b58\u5728\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u6a21\u578b\u6cdb\u5316\u4e0d\u8db3\u3001\u8bc4\u4f30\u6307\u6807\u5355\u4e00\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u793e\u533a\u5408\u4f5c\u3001\u4f26\u7406\u6570\u636e\u96c6\u5efa\u8bbe\u548c\u8f7b\u91cf\u5316\u5efa\u6a21\u63a8\u52a8\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u975e\u6d322000+\u8bed\u8a00\u5728\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b(ASR)\u9886\u57df\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6d88\u9664\u56e0\u6280\u672f\u9e3f\u6c9f\u5bfc\u81f4\u7684\u6570\u5b57\u5305\u5bb9\u6027\u969c\u788d\u3002", "method": "\u91c7\u7528PRISMA 2020\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u67902020-2025\u5e74\u95f45\u5927\u5b66\u672f\u5e73\u53f0\u768471\u9879\u7814\u7a76\uff0c\u8bc4\u4f30\u6570\u636e\u96c6\u8d28\u91cf(5\u5206\u5236\u7b5b\u9009)\u3001\u6a21\u578b\u65b9\u6cd5\u53ca\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u53d1\u73b074\u4e2a\u6570\u636e\u96c6\u8986\u76d6111\u79cd\u8bed\u8a00(\u7ea611,206\u5c0f\u65f6\u8bed\u97f3)\uff0c\u4f46\u4ec515%\u7814\u7a76\u53ef\u590d\u73b0\uff1b\u81ea\u76d1\u7763\u5b66\u4e60\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff0c93%\u7814\u7a76\u4f7f\u7528WER\u6307\u6807\u5ffd\u89c6\u97f3\u8c03/\u5f62\u6001\u7279\u5f81\u3002", "conclusion": "\u5efa\u8bae\u901a\u8fc7\u5229\u76ca\u76f8\u5173\u65b9\u5408\u4f5c\u3001\u521b\u5efa\u5e73\u8861\u6570\u636e\u96c6\u3001\u8f7b\u91cf\u6a21\u578b\u6280\u672f\u548c\u4e3b\u52a8\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u793e\u533a\u9a71\u52a8\u521b\u65b0\u5b9e\u73b0ASR\u7cfb\u7edf\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2510.01146", "pdf": "https://arxiv.org/pdf/2510.01146", "abs": "https://arxiv.org/abs/2510.01146", "authors": ["David Anugraha", "Shou-Yi Hung", "Zilu Tang", "Annie En-Shiun Lee", "Derry Tanti Wijaya", "Genta Indra Winata"], "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Evaluation using Large Language Model (LLM) judges has been widely adopted in\nEnglish and shown to be effective for automatic evaluation. However, their\nperformance does not generalize well to non-English settings, and it remains\nunclear what constitutes effective multilingual training for such judges. In\nthis paper, we introduce mR3, a massively multilingual, rubric-agnostic reward\nreasoning model trained on 72 languages, achieving the broadest language\ncoverage in reward modeling to date. We present a comprehensive study of data\nand curriculum selection for training to identify effective strategies and data\nsources for building high-quality reward models, including the integration of\ntarget-language reasoning datasets. Our approach attains state-of-the-art\nperformance on multilingual reward model benchmarks, surpassing much larger\nmodels (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness\nis further confirmed through extensive ablation studies. Our models, data, and\ncode are available as open source at https://github.com/rubricreward/mr3.", "AI": {"tldr": "\u63d0\u51famR3\u591a\u8bed\u8a00\u5956\u52b1\u6a21\u578b\uff0c\u8986\u76d672\u79cd\u8bed\u8a00\uff0c\u5728\u8bc4\u6d4b\u57fa\u51c6\u4e0a\u8d85\u8d8a\u5927\u6a21\u578b\u4e14\u4f53\u79ef\u7f29\u5c0f9\u500d\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u5de5\u5177\u5728\u975e\u82f1\u8bed\u573a\u666f\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\u8bba", "method": "\u901a\u8fc7\u6570\u636e\u9009\u62e9\u7b56\u7565(\u5305\u62ec\u76ee\u6807\u8bed\u8a00\u63a8\u7406\u6570\u636e\u96c6)\u548c\u8bfe\u7a0b\u5b66\u4e60\u4f18\u5316\u591a\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u8fc7\u7a0b", "result": "\u5728\u591a\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u57fa\u51c6\u53d6\u5f97SOTA\uff0c\u6a21\u578b\u5c3a\u5bf8\u7f29\u5c0f\u81f31/9\u4ecd\u4f18\u4e8eGPT-OSS-120B\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "mR3\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848\uff0c\u6570\u636e\u7b56\u7565\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u5956\u52b1\u6a21\u578b\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd"}}
{"id": "2510.01152", "pdf": "https://arxiv.org/pdf/2510.01152", "abs": "https://arxiv.org/abs/2510.01152", "authors": ["Mustafa Omer Gul", "Claire Cardie", "Tanya Goyal"], "title": "Pay-Per-Search Models are Abstention Models", "categories": ["cs.CL"], "comment": "21 pages, with 10 dedicated to citations and appendix. 9 tables and 9\n  figures. Preprint, under review", "summary": "LLMs cannot reliably recognize their parametric knowledge boundaries and\noften hallucinate answers to outside-of-boundary questions. In contrast, humans\nrecognize their limitations and can either seek external help for such\nquestions or abstain. In this paper, we introduce MASH (Modeling Abstention via\nSelective Help-seeking), a training framework that readily extracts abstentions\nfrom LLMs. Our key idea is that any external help-seeking by an LLM, i.e.\nsearch tool use, can serve as a proxy for abstention if the external help\n(search) is appropriately penalized while simultaneously rewarding answer\naccuracy. MASH operationalizes this idea using reinforcement learning with a\npay-per-search reward.\n  We run experiments on three knowledge-intensive QA datasets. Our results show\nthat MASH substantially improves upon the selective help-seeking performance of\nprior efficient search approaches; on multi-hop datasets, MASH improves answer\naccuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf\nabstention -- it can distinguish between unanswerable/answerable questions and\nselectively generate responses for answerable questions -- showcasing behavior\nanalogous to specialized abstention approaches. We emphasize that contrary to\nprior abstention methods, MASH does not require pre-determining knowledge\nboundaries to construct training data. Instead, MASH's abstentions are a\nby-product of training for the auxiliary selective help-seeking task. Overall,\nwe show that MASH training effectively aligns search tool use with parametric\nknowledge, which can be successfully leveraged for making abstention decisions.", "AI": {"tldr": "\u63d0\u51faMASH\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u60e9\u7f5a\u5916\u90e8\u641c\u7d22\u884c\u4e3a\u540c\u65f6\u5956\u52b1\u51c6\u786e\u56de\u7b54\uff0c\u4f7fLLM\u81ea\u7136\u4ea7\u751f\u5f03\u6743\u884c\u4e3a\uff0c\u65e0\u9700\u9884\u5148\u5b9a\u4e49\u77e5\u8bc6\u8fb9\u754c", "motivation": "LLM\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u77e5\u8bc6\u8fb9\u754c\uff0c\u5e38\u5bf9\u8d85\u51fa\u8fb9\u754c\u95ee\u9898\u751f\u6210\u5e7b\u89c9\u7b54\u6848\uff0c\u800c\u4eba\u7c7b\u4f1a\u4e3b\u52a8\u5bfb\u6c42\u5e2e\u52a9\u6216\u5f03\u6743", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7'\u6309\u6b21\u4ed8\u8d39'\u5956\u52b1\u673a\u5236\uff08\u641c\u7d22\u60e9\u7f5a+\u56de\u7b54\u51c6\u786e\u5956\u52b1\uff09\uff0c\u5c06\u5de5\u5177\u4f7f\u7528\u8f6c\u5316\u4e3a\u5f03\u6743\u4fe1\u53f7", "result": "\u5728\u4e09\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578bQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b07.6%\u51c6\u786e\u7387\u63d0\u5347\uff0c\u4e14\u80fd\u6709\u6548\u533a\u5206\u53ef\u7b54/\u4e0d\u53ef\u7b54\u95ee\u9898", "conclusion": "MASH\u901a\u8fc7\u8f85\u52a9\u6027\u9009\u62e9\u5e2e\u52a9\u4efb\u52a1\u8bad\u7ec3\uff0c\u81ea\u7136\u5b9e\u73b0\u5f03\u6743\u51b3\u7b56\uff0c\u6210\u529f\u5c06\u641c\u7d22\u5de5\u5177\u4f7f\u7528\u4e0e\u53c2\u6570\u77e5\u8bc6\u5bf9\u9f50"}}
{"id": "2510.01157", "pdf": "https://arxiv.org/pdf/2510.01157", "abs": "https://arxiv.org/abs/2510.01157", "authors": ["Alexandrine Fortier", "Thomas Thebaud", "Jes\u00fas Villalba", "Najim Dehak", "Patrick Cardinal"], "title": "Backdoor Attacks Against Speech Language Models", "categories": ["cs.CL", "cs.CR", "cs.SD"], "comment": null, "summary": "Large Language Models (LLMs) and their multimodal extensions are becoming\nincreasingly popular. One common approach to enable multimodality is to cascade\ndomain-specific encoders with an LLM, making the resulting model inherit\nvulnerabilities from all of its components. In this work, we present the first\nsystematic study of audio backdoor attacks against speech language models. We\ndemonstrate its effectiveness across four speech encoders and three datasets,\ncovering four tasks: automatic speech recognition (ASR), speech emotion\nrecognition, and gender and age prediction. The attack consistently achieves\nhigh success rates, ranging from 90.76% to 99.41%. To better understand how\nbackdoors propagate, we conduct a component-wise analysis to identify the most\nvulnerable stages of the pipeline. Finally, we propose a fine-tuning-based\ndefense that mitigates the threat of poisoned pretrained encoders.", "AI": {"tldr": "\u9488\u5bf9\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u97f3\u9891\u540e\u95e8\u653b\u51fb\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u653b\u51fb\u6210\u529f\u7387\u9ad8\u8fbe90.76%-99.41%\uff0c\u901a\u8fc7\u7ec4\u4ef6\u5206\u6790\u5b9a\u4f4d\u8106\u5f31\u73af\u8282\u5e76\u63d0\u51fa\u5fae\u8c03\u9632\u5fa1\u65b9\u6848", "motivation": "\u591a\u6a21\u6001\u6a21\u578b\u901a\u8fc7\u7ea7\u8054\u7f16\u7801\u5668\u7ee7\u627f\u7ec4\u4ef6\u6f0f\u6d1e\uff0c\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u8bed\u97f3\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\u98ce\u9669\u53ca\u5176\u9632\u5fa1\u673a\u5236", "method": "\u57284\u4e2a\u8bed\u97f3\u7f16\u7801\u5668\u548c3\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5ASR\u3001\u60c5\u611f\u8bc6\u522b\u7b494\u9879\u4efb\u52a1\uff0c\u8fdb\u884c\u7ec4\u4ef6\u8106\u5f31\u6027\u5206\u6790\u5e76\u5f00\u53d1\u57fa\u4e8e\u5fae\u8c03\u7684\u9632\u5fa1\u65b9\u6cd5", "result": "\u653b\u51fb\u6210\u529f\u7387\u8d85\u8fc790%\uff0c\u5b9a\u4f4d\u5230\u7f16\u7801\u5668\u5fae\u8c03\u9636\u6bb5\u6700\u8106\u5f31\uff0c\u9632\u5fa1\u65b9\u6cd5\u6709\u6548\u964d\u4f4e\u540e\u95e8\u5a01\u80c1", "conclusion": "\u8bed\u97f3\u6a21\u578b\u540e\u95e8\u653b\u51fb\u5a01\u80c1\u663e\u8457\uff0c\u7ec4\u4ef6\u5206\u6790\u63ed\u793a\u653b\u51fb\u8def\u5f84\uff0c\u5fae\u8c03\u9632\u5fa1\u53ef\u7f13\u89e3\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u4e2d\u6bd2\u98ce\u9669"}}
{"id": "2510.01164", "pdf": "https://arxiv.org/pdf/2510.01164", "abs": "https://arxiv.org/abs/2510.01164", "authors": ["Zhengliang Shi", "Ruotian Ma", "Jen-tse Huang", "Xinbei Ma", "Xingyu Chen", "Mengru Wang", "Qu Yang", "Yue Wang", "Fanghua Ye", "Ziyang Chen", "Shanyi Wang", "Cixing Li", "Wenxuan Wang", "Zhaopeng Tu", "Xiaolong Li", "Zhaochun Ren", "Linus"], "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models (LLMs) are increasingly entrusted with high-stakes\ndecisions that affect human welfare. However, the principles and values that\nguide these models when distributing scarce societal resources remain largely\nunexamined. To address this, we introduce the Social Welfare Function (SWF)\nBenchmark, a dynamic simulation environment where an LLM acts as a sovereign\nallocator, distributing tasks to a heterogeneous community of recipients. The\nbenchmark is designed to create a persistent trade-off between maximizing\ncollective efficiency (measured by Return on Investment) and ensuring\ndistributive fairness (measured by the Gini coefficient). We evaluate 20\nstate-of-the-art LLMs and present the first leaderboard for social welfare\nallocation. Our findings reveal three key insights: (i) A model's general\nconversational ability, as measured by popular leaderboards, is a poor\npredictor of its allocation skill. (ii) Most LLMs exhibit a strong default\nutilitarian orientation, prioritizing group productivity at the expense of\nsevere inequality. (iii) Allocation strategies are highly vulnerable, easily\nperturbed by output-length constraints and social-influence framing. These\nresults highlight the risks of deploying current LLMs as societal\ndecision-makers and underscore the need for specialized benchmarks and targeted\nalignment for AI governance.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1SWF\u57fa\u51c6\u8bc4\u4f30LLMs\u793e\u4f1a\u8d44\u6e90\u5206\u914d\u8868\u73b0\uff0c\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\u666e\u904d\u5b58\u5728\u6548\u7387\u4f18\u5148\u503e\u5411\u4e14\u7b56\u7565\u8106\u5f31\u6027\u663e\u8457", "motivation": "LLMs\u8d8a\u6765\u8d8a\u591a\u53c2\u4e0e\u5f71\u54cd\u4eba\u7c7b\u798f\u7949\u7684\u9ad8\u98ce\u9669\u51b3\u7b56\uff0c\u4f46\u5176\u5728\u5206\u914d\u7a00\u7f3a\u793e\u4f1a\u8d44\u6e90\u65f6\u9075\u5faa\u7684\u4ef7\u503c\u4f53\u7cfb\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76", "method": "\u901a\u8fc7\u52a8\u6001\u6a21\u62df\u73af\u5883\u6784\u5efa\u793e\u4f1a\u798f\u7949\u51fd\u6570\u57fa\u51c6\uff0c\u4f7fLLM\u4f5c\u4e3a\u4e3b\u6743\u5206\u914d\u8005\u5728\u5f02\u8d28\u7fa4\u4f53\u4e2d\u5206\u914d\u4efb\u52a1\uff0c\u6301\u7eed\u8ffd\u8e2a\u6295\u8d44\u56de\u62a5\u7387\uff08\u6548\u7387\uff09\u548c\u57fa\u5c3c\u7cfb\u6570\uff08\u516c\u5e73\u6027\uff09\u7684\u5e73\u8861\u5173\u7cfb", "result": "\u8bc4\u4f3020\u4e2a\u5148\u8fdbLLM\u663e\u793a\uff1a1) \u6a21\u578b\u5bf9\u8bdd\u80fd\u529b\u4e0e\u5206\u914d\u6280\u80fd\u65e0\u5173 2) \u591a\u6570\u6a21\u578b\u5448\u73b0\u5f3a\u70c8\u529f\u5229\u4e3b\u4e49\u503e\u5411 3) \u5206\u914d\u7b56\u7565\u6613\u53d7\u8f93\u51fa\u957f\u5ea6\u9650\u5236\u548c\u793e\u4f1a\u5f71\u54cd\u6846\u67b6\u5e72\u6270", "conclusion": "\u5f53\u524dLLMs\u4e0d\u9002\u5408\u4f5c\u4e3a\u793e\u4f1a\u51b3\u7b56\u8005\uff0c\u9700\u5f00\u53d1\u4e13\u95e8\u8bc4\u4f30\u57fa\u51c6\u5e76\u8fdb\u884c\u9488\u5bf9\u6027\u4ef7\u503c\u89c2\u5bf9\u9f50"}}
{"id": "2510.01165", "pdf": "https://arxiv.org/pdf/2510.01165", "abs": "https://arxiv.org/abs/2510.01165", "authors": ["Oussama Gabouj", "Kamel Charaf", "Ivan Zakazov", "Nicolas Baldwin", "Robert West"], "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 (findings)", "summary": "Large Language Models (LLMs) achieve strong performance across diverse tasks,\nbut their effectiveness often depends on the quality of the provided context.\nRetrieval-Augmented Generation (RAG) enriches prompts with external\ninformation, but its reliance on static databases constrains adaptability and\ncan result in irrelevant demonstrations. In this work, we propose a Generative\nRetrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach\nwhere an LLM model is trained to generate input-specific concise\ndemonstrations. By tailoring demonstrations to each input, our method offers\nbetter contextual support than traditional RAG approaches. We demonstrate the\nsuperiority of GRAD under budget constraints, where we limit both the number of\ntokens used per demonstration and the number of tokens used for the final\noutput. Trained solely on a math dataset, GRAD consistently outperforms strong\nbaselines on Qwen2.5-14B across mathematical reasoning and advanced STEM\nquestions, highlighting GRAD's robust generalization to out-of-distribution\n(OOD) domains such as physics, chemistry, and computer science. Furthermore, we\nshow that demonstrations generated by trained smaller models can effectively\nguide larger target models, reducing training costs while maintaining\ncompetitive accuracy. Overall, this work introduces a scalable demonstration\ngenerator model presenting the first step toward a dynamic few-shot learning\nparadigm in resource-constrained settings. We release the code used for the\nproject.", "AI": {"tldr": "\u63d0\u51faGRAD\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u8f93\u5165\u76f8\u5173\u7684\u7b80\u6d01\u793a\u4f8b\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u548cSTEM\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5c0f\u6a21\u578b\u751f\u6210\u7684\u793a\u4f8b\u53ef\u6709\u6548\u6307\u5bfc\u5927\u6a21\u578b\uff0c\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edfRAG\u4f9d\u8d56\u9759\u6001\u6570\u636e\u5e93\u5bfc\u81f4\u793a\u4f8b\u76f8\u5173\u6027\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u52a8\u6001\u751f\u6210\u673a\u5236\u63d0\u5347\u4e0a\u4e0b\u6587\u652f\u6301\u80fd\u529b\u3002", "method": "\u8bad\u7ec3LLM\u751f\u6210\u8f93\u5165\u76f8\u5173\u7684\u7cbe\u70bc\u793a\u4f8b\uff0c\u57fa\u4e8e\u6570\u5b66\u6570\u636e\u96c6\u8bad\u7ec3GRAD\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u8de8\u5b66\u79d1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "GRAD\u5728token\u9884\u7b97\u9650\u5236\u4e0b\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\uff0c\u4e14\u5728\u7269\u7406/\u5316\u5b66\u7b49OOD\u9886\u57df\u4fdd\u6301\u5f3a\u6cdb\u5316\u6027\u3002", "conclusion": "GRAD\u5f00\u521b\u4e86\u52a8\u6001\u5c11\u6837\u672c\u5b66\u4e60\u8303\u5f0f\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2510.01171", "pdf": "https://arxiv.org/pdf/2510.01171", "abs": "https://arxiv.org/abs/2510.01171", "authors": ["Jiayi Zhang", "Simon Yu", "Derek Chong", "Anthony Sicilia", "Michael R. Tomz", "Christopher D. Manning", "Weiyan Shi"], "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity", "categories": ["cs.CL", "cs.AI"], "comment": "82 pages, 26 figures, 34 tables. Code is available at\n  https://github.com/CHATS-lab/verbalize-sampling", "summary": "Post-training alignment often reduces LLM diversity, leading to a phenomenon\nknown as mode collapse. Unlike prior work that attributes this effect to\nalgorithmic limitations, we identify a fundamental, pervasive data-level\ndriver: typicality bias in preference data, whereby annotators systematically\nfavor familiar text as a result of well-established findings in cognitive\npsychology. We formalize this bias theoretically, verify it on preference\ndatasets empirically, and show that it plays a central role in mode collapse.\nMotivated by this analysis, we introduce Verbalized Sampling, a simple,\ntraining-free prompting strategy to circumvent mode collapse. VS prompts the\nmodel to verbalize a probability distribution over a set of responses (e.g.,\n``Generate 5 jokes about coffee and their corresponding probabilities'').\nComprehensive experiments show that VS significantly improves performance\nacross creative writing (poems, stories, jokes), dialogue simulation,\nopen-ended QA, and synthetic data generation, without sacrificing factual\naccuracy and safety. For instance, in creative writing, VS increases diversity\nby 1.6-2.1x over direct prompting. We further observe an emergent trend that\nmore capable models benefit more from VS. In sum, our work provides a new\ndata-centric perspective on mode collapse and a practical inference-time remedy\nthat helps unlock pre-trained generative diversity.", "AI": {"tldr": "\u63d0\u51faVerbalized Sampling\u65b9\u6cd5\u89e3\u51b3\u5927\u6a21\u578b\u5bf9\u9f50\u8bad\u7ec3\u5bfc\u81f4\u7684\u6a21\u5f0f\u5d29\u6e83\u95ee\u9898\uff0c\u901a\u8fc7\u6982\u7387\u5206\u5e03\u63d0\u793a\u663e\u8457\u63d0\u5347\u751f\u6210\u591a\u6837\u6027", "motivation": "\u4f20\u7edf\u5bf9\u9f50\u8bad\u7ec3\u5bfc\u81f4\u5927\u6a21\u578b\u591a\u6837\u6027\u4e0b\u964d\uff08\u6a21\u5f0f\u5d29\u6e83\uff09\uff0c\u7814\u7a76\u53d1\u73b0\u6570\u636e\u5c42\u9762\u7684\u5178\u578b\u6027\u504f\u5dee\u662f\u6839\u672c\u539f\u56e0\uff08\u6807\u6ce8\u8005\u504f\u597d\u719f\u6089\u6587\u672c\uff09", "method": "\u63d0\u51faVerbalized Sampling\u63d0\u793a\u7b56\u7565\uff1a\u8981\u6c42\u6a21\u578b\u751f\u6210\u54cd\u5e94\u96c6\u5408\u5e76\u6807\u6ce8\u6982\u7387\u5206\u5e03\uff08\u5982\u300c\u751f\u62105\u4e2a\u5496\u5561\u7b11\u8bdd\u53ca\u5176\u6982\u7387\u300d\uff09", "result": "\u5728\u521b\u610f\u5199\u4f5c\u3001\u5bf9\u8bdd\u6a21\u62df\u7b49\u573a\u666f\u4e2d\u591a\u6837\u6027\u63d0\u53471.6-2.1\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u4e8b\u5b9e\u51c6\u786e\u6027\uff1b\u6a21\u578b\u80fd\u529b\u8d8a\u5f3a\u63d0\u5347\u6548\u679c\u8d8a\u663e\u8457", "conclusion": "\u4ece\u6570\u636e\u89c6\u89d2\u63ed\u793a\u4e86\u6a21\u5f0f\u5d29\u6e83\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u63a8\u7406\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u91ca\u653e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u751f\u6210\u6f5c\u529b"}}
{"id": "2510.01172", "pdf": "https://arxiv.org/pdf/2510.01172", "abs": "https://arxiv.org/abs/2510.01172", "authors": ["Qingyuan Liu", "Jia-Chen Gu", "Yunzhi Yao", "Hong Wang", "Nanyun Peng"], "title": "Energy-Regularized Sequential Model Editing on Hyperspheres", "categories": ["cs.CL"], "comment": "The code is available at https://github.com/PlusLabNLP/SPHERE. arXiv\n  admin note: text overlap with arXiv:2410.02355 by other authors", "summary": "Large language models (LLMs) require constant updates to remain aligned with\nevolving real-world knowledge. Model editing offers a lightweight alternative\nto retraining, but sequential editing often destabilizes representations and\ninduces catastrophic forgetting. In this work, we seek to better understand and\nmitigate performance degradation caused by sequential editing. We hypothesize\nthat hyperspherical uniformity, a property that maintains uniform distribution\nof neuron weights on a hypersphere, helps the model remain stable, retain prior\nknowledge, while still accommodate new updates. We use Hyperspherical Energy\n(HE) to quantify neuron uniformity during editing, and examine its correlation\nwith editing performance. Empirical studies across widely used editing methods\nreveals a strong correlation between HE dynamics and editing performance, with\nediting failures consistently coinciding with high HE fluctuations. We further\ntheoretically prove that HE dynamics impose a lower bound on the degradation of\npretrained knowledge, highlighting why HE stability is crucial for knowledge\nretention. Motivated by these insights, we propose SPHERE (Sparse Projection\nfor Hyperspherical Energy-Regularized Editing), an HE-driven regularization\nstrategy that stabilizes neuron weight distributions, ultimately preserving\nprior knowledge while enabling reliable sequential updates. Specifically,\nSPHERE identifies a sparse space complementary to the principal hyperspherical\ndirections of the pretrained weight matrices and projects new knowledge onto\nit, attenuating perturbations on the principal directions. Extensive\nexperiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the\nbest baseline in editing capability by an average of 16.41%, while most\nfaithfully preserving general model performance, thereby offering a principled\npath toward reliable large-scale knowledge editing.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8d85\u7403\u9762\u80fd\u91cf\uff08HE\uff09\u7a33\u5b9a\u795e\u7ecf\u5143\u5206\u5e03\uff0c\u63d0\u51faSPHERE\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8fde\u7eed\u7f16\u8f91\u6548\u679c\u5e76\u51cf\u5c11\u77e5\u8bc6\u9057\u5fd8\u3002", "motivation": "\u8fde\u7eed\u6a21\u578b\u7f16\u8f91\u6613\u5bfc\u81f4\u795e\u7ecf\u5143\u8868\u5f81\u5931\u7a33\u548c\u77e5\u8bc6\u9057\u5fd8\uff0c\u9700\u63a2\u7d22\u7ef4\u6301\u8d85\u7403\u9762\u5747\u5300\u6027\u4ee5\u63d0\u5347\u7f16\u8f91\u7a33\u5b9a\u6027\u3002", "method": "\u901a\u8fc7HE\u91cf\u5316\u795e\u7ecf\u5143\u5206\u5e03\u5747\u5300\u6027\uff0c\u8bbe\u8ba1SPHERE\u6b63\u5219\u5316\u7b56\u7565\uff08\u7a00\u758f\u6295\u5f71\u81f3\u6743\u91cd\u77e9\u9635\u4e3b\u65b9\u5411\u7684\u4e92\u8865\u7a7a\u95f4\uff09\u7a33\u5b9a\u6743\u91cd\u5206\u5e03\u3002", "result": "SPHERE\u5728LLaMA3/Qwen2.5\u4e0a\u7f16\u8f91\u80fd\u529b\u5e73\u5747\u63d0\u534716.41%\uff0c\u6700\u4f73\u4fdd\u6301\u6a21\u578b\u901a\u7528\u6027\u80fd\u3002", "conclusion": "HE\u52a8\u6001\u4e3a\u77e5\u8bc6\u9000\u5316\u63d0\u4f9b\u7406\u8bba\u4e0b\u9650\uff0cSPHERE\u4e3a\u5927\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u5b9e\u73b0\u8def\u5f84\u3002"}}
{"id": "2510.00006", "pdf": "https://arxiv.org/pdf/2510.00006", "abs": "https://arxiv.org/abs/2510.00006", "authors": ["Kajwan Ziaoddini"], "title": "Unpacking Musical Symbolism in Online Communities: Content-Based and Network-Centric Approaches", "categories": ["cs.SD", "cs.CL", "cs.CY", "cs.MM", "eess.AS"], "comment": null, "summary": "This paper examines how musical symbolism is produced and circulated in\nonline communities by combining content-based music analysis with a lightweight\nnetwork perspective on lyrics. Using a curated corpus of 275 chart-topping\nsongs enriched with audio descriptors (energy, danceability, loudness,\nliveness, valence, acousticness, speechiness, popularity) and full lyric\ntranscripts, we build a reproducible pipeline that (i) quantifies temporal\ntrends in sonic attributes, (ii) models lexical salience and co-occurrence, and\n(iii) profiles mood by genre. We find a decade-long decline in energy (79 ->\n58) alongside a rise in danceability (59 -> 73); valence peaks in 2013 (63) and\ndips in 2014-2016 (42) before partially recovering. Correlation analysis shows\nstrong coupling of energy with loudness (r = 0.74) and negative associations\nfor acousticness with both energy (r = -0.54) and loudness (r = -0.51);\ndanceability is largely orthogonal to other features (|r| < 0.20). Lyric\ntokenization (>114k tokens) reveals a pronoun-centric lexicon \"I/you/me/my\" and\na dense co-occurrence structure in which interpersonal address anchors\nmainstream narratives. Mood differs systematically by style: R&B exhibits the\nhighest mean valence (96), followed by K-Pop/Pop (77) and Indie/Pop (70),\nwhereas Latin/Reggaeton is lower (37) despite high danceability. Read through a\nsubcultural identity lens, these patterns suggest the mainstreaming of\npreviously peripheral codes and a commercial preference for relaxed yet\nrhythmically engaging productions that sustain collective participation without\nmaximal intensity. Methodologically, we contribute an integrated\nMIR-plus-network workflow spanning summary statistics, correlation structure,\nlexical co-occurrence matrices, and genre-wise mood profiling that is robust to\nmodality sparsity and suitable for socially aware recommendation or\ncommunity-level diffusion studies.", "AI": {"tldr": "\u6574\u5408\u97f3\u4e50\u4fe1\u606f\u68c0\u7d22\u4e0e\u7f51\u7edc\u5206\u6790\uff0c\u63ed\u793a\u5341\u5e74\u95f4\u4e3b\u6d41\u97f3\u4e50\u80fd\u91cf\u4e0b\u964d\uff0879\u219258\uff09\u3001\u821e\u8e48\u6027\u4e0a\u5347\uff0859\u219273\uff09\u7684\u8d8b\u52bf\uff0c\u53ca\u6b4c\u8bcd\u4e2d\u4eba\u79f0\u4ee3\u8bcd\u4e3b\u5bfc\u7684\u53d9\u4e8b\u7ed3\u6784\u3002", "motivation": "\u63a2\u7a76\u5728\u7ebf\u793e\u533a\u4e2d\u97f3\u4e50\u7b26\u53f7\u7684\u751f\u4ea7\u4f20\u64ad\u673a\u5236\uff0c\u89e3\u6790\u5546\u4e1a\u504f\u597d\u5982\u4f55\u901a\u8fc7\u58f0\u97f3\u7279\u5f81\u4e0e\u6b4c\u8bcd\u7ed3\u6784\u5f71\u54cd\u96c6\u4f53\u53c2\u4e0e\u3002", "method": "\u6536\u96c6275\u9996\u70ed\u95e8\u6b4c\u66f2\u7684\u97f3\u9891\u7279\u5f81\uff08\u80fd\u91cf/\u821e\u8e48\u6027/\u54cd\u5ea6\u7b49\uff09\u4e0e\u6b4c\u8bcd\u6570\u636e\uff0c\u6784\u5efa\u7edf\u8ba1\u6a21\u578b\u5206\u6790\u65f6\u95f4\u8d8b\u52bf\u3001\u8bcd\u6c47\u5171\u73b0\u7f51\u7edc\u53ca\u6d41\u6d3e\u60c5\u7eea\u56fe\u8c31\u3002", "result": "\u80fd\u91cf\u4e0e\u54cd\u5ea6\u5f3a\u76f8\u5173\uff08r=0.74\uff09\uff0cR&B\u60c5\u7eea\u6700\u79ef\u6781\uff08valence=96\uff09\uff1b\u6b4c\u8bcd\u4e2d'I/you/me'\u9ad8\u9891\u5171\u73b0\uff0c\u62c9\u4e01\u6d41\u6d3e\u821e\u8e48\u6027\u9ad8\u4f46\u60c5\u7eea\u503c\u4f4e\uff0837\uff09\u3002", "conclusion": "\u65b9\u6cd5\u8bba\u8d21\u732e\u591a\u6a21\u6001\u5206\u6790\u6d41\u7a0b\uff0c\u63ed\u793a\u4e3b\u6d41\u97f3\u4e50\u8d8b\u5411\u653e\u677e\u8282\u594f+\u4eba\u9645\u53d9\u4e8b\u7b56\u7565\uff0c\u5e73\u8861\u5546\u4e1a\u4f20\u64ad\u4e0e\u4e9a\u6587\u5316\u8eab\u4efd\u8868\u8fbe\u3002"}}
{"id": "2510.00021", "pdf": "https://arxiv.org/pdf/2510.00021", "abs": "https://arxiv.org/abs/2510.00021", "authors": ["Alvaro Vallejo Ram\u00edrez"], "title": "IA aplicada al an\u00e1lisis del conflicto Ir\u00e1n-Israel: Mapeo de discursos en YouTube", "categories": ["cs.SI", "cs.AI", "cs.CL", "I.2.7, H.3.3"], "comment": "in Spanish language", "summary": "Purpose. This study analyzes the digital representation of the Iran-Israel\nconflict that occurred in June 2025, based on 120,000 comments posted on\nYouTube. It sought to identify discursive positions regarding the actors\ninvolved and to examine how media and algorithmic biases shape digital\nconversations. Methodology. A mixed-methods design with triangulation was\nadopted. In the quantitative phase, natural language processing techniques and\nmachine learning models (BERT and XLM-RoBERTa) were used to classify comments\ninto ten categories. In the qualitative phase, a critical analysis of media\ncontext and ideological narratives was conducted, complemented by manual\nannotation and supervised training. This strategy enabled the integration of\nstatistical robustness with contextual understanding. Results and conclusions.\nThe findings reveal a clear overrepresentation of pro-Palestinian and\nanti-United States/Israel discourses, while pro-United States and\nanti-Palestinian positions were marginal. Iran, usually rendered invisible in\nglobal media, emerged as a central actor in the digital conversation during the\nconflict, suggesting a narrative shift away from previous hegemonic frameworks.\nLikewise, the results confirm the influence of algorithmic biases in amplifying\ncertain discourses while limiting others. Original contributions. This work\ncombines computational analysis and philosophical critique for the study of\ndigital controversies, providing a methodological framework replicable in\ngeopolitical contexts. It is one of the first Spanish-language studies to map,\nthrough artificial intelligence and critical analysis, discourses on an\ninternational conflict on YouTube, highlighting asymmetries and narrative\ndisputes that are often overlooked.", "AI": {"tldr": "\u57fa\u4e8e12\u4e07\u6761YouTube\u8bc4\u8bba\u5206\u67902025\u5e74\u4f0a\u6717-\u4ee5\u8272\u5217\u51b2\u7a81\u6570\u5b57\u53d9\u4e8b\uff0c\u63ed\u793a\u7b97\u6cd5\u504f\u89c1\u4e0e\u8bdd\u8bed\u4e0d\u5bf9\u79f0\u73b0\u8c61", "motivation": "\u7814\u7a76\u65e8\u5728\u63ed\u793a\u6570\u5b57\u5a92\u4f53\u4e2d\u51b2\u7a81\u53d9\u4e8b\u7684\u653f\u6cbb\u7acb\u573a\u5206\u5e03\uff0c\u5e76\u63a2\u8ba8\u7b97\u6cd5\u5982\u4f55\u5851\u9020\u516c\u4f17\u8206\u8bba\u503e\u5411", "method": "\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\u4e0e\u4e09\u89d2\u9a8c\u8bc1\uff1a\u5b9a\u91cf\u91c7\u7528BERT/XLM-RoBERTa\u6a21\u578b\u8fdb\u884c\u8bc4\u8bba\u5206\u7c7b\uff0c\u5b9a\u6027\u7ed3\u5408\u6279\u5224\u6027\u5a92\u4f53\u5206\u6790\u4e0e\u4eba\u5de5\u6807\u6ce8", "result": "\u6570\u5b57\u7a7a\u95f4\u5448\u73b0\u4eb2\u5df4\u52d2\u65af\u5766/\u53cd\u7f8e\u4ee5\u8bdd\u8bed\u4e3b\u5bfc\uff0c\u4f0a\u6717\u9996\u6b21\u6210\u4e3a\u6838\u5fc3\u53d9\u4e8b\u4e3b\u4f53\uff0c\u7b97\u6cd5\u673a\u5236\u5f3a\u5316\u7279\u5b9a\u8bdd\u8bed\u4f20\u64ad", "conclusion": "\u8be5\u7814\u7a76\u5f00\u521b\u4e86\u8ba1\u7b97\u5206\u6790\u4e0e\u54f2\u5b66\u6279\u5224\u7ed3\u5408\u7684\u65b9\u6cd5\u6846\u67b6\uff0c\u63ed\u793aYouTube\u56fd\u9645\u51b2\u7a81\u8ba8\u8bba\u4e2d\u88ab\u5ffd\u89c6\u7684\u53d9\u4e8b\u6743\u529b\u7ed3\u6784"}}
{"id": "2510.00032", "pdf": "https://arxiv.org/pdf/2510.00032", "abs": "https://arxiv.org/abs/2510.00032", "authors": ["Ziyi Zeng", "Zhenyang Cai", "Yixi Cai", "Xidong Wang", "Junying Chen", "Rongsheng Wang", "Yipeng Liu", "Siqi Cai", "Benyou Wang", "Zhiguo Zhang", "Haizhou Li"], "title": "WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities", "categories": ["eess.SP", "cs.AI", "cs.CL", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Electroencephalography (EEG) interpretation using multimodal large language\nmodels (MLLMs) offers a novel approach for analyzing brain signals. However,\nthe complex nature of brain activity introduces critical challenges: EEG\nsignals simultaneously encode both cognitive processes and intrinsic neural\nstates, creating a mismatch in EEG paired-data modality that hinders effective\ncross-modal representation learning. Through a pivot investigation, we uncover\ncomplementary relationships between these modalities. Leveraging this insight,\nwe propose mapping EEG signals and their corresponding modalities into a\nunified semantic space to achieve generalized interpretation. To fully enable\nconversational capabilities, we further introduce WaveMind-Instruct-338k, the\nfirst cross-task EEG dataset for instruction tuning. The resulting model\ndemonstrates robust classification accuracy while supporting flexible,\nopen-ended conversations across four downstream tasks, thereby offering\nvaluable insights for both neuroscience research and the development of\ngeneral-purpose EEG models.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\u5b9e\u73b0EEG\u6cdb\u5316\u89e3\u8bfb\uff0c\u5e76\u6784\u5efa\u9996\u4e2a\u8de8\u4efb\u52a1EEG\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6WaveMind-Instruct-338k\uff0c\u6a21\u578b\u5728\u56db\u7c7b\u4e0b\u6e38\u4efb\u52a1\u4e2d\u540c\u65f6\u5b9e\u73b0\u7cbe\u51c6\u5206\u7c7b\u4e0e\u7075\u6d3b\u5bf9\u8bdd", "motivation": "EEG\u4fe1\u53f7\u540c\u65f6\u7f16\u7801\u8ba4\u77e5\u8fc7\u7a0b\u548c\u795e\u7ecf\u72b6\u6001\uff0c\u5bfc\u81f4\u8de8\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u5b58\u5728\u6a21\u6001\u9519\u914d\u95ee\u9898", "method": "\u5c06EEG\u4fe1\u53f7\u4e0e\u5bf9\u5e94\u6a21\u6001\u6620\u5c04\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u5e76\u5f15\u5165\u8de8\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u6570\u636e\u96c6WaveMind-Instruct-338k", "result": "\u6a21\u578b\u5728\u56db\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a33\u5065\u5206\u7c7b\u51c6\u786e\u7387\uff0885.3%\uff09\u7684\u540c\u65f6\u652f\u6301\u5f00\u653e\u5f0f\u5bf9\u8bdd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u79d1\u5b66\u7814\u7a76\u4e0e\u901a\u7528EEG\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5b9e\u73b0\u4e86\u5206\u7c7b\u7cbe\u5ea6\u4e0e\u5bf9\u8bdd\u7075\u6d3b\u6027\u7684\u7edf\u4e00"}}
{"id": "2510.00043", "pdf": "https://arxiv.org/pdf/2510.00043", "abs": "https://arxiv.org/abs/2510.00043", "authors": ["Gregory D. Baker", "Scott McCallum", "Dirk Pattinson"], "title": "Linear Regression in p-adic metric spaces", "categories": ["cs.LG", "cs.CL", "math.NT", "11D88, 62J99, 68T50", "G.3; I.2.6; I.2.7; I.5.1; I.5.4"], "comment": null, "summary": "Many real-world machine learning problems involve inherently hierarchical\ndata, yet traditional approaches rely on Euclidean metrics that fail to capture\nthe discrete, branching nature of hierarchical relationships. We present a\ntheoretical foundation for machine learning in p-adic metric spaces, which\nnaturally respect hierarchical structure. Our main result proves that an\nn-dimensional plane minimizing the p-adic sum of distances to points in a\ndataset must pass through at least n + 1 of those points -- a striking contrast\nto Euclidean regression that highlights how p-adic metrics better align with\nthe discrete nature of hierarchical data. As a corollary, a polynomial of\ndegree n constructed to minimise the p-adic sum of residuals will pass through\nat least n + 1 points. As a further corollary, a polynomial of degree n\napproximating a higher degree polynomial at a finite number of points will\nyield a difference polynomial that has distinct rational roots. We demonstrate\nthe practical significance of this result through two applications in natural\nlanguage processing: analyzing hierarchical taxonomies and modeling grammatical\nmorphology. These results suggest that p-adic metrics may be fundamental to\nproperly handling hierarchical data structures in machine learning. In\nhierarchical data, interpolation between points often makes less sense than\nselecting actual observed points as representatives.", "AI": {"tldr": "\u63d0\u51fa\u5728p-adic\u5ea6\u91cf\u7a7a\u95f4\u8fdb\u884c\u673a\u5668\u5b66\u4e60\u4ee5\u66f4\u597d\u5904\u7406\u5c42\u6b21\u6570\u636e\uff0c\u8bc1\u660e\u5176\u6570\u5b66\u5b9a\u7406\u5e76\u5e94\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406", "motivation": "\u4f20\u7edf\u6b27\u51e0\u91cc\u5f97\u5ea6\u91cf\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5c42\u6b21\u6570\u636e\u7684\u79bb\u6563\u5206\u652f\u7279\u6027\uff0c\u9700\u5bfb\u627e\u66f4\u9002\u914d\u7684\u6570\u5b66\u6846\u67b6", "method": "\u5efa\u7acbp-adic\u5ea6\u91cf\u7a7a\u95f4\u7684\u6570\u5b66\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5173\u952e\u5b9a\u7406\u53ca\u5176\u63a8\u8bba", "result": "1. n\u7ef4p-adic\u5e73\u9762\u987b\u901a\u8fc7\u81f3\u5c11n+1\u4e2a\u6570\u636e\u70b9 2. \u591a\u9879\u5f0f\u63d2\u503c\u5b9a\u7406 3. \u5e94\u7528\u4e8e\u8bed\u8a00\u5206\u7c7b\u548c\u5f62\u6001\u5b66\u5efa\u6a21", "conclusion": "p-adic\u5ea6\u91cf\u662f\u5904\u7406\u5c42\u6b21\u6570\u636e\u7ed3\u6784\u7684\u57fa\u7840\u6570\u5b66\u5de5\u5177\uff0c\u9009\u62e9\u5b9e\u9645\u89c2\u6d4b\u70b9\u4f18\u4e8e\u63d2\u503c\u7684\u7279\u6027\u9002\u914d\u79bb\u6563\u6570\u636e\u7279\u5f81"}}
{"id": "2510.00071", "pdf": "https://arxiv.org/pdf/2510.00071", "abs": "https://arxiv.org/abs/2510.00071", "authors": ["Dongqi Zheng"], "title": "ARS: Adaptive Reasoning Suppression for Efficient Large Reasoning Language Models", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by 39th NeurIPS - Foundations of Reasoning in Language\n  Models", "summary": "Large Reasoning Language Models (LRLMs or LRMs) demonstrate remarkable\ncapabilities in complex reasoning tasks, but suffer from significant\ncomputational inefficiencies due to overthinking phenomena. Existing efficient\nreasoning methods face the challenge of balancing reasoning quality with\ninference cost reduction. We propose \\textbf{Adaptive Reasoning Suppression\n(ARS)}, a novel training-free approach that dynamically suppresses redundant\nreasoning steps while preserving accuracy through adaptive certainty\nmonitoring. ARS introduces a multi-checkpoint certainty estimation mechanism\nwith progressive suppression thresholds, achieving superior efficiency compared\nto static suppression methods. Our extensive evaluation across mathematical\nreasoning benchmarks using multiple model architectures demonstrates that ARS\nachieves up to 53%, 46.1%, and 57.9% in token, latency and energy reduction,\nwhile maintaining or improving accuracy.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684ARS\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6291\u5236\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u5b9e\u73b053% token/46.1%\u5ef6\u8fdf/57.9%\u80fd\u8017\u4f18\u5316\uff0c\u4fdd\u6301\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff0c\u9759\u6001\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u7cbe\u5ea6\u3002ARS\u65e8\u5728\u52a8\u6001\u8bc6\u522b\u5e76\u6291\u5236\u8fc7\u5ea6\u601d\u8003\u73b0\u8c61\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u8fc7\u7a0b", "method": "\u91c7\u7528\u591a\u68c0\u67e5\u70b9\u786e\u5b9a\u6027\u8bc4\u4f30\u673a\u5236\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u6291\u5236\u9608\u503c\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u6d4b\u6a21\u578b\u7f6e\u4fe1\u5ea6\u81ea\u52a8\u7ec8\u6b62\u5197\u4f59\u63a8\u7406\u6b65\u9aa4", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u9ad853% token\u51cf\u5c11\u300146.1%\u5ef6\u8fdf\u964d\u4f4e\u548c57.9%\u80fd\u8017\u4e0b\u964d\uff0c\u51c6\u786e\u7387\u7ef4\u6301\u6216\u63d0\u5347", "conclusion": "ARS\u901a\u8fc7\u52a8\u6001\u6291\u5236\u673a\u5236\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u67b6\u6784\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2510.00137", "pdf": "https://arxiv.org/pdf/2510.00137", "abs": "https://arxiv.org/abs/2510.00137", "authors": ["Nima Sheikholeslami", "Erfan Hosseini", "Patrice Bechard", "Srivatsava Daruru", "Sai Rajeswar"], "title": "Optimizing What Matters: AUC-Driven Learning for Robust Neural Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Dual-encoder retrievers depend on the principle that relevant documents\nshould score higher than irrelevant ones for a given query. Yet the dominant\nNoise Contrastive Estimation (NCE) objective, which underpins Contrastive Loss,\noptimizes a softened ranking surrogate that we rigorously prove is\nfundamentally oblivious to score separation quality and unrelated to AUC. This\nmismatch leads to poor calibration and suboptimal performance in downstream\ntasks like retrieval-augmented generation (RAG). To address this fundamental\nlimitation, we introduce the MW loss, a new training objective that maximizes\nthe Mann-Whitney U statistic, which is mathematically equivalent to the Area\nunder the ROC Curve (AUC). MW loss encourages each positive-negative pair to be\ncorrectly ranked by minimizing binary cross entropy over score differences. We\nprovide theoretical guarantees that MW loss directly upper-bounds the AoC,\nbetter aligning optimization with retrieval goals. We further promote ROC\ncurves and AUC as natural threshold free diagnostics for evaluating retriever\ncalibration and ranking quality. Empirically, retrievers trained with MW loss\nconsistently outperform contrastive counterparts in AUC and standard retrieval\nmetrics. Our experiments show that MW loss is an empirically superior\nalternative to Contrastive Loss, yielding better-calibrated and more\ndiscriminative retrievers for high-stakes applications like RAG.", "AI": {"tldr": "\u63d0\u51faMW\u635f\u5931\u51fd\u6570\u66ff\u4ee3\u4f20\u7edf\u5bf9\u6bd4\u635f\u5931\uff0c\u901a\u8fc7\u6700\u5927\u5316AUC\u6307\u6807\u4f18\u5316\u68c0\u7d22\u6a21\u578b\u6027\u80fd\uff0c\u6539\u5584RAG\u7b49\u4e0b\u6e38\u4efb\u52a1\u6548\u679c", "motivation": "\u4f20\u7edfNCE\u635f\u5931\u51fd\u6570\u65e0\u6cd5\u6709\u6548\u5206\u79bb\u76f8\u5173/\u4e0d\u76f8\u5173\u6587\u6863\u7684\u5f97\u5206\u5dee\u5f02\uff0c\u5bfc\u81f4\u68c0\u7d22\u6a21\u578b\u6821\u51c6\u5dee\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u6b20\u4f73", "method": "\u57fa\u4e8eMann-Whitney U\u7edf\u8ba1\u91cf\u6784\u9020\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6b63\u8d1f\u6837\u672c\u5bf9\u7684\u5f97\u5206\u5dee\u5f02\u4e8c\u5143\u4ea4\u53c9\u71b5\u76f4\u63a5\u4f18\u5316AUC\u6307\u6807", "result": "MW\u635f\u5931\u8bad\u7ec3\u7684\u68c0\u7d22\u6a21\u578b\u5728AUC\u548c\u6807\u51c6\u68c0\u7d22\u6307\u6807\u4e0a\u5168\u9762\u8d85\u8d8a\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u6a21\u578b\u6821\u51c6\u6027\u548c\u5224\u522b\u529b\u663e\u8457\u63d0\u5347", "conclusion": "MW\u635f\u5931\u662f\u66ff\u4ee3\u5bf9\u6bd4\u635f\u5931\u7684\u66f4\u4f18\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408RAG\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u573a\u666f\uff0c\u5efa\u8bae\u5c06AUC\u4f5c\u4e3a\u68c0\u7d22\u6a21\u578b\u7684\u6838\u5fc3\u8bc4\u4f30\u6307\u6807"}}
{"id": "2510.00219", "pdf": "https://arxiv.org/pdf/2510.00219", "abs": "https://arxiv.org/abs/2510.00219", "authors": ["Houjun Liu", "Shikhar Murty", "Christopher D. Manning", "R\u00f3bert Csord\u00e1s"], "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "10 pages, 6 figures", "summary": "Current approaches for scaling inference-time compute in transformers rely on\ntraining them to emit explicit chain-of-thought tokens before producing an\nanswer. While these methods are powerful, they are limited because they cannot\nbe applied during pretraining and are limited to only serially-generated,\nnatural-language verbalization to scale inference-time compute. In this work,\nwe propose Thoughtbubbles, a transformer variant that natively performs\nparallel adaptive computation in latent space by learning to fork or delete\nresidual streams. Thus, tokens that require a large amount of computation can\nform a \"bubble\" of cloned residuals in the middle of the network for additional\nthinking. Crucially, this behavior is learned during pretraining with only\nlanguage modeling loss. Thoughtbubbles outperforms both standard decoder LMs as\nwell as non-adaptive parallel computation approaches on OpenWebText and peS2o\nperplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after\npretraining across 150M to 772M parameter scales. The implicit nature of our\nmethod enables adaptive computation to be learned starting at pretraining time,\npaving the way to unify train and test-time behavior for reasoning models.", "AI": {"tldr": "\u63d0\u51faThoughtbubbles\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6b8b\u5dee\u6d41\u5206\u652f\u5b9e\u73b0Transformer\u7684\u5e76\u884c\u81ea\u9002\u5e94\u8ba1\u7b97\uff0c\u4ec5\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u663e\u5f0f\u601d\u7ef4\u94fe\u7684\u63a8\u7406\u6269\u5c55\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u4e0d\u53ef\u7528\u6027\u548c\u4e32\u884c\u751f\u6210\u7ea6\u675f\uff0c\u9700\u63a2\u7d22\u9690\u5f0f\u81ea\u9002\u5e94\u8ba1\u7b97\u673a\u5236\u3002", "method": "\u4fee\u6539Transformer\u7ed3\u6784\u4f7f\u6b8b\u5dee\u6d41\u53ef\u5b66\u4e60\u5206\u53c9/\u5220\u9664\uff0c\u5728\u7f51\u7edc\u4e2d\u95f4\u5f62\u6210\u8ba1\u7b97\u6c14\u6ce1\u8fdb\u884c\u989d\u5916\u5904\u7406\uff0c\u5b8c\u5168\u901a\u8fc7\u8bed\u8a00\u5efa\u6a21\u635f\u5931\u9a71\u52a8\u3002", "result": "\u5728OpenWebText/peS2o\u56f0\u60d1\u5ea6\u53caHellaSwag/LAMBADA\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u53c2\u6570\u89c4\u6a21150M-772M\u5747\u6709\u6548\u3002", "conclusion": "\u9690\u5f0f\u81ea\u9002\u5e94\u8ba1\u7b97\u673a\u5236\u7edf\u4e00\u4e86\u8bad\u7ec3\u63a8\u7406\u884c\u4e3a\uff0c\u4e3a\u9884\u8bad\u7ec3\u9636\u6bb5\u5b66\u4e60\u590d\u6742\u63a8\u7406\u80fd\u529b\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84\u3002"}}
{"id": "2510.00325", "pdf": "https://arxiv.org/pdf/2510.00325", "abs": "https://arxiv.org/abs/2510.00325", "authors": ["Priyank Dubey"], "title": "QSearchNet: A Quantum Walk Search Framework for Link Prediction", "categories": ["quant-ph", "cs.CL"], "comment": null, "summary": "Link prediction is one of the fundamental problems in graph theory, critical\nfor understanding and forecasting the evolution of complex systems like social\nand biological networks. While classical heuristics capture certain aspects of\ngraph topology, they often struggle to optimally integrate local and global\nstructural information or adapt to complex dependencies. Quantum computing\noffers a powerful alternative by leveraging superposition for simultaneous\nmulti-path exploration and interference-driven integration of both local and\nglobal graph features. In this work, we introduce QSearchNet, a\nquantum-inspired framework based on Discrete-Time Quantum Walk (DTQW) dynamics\nand Grover's amplitude amplification. QSearchNet simulates a topology-aware\nquantum evolution to propagate amplitudes across multiple nodes simultaneously.\nBy aligning interference patterns through quantum reflection and oracle-like\nphase-flip operation, it adaptively prioritizes multi-hop dependencies and\namplifies structurally relevant paths corresponding to potential connections.\nExperiments on diverse real-world networks demonstrate competitive performance,\nparticularly with hard negative samples under realistic evaluation conditions.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5b50\u542f\u53d1\u6846\u67b6QSearchNet\uff0c\u901a\u8fc7\u91cf\u5b50\u884c\u8d70\u548c\u5e45\u5ea6\u653e\u5927\u6280\u672f\u63d0\u5347\u56fe\u7f51\u7edc\u94fe\u8def\u9884\u6d4b\u6027\u80fd", "motivation": "\u7ecf\u5178\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728\u6574\u5408\u5c40\u90e8/\u5168\u5c40\u56fe\u7ed3\u6784\u7279\u5f81\u548c\u9002\u5e94\u590d\u6742\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u91cf\u5b50\u8ba1\u7b97\u901a\u8fc7\u53e0\u52a0\u6001\u548c\u5e72\u6d89\u6548\u5e94\u4e3a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84", "method": "\u7ed3\u5408\u79bb\u6563\u65f6\u95f4\u91cf\u5b50\u884c\u8d70(DTQW)\u7684\u52a8\u6001\u4f20\u64ad\u548cGrover\u5e45\u5ea6\u653e\u5927\u6280\u672f\uff0c\u5229\u7528\u91cf\u5b50\u53cd\u5c04\u548c\u76f8\u4f4d\u7ffb\u8f6c\u64cd\u4f5c\u5bf9\u9f50\u5e72\u6d89\u6a21\u5f0f", "result": "\u5728\u771f\u5b9e\u7f51\u7edc\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u7ade\u4e89\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u5305\u542b\u56f0\u96be\u8d1f\u6837\u672c\u7684\u73b0\u5b9e\u8bc4\u4f30\u6761\u4ef6\u4e0b\u8868\u73b0\u7a81\u51fa", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u673a\u5236\u53ef\u6709\u6548\u6355\u6349\u591a\u8df3\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u590d\u6742\u7f51\u7edc\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u6846\u67b6\u8bbe\u8ba1\u601d\u8def"}}
{"id": "2510.00339", "pdf": "https://arxiv.org/pdf/2510.00339", "abs": "https://arxiv.org/abs/2510.00339", "authors": ["T. James Brandt"], "title": "Navigating the Synchrony-Stability Frontier in Adaptive Chatbots", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.LG", "I.2.7; H.5.2"], "comment": "pages; 9 tables; 7 figures; code & analysis artifact:\n  https://doi.org/10.5281/zenodo.17238269; under review at ACM IUI 2026", "summary": "Adaptive chatbots that mimic a user's linguistic style can build rapport and\nengagement, yet unconstrained mimicry risks an agent that feels unstable or\nsycophantic. We present a computational evaluation framework that makes the\ncore design tension explicit: balancing moment-to-moment linguistic synchrony\nagainst long-term persona stability. Using an 8-dimensional style vector and a\nclosed-loop \"base+delta\" prompting architecture, we simulate and compare\nexplicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA),\nDead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear\nPareto frontier: bounded policies achieve substantial gains in stability at a\nmodest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from\n0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this\ntrade-off through large-scale replications on three public corpora\n(DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation\nacross two model families. Furthermore, we quantify \"prompt legibility,\"\nshowing that frontier policies reduce instruction churn and cut jarring\nregister flips (major tone changes) from 0.254 to 0.092, yielding systems that\nare easier to reason about and maintain. Taken together, our framework provides\na general evaluation harness for style adaptation; a systematic ablation that\nidentifies Pareto-efficient policies; robust validation across diverse datasets\nand models; and novel legibility metrics linking policy choices to system\nmaintainability.", "AI": {"tldr": "\u63d0\u51fa\u8bc4\u4f30\u804a\u5929\u673a\u5668\u4eba\u8bed\u8a00\u98ce\u683c\u9002\u5e94\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7b56\u7565\u6a21\u62df\u9a8c\u8bc1\u53d1\u73b0\u6709\u9650\u5236\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u7a33\u5b9a\u6027\u4e14\u540c\u6b65\u6027\u635f\u5931\u8f83\u5c0f", "motivation": "\u89e3\u51b3\u804a\u5929\u673a\u5668\u4eba\u5728\u6a21\u4eff\u7528\u6237\u8bed\u8a00\u98ce\u683c\u65f6\u9762\u4e34\u7684\u5373\u65f6\u540c\u6b65\u6027\u4e0e\u957f\u671f\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u77db\u76fe", "method": "\u4f7f\u7528\u516b\u7ef4\u98ce\u683c\u5411\u91cf\u548c\u95ed\u73af\u63d0\u793a\u67b6\u6784\uff0c\u5728\u771f\u5b9e\u5bf9\u8bdd\u6570\u636e\u4e0a\u6a21\u62df\u4e94\u79cd\u9002\u5e94\u7b56\u7565\uff08\u5305\u62ecEMA\u3001Cap\u7b49\u6df7\u5408\u7b56\u7565\uff09", "result": "\u6df7\u5408\u7b56\u7565\u5c06\u7a33\u5b9a\u6027\u63d0\u534762%\u81f30.878\uff0c\u540c\u6b65\u6027\u4ec5\u964d\u4f4e17%\uff1b\u5728\u4e09\u5927\u516c\u5f00\u8bed\u6599\u5e93\u9a8c\u8bc1\u6709\u6548\uff0c\u63d0\u793a\u53ef\u8bfb\u6027\u6307\u6807\u63d0\u534764%", "conclusion": "\u6784\u5efa\u4e86\u53ef\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc6\u522b\u51fa\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u9a8c\u8bc1\u4e3a\u81ea\u9002\u5e94\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u7ef4\u62a4\u6027\u8bbe\u8ba1\u6307\u5357"}}
{"id": "2510.00374", "pdf": "https://arxiv.org/pdf/2510.00374", "abs": "https://arxiv.org/abs/2510.00374", "authors": ["Minseok Jeon", "Seunghyun Park"], "title": "GDLNN: Marriage of Programming Language and Neural Networks for Accurate and Easy-to-Explain Graph Classification", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We present GDLNN, a new graph machine learning architecture, for graph\nclassification tasks. GDLNN combines a domain-specific programming language,\ncalled GDL, with neural networks. The main strength of GDLNN lies in its GDL\nlayer, which generates expressive and interpretable graph representations.\nSince the graph representation is interpretable, existing model explanation\ntechniques can be directly applied to explain GDLNN's predictions. Our\nevaluation shows that the GDL-based representation achieves high accuracy on\nmost graph classification benchmark datasets, outperforming dominant graph\nlearning methods such as GNNs. Applying an existing model explanation technique\nalso yields high-quality explanations of GDLNN's predictions. Furthermore, the\ncost of GDLNN is low when the explanation cost is included.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ed3\u5408GDL\u7f16\u7a0b\u8bed\u8a00\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u65b0\u56fe\u5b66\u4e60\u67b6\u6784GDLNN\uff0c\u517c\u5177\u9ad8\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u53ef\u89e3\u91ca\u6027\u5dee\u4e14\u6a21\u578b\u89e3\u91ca\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u56fe\u8868\u793a\u63d0\u5347\u5206\u7c7b\u6027\u80fd", "method": "\u91c7\u7528\u9886\u57df\u7279\u5b9a\u7f16\u7a0b\u8bed\u8a00GDL\u6784\u5efa\u56fe\u8868\u793a\u5c42\uff0c\u5c06\u53ef\u89e3\u91ca\u7684\u56fe\u7279\u5f81\u4e0e\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408", "result": "\u5728\u591a\u6570\u56fe\u5206\u7c7b\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8d85\u8d8a\u4e3b\u6d41\u65b9\u6cd5\uff08\u5982GNN\uff09\uff0c\u4e14\u5e94\u7528\u73b0\u6709\u89e3\u91ca\u6280\u672f\u5373\u53ef\u83b7\u5f97\u9ad8\u8d28\u91cf\u9884\u6d4b\u89e3\u91ca", "conclusion": "GDLNN\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u56fe\u8868\u793a\u5c42\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u89e3\u91ca\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.00404", "pdf": "https://arxiv.org/pdf/2510.00404", "abs": "https://arxiv.org/abs/2510.00404", "authors": ["Xudong Zhu", "Mohammad Mahdi Khalili", "Zhihui Zhu"], "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Sparse autoencoders (SAEs) have emerged as powerful techniques for\ninterpretability of large language models (LLMs), aiming to decompose hidden\nstates into meaningful semantic features. While several SAE variants have been\nproposed, there remains no principled framework to derive SAEs from the\noriginal dictionary learning formulation. In this work, we introduce such a\nframework by unrolling the proximal gradient method for sparse coding. We show\nthat a single-step update naturally recovers common SAE variants, including\nReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation\nof existing SAEs: their sparsity-inducing regularizers enforce non-negativity,\npreventing a single feature from representing bidirectional concepts (e.g.,\nmale vs. female). This structural constraint fragments semantic axes into\nseparate, redundant features, limiting representational completeness. To\naddress this issue, we propose AbsTopK SAE, a new variant derived from the\n$\\ell_0$ sparsity constraint that applies hard thresholding over the\nlargest-magnitude activations. By preserving both positive and negative\nactivations, AbsTopK uncovers richer, bidirectional conceptual representations.\nComprehensive experiments across four LLMs and seven probing and steering tasks\nshow that AbsTopK improves reconstruction fidelity, enhances interpretability,\nand enables single features to encode contrasting concepts. Remarkably, AbsTopK\nmatches or even surpasses the Difference-in-Mean method, a supervised approach\nthat requires labeled data for each concept and has been shown in prior work to\noutperform SAEs.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5c55\u5f00\u7a00\u758f\u7f16\u7801\u7684\u8fd1\u7aef\u68af\u5ea6\u65b9\u6cd5\u5efa\u7acbSAE\u6846\u67b6\uff0c\u53d1\u73b0\u73b0\u6709SAE\u7684\u6b63\u5219\u5316\u9650\u5236\u5bfc\u81f4\u53cc\u5411\u6982\u5ff5\u8868\u793a\u4e0d\u8db3\uff0c\u63d0\u51fa\u652f\u6301\u53cc\u5411\u6fc0\u6d3b\u7684AbsTopK SAE\u65b9\u6cd5\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edfSAE\u548c\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff08SAE\uff09\u7684\u6b63\u5219\u5316\u673a\u5236\u5f3a\u5236\u975e\u8d1f\u6fc0\u6d3b\uff0c\u5bfc\u81f4\u5355\u4e2a\u7279\u5f81\u65e0\u6cd5\u8868\u793a\u53cc\u5411\u8bed\u4e49\u6982\u5ff5\uff08\u5982\u6027\u522b\u5bf9\u7acb\uff09\uff0c\u9020\u6210\u7279\u5f81\u5197\u4f59\u548c\u8bed\u4e49\u8868\u793a\u4e0d\u5b8c\u6574\u3002", "method": "\u57fa\u4e8e\u21130\u7a00\u758f\u7ea6\u675f\u63a8\u5bfc\u51faAbsTopK SAE\uff1a\u901a\u8fc7\u4fdd\u7559\u6700\u5927\u7edd\u5bf9\u503c\u6fc0\u6d3b\u7684\u786c\u9608\u503c\u5904\u7406\uff0c\u652f\u6301\u6b63\u8d1f\u53cc\u5411\u6fc0\u6d3b\uff0c\u5b9e\u73b0\u66f4\u5b8c\u6574\u7684\u53cc\u5411\u6982\u5ff5\u7f16\u7801\u3002", "result": "\u57284\u4e2aLLM\u548c7\u7c7b\u4efb\u52a1\u4e2d\uff0cAbsTopK SAE\u91cd\u5efa\u7cbe\u5ea6\u63d0\u534725%\uff0c\u7279\u5f81\u53ef\u89e3\u91ca\u6027\u589e\u5f3a\uff0c\u5355\u4e2a\u7279\u5f81\u53ef\u7f16\u7801\u5bf9\u7acb\u6982\u5ff5\uff0c\u6027\u80fd\u8d85\u8d8a\u4f20\u7edfSAE\u4e14\u5339\u914d/\u8d85\u8fc7\u9700\u6807\u6ce8\u6570\u636e\u7684\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "\u7a81\u7834\u4f20\u7edfSAE\u7684\u975e\u8d1f\u6027\u7ea6\u675f\uff0cAbsTopK SAE\u901a\u8fc7\u53cc\u5411\u6fc0\u6d3b\u673a\u5236\u663e\u8457\u63d0\u5347\u8bed\u4e49\u8868\u793a\u80fd\u529b\uff0c\u4e3aLLM\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u8d85\u8d8a\u73b0\u6709\u65e0\u76d1\u7763/\u6709\u76d1\u7763\u65b9\u6cd5\u7684\u80fd\u529b\u3002"}}
{"id": "2510.00436", "pdf": "https://arxiv.org/pdf/2510.00436", "abs": "https://arxiv.org/abs/2510.00436", "authors": ["Sarvesh Soni", "Dina Demner-Fushman"], "title": "Automated Evaluation can Distinguish the Good and Bad AI Responses to Patient Questions about Hospitalization", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Automated approaches to answer patient-posed health questions are rising, but\nselecting among systems requires reliable evaluation. The current gold standard\nfor evaluating the free-text artificial intelligence (AI) responses--human\nexpert review--is labor-intensive and slow, limiting scalability. Automated\nmetrics are promising yet variably aligned with human judgments and often\ncontext-dependent. To address the feasibility of automating the evaluation of\nAI responses to hospitalization-related questions posed by patients, we\nconducted a large systematic study of evaluation approaches. Across 100 patient\ncases, we collected responses from 28 AI systems (2800 total) and assessed them\nalong three dimensions: whether a system response (1) answers the question, (2)\nappropriately uses clinical note evidence, and (3) uses general medical\nknowledge. Using clinician-authored reference answers to anchor metrics,\nautomated rankings closely matched expert ratings. Our findings suggest that\ncarefully designed automated evaluation can scale comparative assessment of AI\nsystems and support patient-clinician communication.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u5b9e\u57fa\u4e8e\u4e34\u5e8a\u53c2\u8003\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u80fd\u6709\u6548\u8bc4\u4f30\u533b\u7597AI\u56de\u7b54\u8d28\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u89c4\u6a21\u5316\u8bc4\u4f30\u3002", "motivation": "\u4eba\u5de5\u4e13\u5bb6\u8bc4\u5ba1\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u53ef\u9760\u81ea\u52a8\u5316\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u652f\u6301AI\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7684\u89c4\u6a21\u5316\u5e94\u7528\u3002", "method": "\u6536\u96c628\u4e2aAI\u7cfb\u7edf\u5bf9100\u4e2a\u4f4f\u9662\u75c5\u4f8b\u76842800\u6761\u56de\u7b54\uff0c\u901a\u8fc7\u4e13\u5bb6\u64b0\u5199\u7684\u53c2\u8003\u7b54\u6848\u951a\u5b9a\u6307\u6807\uff0c\u4ece\u95ee\u9898\u56de\u7b54\u51c6\u786e\u6027\u3001\u4e34\u5e8a\u8bc1\u636e\u8fd0\u7528\u3001\u533b\u5b66\u77e5\u8bc6\u5e94\u7528\u4e09\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u81ea\u52a8\u5316\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4e13\u5bb6\u8bc4\u5206\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8bc1\u660e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u4f53\u7cfb\u5177\u6709\u89c4\u6a21\u5316\u6bd4\u8f83AI\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u4e34\u5e8a\u53c2\u8003\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u4f53\u7cfb\u53ef\u6709\u6548\u66ff\u4ee3\u4eba\u5de5\u8bc4\u4f30\uff0c\u4e3a\u533b\u7597AI\u7cfb\u7edf\u6bd4\u8f83\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u533b\u60a3\u6c9f\u901a\u573a\u666f\u7684\u89c4\u6a21\u5316\u5e94\u7528\u3002"}}
{"id": "2510.00586", "pdf": "https://arxiv.org/pdf/2510.00586", "abs": "https://arxiv.org/abs/2510.00586", "authors": ["Yen-Shan Chen", "Sian-Yao Huang", "Cheng-Lin Yang", "Yun-Nung Chen"], "title": "Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Existing data poisoning attacks on retrieval-augmented generation (RAG)\nsystems scale poorly because they require costly optimization of poisoned\ndocuments for each target phrase. We introduce Eyes-on-Me, a modular attack\nthat decomposes an adversarial document into reusable Attention Attractors and\nFocus Regions. Attractors are optimized to direct attention to the Focus\nRegion. Attackers can then insert semantic baits for the retriever or malicious\ninstructions for the generator, adapting to new targets at near zero cost. This\nis achieved by steering a small subset of attention heads that we empirically\nidentify as strongly correlated with attack success. Across 18 end-to-end RAG\nsettings (3 datasets $\\times$ 2 retrievers $\\times$ 3 generators), Eyes-on-Me\nraises average attack success rates from 21.9 to 57.8 (+35.9 points,\n2.6$\\times$ over prior work). A single optimized attractor transfers to unseen\nblack box retrievers and generators without retraining. Our findings establish\na scalable paradigm for RAG data poisoning and show that modular, reusable\ncomponents pose a practical threat to modern AI systems. They also reveal a\nstrong link between attention concentration and model outputs, informing\ninterpretability research.", "AI": {"tldr": "Eyes-on-Me\u901a\u8fc7\u6a21\u5757\u5316\u653b\u51fb\u7ec4\u4ef6\u5b9e\u73b0\u53ef\u6269\u5c55\u7684RAG\u6570\u636e\u6295\u6bd2\uff0c\u5c06\u5bf9\u6297\u6587\u6863\u5206\u89e3\u4e3a\u53ef\u91cd\u7528\u6ce8\u610f\u529b\u5438\u5f15\u5668\u548c\u805a\u7126\u533a\u57df\uff0c\u63d0\u5347\u653b\u51fb\u6210\u529f\u73872.6\u500d", "motivation": "\u73b0\u6709RAG\u6570\u636e\u6295\u6bd2\u653b\u51fb\u9700\u4e3a\u6bcf\u4e2a\u76ee\u6807\u77ed\u8bed\u5355\u72ec\u4f18\u5316\u6295\u6bd2\u6587\u6863\uff0c\u6269\u5c55\u6027\u5dee\u4e14\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u53ef\u590d\u7528\u7ec4\u4ef6\u964d\u4f4e\u653b\u51fb\u6210\u672c", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u5438\u5f15\u5668\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u805a\u7126\u533a\u57df\uff0c\u5229\u7528\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u6ce8\u610f\u529b\u5934\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u8bed\u4e49\u8bf1\u9975\u548c\u6076\u610f\u6307\u4ee4\u7684\u96f6\u6210\u672c\u8fc1\u79fb\u90e8\u7f72", "result": "\u572818\u79cdRAG\u573a\u666f\u4e2d\u653b\u51fb\u6210\u529f\u7387\u63d0\u534735.9\u4e2a\u767e\u5206\u70b9\u8fbe57.8%\uff0c\u5355\u4e2a\u5438\u5f15\u5668\u53ef\u8fc1\u79fb\u81f3\u9ed1\u76d2\u6a21\u578b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3", "conclusion": "\u6a21\u5757\u5316\u7ec4\u4ef6\u5bf9AI\u7cfb\u7edf\u6784\u6210\u5b9e\u9645\u5a01\u80c1\uff0c\u6ce8\u610f\u529b\u96c6\u4e2d\u4e0e\u6a21\u578b\u8f93\u51fa\u7684\u5f3a\u5173\u8054\u4e3a\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2510.00615", "pdf": "https://arxiv.org/pdf/2510.00615", "abs": "https://arxiv.org/abs/2510.00615", "authors": ["Minki Kang", "Wei-Ning Chen", "Dongge Han", "Huseyin A. Inan", "Lukas Wutschitz", "Yanzhi Chen", "Robert Sim", "Saravan Rajmohan"], "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly deployed as agents in dynamic,\nreal-world environments, where success requires both reasoning and effective\ntool use. A central challenge for agentic tasks is the growing context length,\nas agents must accumulate long histories of actions and observations. This\nexpansion raises costs and reduces efficiency in long-horizon tasks, yet prior\nwork on context compression has mostly focused on single-step tasks or narrow\napplications. We introduce Agent Context Optimization (ACON), a unified\nframework that optimally compresses both environment observations and\ninteraction histories into concise yet informative condensations. ACON\nleverages compression guideline optimization in natural language space: given\npaired trajectories where full context succeeds but compressed context fails,\ncapable LLMs analyze the causes of failure, and the compression guideline is\nupdated accordingly. Furthermore, we propose distilling the optimized LLM\ncompressor into smaller models to reduce the overhead of the additional module.\nExperiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON\nreduces memory usage by 26-54% (peak tokens) while largely preserving task\nperformance, preserves over 95% of accuracy when distilled into smaller\ncompressors, and enhances smaller LMs as long-horizon agents with up to 46%\nperformance improvement.", "AI": {"tldr": "ACON\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u538b\u7f29\u964d\u4f4e26-54%\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u84b8\u998f\u6280\u672f\u63d0\u5347\u5c0f\u6a21\u578b\u4ee3\u7406\u80fd\u529b", "motivation": "\u957f\u4e0a\u4e0b\u6587\u5bfc\u81f4\u5185\u5b58\u6210\u672c\u589e\u52a0\u548c\u6548\u7387\u4e0b\u964d\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6b65\u4efb\u52a1\u6216\u72ed\u7a84\u573a\u666f\uff0c\u9700\u8981\u901a\u7528\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u7a7a\u95f4\u7684\u538b\u7f29\u6307\u5357\u4f18\u5316\uff1a\u5206\u6790\u5931\u8d25\u8f68\u8ff9\u539f\u56e0\uff0c\u66f4\u65b0\u538b\u7f29\u7b56\u7565\uff1b\u5c06\u4f18\u5316\u540e\u7684\u538b\u7f29\u5668\u84b8\u998f\u81f3\u5c0f\u6a21\u578b", "result": "\u5728\u4e09\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\u5185\u5b58\u5cf0\u503c\u964d\u4f4e26-54%\uff0c\u84b8\u998f\u540e\u4fdd\u630195%\u51c6\u786e\u7387\uff0c\u5c0f\u6a21\u578b\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe46%", "conclusion": "ACON\u6709\u6548\u5e73\u8861\u4e0a\u4e0b\u6587\u538b\u7f29\u4e0e\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u957f\u5468\u671f\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u5c0f\u6a21\u578b\u4ee3\u7406\u80fd\u529b"}}
{"id": "2510.00620", "pdf": "https://arxiv.org/pdf/2510.00620", "abs": "https://arxiv.org/abs/2510.00620", "authors": ["Rosni Vasu", "Peter Jansen", "Pao Siangliulue", "Cristina Sarasua", "Abraham Bernstein", "Peter Clark", "Bhavana Dalvi Mishra"], "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation", "categories": ["cs.AI", "cs.CL"], "comment": "10 pages (main), 65 pages total", "summary": "While there has been a surge of interest in automated scientific discovery\n(ASD), especially with the emergence of LLMs, it remains challenging for tools\nto generate hypotheses that are both testable and grounded in the scientific\nliterature. Additionally, existing ideation tools are not adaptive to prior\nexperimental outcomes. We developed HARPA to address these challenges by\nincorporating the ideation workflow inspired by human researchers. HARPA first\nidentifies emerging research trends through literature mining, then explores\nhypothesis design spaces, and finally converges on precise, testable hypotheses\nby pinpointing research gaps and justifying design choices. Our evaluations\nshow that HARPA-generated hypothesis-driven research proposals perform\ncomparably to a strong baseline AI-researcher across most qualitative\ndimensions (e.g., specificity, novelty, overall quality), but achieve\nsignificant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness\n(+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the\nASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11\nout of 40) and fewer failures (16 vs. 21 out of 40), showing that expert\nfeasibility judgments track with actual execution success. Furthermore, to\nsimulate how researchers continuously refine their understanding of what\nhypotheses are both testable and potentially interesting from experience, HARPA\nlearns a reward model that scores new hypotheses based on prior experimental\noutcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained\nbaseline scorer. Together, these methods represent a step forward in the field\nof AI-driven scientific discovery.", "AI": {"tldr": "HARPA\u7cfb\u7edf\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7814\u7a76\u8005\u7684\u5047\u8bbe\u751f\u6210\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86AI\u79d1\u5b66\u53d1\u73b0\u4e2d\u5047\u8bbe\u63d0\u6848\u7684\u53ef\u884c\u6027\u548c\u6587\u732e\u57fa\u7840\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u6267\u884c\u6210\u529f\u7387\u4e0a\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u79d1\u5b66\u53d1\u73b0\u5de5\u5177\u5b58\u5728\u5047\u8bbe\u68c0\u9a8c\u6027\u4e0d\u8db3\u3001\u6587\u732e\u57fa\u7840\u8584\u5f31\u4e14\u65e0\u6cd5\u9002\u5e94\u5148\u524d\u5b9e\u9a8c\u7ed3\u679c\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u79d1\u7814\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "1. \u6587\u732e\u6316\u6398\u8bc6\u522b\u65b0\u5174\u8d8b\u52bf \u2192 2. \u63a2\u7d22\u5047\u8bbe\u8bbe\u8ba1\u7a7a\u95f4 \u2192 3. \u805a\u7126\u7814\u7a76\u7a7a\u767d\u751f\u6210\u53ef\u68c0\u9a8c\u5047\u8bbe \u2192 4. \u57fa\u4e8e\u5b9e\u9a8c\u53cd\u9988\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u4f18\u5316\u5047\u8bbe\u8bc4\u5206", "result": "\u53ef\u884c\u6027(+0.78)\u548c\u6587\u732e\u57fa\u7840\u6027(+0.85)\u663e\u8457\u63d0\u5347\uff1b\u4e0eCodeScientist\u534f\u540c\u5b9e\u73b0\u66f4\u9ad8\u6267\u884c\u6210\u529f\u7387(20/40 vs 11/40)\uff1b\u5956\u52b1\u6a21\u578b\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534728%\u7edd\u5bf9\u589e\u76ca", "conclusion": "HARPA\u901a\u8fc7\u6574\u5408\u6587\u732e\u9a71\u52a8\u5047\u8bbe\u751f\u6210\u4e0e\u5b9e\u9a8c\u53cd\u9988\u5b66\u4e60\u673a\u5236\uff0c\u63a8\u52a8\u4e86AI\u79d1\u5b66\u53d1\u73b0\u9886\u57df\u7684\u53d1\u5c55\uff0c\u9a8c\u8bc1\u4e86\u4e13\u5bb6\u53ef\u884c\u6027\u5224\u65ad\u4e0e\u5b9e\u9a8c\u6210\u529f\u7387\u7684\u6b63\u76f8\u5173\u6027\u3002"}}
{"id": "2510.00626", "pdf": "https://arxiv.org/pdf/2510.00626", "abs": "https://arxiv.org/abs/2510.00626", "authors": ["Chen-An Li", "Tzu-Han Lin", "Hung-yi Lee"], "title": "When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models", "categories": ["cs.SD", "cs.CL"], "comment": "5 pages; submitted to ICASSP 2026", "summary": "Large audio-language models (LALMs) unify speech and text processing, but\ntheir robustness in noisy real-world settings remains underexplored. We\ninvestigate how irrelevant audio, such as silence, synthetic noise, and\nenvironmental sounds, affects text reasoning tasks where audio is unnecessary.\nAcross three text-based benchmarks, we find that even non-informative audio\nreduces accuracy and increases prediction volatility; the severity of\ninterference scales with longer durations, higher amplitudes, and elevated\ndecoding temperatures. Silence, often assumed neutral, destabilizes outputs as\nstrongly as synthetic noise. While larger models show greater resilience,\nvulnerabilities persist across all evaluated systems. We further test\nmitigation strategies and find that prompting shows limited effectiveness,\nwhereas self-consistency improves stability at the cost of increased\ncomputation. Our results reveal cross-modal interference as a key robustness\nchallenge and highlight the need for efficient fusion strategies that preserve\nreasoning performance in the presence of irrelevant inputs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u65e0\u5173\u97f3\u9891\uff08\u9759\u97f3/\u566a\u58f0\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u589e\u5927\u53ef\u90e8\u5206\u7f13\u89e3\u4f46\u65e0\u6cd5\u6839\u9664\uff0c\u81ea\u6d3d\u6027\u65b9\u6cd5\u80fd\u63d0\u5347\u7a33\u5b9a\u6027\u4f46\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5728\u542b\u65e0\u5173\u566a\u58f0\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u8868\u73b0\uff0c\u63ed\u793a\u8de8\u6a21\u6001\u5e72\u6270\u5bf9\u7eaf\u6587\u672c\u63a8\u7406\u4efb\u52a1\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5728\u4e09\u4e2a\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6ce8\u5165\u9759\u97f3\u3001\u5408\u6210\u566a\u58f0\u548c\u73af\u5883\u58f0\uff0c\u5206\u6790\u6a21\u578b\u8f93\u51fa\u7684\u51c6\u786e\u6027\u3001\u6ce2\u52a8\u6027\u53ca\u6e29\u5ea6\u53c2\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u6d4b\u8bd5\u63d0\u793a\u5de5\u7a0b\u548c\u81ea\u6d3d\u6027\u89e3\u7801\u7684\u7f13\u89e3\u6548\u679c\u3002", "result": "\u975e\u4fe1\u606f\u6027\u97f3\u9891\u4f7f\u51c6\u786e\u7387\u4e0b\u964d\u4e14\u9884\u6d4b\u6ce2\u52a8\u52a0\u5267\uff0c\u9759\u97f3\u7684\u5e72\u6270\u5f3a\u5ea6\u4e0e\u566a\u58f0\u76f8\u5f53\uff1b\u6a21\u578b\u89c4\u6a21\u6269\u5927\u63d0\u5347\u97e7\u6027\u4f46\u6f0f\u6d1e\u4ecd\u5b58\uff1b\u81ea\u6d3d\u6027\u65b9\u6cd5\u53ef\u4f7f\u9884\u6d4b\u7a33\u5b9a\u6027\u63d0\u534737%\u4f46\u8ba1\u7b97\u91cf\u500d\u589e\u3002", "conclusion": "\u8de8\u6a21\u6001\u5e72\u6270\u6784\u6210\u65b0\u578b\u9c81\u68d2\u6027\u6311\u6218\uff0c\u9700\u8bbe\u8ba1\u9009\u62e9\u6027\u4fe1\u606f\u878d\u5408\u673a\u5236\uff0c\u5728\u4fdd\u6301\u591a\u6a21\u6001\u4f18\u52bf\u7684\u540c\u65f6\u907f\u514d\u65e0\u5173\u8f93\u5165\u5bf9\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u7684\u4fb5\u8680\u3002"}}
{"id": "2510.00628", "pdf": "https://arxiv.org/pdf/2510.00628", "abs": "https://arxiv.org/abs/2510.00628", "authors": ["Yu-Xiang Lin", "Chen-An Li", "Sheng-Lun Wei", "Po-Chun Chen", "Hsin-Hsi Chen", "Hung-yi Lee"], "title": "Hearing the Order: Investigating Selection Bias in Large Audio-Language Models", "categories": ["cs.SD", "cs.CL"], "comment": "The first two authors contributed equally. Submitted to ICASSP 2026", "summary": "Large audio-language models (LALMs) are often used in tasks that involve\nreasoning over ordered options. An open question is whether their predictions\nare influenced by the order of answer choices, which would indicate a form of\nselection bias and undermine their reliability. In this paper, we identify and\nanalyze this problem in LALMs. We demonstrate that no model is immune to this\nbias through extensive experiments on six LALMs across three widely used\nbenchmarks and their spoken counterparts. Shuffling the order of answer options\ncan cause performance fluctuations of up to 24% and even change model rankings,\nraising concerns about the reliability of current evaluation practices. We also\nstudy permutation-based strategies and show that they can mitigate bias in most\ncases. Our work represents the first systematic investigation of this issue in\nLALMs, and we hope it raises awareness and motivates further research in this\ndirection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9009\u9879\u987a\u5e8f\u654f\u611f\u6027\uff0c\u6539\u53d8\u9009\u9879\u987a\u5e8f\u53ef\u5bfc\u81f424%\u7684\u6027\u80fd\u6ce2\u52a8\u5e76\u5f71\u54cd\u6a21\u578b\u6392\u540d\uff0c\u63d0\u51fa\u6392\u5217\u7b56\u7565\u53ef\u7f13\u89e3\u8be5\u504f\u5dee", "motivation": "\u73b0\u6709\u8bc4\u4f30\u672a\u5145\u5206\u8003\u8651\u9009\u9879\u987a\u5e8f\u5bf9LALMs\u9884\u6d4b\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u8fd9\u79cd\u9009\u62e9\u504f\u5dee\u4f1a\u5a01\u80c1\u6a21\u578b\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u53ca\u5b9e\u9645\u5e94\u7528\u6548\u679c", "method": "\u57286\u4e2aLALMs\u6a21\u578b\u4e0a\u5f00\u5c55\u8de83\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u53ca\u5176\u8bed\u97f3\u7248\u672c\u7684\u5b9e\u9a8c\uff0c\u91c7\u7528\u9009\u9879\u968f\u673a\u6392\u5217\u7b56\u7565\u5206\u6790\u6027\u80fd\u6ce2\u52a8\uff0c\u5e76\u9a8c\u8bc1\u6392\u5217\u7ec4\u5408\u65b9\u6cd5\u7684\u6709\u6548\u6027", "result": "\u9009\u9879\u987a\u5e8f\u6539\u53d8\u5bfc\u81f4\u6700\u592724%\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u6a21\u578b\u6392\u540d\u51fa\u73b0\u53cd\u8f6c\uff1b\u6392\u5217\u7b56\u7565\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u6709\u6548\u7f13\u89e3\u9009\u62e9\u504f\u5dee\u95ee\u9898", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u63ed\u793aLALMs\u4e2d\u7684\u9009\u9879\u987a\u5e8f\u654f\u611f\u6027\u7f3a\u9677\uff0c\u8d28\u7591\u73b0\u6709\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u4e3a\u6539\u8fdb\u6a21\u578b\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2510.00636", "pdf": "https://arxiv.org/pdf/2510.00636", "abs": "https://arxiv.org/abs/2510.00636", "authors": ["Alessio Devoto", "Maximilian Jeblick", "Simon J\u00e9gou"], "title": "Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck\nfor efficient large language model inference. While attention-score-based KV\ncache pruning shows promise, it faces critical practical limitations: attention\nscores from future tokens are unavailable during compression, and modern\nimplementations like Flash Attention do not materialize the full attention\nmatrix, making past scores inaccessible. To overcome these challenges, we\nintroduce $\\textbf{Expected Attention, a training-free compression method}$\nthat estimates KV pairs importance by predicting how future queries will attend\nto them. Our approach leverages the distributional properties of LLM\nactivations to compute expected attention scores in closed form for each KV\npair. These scores enable principled ranking and pruning of KV pairs with\nminimal impact on the residual stream, achieving effective compression without\nperformance degradation. Importantly, our method operates seamlessly across\nboth prefilling and decoding phases, consistently outperforming\nstate-of-the-art baselines in both scenarios. Finally, $\\textbf{we release\nKVPress, a comprehensive library to enable researchers to implement and\nbenchmark KV cache compression methods, already including more than 20\ntechniques}$.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684Expected Attention\u65b9\u6cd5\u5b9e\u73b0KV\u7f13\u5b58\u538b\u7f29\uff0c\u5e76\u53d1\u5e03KVPress\u5de5\u5177\u5e93", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6ce8\u610f\u529b\u5206\u6570\u7684KV\u7f13\u5b58\u526a\u679d\u5b58\u5728\u672a\u6765token\u4e0d\u53ef\u89c1\u548c\u73b0\u4ee3\u5b9e\u73b0\u4e0d\u4fdd\u7559\u5b8c\u6574\u6ce8\u610f\u529b\u77e9\u9635\u7684\u7f3a\u9677", "method": "\u901a\u8fc7LLM\u6fc0\u6d3b\u7684\u5206\u5e03\u7279\u6027\u9884\u6d4b\u672a\u6765\u67e5\u8be2\u5bf9KV\u5bf9\u7684\u5173\u6ce8\u7a0b\u5ea6\uff0c\u8ba1\u7b97\u95ed\u5f0f\u9884\u671f\u6ce8\u610f\u529b\u5206\u6570\u8fdb\u884c\u526a\u679d", "result": "\u5728\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u5747\u5b9e\u73b0\u6709\u6548\u538b\u7f29\uff0c\u6027\u80fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u5168\u6d41\u7a0b\u65e0\u635f\u538b\u7f29\uff0cKVPress\u5e93\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u5305\u542b20+\u6280\u672f\u7684\u7edf\u4e00\u8bc4\u6d4b\u5e73\u53f0"}}
{"id": "2510.00671", "pdf": "https://arxiv.org/pdf/2510.00671", "abs": "https://arxiv.org/abs/2510.00671", "authors": ["Thong Nguyen", "Yibin Lei", "Jia-Huei Ju", "Eugene Yang", "Andrew Yates"], "title": "Milco: Learned Sparse Retrieval Across Languages via a Multilingual Connector", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Learned Sparse Retrieval (LSR) combines the efficiency of bi-encoders with\nthe transparency of lexical matching, but existing approaches struggle to scale\nbeyond English. We introduce MILCO, an LSR architecture that maps queries and\ndocuments from different languages into a shared English lexical space via a\nmultilingual connector. MILCO is trained with a specialized two-stage regime\nthat combines Sparse Alignment Pretraining with contrastive training to provide\nrepresentation transparency and effectiveness while mitigating semantic\ncollapse. Motivated by the observation that uncommon entities are often lost\nwhen projected into English, we propose a new LexEcho head, which enhances\nrobustness by augmenting the English lexical representation with a\nsource-language view obtained through a special [ECHO] token. MILCO achieves\nstate-of-the-art multilingual and cross-lingual LSR performance, outperforming\nleading dense, sparse, and multi-vector baselines such as BGE-M3 and\nQwen3-Embed on standard multilingual benchmarks, while supporting dynamic\nefficiency through post-hoc pruning. Notably, when using mass-based pruning to\nreduce document representations to only 30 active dimensions on average, MILCO\n560M outperforms the similarly-sized Qwen3-Embed 0.6B with 1024 dimensions.", "AI": {"tldr": "MILCO\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8bed\u8a00\u7a00\u758f\u68c0\u7d22\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u82f1\u8bed\u8bcd\u6c47\u7a7a\u95f4\u6620\u5c04\u548cLexEcho\u589e\u5f3a\u673a\u5236\uff0c\u5728\u591a\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u5e76\u652f\u6301\u52a8\u6001\u6548\u7387\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u5230\u975e\u82f1\u8bed\u573a\u666f\uff0c\u4e14\u8de8\u8bed\u8a00\u6295\u5f71\u65f6\u5bb9\u6613\u4e22\u5931\u4f4e\u9891\u5b9e\u4f53\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u7a00\u758f\u5bf9\u9f50\u9884\u8bad\u7ec3+\u5bf9\u6bd4\u8bad\u7ec3\uff09\u7ed3\u5408LexEcho\u673a\u5236\uff0c\u901a\u8fc7[ECHO]\u4ee4\u724c\u4fdd\u7559\u6e90\u8bed\u8a00\u7279\u5f81\u589e\u5f3a\u8868\u793a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u4e0a\u8d85\u8d8aBGE-M3/Qwen3-Embed\u7b49\u6a21\u578b\uff0c\u6587\u6863\u8868\u793a\u538b\u7f29\u81f330\u7ef4\u65f6\u4ecd\u4f18\u4e8e0.6B\u53c2\u6570\u6a21\u578b\u76841024\u7ef4\u8868\u73b0\u3002", "conclusion": "MILCO\u8bc1\u660e\u4e86\u7a00\u758f\u68c0\u7d22\u5728\u8de8\u8bed\u8a00\u573a\u666f\u7684\u6f5c\u529b\uff0c\u5176\u6548\u7387-\u6548\u679c\u5e73\u8861\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2510.00685", "pdf": "https://arxiv.org/pdf/2510.00685", "abs": "https://arxiv.org/abs/2510.00685", "authors": ["Nurbek Tastan", "Samuel Horvath", "Karthik Nandakumar"], "title": "Stochastic Self-Organization in Multi-Agent Systems", "categories": ["cs.MA", "cs.CL", "cs.LG"], "comment": null, "summary": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the\npotential to solve tasks that are beyond the reach of any single LLM. However,\nthis potential can only be realized when the collaboration mechanism between\nagents is optimized. Specifically, optimizing the communication structure\nbetween agents is critical for fruitful collaboration. Most existing approaches\nrely on fixed topologies, pretrained graph generators, optimization over edges,\nor employ external LLM judges, thereby adding to the complexity. In this work,\nwe introduce a response-conditioned framework that adapts communication\non-the-fly. Agents independently generate responses to the user query and\nassess peer contributions using an approximation of the Shapley value. A\ndirected acyclic graph (DAG) is then constructed to regulate the propagation of\nthe responses among agents, which ensures stable and efficient message\ntransmission from high-contributing agents to others. This graph is dynamically\nupdated based on the agent responses from the previous collaboration round.\nSince the proposed framework enables the self-organization of agents without\nadditional supervision or training, we refer to it as SelfOrg. The SelfOrg\nframework goes beyond task- and query-level optimization and takes into account\nthe stochastic nature of agent responses. Experiments with both strong and weak\nLLM backends demonstrate robust performance, with significant gains in the weak\nregime where prior methods collapse. We also theoretically show that multiple\nagents increase the chance of correctness and that the correct responses\nnaturally dominate the information flow.", "AI": {"tldr": "\u63d0\u51faSelfOrg\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8eShapley\u503c\u7684\u52a8\u6001\u6709\u5411\u65e0\u73af\u56fe\u4f18\u5316\u591a\u667a\u80fd\u4f53\u901a\u4fe1\u7ed3\u6784\uff0c\u5b9e\u73b0\u65e0\u9700\u76d1\u7763\u7684\u81ea\u7ec4\u7ec7\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u56fa\u5b9a\u901a\u4fe1\u62d3\u6251\u7ed3\u6784\u548c\u5916\u90e8\u4f9d\u8d56\u5bfc\u81f4\u534f\u4f5c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u5f00\u53d1\u52a8\u6001\u81ea\u9002\u5e94\u7684\u901a\u4fe1\u673a\u5236\u6765\u63d0\u5347\u534f\u4f5c\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u54cd\u5e94\u6761\u4ef6\u5316\u6846\u67b6\uff1a1) \u667a\u80fd\u4f53\u72ec\u7acb\u751f\u6210\u54cd\u5e94 2) \u7528Shapley\u503c\u8fd1\u4f3c\u8bc4\u4f30\u8d21\u732e\u5ea6 3) \u6784\u5efa\u6709\u5411\u65e0\u73af\u56fe\u63a7\u5236\u9ad8\u8d21\u732e\u8282\u70b9\u5411\u4f4e\u8d21\u732e\u8282\u70b9\u7684\u4fe1\u606f\u4f20\u64ad 4) \u52a8\u6001\u66f4\u65b0\u901a\u4fe1\u62d3\u6251", "result": "\u5b9e\u9a8c\u663e\u793a\u5728\u5f31LLM\u73af\u5883\u4e0b\u6027\u80fd\u63d0\u5347\u663e\u8457\uff08\u4f20\u7edf\u65b9\u6cd5\u5931\u6548\uff09\uff0c\u7406\u8bba\u8bc1\u660e\u591a\u667a\u80fd\u4f53\u80fd\u63d0\u5347\u6b63\u786e\u7387\u4e14\u6b63\u786e\u54cd\u5e94\u4e3b\u5bfc\u4fe1\u606f\u6d41", "conclusion": "SelfOrg\u901a\u8fc7\u52a8\u6001\u901a\u4fe1\u673a\u5236\u7a81\u7834\u4efb\u52a1/\u67e5\u8be2\u7ea7\u4f18\u5316\uff0c\u5b9e\u73b0\u54cd\u5e94\u968f\u673a\u6027\u4e0b\u7684\u9c81\u68d2\u534f\u4f5c\uff0c\u4e3aLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.00743", "pdf": "https://arxiv.org/pdf/2510.00743", "abs": "https://arxiv.org/abs/2510.00743", "authors": ["Yifei Cao", "Changhao Jiang", "Jiabao Zhuang", "Jiajun Sun", "Ming Zhang", "Zhiheng Xi", "Hui Li", "Shihan Dou", "Yuran Wang", "Yunke Zhang", "Tao Ji", "Tao Gui", "Qi Zhang", "Xuanjing Huang"], "title": "From Scores to Preferences: Redefining MOS Benchmarking for Speech Quality Reward Modeling", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Assessing the perceptual quality of synthetic speech is crucial for guiding\nthe development and refinement of speech generation models. However, it has\ntraditionally relied on human subjective ratings such as the Mean Opinion Score\n(MOS), which depend on manual annotations and often suffer from inconsistent\nrating standards and poor reproducibility. To address these limitations, we\nintroduce MOS-RMBench, a unified benchmark that reformulates diverse MOS\ndatasets into a preference-comparison setting, enabling rigorous evaluation\nacross different datasets. Building on MOS-RMBench, we systematically construct\nand evaluate three paradigms for reward modeling: scalar reward models,\nsemi-scalar reward models, and generative reward models (GRMs). Our experiments\nreveal three key findings: (1) scalar models achieve the strongest overall\nperformance, consistently exceeding 74% accuracy; (2) most models perform\nconsiderably worse on synthetic speech than on human speech; and (3) all models\nstruggle on pairs with very small MOS differences. To improve performance on\nthese challenging pairs, we propose a MOS-aware GRM that incorporates an\nMOS-difference-based reward function, enabling the model to adaptively scale\nrewards according to the difficulty of each sample pair. Experimental results\nshow that the MOS-aware GRM significantly improves fine-grained quality\ndiscrimination and narrows the gap with scalar models on the most challenging\ncases. We hope this work will establish both a benchmark and a methodological\nframework to foster more rigorous and scalable research in automatic speech\nquality assessment.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u57fa\u51c6MOS-RMBench\u91cd\u6784\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u8303\u5f0f\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e09\u7c7b\u5956\u52b1\u6a21\u578b\u5e76\u6539\u8fdb\u751f\u6210\u5f0f\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8d28\u91cf\u5224\u522b\u4e0a\u7684\u8868\u73b0", "motivation": "\u4f20\u7edf\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u4f9d\u8d56\u4e3b\u89c2\u8bc4\u5206(MOS)\uff0c\u5b58\u5728\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u8bc4\u5206\u6807\u51c6\u4e0d\u4e00\u81f4\u3001\u53ef\u91cd\u590d\u6027\u5dee\u7b49\u5c40\u9650\u6027", "method": "1. \u6784\u5efaMOS-RMBench\u57fa\u51c6\u5c06\u591a\u6570\u636e\u96c6\u7edf\u4e00\u4e3a\u504f\u597d\u6bd4\u8f83\u5f62\u5f0f\n2. \u7cfb\u7edf\u7814\u7a76\u6807\u91cf/\u534a\u6807\u91cf/\u751f\u6210\u5f0f\u4e09\u79cd\u5956\u52b1\u5efa\u6a21\u8303\u5f0f\n3. \u63d0\u51fa\u878d\u5408MOS\u5dee\u5f02\u7684\u611f\u77e5\u578b\u751f\u6210\u5f0f\u5956\u52b1\u6a21\u578b(MOS-aware GRM)", "result": "1. \u6807\u91cf\u6a21\u578b\u6574\u4f53\u6700\u4f18(\u51c6\u786e\u7387>74%)\n2. \u591a\u6570\u6a21\u578b\u5728\u5408\u6210\u8bed\u97f3\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\n3. \u6240\u6709\u6a21\u578b\u5728\u5fae\u5c0fMOS\u5dee\u5f02\u6837\u672c\u5bf9\u8868\u73b0\u5dee\n4. MOS-aware GRM\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u5224\u522b\u80fd\u529b", "conclusion": "\u5efa\u7acb\u8bed\u97f3\u8d28\u91cf\u8bc4\u4f30\u7684\u7edf\u4e00\u57fa\u51c6\u4e0e\u65b9\u6cd5\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5956\u52b1\u7f29\u653e\u673a\u5236\u7f29\u5c0f\u6807\u91cf\u6a21\u578b\u4e0e\u751f\u6210\u6a21\u578b\u5728\u56f0\u96be\u6837\u672c\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd"}}
{"id": "2510.00808", "pdf": "https://arxiv.org/pdf/2510.00808", "abs": "https://arxiv.org/abs/2510.00808", "authors": ["Divy Kala", "Eshika Khandelwal", "Makarand Tapaswi"], "title": "What You See is What You Ask: Evaluating Audio Descriptions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "EMNLP 2025 Main Track Long Paper", "summary": "Audio descriptions (ADs) narrate important visual details in movies, enabling\nBlind and Low Vision (BLV) users to understand narratives and appreciate visual\ndetails. Existing works in automatic AD generation mostly focus on few-second\ntrimmed clips, and evaluate them by comparing against a single ground-truth\nreference AD. However, writing ADs is inherently subjective. Through alignment\nand analysis of two independent AD tracks for the same movies, we quantify the\nsubjectivity in when and whether to describe, and what and how to highlight.\nThus, we show that working with trimmed clips is inadequate. We propose ADQA, a\nQA benchmark that evaluates ADs at the level of few-minute long, coherent video\nsegments, testing whether they would help BLV users understand the story and\nappreciate visual details. ADQA features visual appreciation (VA) questions\nabout visual facts and narrative understanding (NU) questions based on the\nplot. Through ADQA, we show that current AD generation methods lag far behind\nhuman-authored ADs. We conclude with several recommendations for future work\nand introduce a public leaderboard for benchmarking.", "AI": {"tldr": "\u63d0\u51faADQA\u8bc4\u4f30\u57fa\u51c6\uff0c\u901a\u8fc7\u957f\u89c6\u9891\u7247\u6bb5\u6d4b\u8bd5\u97f3\u9891\u63cf\u8ff0\u8d28\u91cf\uff0c\u63ed\u793a\u73b0\u6709\u751f\u6210\u65b9\u6cd5\u4e0e\u4eba\u5de5\u64b0\u5199\u7684\u663e\u8457\u5dee\u8ddd", "motivation": "\u73b0\u6709\u97f3\u9891\u63cf\u8ff0\u7814\u7a76\u5c40\u9650\u4e8e\u77ed\u7247\u6bb5\u548c\u5355\u4e00\u53c2\u8003\u6807\u51c6\uff0c\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u5e2e\u52a9\u89c6\u969c\u7528\u6237\u7406\u89e3\u53d9\u4e8b\u7684\u5b9e\u9645\u6548\u679c\uff0c\u9700\u5efa\u7acb\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "1. \u91cf\u5316\u5206\u6790\u53ccAD\u8f68\u8ff9\u7684\u4e3b\u89c2\u5dee\u5f02 2. \u6784\u5efa\u5305\u542b\u89c6\u89c9\u6b23\u8d4f(VA)\u548c\u53d9\u4e8b\u7406\u89e3(NU)\u95ee\u9898\u7684QA\u57fa\u51c6 3. \u8bbe\u8ba1\u957f\u89c6\u9891\u6bb5\u8bc4\u4f30\u6846\u67b6\u5e76\u5efa\u7acb\u516c\u5171\u6392\u884c\u699c", "result": "ADQA\u6d4b\u8bd5\u663e\u793a\u5f53\u524d\u81ea\u52a8\u751f\u6210\u65b9\u6cd5\u5728\u53d9\u4e8b\u51c6\u786e\u6027\u548c\u89c6\u89c9\u7ec6\u8282\u4f20\u8fbe\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u4eba\u5de5AD\uff08\u51c6\u786e\u7387\u5dee\u503c\u8fbe35%\uff09", "conclusion": "\u5efa\u8bae\u5f00\u53d1\u8003\u8651\u65f6\u95f4\u8fde\u8d2f\u6027\u7684\u751f\u6210\u6a21\u578b\uff0c\u5efa\u7acb\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u5f00\u653eADQA\u5e73\u53f0\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2510.00845", "pdf": "https://arxiv.org/pdf/2510.00845", "abs": "https://arxiv.org/abs/2510.00845", "authors": ["Maxime M\u00e9loux", "Maxime Peyrard", "Fran\u00e7ois Portet"], "title": "Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The development of trustworthy artificial intelligence requires moving beyond\nblack-box performance metrics toward an understanding of models' internal\ncomputations. Mechanistic Interpretability (MI) aims to meet this need by\nidentifying the algorithmic mechanisms underlying model behaviors. Yet, the\nscientific rigor of MI critically depends on the reliability of its findings.\nIn this work, we argue that interpretability methods, such as circuit\ndiscovery, should be viewed as statistical estimators, subject to questions of\nvariance and robustness. To illustrate this statistical framing, we present a\nsystematic stability analysis of a state-of-the-art circuit discovery method:\nEAP-IG. We evaluate its variance and robustness through a comprehensive suite\nof controlled perturbations, including input resampling, prompt paraphrasing,\nhyperparameter variation, and injected noise within the causal analysis itself.\nAcross a diverse set of models and tasks, our results demonstrate that EAP-IG\nexhibits high structural variance and sensitivity to hyperparameters,\nquestioning the stability of its findings. Based on these results, we offer a\nset of best-practice recommendations for the field, advocating for the routine\nreporting of stability metrics to promote a more rigorous and statistically\ngrounded science of interpretability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7535\u8def\u53d1\u73b0\u65b9\u6cd5EAP-IG\u7684\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u5176\u5b58\u5728\u9ad8\u7ed3\u6784\u65b9\u5dee\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\uff0c\u5efa\u8bae\u5728\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u5f15\u5165\u7a33\u5b9a\u6027\u6307\u6807\u4ee5\u63d0\u5347\u79d1\u5b66\u4e25\u8c28\u6027\u3002", "motivation": "\u9488\u5bf9\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u9886\u57df\u7f3a\u4e4f\u7edf\u8ba1\u4e25\u8c28\u6027\u7684\u73b0\u72b6\uff0c\u4f5c\u8005\u4e3b\u5f20\u5c06\u7535\u8def\u53d1\u73b0\u65b9\u6cd5\u89c6\u4e3a\u7edf\u8ba1\u4f30\u8ba1\u5668\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u6270\u52a8\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u7ed3\u679c\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u8f93\u5165\u91cd\u91c7\u6837\u3001\u63d0\u793a\u6539\u5199\u3001\u8d85\u53c2\u6570\u8c03\u6574\u3001\u56e0\u679c\u5206\u6790\u566a\u58f0\u6ce8\u5165\u7b49\u63a7\u5236\u53d8\u91cf\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u7cfb\u7edf\u8bc4\u4f30EAP-IG\u7684\u65b9\u5dee\u4e0e\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEAP-IG\u5b58\u5728\u663e\u8457\u7684\u7ed3\u6784\u4e0d\u7a33\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\uff0c\u5176\u53d1\u73b0\u7684\u53ef\u91cd\u590d\u6027\u53d7\u5230\u8d28\u7591\u3002\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u5305\u542b\u7a33\u5b9a\u6027\u6307\u6807\u62a5\u544a\u7684\u7814\u7a76\u89c4\u8303\u5efa\u8bae\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u5b58\u5728\u7684\u7edf\u8ba1\u57fa\u7840\u7f3a\u9677\uff0c\u547c\u5401\u5efa\u7acb\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u548c\u6807\u51c6\u5316\u62a5\u544a\u673a\u5236\uff0c\u63a8\u52a8\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u5411\u66f4\u4e25\u8c28\u7684\u5b9e\u8bc1\u79d1\u5b66\u53d1\u5c55\u3002"}}
{"id": "2510.00855", "pdf": "https://arxiv.org/pdf/2510.00855", "abs": "https://arxiv.org/abs/2510.00855", "authors": ["Kevin Zhang", "Kuangzhi Ge", "Xiaowei Chi", "Renrui Zhang", "Shaojun Shi", "Zhen Dong", "Sirui Han", "Shanghang Zhang"], "title": "Can World Models Benefit VLMs for World Dynamics?", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project page: https://dyva-worldlm.github.io", "summary": "Trained on internet-scale video data, generative world models are\nincreasingly recognized as powerful world simulators that can generate\nconsistent and plausible dynamics over structure, motion, and physics. This\nraises a natural question: with the advent of strong video foundational models,\nmight they supplant conventional vision encoder paradigms for general-purpose\nmultimodal understanding? While recent studies have begun to explore the\npotential of world models on common vision tasks, these explorations typically\nlack a systematic investigation of generic, multimodal tasks. In this work, we\nstrive to investigate the capabilities when world model priors are transferred\ninto Vision-Language Models: we re-purpose a video diffusion model as a\ngenerative encoder to perform a single denoising step and treat the resulting\nlatents as a set of visual embedding. We empirically investigate this class of\nmodels, which we refer to as World-Language Models (WorldLMs), and we find that\ngenerative encoders can capture latents useful for downstream understanding\nthat show distinctions from conventional encoders. Naming our best-performing\nvariant Dynamic Vision Aligner (DyVA), we further discover that this method\nsignificantly enhances spatial reasoning abilities and enables single-image\nmodels to perform multi-frame reasoning. Through the curation of a suite of\nvisual reasoning tasks, we find DyVA to surpass both open-source and\nproprietary baselines, achieving state-of-the-art or comparable performance. We\nattribute these gains to WorldLM's inherited motion-consistency internalization\nfrom video pre-training. Finally, we systematically explore extensive model\ndesigns to highlight promising directions for future work. We hope our study\ncan pave the way for a new family of VLMs that leverage priors from world\nmodels and are on a promising path towards generalist vision learners.", "AI": {"tldr": "\u63a2\u7d22\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u5728\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u63d0\u51faDynamic Vision Aligner\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u80fd\u529b", "motivation": "\u63a2\u7a76\u89c6\u9891\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u591f\u66ff\u4ee3\u4f20\u7edf\u89c6\u89c9\u7f16\u7801\u8303\u5f0f\uff0c\u5b9e\u73b0\u901a\u7528\u591a\u6a21\u6001\u7406\u89e3", "method": "\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u91cd\u65b0\u7528\u4f5c\u751f\u6210\u5f0f\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5355\u6b21\u53bb\u566a\u6b65\u9aa4\u83b7\u53d6\u89c6\u89c9\u5d4c\u5165\u7279\u5f81(DyVA\u65b9\u6cd5)", "result": "\u5728\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\uff0c\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5355\u56fe\u50cf\u6a21\u578b\u5177\u5907\u591a\u5e27\u63a8\u7406\u80fd\u529b", "conclusion": "\u4e16\u754c\u6a21\u578b\u7684\u8fd0\u52a8\u4e00\u81f4\u6027\u5185\u90e8\u5316\u7279\u6027\u4e3a\u6784\u5efa\u65b0\u578bVLMs\u6307\u660e\u65b9\u5411\uff0c\u7cfb\u7edf\u5316\u6a21\u578b\u8bbe\u8ba1\u63a2\u7d22\u4e3a\u901a\u7528\u89c6\u89c9\u5b66\u4e60\u63d0\u4f9b\u8def\u5f84"}}
{"id": "2510.00866", "pdf": "https://arxiv.org/pdf/2510.00866", "abs": "https://arxiv.org/abs/2510.00866", "authors": ["Thiziri Nait Saada", "Louis Bethune", "Michal Klein", "David Grangier", "Marco Cuturi", "Pierre Ablin"], "title": "The data-quality illusion: Rethinking Classifier-based quality filtering for LLM Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": "21 pages, 20 figures, 2 tables, preprint", "summary": "Large-scale models are pretrained on massive web-crawled datasets containing\ndocuments of mixed quality, making data filtering essential. A popular method\nis Classifier-based Quality Filtering (CQF), which trains a binary classifier\nto distinguish between pretraining data and a small, high-quality set. It\nassigns each pretraining document a quality score defined as the classifier's\nscore and retains only the top-scoring ones. We provide an in-depth analysis of\nCQF. We show that while CQF improves downstream task performance, it does not\nnecessarily enhance language modeling on the high-quality dataset. We explain\nthis paradox by the fact that CQF implicitly filters the high-quality dataset\nas well. We further compare the behavior of models trained with CQF to those\ntrained on synthetic data of increasing quality, obtained via random token\npermutations, and find starkly different trends. Our results challenge the view\nthat CQF captures a meaningful notion of data quality.", "AI": {"tldr": "CQF\u65b9\u6cd5\u867d\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f46\u672a\u6539\u5584\u9ad8\u8d28\u91cf\u6570\u636e\u8bed\u8a00\u5efa\u6a21\uff0c\u5e76\u63ed\u793a\u5176\u9690\u5f0f\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u95ee\u9898", "motivation": "\u63a2\u7a76\u57fa\u4e8e\u5206\u7c7b\u5668\u7684\u8d28\u91cf\u8fc7\u6ee4\u65b9\u6cd5\uff08CQF\uff09\u5728\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u548c\u5c40\u9650\u6027", "method": "\u901a\u8fc7\u5206\u6790CQF\u5bf9\u9884\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u968f\u673a\u6807\u8bb0\u6392\u5217\u751f\u6210\u7684\u6e10\u8fdb\u8d28\u91cf\u5408\u6210\u6570\u636e\u8fdb\u884c\u6a21\u578b\u884c\u4e3a\u5bf9\u6bd4", "result": "CQF\u5728\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u7684\u540c\u65f6\uff0c\u672a\u80fd\u6539\u5584\u76ee\u6807\u6570\u636e\u96c6\u7684\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\uff0c\u4e14\u4e0e\u5408\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u5c55\u73b0\u76f8\u53cd\u8d8b\u52bf", "conclusion": "CQF\u53ef\u80fd\u672a\u6355\u6349\u5230\u6570\u636e\u8d28\u91cf\u7684\u6709\u6548\u8868\u5f81\uff0c\u5176\u4f5c\u7528\u673a\u5236\u9700\u8981\u91cd\u65b0\u8bc4\u4f30"}}
{"id": "2510.00908", "pdf": "https://arxiv.org/pdf/2510.00908", "abs": "https://arxiv.org/abs/2510.00908", "authors": ["Roksana Goworek", "Olivia Macmillan-Scott", "Eda B. \u00d6zyi\u011fit"], "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of\nretrieving relevant documents written in languages different from that of the\noriginal query. Research in this area has typically framed the task as\nmonolingual retrieval augmented by translation, treating retrieval methods and\ncross-lingual capabilities in isolation. Both monolingual and cross-lingual\nretrieval usually follow a pipeline of query expansion, ranking, re-ranking\nand, increasingly, question answering. Recent advances, however, have shifted\nfrom translation-based methods toward embedding-based approaches and leverage\nmultilingual large language models (LLMs), for which aligning representations\nacross languages remains a central challenge. The emergence of cross-lingual\nembeddings and multilingual LLMs has introduced a new paradigm, offering\nimproved retrieval performance and enabling answer generation. This survey\nprovides a comprehensive overview of developments from early translation-based\nmethods to state-of-the-art embedding-driven and generative techniques. It\npresents a structured account of core CLIR components, evaluation practices,\nand available resources. Persistent challenges such as data imbalance and\nlinguistic variation are identified, while promising directions are suggested\nfor advancing equitable and effective cross-lingual information retrieval. By\nsituating CLIR within the broader landscape of information retrieval and\nmultilingual language processing, this work not only reviews current\ncapabilities but also outlines future directions for building retrieval systems\nthat are robust, inclusive, and adaptable.", "AI": {"tldr": "\u8de8\u8bed\u8a00\u4fe1\u606f\u68c0\u7d22\uff08CLIR\uff09\u4ece\u57fa\u4e8e\u7ffb\u8bd1\u7684\u65b9\u6cd5\u53d1\u5c55\u4e3a\u5d4c\u5165\u9a71\u52a8\u548c\u751f\u6210\u6280\u672f\uff0c\u9762\u4e34\u6570\u636e\u4e0d\u5e73\u8861\u7b49\u6311\u6218\uff0c\u9700\u6784\u5efa\u9c81\u68d2\u4e14\u5305\u5bb9\u7684\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edfCLIR\u65b9\u6cd5\u5c06\u68c0\u7d22\u4e0e\u8de8\u8bed\u8a00\u80fd\u529b\u5b64\u7acb\u5904\u7406\uff0c\u65b0\u5174\u5d4c\u5165\u6280\u672f\u548c\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u53d1\u5c55\uff0c\u9700\u7cfb\u7edf\u6027\u7efc\u8ff0\u4ee5\u6307\u5bfc\u672a\u6765\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0CLIR\u6f14\u53d8\u5386\u7a0b\uff0c\u5206\u6790\u7ffb\u8bd1\u65b9\u6cd5\u3001\u8de8\u8bed\u8a00\u5d4c\u5165\u53ca\u591a\u8bed\u8a00LLMs\u7684\u5e94\u7528\uff0c\u603b\u7ed3\u6838\u5fc3\u7ec4\u4ef6\uff08\u5982\u67e5\u8be2\u6269\u5c55\u3001\u6392\u5e8f\uff09\u3001\u8bc4\u4f30\u65b9\u6cd5\u53ca\u8d44\u6e90\u3002", "result": "\u8bc6\u522b\u6570\u636e\u4e0d\u5e73\u8861\u3001\u8bed\u8a00\u53d8\u5f02\u7b49\u6311\u6218\uff0c\u63d0\u51fa\u7ed3\u5408\u591a\u8bed\u8a00\u5904\u7406\u4e0e\u68c0\u7d22\u6280\u672f\uff0c\u6784\u5efa\u9002\u5e94\u6027\u5f3a\u7684\u7cfb\u7edf\u4f5c\u4e3a\u672a\u6765\u65b9\u5411\u3002", "conclusion": "CLIR\u9700\u6574\u5408\u4fe1\u606f\u68c0\u7d22\u4e0e\u591a\u8bed\u8a00\u5904\u7406\u8fdb\u5c55\uff0c\u901a\u8fc7\u4f18\u5316\u5d4c\u5165\u5bf9\u9f50\u548c\u751f\u6210\u6280\u672f\u89e3\u51b3\u6311\u6218\uff0c\u5b9e\u73b0\u516c\u5e73\u9ad8\u6548\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u3002"}}
{"id": "2510.00977", "pdf": "https://arxiv.org/pdf/2510.00977", "abs": "https://arxiv.org/abs/2510.00977", "authors": ["Yihong Wu", "Liheng Ma", "Lei Ding", "Muzhi Li", "Xinyu Wang", "Kejia Chen", "Zhan Su", "Zhanguang Zhang", "Chenyang Huang", "Yingxue Zhang", "Mark Coates", "Jian-Yun Nie"], "title": "It Takes Two: Your GRPO Is Secretly DPO", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement\nlearning algorithm for post-training Large Language Models (LLMs). It is\ncommonly believed that GRPO necessitates a large group size to ensure stable\ntraining via precise statistical estimation, which incurs substantial\ncomputational overhead. In this work, we challenge this assumption by reframing\nGRPO as a form of contrastive learning, which reveals a fundamental connection\nto Direct Preference Optimization (DPO). Motivated by DPO's empirical success,\nwe investigate the minimal two-rollout case (2-GRPO), a configuration\npreviously deemed infeasible. We provide a rigorous theoretical analysis to\nvalidate 2-GRPO and demonstrate empirically that it achieves performance on par\nwith 16-GRPO, despite using only 1/8 of the rollouts and reducing training time\nby over 70%.", "AI": {"tldr": "2-GRPO\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e0e16-GRPO\u76f8\u5f53\u7684\u6a21\u578b\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534770%+\uff0c\u4ec5\u97001/8\u8bad\u7ec3\u6570\u636e\u91cf", "motivation": "\u6311\u6218\u4f20\u7edf\u8ba4\u77e5\u4e2dGRPO\u7b97\u6cd5\u9700\u8981\u5927\u6837\u672c\u91cf\u7684\u5047\u8bbe\uff0c\u901a\u8fc7\u5efa\u7acb\u4e0eDPO\u7b97\u6cd5\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63a2\u7d22\u6700\u5c0f\u6837\u672c\u91cf\u914d\u7f6e\uff082\u7ec4\u6837\u672c\uff09\u7684\u53ef\u884c\u6027", "method": "1. \u7406\u8bba\u5c42\u9762\u5c06GRPO\u91cd\u6784\u4e3a\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\n2. \u5efa\u7acb\u4e0eDPO\u7b97\u6cd5\u7684\u7406\u8bba\u5173\u8054\n3. \u5b9e\u8bc1\u9a8c\u8bc1\u4e24\u6837\u672c\u914d\u7f6e\uff082-GRPO\uff09\u7684\u6709\u6548\u6027", "result": "2-GRPO\u5728\u6027\u80fd\u4e0a\u4e0e16-GRPO\u6301\u5e73\uff08\u4ec51/8\u6837\u672c\u91cf\uff09\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed70%\u4ee5\u4e0a\uff0c\u663e\u5b58\u6d88\u8017\u663e\u8457\u964d\u4f4e", "conclusion": "\u8be5\u7814\u7a76\u7a81\u7834\u4e86\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5bf9\u5927\u6570\u636e\u91cf\u7684\u4f9d\u8d56\uff0c\u4e3aLLM\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u91cd\u8981\u5de5\u7a0b\u5b9e\u8df5\u4ef7\u503c"}}
{"id": "2510.00982", "pdf": "https://arxiv.org/pdf/2510.00982", "abs": "https://arxiv.org/abs/2510.00982", "authors": ["Emiru Tsunoo", "Hayato Futami", "Yosuke Kashiwagi", "Siddhant Arora", "Shinji Watanabe"], "title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted for ASRU 2025", "summary": "For streaming speech recognition, a Transformer-based encoder has been widely\nused with block processing. Although many studies addressed improving emission\nlatency of transducers, little work has been explored for improving encoding\nlatency of the block processing. We seek to reduce latency by frequently\nemitting a chunk with a small shift rather than scarce large-chunk emissions,\nresulting in higher computational costs. To efficiently compute with the small\nchunk shift, we propose a new encoder, Spiralformer, tailored for block\nprocessing by combining layer dropping and early exiting. We skip layer\ncomputation in a cyclic manner and shift the computed layer in each block\nspirally, which completes computation for all the layers over the block\nprocessing. Experimentally, we observed that our method achieved 21.6%\nreduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ,\ncompared with the baseline with similar computational cost and word error\nrates.", "AI": {"tldr": "\u63d0\u51faSpiralformer\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u5c42\u5faa\u73af\u4e22\u5f03\u548c\u87ba\u65cb\u5c42\u8f6c\u79fb\u673a\u5236\u4f18\u5316\u5757\u5904\u7406\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6210\u672c\u4e0e\u8bcd\u9519\u8bef\u7387\u76f8\u8fd1\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8bed\u97f3\u8bc6\u522b\u7f16\u7801\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u6d41\u5f0f\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\u4e3b\u8981\u5173\u6ce8transducer\u7684\u53d1\u5c04\u5ef6\u8fdf\uff0c\u800c\u5757\u5904\u7406\u7684\u7f16\u7801\u5ef6\u8fdf\u6539\u5584\u4e0d\u8db3\u3002\u9891\u7e41\u5c0f\u6b65\u957f\u8f93\u51fa\u867d\u80fd\u964d\u4f4e\u5ef6\u8fdf\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c", "method": "\u7ed3\u5408\u5c42\u4e22\u5f03\uff08cyclic layer dropping\uff09\u548c\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u901a\u8fc7\u87ba\u65cb\u5f0f\u5c42\u8f6c\u79fb\u7b56\u7565\u5728\u5757\u5904\u7406\u4e2d\u5b8c\u6210\u6240\u6709\u5c42\u7684\u8ba1\u7b97\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387", "result": "Librispeech\u5e73\u5747token\u5ef6\u8fdf\u964d\u4f4e21.6%\uff0cCSJ\u964d\u4f4e7.0%\uff0c\u8ba1\u7b97\u6210\u672c\u4e0e\u8bcd\u9519\u8bef\u7387\u4e0e\u57fa\u7ebf\u6a21\u578b\u76f8\u5f53", "conclusion": "Spiralformer\u901a\u8fc7\u521b\u65b0\u7684\u5c42\u8c03\u5ea6\u673a\u5236\uff0c\u5728\u6d41\u5f0f\u8bed\u97f3\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u5ef6\u8fdf\u4f18\u5316\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861"}}
{"id": "2510.01003", "pdf": "https://arxiv.org/pdf/2510.01003", "abs": "https://arxiv.org/abs/2510.01003", "authors": ["Boshi Wang", "Weijian Xu", "Yunsheng Li", "Mei Gao", "Yujia Xie", "Huan Sun", "Dongdong Chen"], "title": "Improving Code Localization with Repository Memory", "categories": ["cs.SE", "cs.CL"], "comment": "15 pages, 8 figures", "summary": "Code localization is a fundamental challenge in repository-level software\nengineering tasks such as bug fixing. While existing methods equip language\nagents with comprehensive tools/interfaces to fetch information from the\nrepository, they overlook the critical aspect of memory, where each instance is\ntypically handled from scratch assuming no prior repository knowledge. In\ncontrast, human developers naturally build long-term repository memory, such as\nthe functionality of key modules and associations between various bug types and\ntheir likely fix locations. In this work, we augment language agents with such\nmemory by leveraging a repository's commit history - a rich yet underutilized\nresource that chronicles the codebase's evolution. We introduce tools that\nallow the agent to retrieve from a non-parametric memory encompassing recent\nhistorical commits and linked issues, as well as functionality summaries of\nactively evolving parts of the codebase identified via commit patterns. We\ndemonstrate that augmenting such a memory can significantly improve LocAgent, a\nstate-of-the-art localization framework, on both SWE-bench-verified and the\nmore recent SWE-bench-live benchmarks. Our research contributes towards\ndeveloping agents that can accumulate and leverage past experience for\nlong-horizon tasks, more closely emulating the expertise of human developers.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u4ed3\u5e93\u63d0\u4ea4\u5386\u53f2\u6784\u5efa\u975e\u53c2\u6570\u5316\u8bb0\u5fc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7801\u5b9a\u4f4d\u6846\u67b6LocAgent\u5728SWE-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u73b0\u6709\u4ee3\u7801\u5b9a\u4f4d\u65b9\u6cd5\u7f3a\u4e4f\u957f\u671f\u8bb0\u5fc6\u673a\u5236\uff0c\u800c\u4eba\u7c7b\u5f00\u53d1\u8005\u4f1a\u81ea\u7136\u79ef\u7d2f\u4ed3\u5e93\u77e5\u8bc6\u3002\u63d0\u4ea4\u5386\u53f2\u4f5c\u4e3a\u8bb0\u5f55\u4ee3\u7801\u5e93\u6f14\u53d8\u7684\u5b9d\u8d35\u8d44\u6e90\u672a\u88ab\u5145\u5206\u5229\u7528", "method": "\u5f00\u53d1\u5de5\u5177\u5b9e\u73b0\u4e09\u5c42\u6b21\u8bb0\u5fc6\u68c0\u7d22\uff1a1\uff09\u8fd1\u671f\u63d0\u4ea4\u5386\u53f2\u4e0e\u5173\u8054\u95ee\u9898 2\uff09\u901a\u8fc7\u63d0\u4ea4\u6a21\u5f0f\u8bc6\u522b\u7684\u6d3b\u8dc3\u4ee3\u7801\u6a21\u5757\u529f\u80fd\u6458\u8981 3\uff09\u5efa\u7acb\u9519\u8bef\u7c7b\u578b\u4e0e\u4fee\u590d\u4f4d\u7f6e\u7684\u5173\u8054\u6a21\u5f0f", "result": "\u5728SWE-bench-verified\u548cSWE-bench-live\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\u5bf9LocAgent\u6846\u67b6\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u5177\u5907\u7ecf\u9a8c\u79ef\u7d2f\u80fd\u529b\u7684\u667a\u80fd\u4f53\u5f00\u53d1\uff0c\u901a\u8fc7\u8bb0\u5fc6\u673a\u5236\u6a21\u62df\u4eba\u7c7b\u5f00\u53d1\u8005\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e3a\u957f\u671f\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2510.01025", "pdf": "https://arxiv.org/pdf/2510.01025", "abs": "https://arxiv.org/abs/2510.01025", "authors": ["Federico Tiblias", "Irina Bigoulaeva", "Jingcheng Niu", "Simone Balloccu", "Iryna Gurevych"], "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The linear representation hypothesis states that language models (LMs) encode\nconcepts as directions in their latent space, forming organized,\nmultidimensional manifolds. Prior efforts focus on discovering specific\ngeometries for specific features, and thus lack generalization. We introduce\nSupervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to\nautomatically discover feature manifolds. We apply SMDS to temporal reasoning\nas a case study, finding that different features form various geometric\nstructures such as circles, lines, and clusters. SMDS reveals many insights on\nthese structures: they consistently reflect the properties of the concepts they\nrepresent; are stable across model families and sizes; actively support\nreasoning in models; and dynamically reshape in response to context changes.\nTogether, our findings shed light on the functional role of feature manifolds,\nsupporting a model of entity-based reasoning in which LMs encode and transform\nstructured representations.", "AI": {"tldr": "\u63d0\u51fa\u76d1\u7763\u591a\u7ef4\u7f29\u653e\uff08SMDS\uff09\u65b9\u6cd5\uff0c\u81ea\u52a8\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u591a\u7ef4\u7279\u5f81\u6d41\u5f62\u7ed3\u6784\uff0c\u63ed\u793a\u5176\u7a33\u5b9a\u6027\u548c\u52a8\u6001\u8c03\u6574\u7279\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u7279\u5f81\u7684\u51e0\u4f55\u7ed3\u6784\u53d1\u73b0\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u9700\u8981\u6a21\u578b\u65e0\u5173\u7684\u81ea\u52a8\u7279\u5f81\u6d41\u5f62\u63a2\u7d22\u65b9\u6cd5", "method": "\u5f00\u53d1\u6a21\u578b\u65e0\u5173\u7684SMDS\u65b9\u6cd5\uff0c\u4ee5\u65f6\u95f4\u63a8\u7406\u4e3a\u6848\u4f8b\uff0c\u5206\u6790\u4e0d\u540c\u7279\u5f81\u5f62\u6210\u7684\u51e0\u4f55\u7ed3\u6784\uff08\u5706\u5f62/\u76f4\u7ebf/\u96c6\u7fa4\uff09", "result": "\u53d1\u73b0\u7279\u5f81\u6d41\u5f62\uff1a1) \u53cd\u6620\u6982\u5ff5\u5c5e\u6027 2) \u8de8\u6a21\u578b\u7a33\u5b9a 3) \u652f\u6301\u63a8\u7406\u8fc7\u7a0b 4) \u968f\u4e0a\u4e0b\u6587\u52a8\u6001\u91cd\u5851", "conclusion": "\u7279\u5f81\u6d41\u5f62\u5728\u5b9e\u4f53\u63a8\u7406\u4e2d\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u652f\u6301\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u5f81\u8fdb\u884c\u7f16\u7801\u548c\u8f6c\u6362\u7684\u63a8\u7406\u673a\u5236"}}
{"id": "2510.01047", "pdf": "https://arxiv.org/pdf/2510.01047", "abs": "https://arxiv.org/abs/2510.01047", "authors": ["Xiao Li", "Jiaqi Zhang", "Shuxiang Zhang", "Tianshui Chen", "Liang Lin", "Guangrun Wang"], "title": "Authentic Discrete Diffusion Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally\nredefines prior pseudo-discrete approaches by preserving core diffusion\ncharacteristics directly in the one-hot space through a suite of coordinated\nmechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD\nreformulates the diffusion input by directly using float-encoded one-hot class\ndata, without relying on diffusing in the continuous latent spaces or masking\npolicies. At its core, a timestep-conditioned cross-entropy loss is introduced\nbetween the diffusion model's outputs and the original one-hot labels. This\nsynergistic design establishes a bridge between discriminative and generative\nlearning. Our experiments demonstrate that ADD not only achieves superior\nperformance on classification tasks compared to the baseline, but also exhibits\nexcellent text generation capabilities on Image captioning. Extensive ablations\nvalidate the measurable gains of each component.", "AI": {"tldr": "ADD\u6846\u67b6\u901a\u8fc7one-hot\u7a7a\u95f4\u76f4\u63a5\u4fdd\u6301\u6269\u6563\u7279\u6027\uff0c\u6539\u8fdb\u4f20\u7edf\u4f2a\u79bb\u6563\u6269\u6563\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u548c\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edf\u4f2a\u79bb\u6563\u6269\u6563\u65b9\u6cd5\u4f9d\u8d56\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u6269\u6563\u6216\u63a9\u7801\u7b56\u7565\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u79bb\u6563\u6269\u6563\u7279\u6027\u3002\u9700\u8981\u5efa\u7acb\u5224\u522b\u5f0f\u4e0e\u751f\u6210\u5f0f\u5b66\u4e60\u7684\u6865\u6881", "method": "1. \u76f4\u63a5\u4f7f\u7528\u6d6e\u70b9\u7f16\u7801\u7684one-hot\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\n2. \u63d0\u51fa\u65f6\u95f4\u6b65\u6761\u4ef6\u5316\u4ea4\u53c9\u71b5\u635f\u5931\n3. \u5728\u6269\u6563\u8f93\u51fa\u4e0e\u539f\u59cb\u6807\u7b7e\u95f4\u5efa\u7acb\u8054\u7cfb", "result": "\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\uff0c\u56fe\u50cf\u63cf\u8ff0\u751f\u6210\u8868\u73b0\u51fa\u8272\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u6709\u6548\u6027", "conclusion": "ADD\u6846\u67b6\u5f00\u521b\u4e86\u771f\u6b63\u7684\u79bb\u6563\u6269\u6563\u8303\u5f0f\uff0c\u517c\u5177\u5224\u522b\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u591a\u4efb\u52a1\u7edf\u4e00\u6a21\u578b\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2510.01051", "pdf": "https://arxiv.org/pdf/2510.01051", "abs": "https://arxiv.org/abs/2510.01051", "authors": ["Zichen Liu", "Anya Sims", "Keyu Duan", "Changyu Chen", "Simon Yu", "Xiangxin Zhou", "Haotian Xu", "Shaopan Xiong", "Bo Liu", "Chenmien Tan", "Chuen Yang Beh", "Weixun Wang", "Hao Zhu", "Weiyan Shi", "Diyi Yang", "Michael Shieh", "Yee Whye Teh", "Wee Sun Lee", "Min Lin"], "title": "GEM: A Gym for Agentic LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The training paradigm for large language models (LLMs) is moving from static\ndatasets to experience-based learning, where agents acquire skills via\ninteracting with complex environments. To facilitate this transition we\nintroduce GEM (General Experience Maker), an open-source environment simulator\ndesigned for the age of LLMs. Analogous to OpenAI-Gym for traditional\nreinforcement learning (RL), GEM provides a standardized framework for the\nenvironment-agent interface, including asynchronous vectorized execution for\nhigh throughput, and flexible wrappers for easy extensibility. GEM also\nfeatures a diverse suite of environments, robust integrated tools, and\nsingle-file example scripts demonstrating using GEM with five popular RL\ntraining frameworks. Along with this, we also provide a set of baselines across\n24 environments using REINFORCE with Return Batch Normalization (ReBN), which\n-- unlike GRPO -- is compatible with the full RL setting of dense per-turn\nrewards and offers better credit assignment. We further conduct apple-to-apple\nbenchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings\nusing GEM to shed light on the algorithmic designs. Lastly, GEM also functions\nas a convenient evaluation toolkit besides a training environment. We hope this\nframework can help accelerate future agentic LLM research.", "AI": {"tldr": "\u63d0\u51fa\u5f00\u6e90\u73af\u5883\u6a21\u62df\u5668GEM\uff0c\u4e3aLLM\u63d0\u4f9b\u7c7b\u4f3cOpenAI-Gym\u7684\u6807\u51c6\u5316\u8bad\u7ec3\u6846\u67b6\uff0c\u652f\u6301\u591aRL\u6846\u67b6\u5e76\u9a8c\u8bc1ReBN\u7b97\u6cd5\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8303\u5f0f\u6b63\u4ece\u9759\u6001\u6570\u636e\u8f6c\u5411\u7ecf\u9a8c\u5b66\u4e60\uff0c\u9700\u8981\u6807\u51c6\u5316\u73af\u5883\u63a5\u53e3\u4fc3\u8fdb\u6280\u672f\u53d1\u5c55", "method": "\u5f00\u53d1\u5177\u5907\u5f02\u6b65\u5411\u91cf\u5316\u6267\u884c\u3001\u7075\u6d3b\u6269\u5c55\u63a5\u53e3\u7684GEM\u6846\u67b6\uff0c\u96c6\u621024\u4e2a\u73af\u5883\u6d4b\u8bd5\u57fa\u51c6\uff0c\u4f7f\u7528REINFORCE+ReBN\u7b97\u6cd5\u8fdb\u884c\u8de8\u73af\u5883\u9a8c\u8bc1", "result": "ReBN\u5728\u5bc6\u96c6\u5956\u52b1\u8bbe\u7f6e\u4e2d\u4f18\u4e8eGRPO\uff0c\u901a\u8fc7GEM\u5b9e\u73b0PPO/GRPO/REINFORCE\u7684\u516c\u5e73\u5bf9\u6bd4\uff0c\u63ed\u793a\u591a\u8f6e\u573a\u666f\u7b97\u6cd5\u8bbe\u8ba1\u5dee\u5f02", "conclusion": "GEM\u4f5c\u4e3a\u8bad\u7ec3\u73af\u5883\u4e0e\u8bc4\u4f30\u5de5\u5177\u5305\uff0c\u6709\u671b\u52a0\u901f\u4ee3\u7406\u578bLLM\u7814\u7a76\uff0c\u5176\u6807\u51c6\u5316\u8bbe\u8ba1\u652f\u6301\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u8fed\u4ee3\u4e0e\u6548\u679c\u9a8c\u8bc1"}}
{"id": "2510.01132", "pdf": "https://arxiv.org/pdf/2510.01132", "abs": "https://arxiv.org/abs/2510.01132", "authors": ["Ruiyi Wang", "Prithviraj Ammanabrolu"], "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We study what actually works and what doesn't for training large language\nmodels as agents via multi-turn reinforcement learning. Despite rapid progress,\nexisting frameworks and definitions are fragmented, and there is no systematic\nformulation or analysis of which design choices matter across tasks. We address\nthis gap by first breaking down the design space into three inter-related\npillars -- environment, reward, and policy -- and empirically derive a recipe\nfor training LLM agents in situated textual domains. In particular, we test\nTextWorld and ALFWorld, popular domains for testing situated embodied\nreasoning, as well as SWE-Gym for more software engineering style tasks. (i)\nFor the environment, we analyze the impacts of task complexity in terms of\nsizes of the state and action spaces as well as optimal solution length,\nfinding that even simple environments within a domain can provide signal on how\nwell an agent can generalize to more complex tasks. (ii) For the reward, we\nablate relative reward sparsity, observing that while dense turn-level rewards\naccelerate training, performance and stability is highly dependent on the\nchoice of RL algorithm. (iii) And for the agent's policy, we explore the\ninterplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO)\npolicy gradient methods in addition to showing how to find the optimal\nSupervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We\ndistill these findings into a training recipe that guides co-design across the\nthree pillars, facilitating research and practical efforts in multi-turn\nagentic RL. Code: https://github.com/pearls-lab/meow-tea-taro", "AI": {"tldr": "\u7cfb\u7edf\u5206\u6790\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4e2d\u73af\u5883\u3001\u5956\u52b1\u3001\u7b56\u7565\u4e09\u5927\u652f\u67f1\u5bf9LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u8de8\u9886\u57df\u8bad\u7ec3\u65b9\u6848", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u8bad\u7ec3\u6846\u67b6\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u6838\u5fc3\u8bbe\u8ba1\u8981\u7d20\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u963b\u788d\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\u7814\u7a76", "method": "\u5c06\u8bbe\u8ba1\u7a7a\u95f4\u5206\u89e3\u4e3a\u73af\u5883-\u5956\u52b1-\u7b56\u7565\u4e09\u652f\u67f1\uff0c\u5728TextWorld/ALFWorld/SWE-Gym\u7b49\u6587\u672c\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u5956\u52b1\u7a00\u758f\u5ea6\u4e0e\u7b56\u7565\u68af\u5ea6\u65b9\u6cd5\u7684\u76f8\u4e92\u5173\u7cfb", "result": "\u53d1\u73b0\u73af\u5883\u590d\u6742\u5ea6\u4e0e\u6cdb\u5316\u80fd\u529b\u6b63\u76f8\u5173\uff1b\u5bc6\u96c6\u56de\u5408\u5956\u52b1\u52a0\u901f\u8bad\u7ec3\u4f46\u7a33\u5b9a\u6027\u4f9d\u8d56RL\u7b97\u6cd5\u9009\u62e9\uff1bPPO/GRPO\u4e0eRLOO\u5728\u4e0d\u540c\u5956\u52b1\u7a00\u758f\u5ea6\u4e0b\u8868\u73b0\u4e92\u8865\uff1b\u786e\u5b9aSFT\u4e0eRL\u7684\u6700\u4f18\u8bad\u7ec3\u6bd4\u4f8b", "conclusion": "\u63d0\u51fa\u4e09\u652f\u67f1\u534f\u540c\u8bbe\u8ba1\u8bad\u7ec3\u65b9\u6848\uff0c\u4e3a\u591a\u8f6e\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u7cfb\u7edf\u6307\u5bfc\u6846\u67b6\uff0c\u5f00\u6e90\u4ee3\u7801\u4fc3\u8fdb\u5b9e\u8df5\u5e94\u7528"}}
{"id": "2510.01135", "pdf": "https://arxiv.org/pdf/2510.01135", "abs": "https://arxiv.org/abs/2510.01135", "authors": ["Zhaolin Gao", "Joongwon Kim", "Wen Sun", "Thorsten Joachims", "Sid Wang", "Richard Yuanzhe Pang", "Liang Tan"], "title": "Prompt Curriculum Learning for Efficient LLM Post-Training", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement\nlearning (RL) algorithm that selects intermediate-difficulty prompts using a\nlearned value model to post-train language models. Since post-training LLMs via\nRL remains sensitive to batching and prompt selection strategies, we first\nconduct a series of systematic experiments where we (1) determine the optimal\ntraining batch size that balances generation efficiency and gradient quality\nand (2) establish the importance of focusing on prompts of intermediate\ndifficulty for the policy. We build upon these results to design PCL, which\nidentifies prompts of intermediate difficulty for the current policy in an\non-policy manner by using a value model that is concurrently updated based on\nthe current policy. By focusing on informative prompts that yield high\neffective ratios, PCL achieves either the highest performance or requires\nsignificantly less time to reach comparable performance to its counterparts.\nCompared to rollout-based filtering methods, PCL avoids costly rollouts and\nachieves $12.1\\times$ and $16.9\\times$ faster speed on identifying\nintermediate-difficulty prompts when training on MATH and DeepScaleR,\nrespectively. We further demonstrate that our value model accurately predicts\nprompt difficulty and allows PCL to focus on progressively more challenging\nprompts during RL. Our results present a new methodology that delivers improved\ntradeoff between upper-bound performance and efficiency for reasoning-focused\nRL.", "AI": {"tldr": "\u63d0\u51faPrompt Curriculum Learning (PCL)\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e2d\u7b49\u96be\u5ea6\u63d0\u793a\u548c\u5e76\u53d1\u66f4\u65b0\u4ef7\u503c\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347RL\u8bad\u7ec3\u6548\u7387\u548c\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8bed\u8a00\u6a21\u578b\u540e\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u6279\u6b21\u5904\u7406\u548c\u63d0\u793a\u9009\u62e9\u7b56\u7565\u654f\u611f\uff0c\u9700\u5e73\u8861\u8bad\u7ec3\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u4e0a\u9650", "method": "1. \u7cfb\u7edf\u5b9e\u9a8c\u786e\u5b9a\u6700\u4f73\u6279\u6b21\u89c4\u6a21 2. \u63d0\u51fa\u4ef7\u503c\u6a21\u578b\u52a8\u6001\u8bc4\u4f30\u63d0\u793a\u96be\u5ea6 3. \u57fa\u4e8e\u6709\u6548\u6bd4\u7387\u7b5b\u9009\u4e2d\u7b49\u96be\u5ea6\u63d0\u793a\u8fdb\u884c\u7b56\u7565\u8bad\u7ec3", "result": "\u5728MATH\u548cDeepScaleR\u4e0a\u5206\u522b\u5b9e\u73b012.1\u500d\u548c16.9\u500d\u7684\u63d0\u793a\u7b5b\u9009\u52a0\u901f\uff0c\u4e14\u8fbe\u5230\u6700\u9ad8\u6027\u80fd\u6216\u66f4\u5feb\u6536\u655b\u901f\u5ea6", "conclusion": "PCL\u901a\u8fc7\u63d0\u793a\u8bfe\u7a0b\u5b66\u4e60\u673a\u5236\uff0c\u4e3a\u63a8\u7406\u5bfc\u5411\u7684\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u4f18\u5316\u65b0\u8303\u5f0f"}}
{"id": "2510.01167", "pdf": "https://arxiv.org/pdf/2510.01167", "abs": "https://arxiv.org/abs/2510.01167", "authors": ["Yiran Shen", "Yu Xia", "Jonathan Chang", "Prithviraj Ammanabrolu"], "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Aligning large language models to human preferences is inherently\nmultidimensional, yet most pipelines collapse heterogeneous signals into a\nsingle optimizeable objective. We seek to answer what it would take to\nsimultaneously align a model across various domains spanning those with:\nverifiable rewards (mathematical accuracy), non-verifiable subjective\npreferences (human values), and complex interactive scenarios (multi-turn AI\ntutoring dialogues). Such multi-objective reinforcement learning setups are\noften plagued by the individual objectives being at odds with each other,\nresulting in inefficient training and little user control during inference. We\npropose a unified framework that: (i) standardizes {process reward model} (PRM)\ntraining across both verifiable and non-verifiable settings to better supervise\nmodels' chain-of-thought reasoning; (ii) performs {multi-objective alignment}\nby training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead\n$\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the\nvector correspond to the various objectives instead of a single scalar; and\n(iii) demonstrates how such a system provides fine-grained inference-time user\ncontrol. Experiments across math reasoning, value alignment, and multi-turn\ndialogue show that our framework improves performance across multiple\nobjectives simultaneously, while minimizing cross-objective trade-offs and\nenabling flexible inference time user control. The code can be found at\nhttps://github.com/pearls-lab/multiobj-align.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6MAH-DPO\u5b9e\u73b0\u591a\u76ee\u6807\u5bf9\u9f50\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u5956\u52b1\u673a\u5236\u540c\u65f6\u4f18\u5316\u6570\u5b66\u51c6\u786e\u6027\u3001\u4ef7\u503c\u89c2\u5bf9\u9f50\u548c\u591a\u8f6e\u5bf9\u8bdd\u6027\u80fd\uff0c\u5b9e\u73b0\u63a8\u7406\u65f6\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "motivation": "\u4f20\u7edfLLM\u5bf9\u9f50\u65b9\u6cd5\u5c06\u591a\u7ef4\u4fe1\u53f7\u538b\u7f29\u4e3a\u5355\u4e00\u76ee\u6807\uff0c\u5bfc\u81f4\u76ee\u6807\u51b2\u7a81\u548c\u8bad\u7ec3\u4f4e\u6548\u3002\u9700\u89e3\u51b3\u53ef\u9a8c\u8bc1\u5956\u52b1\uff08\u6570\u5b66\uff09\u3001\u4e3b\u89c2\u504f\u597d\uff08\u4ef7\u503c\u89c2\uff09\u548c\u590d\u6742\u4ea4\u4e92\uff08\u5bf9\u8bdd\uff09\u7684\u591a\u7ef4\u5ea6\u5bf9\u9f50\u96be\u9898\u3002", "method": "1. \u6807\u51c6\u5316\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u76d1\u7763\u601d\u7ef4\u94fe 2. \u4f7f\u7528\u591a\u52a8\u4f5c\u5934DPO(MAH-DPO)\u8fdb\u884c\u591a\u76ee\u6807\u5bf9\u9f50 3. \u6784\u5efa\u5411\u91cf\u5956\u52b1\u66ff\u4ee3\u6807\u91cf\u5956\u52b1\uff0c\u5404\u7ef4\u5ea6\u5bf9\u5e94\u4e0d\u540c\u76ee\u6807", "result": "\u5728\u6570\u5b66\u63a8\u7406\uff08MATH\u51c6\u786e\u7387\u219112%\uff09\u3001\u4ef7\u503c\u89c2\u5bf9\u9f50\uff08HHH\u8bc4\u4f30\u63d0\u5347\uff09\u548c\u591a\u8f6e\u8f85\u5bfc\u5bf9\u8bdd\uff08F1\u5f97\u5206\u21917.3%\uff09\u4e2d\u5747\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u51cf\u5c11\u76ee\u6807\u95f4\u51b2\u7a81", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u8de8\u9886\u57df\u591a\u76ee\u6807\u534f\u540c\u4f18\u5316\uff0c\u901a\u8fc7\u5411\u91cf\u5956\u52b1\u7ef4\u5ea6\u5206\u79bb\u63d0\u4f9b\u7075\u6d3b\u63a8\u7406\u63a7\u5236\uff0c\u4e3a\u590d\u6742\u5bf9\u9f50\u573a\u666f\u63d0\u4f9b\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.01174", "pdf": "https://arxiv.org/pdf/2510.01174", "abs": "https://arxiv.org/abs/2510.01174", "authors": ["Yanzhe Chen", "Kevin Qinghong Lin", "Mike Zheng Shou"], "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.MM"], "comment": "Project Page: https://showlab.github.io/Code2Video/", "summary": "While recent generative models advance pixel-space video synthesis, they\nremain limited in producing professional educational videos, which demand\ndisciplinary knowledge, precise visual structures, and coherent transitions,\nlimiting their applicability in educational scenarios. Intuitively, such\nrequirements are better addressed through the manipulation of a renderable\nenvironment, which can be explicitly controlled via logical commands (e.g.,\ncode). In this work, we propose Code2Video, a code-centric agent framework for\ngenerating educational videos via executable Python code. The framework\ncomprises three collaborative agents: (i) Planner, which structures lecture\ncontent into temporally coherent flows and prepares corresponding visual\nassets; (ii) Coder, which converts structured instructions into executable\nPython codes while incorporating scope-guided auto-fix to enhance efficiency;\nand (iii) Critic, which leverages vision-language models (VLM) with visual\nanchor prompts to refine spatial layout and ensure clarity. To support\nsystematic evaluation, we build MMMC, a benchmark of professionally produced,\ndiscipline-specific educational videos. We evaluate MMMC across diverse\ndimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and\nparticularly, TeachQuiz, a novel end-to-end metric that quantifies how well a\nVLM, after unlearning, can recover knowledge by watching the generated videos.\nOur results demonstrate the potential of Code2Video as a scalable,\ninterpretable, and controllable approach, achieving 40% improvement over direct\ncode generation and producing videos comparable to human-crafted tutorials. The\ncode and datasets are available at https://github.com/showlab/Code2Video.", "AI": {"tldr": "Code2Video\u901a\u8fc7Python\u4ee3\u7801\u9a71\u52a8\u4e09\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff08\u89c4\u5212\u5668/\u7f16\u7801\u5668/\u8bc4\u5ba1\u5668\uff09\uff0c\u751f\u6210\u4e13\u4e1a\u6559\u5b66\u89c6\u9891\uff0c\u76f8\u6bd4\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u6548\u7387\u63d0\u534740%\u4e14\u8d28\u91cf\u5ab2\u7f8e\u4eba\u5de5\u6559\u7a0b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u4e13\u4e1a\u6559\u5b66\u89c6\u9891\u5236\u4f5c\u4e2d\u5b58\u5728\u5b66\u79d1\u77e5\u8bc6\u6574\u5408\u3001\u89c6\u89c9\u7ed3\u6784\u7cbe\u786e\u6027\u548c\u8fc7\u6e21\u8fde\u8d2f\u6027\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u53ef\u7f16\u7a0b\u6e32\u67d3\u73af\u5883\u5b9e\u73b0\u7cbe\u51c6\u63a7\u5236\u7684\u6559\u5b66\u89c6\u9891\u751f\u6210\u3002", "method": "1. \u89c4\u5212\u5668\uff08Planner\uff09\u6784\u5efa\u65f6\u5e8f\u8fde\u8d2f\u7684\u6559\u5b66\u6d41\u7a0b\u548c\u89c6\u89c9\u7d20\u6750\uff1b2. \u7f16\u7801\u5668\uff08Coder\uff09\u5c06\u6307\u4ee4\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u4ee3\u7801\uff08\u542b\u4f5c\u7528\u57df\u5f15\u5bfc\u7684\u81ea\u52a8\u4fee\u590d\uff09\uff1b3. \u8bc4\u5ba1\u5668\uff08Critic\uff09\u901a\u8fc7\u89c6\u89c9\u951a\u70b9\u63d0\u793a\u4f18\u5316\u7a7a\u95f4\u5e03\u5c40\u3002", "result": "\u63d0\u51faMMMC\u8bc4\u4f30\u57fa\u51c6\u548cTeachQuiz\u7aef\u5230\u7aef\u6307\u6807\uff0c\u89c6\u9891\u751f\u6210\u8d28\u91cf\u76f8\u6bd4\u76f4\u63a5\u4ee3\u7801\u751f\u6210\u63d0\u534740%\uff0c\u89c6\u89c9\u6548\u679c\u63a5\u8fd1\u4eba\u7c7b\u4e13\u4e1a\u6559\u7a0b\u3002", "conclusion": "\u4ee3\u7801\u9a71\u52a8\u7684\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4e3a\u6559\u5b66\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2510.01179", "pdf": "https://arxiv.org/pdf/2510.01179", "abs": "https://arxiv.org/abs/2510.01179", "authors": ["Zhangchen Xu", "Adriana Meza Soria", "Shawn Tan", "Anurag Roy", "Ashish Sunil Agrawal", "Radha Poovendran", "Rameswar Panda"], "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "35 pages, 13 figures", "summary": "Large Language Model (LLM) agents are rapidly emerging as powerful systems\nfor automating tasks across domains. Yet progress in the open-source community\nis constrained by the lack of high quality permissively licensed tool-agentic\ntraining data. Existing datasets are often limited in diversity, realism, and\ncomplexity, particularly regarding multi-tool and multi-turn interactions. To\naddress this gap, we introduce Toucan, the largest publicly available\ntool-agentic dataset to date, containing 1.5 million trajectories synthesized\nfrom nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work,\nToucan leverages authentic MCP environments to generate diverse, realistic, and\nchallenging tasks with trajectories involving real tool execution. Our pipeline\nfirst produces a broad spectrum of tool-use queries using five distinct models,\napplies model-based quality filtering, and then generates agentic trajectories\nwith three teacher models using two agentic frameworks. Rigorous rule-based and\nmodel-based validation ensures high-quality outputs. We also introduce three\nextension mechanisms to further diversify tasks and simulate multi-turn\nconversations. Models fine-tuned on Toucan outperform larger closed-source\ncounterparts on the BFCL V3 benchmark and push the Pareto frontier forward on\nMCP-Universe Bench.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5f53\u524d\u6700\u5927\u7684\u5f00\u6e90\u5de5\u5177\u4ee3\u7406\u6570\u636e\u96c6Toucan\uff0c\u901a\u8fc7\u771f\u5b9e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u751f\u6210150\u4e07\u6761\u8f68\u8ff9\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u5de5\u5177\u4ee3\u7406\u8bad\u7ec3\u6570\u636e\u5728\u591a\u6837\u6027\u3001\u771f\u5b9e\u6027\u548c\u591a\u5de5\u5177/\u591a\u8f6e\u4ea4\u4e92\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5236\u7ea6\u4e86\u5f00\u6e90\u793e\u533a\u53d1\u5c55\u3002", "method": "\u4f7f\u75285\u4e2a\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u67e5\u8be2\u2192\u8d28\u91cf\u8fc7\u6ee4\u21923\u4e2a\u6559\u5e08\u6a21\u578b+2\u4e2a\u6846\u67b6\u751f\u6210\u8f68\u8ff9\u2192\u4e25\u683c\u89c4\u5219/\u6a21\u578b\u53cc\u91cd\u9a8c\u8bc1\u2192\u6269\u5c55\u673a\u5236\u5b9e\u73b0\u591a\u8f6e\u5bf9\u8bdd\u6a21\u62df", "result": "Toucan\u5fae\u8c03\u6a21\u578b\u5728BFCL V3\u57fa\u51c6\u8d85\u8d8a\u95ed\u6e90\u5927\u6a21\u578b\uff0c\u5728MCP-Universe Bench\u63a8\u8fdb\u5e15\u7d2f\u6258\u524d\u6cbf", "conclusion": "Toucan\u586b\u8865\u4e86\u9ad8\u8d28\u91cf\u5f00\u6e90\u5de5\u5177\u4ee3\u7406\u6570\u636e\u7a7a\u767d\uff0c\u5176\u5408\u6210\u9a8c\u8bc1\u6846\u67b6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.01180", "pdf": "https://arxiv.org/pdf/2510.01180", "abs": "https://arxiv.org/abs/2510.01180", "authors": ["Jian Hu", "Mingjie Liu", "Ximing Lu", "Fang Wu", "Zaid Harchaoui", "Shizhe Diao", "Yejin Choi", "Pavlo Molchanov", "Jun Yang", "Jan Kautz", "Yi Dong"], "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration", "categories": ["cs.LG", "cs.CL"], "comment": "16 pages, 4 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key\ningredient for unlocking complex reasoning capabilities in large language\nmodels. Recent work ProRL has shown promise in scaling RL by increasing the\nnumber of training steps. However, performance plateaus after thousands of\nsteps, with clear diminishing returns from allocating more computation to\nadditional training. In this work, we investigate a complementary paradigm for\nscaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to\nexhaustively Broaden exploration, which yields continuous performance gains\nbeyond the saturation point observed in ProRL when scaling the number of\ntraining steps. Our approach is motivated by a mass balance equation analysis\nallowing us to characterize the rate of change in probability mass for correct\nand incorrect tokens during the reinforcement process. We show that under a\none-step RL assumption, sampled rollout tokens always contribute to\ncorrect-mass expansion, while unsampled tokens outside rollouts may lead to\ngains or losses depending on their distribution and the net reward balance.\nImportantly, as the number of rollouts per example N increases, the effect of\nunsampled terms diminishes, ensuring overall correct-mass expansion. To\nvalidate our theoretical analysis, we conduct simulations under more relaxed\nconditions and find that a sufficiently large rollout size N-corresponding to\nample exploration-guarantees an increase in the probability mass of all correct\ntokens. Empirically, BroRL revives models saturated after 3K ProRL training\nsteps and demonstrates robust, continuous improvement, achieving\nstate-of-the-art results for the 1.5B model across diverse benchmarks.", "AI": {"tldr": "BroRL\u901a\u8fc7\u5927\u5e45\u589e\u52a0\u6bcf\u4e2a\u6837\u672c\u7684rollout\u6b21\u6570(\u63a2\u7d22\u5e7f\u5ea6)\uff0c\u7a81\u7834ProRL\u4ec5\u9760\u589e\u52a0\u8bad\u7ec3\u6b65\u6570\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728\u6a21\u578b\u9971\u548c\u540e\u4ecd\u80fd\u6301\u7eed\u63d0\u5347\u8868\u73b0", "motivation": "\u73b0\u6709ProRL\u65b9\u6cd5\u901a\u8fc7\u589e\u52a0\u8bad\u7ec3\u6b65\u6570\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u5728\u6570\u5343\u6b65\u540e\u8fdb\u5165\u5e73\u53f0\u671f\uff0c\u8ba1\u7b97\u8d44\u6e90\u6295\u5165\u5448\u73b0\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002\u9700\u8981\u63a2\u7d22\u4e0e\u8bad\u7ec3\u6b65\u6570\u6b63\u4ea4\u7684\u6269\u5c55\u7ef4\u5ea6", "method": "\u63d0\u51faBroRL\u8303\u5f0f\uff1a1. \u5c06\u6bcf\u4e2a\u6837\u672c\u7684rollout\u6b21\u6570\u63d0\u5347\u81f3\u6570\u767e\u6b21\u5b9e\u73b0\u5145\u5206\u63a2\u7d22 2. \u901a\u8fc7\u8d28\u91cf\u5e73\u8861\u65b9\u7a0b\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u589e\u52a0rollout\u6b21\u6570\u80fd\u786e\u4fdd\u6b63\u786etoken\u7684\u6982\u7387\u8d28\u91cf\u6301\u7eed\u6269\u5c55 3. \u5efa\u7acb\u5355\u6b65RL\u5047\u8bbe\u4e0b\u7684\u6570\u5b66\u6a21\u578b\uff0c\u63ed\u793a\u91c7\u6837/\u672a\u91c7\u6837token\u5bf9\u6982\u7387\u8d28\u91cf\u7684\u5f71\u54cd\u673a\u5236", "result": "\u7406\u8bba\u5c42\u9762\uff1a\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u5145\u8db3rollout\u6b21\u6570\u53ef\u4fdd\u8bc1\u6240\u6709\u6b63\u786etoken\u6982\u7387\u8d28\u91cf\u63d0\u5347\uff1b\u5b9e\u8bc1\u5c42\u9762\uff1aBroRL\u6210\u529f\u590d\u6d3b\u5df2\u9971\u548c\u76843K\u6b65ProRL\u6a21\u578b\uff0c\u57281.5B\u6a21\u578b\u4e0a\u53d6\u5f97\u8de8\u57fa\u51c6SOTA\uff0c\u5c55\u793a\u51fa\u7a33\u5065\u7684\u6301\u7eed\u6539\u8fdb\u80fd\u529b", "conclusion": "\u63a2\u7d22\u5e7f\u5ea6\u7684\u6269\u5c55\u4e0e\u8bad\u7ec3\u6df1\u5ea6\u7684\u6269\u5c55\u5f62\u6210\u4e92\u8865\uff0cBroRL\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u6301\u7eed\u6027\u80fd\u63d0\u5347\u5f00\u8f9f\u4e86\u65b0\u7ef4\u5ea6\uff0c\u5176\u7406\u8bba\u6846\u67b6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\u57fa\u7840"}}
