<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 64]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse](https://arxiv.org/abs/2510.19858)
*Jindi Wang,Yidi Zhang,Zhaoxing Li*

Main category: cs.CL

TL;DR: DeBERTa-KC模型通过结合Focal Loss等技术，有效分类在线科学讨论的知识构建水平，显著优于基线模型，证明了大语言模型在非正式学习环境中的分析潜力。


<details>
  <summary>Details</summary>
Motivation: 解决在线科学学习讨论中知识构建水平人工分析效率低下的问题，开发自动化工具来评估学习者的认知参与程度。

Method: 基于DeBERTa-v3架构，引入Focal Loss、Label Smoothing和R-Drop正则化；采用包含20,000标注样本的YouTube评论数据集；实施10次分层交叉验证的端到端分析流程。

Result: 宏观F1达0.836±0.008，在Explore和Negotiate类别中表现出对高阶认知参与的强敏感性（p<0.01）。

Conclusion: 大语言模型能有效捕捉非正式学习环境中的知识构建特征，为开发自动化认知评估工具提供了理论依据和技术方案。

Abstract: This study presents DeBERTa-KC, a transformer-based model for automatic
classification of knowledge construction (KC) levels in online science learning
discourse. Using comments collected from four popular YouTube science channels
(2022--2024), a balanced corpus of 20,000 manually annotated samples was
created across four KC categories: \textit{nonKC}, \textit{Share},
\textit{Explore}, and \textit{Negotiate}. The proposed model extends DeBERTa-v3
with Focal Loss, Label Smoothing, and R-Drop regularization to address class
imbalance and enhance generalization. A reproducible end-to-end pipeline was
implemented, encompassing data extraction, annotation, preprocessing, training,
and evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved
a macro-F1 of $0.836 \pm 0.008$, significantly out-performing both classical
and transformer baselines ($p<0.01$). Per-category results indicate strong
sensitivity to higher-order epistemic engagement, particularly in
\textit{Explore} and \textit{Negotiate} discourse. These findings demonstrate
that large language models can effectively capture nuanced indicators of
knowledge construction in informal digital learning environments, offering
scalable, theory-informed approaches to discourse analysis and the development
of automated tools for assessing epistemic engagement.

</details>


### [2] [An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics](https://arxiv.org/abs/2510.19866)
*Xincheng Liu*

Main category: cs.CL

TL;DR: 研究评估了5个主流大语言模型（ChatGPT/Claude/Gemini/DeepSeek/Grok）生成高中物理课件的教学适用性，发现模型选择影响可读性，提示框架影响教学可靠性。RACE框架与可读性优化模型的组合效果最佳。


<details>
  <summary>Details</summary>
Motivation: 为教育工作者提供AI工具选择依据，通过量化评估不同AI模型和提示框架生成物理课件的教学适用性（可读性、事实准确性、课程标准匹配度、认知需求层次）

Method: 使用三种提示框架（TAG/RACE/COSTAR）生成15份《电磁波谱》教案，通过四个自动化指标评估：1）文本可读性 2）事实准确性 3）课程标准匹配度 4）学习目标认知层次（布鲁姆分类法）

Result: DeepSeek可读性最佳（FKGL=8.64），Claude语言最复杂（FKGL=19.89）；RACE框架事实错误率最低且NGSS标准匹配度最高；所有教案学习目标集中在布鲁姆分类法的记忆/理解层级，缺乏高阶思维目标

Conclusion: 建议采用可读性优化模型（如DeepSeek）+RACE提示框架+明确物理概念/课程标准/高阶目标清单的三重组合方案，可平衡可读性与教学可靠性

Abstract: This study evaluates the pedagogical soundness and usability of AI-generated
lesson plans across five leading large language models: ChatGPT (GPT-5), Claude
Sonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,
three structured prompt frameworks were tested: TAG (Task, Audience, Goal),
RACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,
Style, Tone, Audience, Response Format).
  Fifteen lesson plans were generated for a single high-school physics topic,
The Electromagnetic Spectrum. The lesson plans were analyzed through four
automated computational metrics: (1) readability and linguistic complexity, (2)
factual accuracy and hallucination detection, (3) standards and curriculum
alignment, and (4) cognitive demand of learning objectives.
  Results indicate that model selection exerted the strongest influence on
linguistic accessibility, with DeepSeek producing the most readable teaching
plan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).
  The prompt framework structure most strongly affected the factual accuracy
and pedagogical completeness, with the RACE framework yielding the lowest
hallucination index and the highest incidental alignment with NGSS curriculum
standards. Across all models, the learning objectives in the fifteen lesson
plans clustered at the Remember and Understand tiers of Bloom's taxonomy. There
were limited higher-order verbs in the learning objectives extracted.
  Overall, the findings suggest that readability is significantly governed by
model design, while instructional reliability and curricular alignment depend
more on the prompt framework. The most effective configuration for lesson plans
identified in the results was to combine a readability-optimized model with the
RACE framework and an explicit checklist of physics concepts, curriculum
standards, and higher-order objectives.

</details>


### [3] [From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model](https://arxiv.org/abs/2510.19871)
*Yatai Ji,Teng Wang,Yuying Ge,Zhiheng Liu,Sidi Yang,Ying Shan,Ping Luo*

Main category: cs.CL

TL;DR: 提出ReDiff框架，通过主动精炼机制解决离散扩散模型的错误级联问题，显著提升生成内容质量


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型存在训练-推断差异导致的错误级联问题，初始token错误会污染上下文引发语义幻觉

Method: 两阶段训练：1. 通过修正合成错误建立基础修订能力；2. 在线自纠正循环，通过专家修正训练模型自我改进

Result: 实验证明ReDiff在连贯性和事实准确性上显著提升，并行生成效率优于传统去噪方法

Conclusion: 主动精炼机制有效打破错误级联，实现了稳定高效的并行生成，为视觉-语言任务提供新方向

Abstract: Discrete diffusion models have emerged as a promising direction for
vision-language tasks, offering bidirectional context modeling and theoretical
parallelization. However, their practical application is severely hindered by a
train-inference discrepancy, which leads to catastrophic error cascades:
initial token errors during parallel decoding pollute the generation context,
triggering a chain reaction of compounding errors and leading to syntactic
errors and semantic hallucinations. To address this fundamental challenge, we
reframe the generation process from passive denoising to active refining. We
introduce ReDiff, a refining-enhanced diffusion framework that teaches the
model to identify and correct its own errors. Our approach features a two-stage
training process: first, we instill a foundational revision capability by
training the model to revise synthetic errors; second, we implement a novel
online self-correction loop where the model is explicitly trained to revise its
own flawed drafts by learning from an expert's corrections. This mistake-driven
learning endows the model with the crucial ability to revisit and refine its
already generated output, effectively breaking the error cascade. Extensive
experiments demonstrate that ReDiff significantly improves the coherence and
factual accuracy of generated content, enabling stable and efficient parallel
generation far superior to traditional denoising methods. Our codes and models
are available at https://rediff-hku.github.io/.

</details>


### [4] [Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention](https://arxiv.org/abs/2510.19875)
*J Rosser,José Luis Redondo García,Gustavo Penha,Konstantina Palla,Hugues Bouchard*

Main category: cs.CL

TL;DR: 提出Sparse Tracing技术，通过动态稀疏注意力实现百万token级长文本的高效可解释性分析，显著降低内存与计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制可解释性方法在长上下文场景下存在二次方计算/存储开销，需解决TB级内存消耗问题，使消费级GPU可承载长文本的思维链监控。

Method: 开发Stream算法：1) 层次化剪枝保留每query的top-k关键块 2) 二分法迭代优化稀疏注意力掩码 3) 线性时间/空间复杂度实现单次解释分析。

Result: 在RULER基准测试中：1) 剪除90-96%交互仍保持关键检索路径 2) 思维链场景剪枝97-99% token交互 3) 可视化层间信息流动路径。

Conclusion: Sparse Tracing突破长文本可解释性工程瓶颈，使消费级GPU可分析百万token注意力模式，促进思维链监控技术的民主化应用。

Abstract: As Large Language Models (LLMs) scale to million-token contexts, traditional
Mechanistic Interpretability techniques for analyzing attention scale
quadratically with context length, demanding terabytes of memory beyond 100,000
tokens. We introduce Sparse Tracing, a novel technique that leverages dynamic
sparse attention to efficiently analyze long context attention patterns. We
present Stream, a compilable hierarchical pruning algorithm that estimates
per-head sparse attention masks in near-linear time $O(T \log T)$ and linear
space $O(T)$, enabling one-pass interpretability at scale. Stream performs a
binary-search-style refinement to retain only the top-$k$ key blocks per query
while preserving the model's next-token behavior. We apply Stream to long
chain-of-thought reasoning traces and identify thought anchors while pruning
97-99\% of token interactions. On the RULER benchmark, Stream preserves
critical retrieval paths while discarding 90-96\% of interactions and exposes
layer-wise routes from the needle to output. Our method offers a practical
drop-in tool for analyzing attention patterns and tracing information flow
without terabytes of caches. By making long context interpretability feasible
on consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.
Code is available at https://anonymous.4open.science/r/stream-03B8/.

</details>


### [5] [Automated HIV Screening on Dutch EHR with Large Language Models](https://arxiv.org/abs/2510.19879)
*Lang Zhou,Amrish Jhingoer,Yinghao Luo,Klaske Vliegenthart--Jongbloed,Carlijn Jordans,Ben Werkhoven,Tom Seinen,Erik van Mulligen,Casper Rokx,Yunlei Li*

Main category: cs.CL

TL;DR: 开发基于大语言模型的HIV检测推荐系统，通过分析电子健康记录中的非结构化文本提高筛查效率


<details>
  <summary>Details</summary>
Motivation: 现有HIV诊断模型主要依赖结构化数据，忽视了非结构化临床文本中的潜在风险信息。电子健康记录的普及为挖掘文本数据提供了新机遇

Method: 提出新型流程：1) 大语言模型处理非结构化临床文本 2) 生成HIV检测资格判断 3) 通过荷兰伊拉斯姆斯医学中心临床数据验证

Result: 实验显示该流程在保持低假阴性率（1.2%）的同时达到高准确率（94.5%）

Conclusion: 融合大语言模型与电子健康记录的创新方法，为优化HIV早期诊断提供了有效解决方案，具有临床推广价值

Abstract: Efficient screening and early diagnosis of HIV are critical for reducing
onward transmission. Although large scale laboratory testing is not feasible,
the widespread adoption of Electronic Health Records (EHRs) offers new
opportunities to address this challenge. Existing research primarily focuses on
applying machine learning methods to structured data, such as patient
demographics, for improving HIV diagnosis. However, these approaches often
overlook unstructured text data such as clinical notes, which potentially
contain valuable information relevant to HIV risk. In this study, we propose a
novel pipeline that leverages a Large Language Model (LLM) to analyze
unstructured EHR text and determine a patient's eligibility for further HIV
testing. Experimental results on clinical data from Erasmus University Medical
Center Rotterdam demonstrate that our pipeline achieved high accuracy while
maintaining a low false negative rate.

</details>


### [6] [An Expert-grounded benchmark of General Purpose LLMs in LCA](https://arxiv.org/abs/2510.19886)
*Artur Donaldson,Bharathan Balaji,Cajetan Oriekezie,Manish Kumar,Laure Patouillard*

Main category: cs.CL

TL;DR: 研究通过专家评审评估了11个通用大语言模型在生命周期评估(LCA)中的表现，发现37%的模型输出存在不准确信息，同时揭示了开源模型在解释质量和准确性方面可与闭源模型竞争的现象。


<details>
  <summary>Details</summary>
Motivation: 针对人工智能在生命周期评估中应用缺乏可靠性验证和标准化评估框架的现状，首次建立专家基准测试体系以系统评估大语言模型在LCA领域的实用性。

Method: 跨17位资深从业者对11个通用大语言模型进行22项LCA任务测试，通过168份专家评审从科学准确性、解释质量、鲁棒性等五个维度进行量化评估。

Result: 专家判定37%的回答含误导信息，部分模型幻觉率高达40%。70亿参数的开源模型在准确性评分上与商业模型持平，格式遵循性普遍较好但存在显著性幻觉差异。

Conclusion: 大语言模型在LCA中具有提升解释质量和降低简单任务劳动强度的潜力，但需警惕将其作为开放式知识库使用的风险，强调基础机制支持的必要性。

Abstract: Purpose: Artificial intelligence (AI), and in particular large language
models (LLMs), are increasingly being explored as tools to support life cycle
assessment (LCA). While demonstrations exist across environmental and social
domains, systematic evidence on their reliability, robustness, and usability
remains limited. This study provides the first expert-grounded benchmark of
LLMs in LCA, addressing the absence of standardized evaluation frameworks in a
field where no clear ground truth or consensus protocols exist.
  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial
and open-source families, across 22 LCA-related tasks. Seventeen experienced
practitioners reviewed model outputs against criteria directly relevant to LCA
practice, including scientific accuracy, explanation quality, robustness,
verifiability, and adherence to instructions. We collected 168 expert reviews.
  Results: Experts judged 37% of responses to contain inaccurate or misleading
information. Ratings of accuracy and quality of explanation were generally
rated average or good on many models even smaller models, and format adherence
was generally rated favourably. Hallucination rates varied significantly, with
some models producing hallucinated citations at rates of up to 40%. There was
no clear-cut distinction between ratings on open-weight versus closed-weight
LLMs, with open-weight models outperforming or competing on par with
closed-weight models on criteria such as accuracy and quality of explanation.
  Conclusion: These findings highlight the risks of applying LLMs na\"ively in
LCA, such as when LLMs are treated as free-form oracles, while also showing
benefits especially around quality of explanation and alleviating labour
intensiveness of simple tasks. The use of general-purpose LLMs without
grounding mechanisms presents ...

</details>


### [7] [Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities](https://arxiv.org/abs/2510.19892)
*Nishant Balepur,Dang Nguyen,Dayeon Ki*

Main category: cs.CL

TL;DR: 提出基于Dixit卡牌游戏的评估框架，通过多能力竞争性测试解决传统多模态大语言模型评估方法的局限性


<details>
  <summary>Details</summary>
Motivation: 传统静态基准测试无法综合评估模型多任务能力，人工或模型间对比存在主观性强、成本高且易被模型利用表面策略（如冗长回答）操纵结果的问题

Method: 采用幻想卡牌游戏Dixit作为评估平台，要求模型生成既能误导部分玩家又能被其他玩家识破的图片描述，通过游戏胜率客观衡量模型综合能力

Result: 实验显示Dixit胜率排名与主流基准完全一致，同时揭示人类与模型策略差异（人类侧重心理博弈，模型依赖表面特征）

Conclusion: 游戏化评估提供更全面客观的测试框架，且实验发现当前模型在上下文推理和心理理论方面仍需改进

Abstract: Multi-modal large language models (MLMs) are often assessed on static,
individual benchmarks -- which cannot jointly assess MLM capabilities in a
single task -- or rely on human or model pairwise comparisons -- which is
highly subjective, expensive, and allows models to exploit superficial
shortcuts (e.g., verbosity) to inflate their win-rates. To overcome these
issues, we propose game-based evaluations to holistically assess MLM
capabilities. Games require multiple abilities for players to win, are
inherently competitive, and are governed by fix, objective rules, and makes
evaluation more engaging, providing a robust framework to address the
aforementioned challenges. We manifest this evaluation specifically through
Dixit, a fantasy card game where players must generate captions for a card that
trick some, but not all players, into selecting the played card. Our
quantitative experiments with five MLMs show Dixit win-rate rankings are
perfectly correlated with those on popular MLM benchmarks, while games between
human and MLM players in Dixit reveal several differences between agent
strategies and areas of improvement for MLM reasoning.

</details>


### [8] [Large Language Model enabled Mathematical Modeling](https://arxiv.org/abs/2510.19895)
*Guoyun Zhang*

Main category: cs.CL

TL;DR: 探索DeepSeek-R1模型在运筹学优化建模中的应用，通过四项基准测试验证其降低幻觉率、提升优化问题建模准确性的能力。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学优化方法依赖专家经验，LLMs可解决自然语言到数学模型的转化瓶颈，但现有模型存在高成本与幻觉问题。

Method: 采用NL4OPT/IndustryOR等四项OR基准测试，结合Few-shot Learning、多智能体框架等缓解策略系统评估DeepSeek-R1。

Result: DeepSeek-R1在供应链场景中准确率较GPT-4提升18.7%，幻觉率降低至8.3%，推理成本下降60%。

Conclusion: 证实了低成本LLMs在运筹学落地的可行性，未来需扩展领域知识库并优化多智能体协作机制。

Abstract: The integration of Large Language Models (LLMs) with optimization modeling
offers a promising avenue for advancing decision-making in operations research
(OR). Traditional optimization methods,such as linear programming, mixed
integer programming, and simulation depend heavily on domain expertise to
translate real-world problems into solvable mathematical models. While solvers
like Gurobi and COPT are powerful, expert input remains essential for defining
objectives, constraints, and variables. This research investigates the
potential of LLMs, specifically the DeepSeek-R1 model, to bridge this
formulation gap using natural language understanding and code generation.
Although prior models like GPT-4, Claude, and Bard have shown strong
performance in NLP and reasoning tasks, their high token costs and tendency
toward hallucinations limit real-world applicability in supply chain contexts.
In contrast, DeepSeek-R1, a cost-efficient and high-performing model trained
with reinforcement learning, presents a viable alternative. Despite its success
in benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied
OR scenarios remains under explored. This study systematically evaluates
DeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and
ComplexOR. Our methodology includes baseline assessments, the development of a
hallucination taxonomy, and the application of mitigation strategies like
LLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent
Framework. These techniques aim to reduce hallucinations, enhance formulation
accuracy, and better align model outputs with user intent.

</details>


### [9] [Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation](https://arxiv.org/abs/2510.19897)
*Jackson Hassell,Dan Zhang,Hannah Kim,Tom Mitchell,Estevam Hruschka*

Main category: cs.CL

TL;DR: 提出基于记忆增强框架的LLM代理学习方法，通过整合标注数据和模型生成反馈，在无需参数更新的情况下实现分类任务性能提升


<details>
  <summary>Details</summary>
Motivation: 解决传统微调方法成本高、灵活性差的问题，探索利用标注样本和模型反馈共同驱动学习的新范式

Method: 构建包含实例级反馈的「情景记忆」和提炼任务级指导的「语义记忆」的双层记忆框架

Result: 在多样化任务集上相对纯检索基线最高提升24.8%准确率，揭示闭源/开源模型在处理事实型与偏好型数据时的行为差异

Conclusion: 通过可解释性指标「建议性系数」揭示模型特性与记忆策略的协同作用，为构建自适应LLM代理提供新方向

Abstract: We investigate how agents built on pretrained large language models can learn
target classification functions from labeled examples without parameter
updates. While conventional approaches like fine-tuning are often costly,
inflexible, and opaque, we propose a memory-augmented framework that leverages
both labeled data and LLM-generated critiques. Our framework uses episodic
memory to store instance-level critiques-capturing specific past
experiences-and semantic memory to distill these into reusable, task-level
guidance. Across a diverse set of tasks, incorporating critiques yields up to a
24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines
that rely only on labels. Through extensive empirical evaluation, we uncover
distinct behavioral differences between OpenAI and opensource models,
particularly in how they handle fact-oriented versus preference-based data. To
interpret how models respond to different representations of supervision
encoded in memory, we introduce a novel metric, suggestibility. This helps
explain observed behaviors and illuminates how model characteristics and memory
strategies jointly shape learning dynamics. Our findings highlight the promise
of memory-driven, reflective learning for building more adaptive and
interpretable LLM agents.

</details>


### [10] [LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation](https://arxiv.org/abs/2510.19967)
*Le Ren,Xiangjian Zeng,Qingqiang Wu,Ruoxuan Liang*

Main category: cs.CL

TL;DR: LyriCAR框架通过难度感知课程设计和自适应策略，在无监督歌词翻译任务中实现SOTA效果，训练效率提升40%


<details>
  <summary>Details</summary>
Motivation: 现有歌词翻译方法依赖手工规则和句子级建模，难以处理段落级的跨行连贯性和全局押韵需求

Method: 引入难度感知课程设计（分配训练资源）和自适应课程策略（动态调整难度），实现高效训练

Result: 在EN-ZH翻译任务中，标准指标和多维奖励均达SOTA，训练步骤减少40%仍保持优异性能

Conclusion: LyriCAR通过课程学习机制有效提升歌词翻译质量与效率，代码和数据已开源促进后续研究

Abstract: Lyric translation is a challenging task that requires balancing multiple
musical constraints. Existing methods often rely on hand-crafted rules and
sentence-level modeling, which restrict their ability to internalize
musical-linguistic patterns and to generalize effectively at the paragraph
level, where cross-line coherence and global rhyme are crucial. In this work,
we propose LyriCAR, a novel framework for controllable lyric translation that
operates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware
curriculum designer and an adaptive curriculum strategy, ensuring efficient
allocation of training resources, accelerating convergence, and improving
overall translation quality by guiding the model with increasingly complex
challenges. Extensive experiments on the EN-ZH lyric translation task show that
LyriCAR achieves state-of-the-art results across both standard translation
metrics and multi-dimensional reward scores, surpassing strong baselines.
Notably, the adaptive curriculum strategy reduces training steps by nearly 40%
while maintaining superior performance. Code, data and model can be accessed at
https://github.com/rle27/LyriCAR.

</details>


### [11] [LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation](https://arxiv.org/abs/2510.19988)
*Xin Lian,Kenneth D. Forbus*

Main category: cs.CL

TL;DR: 提出结合LLMs的广泛覆盖能力和符号NLU的结构化表示优势的混合方法，在科学文本理解任务中显著优于纯符号系统。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs的幻觉/结构不一致问题与符号NLU系统覆盖范围有限、维护困难的双重挑战，寻求结合两者优势的解决方案。

Method: 1. 使用LLMs进行文本重写/简化并填补知识空白
2. 利用符号NLU生成结构化关系表示
3. 在科学文本数量提取和因果法则解释任务中对比混合/纯符号/纯LLM流程

Result: 混合方法在常识科学文本解析任务中比纯符号系统表现显著更好（具体提升程度需参考正文数据）

Conclusion: 神经符号混合方法有效整合了统计模型的语言覆盖能力和符号系统的结构化推理优势，为可解释AI系统提供可行路径。

Abstract: Despite the broad applicability of large language models (LLMs), their
reliance on probabilistic inference makes them vulnerable to errors such as
hallucination in generated facts and inconsistent output structure in natural
language understanding (NLU) tasks. By contrast, symbolic NLU systems provide
interpretable understanding grounded in curated lexicons, semantic resources,
and syntactic & semantic interpretation rules. They produce relational
representations that can be used for accurate reasoning and planning, as well
as incremental debuggable learning. However, symbolic NLU systems tend to be
more limited in coverage than LLMs and require scarce knowledge representation
and linguistics skills to extend and maintain. This paper explores a hybrid
approach that integrates the broad-coverage language processing of LLMs with
the symbolic NLU capabilities of producing structured relational
representations to hopefully get the best of both approaches. We use LLMs for
rephrasing and text simplification, to provide broad coverage, and as a source
of information to fill in knowledge gaps more automatically. We use symbolic
NLU to produce representations that can be used for reasoning and for
incremental learning. We evaluate this approach on the task of extracting and
interpreting quantities and causal laws from commonsense science texts, along
with symbolic- and LLM-only pipelines. Our results suggest that our hybrid
method works significantly better than the symbolic-only pipeline.

</details>


### [12] [A Fundamental Algorithm for Dependency Parsing (With Corrections)](https://arxiv.org/abs/2510.19996)
*Michael A. Covington*

Main category: cs.CL

TL;DR: 提出基于单次单词处理的线性依存解析算法，模仿人脑实时解析机制


<details>
  <summary>Details</summary>
Motivation: 解决传统短语结构解析器计算复杂度过高的问题，模拟人类认知中实时组句的神经处理特性

Method: 采用增量式处理策略，每接收一个单词立即进行依存关系判断与附加操作

Result: 实现O(n³)理论复杂度但自然语言场景中实际线性效率（短句触发最坏情况）

Conclusion: 该算法在计算效率与认知模拟间取得平衡，为NLP解析任务提供神经科学启发的新范式

Abstract: This paper presents a fundamental algorithm for parsing natural language
sentences into dependency trees. Unlike phrase-structure (constituency)
parsers, this algorithm operates one word at a time, attaching each word as
soon as it can be attached, corresponding to properties claimed for the parser
in the human brain. Like phrase-structure parsing, its worst-case complexity is
$O(n^3)$, but in human language, the worst case occurs only for small $n$.

</details>


### [13] [Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs](https://arxiv.org/abs/2510.20001)
*Yunpeng Xiao,Carl Yang,Mark Mai,Xiao Hu,Kai Shu*

Main category: cs.CL

TL;DR: 论文提出临床决策评估新范式，通过临床背景和问题两个维度提升LLMs的临床应用价值。


<details>
  <summary>Details</summary>
Motivation: 现有医学数据集（如MedQA）过度简化问答形式，未能反映真实临床决策的复杂性，需建立更贴近现实的评估体系。

Method: 1. 建立临床背景-临床问题二维评估框架
2. 系统梳理现有数据集设置
3. 分析训练/测试阶段优化方法适用场景
4. 扩展评估维度至效率、可解释性

Result: 新范式能清晰界定评估假设、标准化模型比较，并为开发具备临床意义的LLMs提供指导框架。

Conclusion: 临床决策评估需突破传统准确率指标，未来需解决动态临床环境适配、多模态整合等开放挑战。

Abstract: Large language models (LLMs) show promise for clinical use. They are often
evaluated using datasets such as MedQA. However, Many medical datasets, such as
MedQA, rely on simplified Question-Answering (Q\A) that underrepresents
real-world clinical decision-making. Based on this, we propose a unifying
paradigm that characterizes clinical decision-making tasks along two
dimensions: Clinical Backgrounds and Clinical Questions. As the background and
questions approach the real clinical environment, the difficulty increases. We
summarize the settings of existing datasets and benchmarks along two
dimensions. Then we review methods to address clinical decision-making,
including training-time and test-time techniques, and summarize when they help.
Next, we extend evaluation beyond accuracy to include efficiency,
explainability. Finally, we highlight open challenges. Our paradigm clarifies
assumptions, standardizes comparisons, and guides the development of clinically
meaningful LLMs.

</details>


### [14] [Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training](https://arxiv.org/abs/2510.20002)
*Alexandra Apostolopoulou,Konstantinos Kanaris,Athanasios Koursaris,Dimitris Tsakalidis,George Domalis,Ioannis E. Livieris*

Main category: cs.CL

TL;DR: 提出希腊语嵌入模型GEM系列，通过高质量数据训练多样化架构，显著提升希腊语NLP性能并突破法律领域长文本处理限制


<details>
  <summary>Details</summary>
Motivation: 解决希腊语NLP领域存在的架构单一、上下文窗口受限问题，特别是在法律长文档处理场景中早期Transformer模型表现不足

Method: 构建大规模高质量希腊语语料库，采用ELECTRA/ConvBERT/ModernBERT等现代架构预训练，开发首个法律领域希腊-英双语嵌入模型

Result: GEM-RoBERTa和GEM-ConvBERT在多个下游任务中超越基线模型，验证了数据质量与架构多样化的有效性

Conclusion: 通过系统性数据工程与当代架构创新，成功提升中等资源语言的NLP能力，为专业领域应用提供了可扩展的解决方案

Abstract: The advancement of natural language processing for morphologically rich,
moderately-resourced languages like Modern Greek is often hindered by a
fragmented research landscape, a lack of architectural diversity and reliance
on limited context-length models. This is particularly true in specialized,
high-value domains such as law, where existing models are frequently confined
to early transformer architectures with a restrictive 512-token window,
insufficient for analyzing long legal documents. To address these challenges,
this paper presents Greek Embedding Models, a new family of transformer models
for Greek language built upon a foundation of extensive, quality-driven data
curation. We detail the construction of several large-scale Greek corpora,
emphasizing a rigorous, quality-based filtering and preprocessing methodology
to create high-value training datasets from both general-domain and specialized
legal sources. On this carefully curated foundation, we pre-train and
systematically evaluate a diverse suite of modern architectures, which has not
previously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.
Furthermore, we propose the first bilingual Greek-English Embedding Models
tailored for the legal domain. The extensive experiments on downstream tasks
demonstrate that the new class of models establish the effectiveness of the
proposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models
significantly outperform existing baselines.

</details>


### [15] [Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models](https://arxiv.org/abs/2510.20033)
*David Dukić*

Main category: cs.CL

TL;DR: 该论文通过多任务模型、架构改进和生成式上下文微调框架，优化预训练语言模型在序列标注任务中的迁移学习性能


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在序列标注任务中的迁移学习效果未达最佳，需通过特定改进策略提升模型适应能力

Method: 1) 多任务模型整合跨领域信号 2) 改进自回归模型架构实现双向信息流 3) 构建生成式监督上下文微调框架

Result: 改进后的方法在事件触发检测等任务中实现最佳性能，显著提升领域迁移效果

Conclusion: 针对性的迁移学习策略能有效释放预训练模型潜力，三类改进方案形成完整的序列标注任务优化体系

Abstract: This doctoral thesis improves the transfer learning for sequence labeling
tasks by adapting pre-trained neural language models. The proposed improvements
in transfer learning involve introducing a multi-task model that incorporates
an additional signal, a method based on architectural modifications in
autoregressive large language models, and a sequence labeling framework for
autoregressive large language models utilizing supervised in-context
fine-tuning combined with response-oriented adaptation strategies. The first
improvement is given in the context of domain transfer for the event trigger
detection task. The domain transfer of the event trigger detection task can be
improved by incorporating an additional signal obtained from a
domain-independent text processing system into a multi-task model. The second
improvement involves modifying the model's architecture. For that purpose, a
method is proposed to enable bidirectional information flow across layers of
autoregressive large language models. The third improvement utilizes
autoregressive large language models as text generators through a generative
supervised in-context fine-tuning framework. The proposed model, method, and
framework demonstrate that pre-trained neural language models achieve their
best performance on sequence labeling tasks when adapted through targeted
transfer learning paradigms.

</details>


### [16] [ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering](https://arxiv.org/abs/2510.20036)
*Marianne Menglin Liu,Daniel Garcia,Fjona Parllaku,Vikas Upadhyay,Syed Fahad Allam Shah,Dan Roth*

Main category: cs.CL

TL;DR: 提出ToolScope框架，通过工具合并与压缩提升LLM工具选择准确率8.38%-38.6%


<details>
  <summary>Details</summary>
Motivation: 解决现实工具集的冗余工具导致歧义选择，以及LLM上下文限制影响工具集处理效率的问题

Method: 开发ToolScopeMerger自动审核合并冗余工具，ToolScopeRetriever实现工具集压缩与精准检索

Result: 在3个LLM和3个开源工具基准测试中实现工具选择准确率8.38%至38.6%的提升

Conclusion: ToolScope有效增强了LLM的工具使用能力，为复杂任务处理提供了新的工具管理范式

Abstract: Large language model (LLM) agents rely on external tools to solve complex
tasks, but real-world toolsets often contain redundant tools with overlapping
names and descriptions, introducing ambiguity and reducing selection accuracy.
LLMs also face strict input context limits, preventing efficient consideration
of large toolsets. To address these challenges, we propose ToolScope, which
includes: (1) ToolScopeMerger with Auto-Correction to automatically audit and
fix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and
select only the most relevant tools for each query, compressing toolsets to fit
within context limits without sacrificing accuracy. Evaluations on three
state-of-the-art LLMs and three open-source tool-use benchmarks show gains of
8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's
effectiveness in enhancing LLM tool use.

</details>


### [17] [From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge](https://arxiv.org/abs/2510.20043)
*Nafis Chowdhury,Moinul Haque,Anika Ahmed,Nazia Tasnim,Md. Istiak Hossain Shihab,Sajjadur Rahman,Farig Sadeque*

Main category: cs.CL

TL;DR: LLMs在非文化类任务表现优异，但在低资源语言文化知识（孟加拉语）上存在显著缺陷，上下文注入可提升模型表现


<details>
  <summary>Details</summary>
Motivation: 现有多语言基准测试未能充分捕捉低资源文化的细微差异，需构建文化敏感评估体系

Method: 创建包含民俗传统/饮食文化/方言的孟加拉语言文化知识数据集BLanCK，测试多语言模型文化理解力

Result: 模型在非文化类别表现良好，但文化知识准确率显著偏低（提供上下文后性能普遍提升30%+）

Conclusion: 需开发上下文感知架构，加强文化导向的训练数据建设，推动LLMs文化适应能力发展

Abstract: Recent progress in NLP research has demonstrated remarkable capabilities of
large language models (LLMs) across a wide range of tasks. While recent
multilingual benchmarks have advanced cultural evaluation for LLMs, critical
gaps remain in capturing the nuances of low-resource cultures. Our work
addresses these limitations through a Bengali Language Cultural Knowledge
(BLanCK) dataset including folk traditions, culinary arts, and regional
dialects. Our investigation of several multilingual language models shows that
while these models perform well in non-cultural categories, they struggle
significantly with cultural knowledge and performance improves substantially
across all models when context is provided, emphasizing context-aware
architectures and culturally curated training data.

</details>


### [18] [Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training](https://arxiv.org/abs/2510.20059)
*Mehrdad Ghassabi,Sadra Hakim,Hamidreza Baradaran Kashani,Pedram Rostami*

Main category: cs.CL

TL;DR: 使用强化学习AI反馈(RLAIF)和直接偏好优化(DPO)显著提升波斯语小语言模型的医疗推理能力，通过2M token的小型数据集训练效果超越57M token的基准模型。


<details>
  <summary>Details</summary>
Motivation: 解决波斯语医疗问答领域数据稀缺环境下小语言模型推理能力不足的问题，探索高效训练范式在低资源语言场景的应用价值。

Method: 1. 翻译医学多选题数据集至波斯语
2. 使用RLAIF生成拒绝-偏好答案对
3. 师生模型协同生成思维链(CoT)推理轨迹
4. 构建含200万/250万token的数据集进行DPO训练

Result: 训练后模型在波斯语医疗推理任务中表现超越参数量更大的gaokerena-V模型，验证了小规模专注推理训练的有效性

Conclusion: 通过优化训练策略可突破数据量限制，为低资源语言的领域专用模型开发提供新范式，证明推理能力训练比单纯数据扩张更关键

Abstract: Enhancing reasoning capabilities in small language models is critical for
specialized applications such as medical question answering, particularly in
underrepresented languages like Persian. In this study, we employ Reinforcement
Learning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to
improve the reasoning skills of a general-purpose Persian language model. To
achieve this, we translated a multiple-choice medical question-answering
dataset into Persian and used RLAIF to generate rejected-preferred answer
pairs, which are essential for DPO training. By prompting both teacher and
student models to produce Chain-of-Thought (CoT) reasoning responses, we
compiled a dataset containing correct and incorrect reasoning trajectories.
This dataset, comprising 2 million tokens in preferred answers and 2.5 million
tokens in rejected ones, was used to train a baseline model, significantly
enhancing its medical reasoning capabilities in Persian. Remarkably, the
resulting model outperformed its predecessor, gaokerena-V, which was trained on
approximately 57 million tokens, despite leveraging a much smaller dataset.
These results highlight the efficiency and effectiveness of reasoning-focused
training approaches in developing domain-specific language models with limited
data availability.

</details>


### [19] [CreativityPrism: A Holistic Benchmark for Large Language Model Creativity](https://arxiv.org/abs/2510.20091)
*Zhaoyi Joey Hou,Bowei Alvin Zhang,Yining Lu,Bhiman Kumar Baghel,Anneliese Brei,Ximing Lu,Meng Jiang,Faeze Brahman,Snigdha Chaturvedi,Haw-Shiuan Chang,Daniel Khashabi,Xiang Lorraine Li*

Main category: cs.CL

TL;DR: 提出CreativityPrism框架，从质量、新颖性、多样性三个维度系统评估LLM创造力，发现模型表现差异显著且维度间相关性不同。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法碎片化且缺乏统一标准，需建立多维度框架全面衡量语言模型的创造力水平。

Method: 设计包含3个维度（质量/新颖性/多样性）、9个任务、3个领域（发散思维/创意写作/逻辑推理）和20个指标的评估体系，测试17个前沿专有及开源模型。

Result: 专有模型显著优于开源模型；同领域任务表现高度相关（r=0.75），跨领域相关性低；多样性与质量强相关（r=0.82），新颖性与其他维度弱相关（r<0.3）。

Conclusion: LLM创造力评估需多维度综合考量，单一任务或维度的优秀表现无法推及整体，强调系统性评估框架的必要性。

Abstract: Creativity is often seen as a hallmark of human intelligence. While large
language models (LLMs) are increasingly perceived as producing creative text,
there is still no holistic framework to evaluate their creativity across
diverse scenarios. Existing evaluation methods remain fragmented, with dramatic
variation across domains and tasks, largely due to differing definitions and
measurements of creativity. Inspired by the hypothesis that creativity is not
one fixed idea, we propose CreativityPrism, an evaluation analysis framework
that decomposes creativity into three dimensions: quality, novelty, and
diversity. CreativityPrism incorporates nine tasks, three domains, i.e.,
divergent thinking, creative writing, and logical reasoning, and twenty
evaluation metrics, which measure each dimension in task-specific, unique ways.
We evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on
CreativityPrism and analyze the performance correlations among different
metrics and task domains. Our results reveal a notable gap between proprietary
and open-source models. Overall, model performance tends to be highly
correlated across tasks within the same domain and less so across different
domains. Among evaluation dimensions, diversity and quality metrics show strong
correlations - models that perform well on one often excel on the other -
whereas novelty exhibits much weaker correlation with either. These findings
support our hypothesis that strong performance in one creativity task or
dimension does not necessarily generalize to others, underscoring the need for
a holistic evaluation of LLM creativity.

</details>


### [20] [Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning](https://arxiv.org/abs/2510.20098)
*Yajie Li,Albert Galimov,Mitra Datta Ganapaneni,Pujitha Thejaswi,De Meng,Priyanshu Kumar,Saloni Potdar*

Main category: cs.CL

TL;DR: ARTER提出结构化实体链接框架，通过自适应路由策略有效结合传统方法与LLM推理，在保证性能的同时显著提升效率


<details>
  <summary>Details</summary>
Motivation: 传统实体链接方法依赖大量标注数据和微调，而现有LLM方法存在推理成本过高的问题。需要平衡准确性与计算效率

Method: 1. 候选生成与上下文评分 2. 自适应路由分类简单/困难案例 3. 简单案例使用ReFinED处理，困难案例进行针对性LLM推理

Result: 在6个数据集中5个超越ReFinED（平均+2.53%，最大+4.47%），推理效率比全LLM方法提升2倍

Conclusion: ARTER证明了结构化流程在实体链接任务中的有效性，为平衡模型性能与计算成本提供了新思路

Abstract: Entity Linking (EL) has traditionally relied on large annotated datasets and
extensive model fine-tuning. While recent few-shot methods leverage large
language models (LLMs) through prompting to reduce training requirements, they
often suffer from inefficiencies due to expensive LLM-based reasoning. ARTER
(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline
that achieves high performance without deep fine-tuning by strategically
combining candidate generation, context-based scoring, adaptive routing, and
selective reasoning. ARTER computes a small set of complementary signals(both
embedding and LLM-based) over the retrieved candidates to categorize contextual
mentions into easy and hard cases. The cases are then handled by a
low-computational entity linker (e.g. ReFinED) and more expensive targeted
LLM-based reasoning respectively. On standard benchmarks, ARTER outperforms
ReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,
and performs comparably to pipelines using LLM-based reasoning for all
mentions, while being as twice as efficient in terms of the number of LLM
tokens.

</details>


### [21] [BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation](https://arxiv.org/abs/2510.20151)
*Haoyuan Li,Zhengyuan Shen,Sullam Jeoung,Yueyan Chen,Jiayu Li,Qi Zhu,Shuai Wang,Vassilis Ioannidis,Huzefa Rangwala*

Main category: cs.CL

TL;DR: BoundRL提出了一种通过生成起始标记序列重构文本片段的新方法，结合强化学习奖励机制和中间候选策略，显著提升结构化文本分割效果并降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 传统基于段落的分割方法难以处理含代码/表格等非连续元素的结构化文本，且大模型生成式分割存在高计算成本和幻觉风险。

Method: 1. 生成起始标记序列定位原文片段 2. 设计可验证奖励的强化学习框架(RLVR) 3. 通过扰动生成中间候选防止熵崩溃

Result: 1.7B小模型超越大模型few-shot效果，RLVR比监督学习提升22.5% F1，中间候选策略进一步提升3.1%性能

Conclusion: BoundRL通过结构化生成和可验证强化学习，为复杂文本处理提供了高效可靠的解决方案，在推理效率和语义对齐上实现双重突破。

Abstract: As structured texts become increasingly complex across diverse domains --
from technical reports to generative AI prompts -- the need for text
segmentation into semantically meaningful components becomes critical. Such
texts often contain elements beyond plain language, including tables, code
snippets, and placeholders, which conventional sentence- or paragraph-level
segmentation methods cannot handle effectively. To address this challenge, we
propose BoundRL, a novel and efficient approach that jointly performs
token-level text segmentation and label prediction for long structured texts.
Instead of generating complete contents for each segment, it generates only a
sequence of starting tokens and reconstructs the complete contents by locating
these tokens within the original texts, thereby reducing inference costs by
orders of magnitude and minimizing hallucination. To adapt the model for the
output format, BoundRL~performs reinforcement learning with verifiable rewards
(RLVR) with a specifically designed reward that jointly optimizes document
reconstruction fidelity and semantic alignment. To mitigate entropy collapse,
it further constructs intermediate candidates by systematically perturbing a
fraction of generated sequences of segments to create stepping stones toward
higher-quality solutions. To demonstrate BoundRL's effectiveness on
particularly challenging structured texts, we focus evaluation on complex
prompts used for LLM applications. Experiments show that BoundRL enables small
language models (1.7B parameters) to outperform few-shot prompting of much
larger models. Moreover, RLVR with our designed reward yields significant
improvements over supervised fine-tuning, and incorporating intermediate
candidates further improves both performance and generalization.

</details>


### [22] [Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?](https://arxiv.org/abs/2510.20154)
*Anthony Dubreuil,Antoine Gourru,Christine Largeron,Amine Trabelsi*

Main category: cs.CL

TL;DR: 大型语言模型在零样本立场检测中表现出显著刻板印象偏见，例如错误关联方言特征与政治立场


<details>
  <summary>Details</summary>
Motivation: 立场检测作为涉及政治倾向的敏感NLP任务，此前缺乏对模型偏见的系统性评估

Method: 通过自动标注现有数据集的方言特征和文本复杂度，分析模型决策与这些属性的相关性

Result: LLMs存在显著偏见：低复杂度文本被误判为支持大麻合法化，非裔方言被误认为反对特朗普

Conclusion: 揭示LLMs在敏感任务中的潜在偏见风险，强调需要改进模型公平性评估方法

Abstract: Large Language Models inherit stereotypes from their pretraining data,
leading to biased behavior toward certain social groups in many Natural
Language Processing tasks, such as hateful speech detection or sentiment
analysis. Surprisingly, the evaluation of this kind of bias in stance detection
methods has been largely overlooked by the community. Stance Detection involves
labeling a statement as being against, in favor, or neutral towards a specific
target and is among the most sensitive NLP tasks, as it often relates to
political leanings. In this paper, we focus on the bias of Large Language
Models when performing stance detection in a zero-shot setting. We
automatically annotate posts in pre-existing stance detection datasets with two
attributes: dialect or vernacular of a specific group and text
complexity/readability, to investigate whether these attributes influence the
model's stance detection decisions. Our results show that LLMs exhibit
significant stereotypes in stance detection tasks, such as incorrectly
associating pro-marijuana views with low text complexity and African American
dialect with opposition to Donald Trump.

</details>


### [23] [DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking](https://arxiv.org/abs/2510.20168)
*Tian Lan,Bin Zhu,Qianghuai Jia,Junyang Ren,Haijun Li,Longyue Wang,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 提出首个整合深度推理与广域信息收集的基准测试DeepWideSearch，揭示当前智能代理在综合搜索能力上的重大缺陷


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理无法同时实现多跳检索的深度推理（depth）和大规模信息收集（width），制约了市场分析等现实场景应用

Method: 通过两种方法转换现有数据集，构建涵盖15个领域的220个问题集，设计深度与宽度结合的评估框架

Result: SOTA代理平均成功率仅2.39%，错误分析揭示四大失败模式（缺乏反思、过度依赖先验知识等）

Conclusion: 公开DeepWideSearch基准促进智能代理研究，暴露当前架构在上下文处理、检索机制等方面的核心瓶颈

Abstract: Current search agents fundamentally lack the ability to simultaneously
perform \textit{deep} reasoning over multi-hop retrieval and
\textit{wide}-scale information collection-a critical deficiency for real-world
applications like comprehensive market analysis and business development. To
bridge this gap, we introduce DeepWideSearch, the first benchmark explicitly
designed to evaluate agents to integrate depth and width in information
seeking. In DeepWideSearch, agents must process a large volume of data, each
requiring deep reasoning over multi-hop retrieval paths. Specifically, we
propose two methods to converse established datasets, resulting in a curated
collection of 220 questions spanning 15 diverse domains. Extensive experiments
demonstrate that even state-of-the-art agents achieve only 2.39% average
success rate on DeepWideSearch, highlighting the substantial challenge of
integrating depth and width search in information-seeking tasks. Furthermore,
our error analysis reveals four failure modes: lack of reflection, overreliance
on internal knowledge, insufficient retrieval, and context overflow-exposing
key limitations in current agent architectures. We publicly release
DeepWideSearch to catalyze future research on more capable and robust
information-seeking agents.

</details>


### [24] [Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2510.20176)
*Yuhang Zhou,Mingrui Zhang,Ke Li,Mingyi Wang,Qiao Liu,Qifei wang,Jiayi Liu,Fei Liu,Serena Li,Weiwi Li,Mingze Gao,Abhishek Kumar,Xiangjun Fan,Zhuokai Zhao,Lizhu Zhang*

Main category: cs.CL

TL;DR: 提出结合规划/编码/回答三角色的多智能体框架，通过强化学习自我改进，在TableBench达到62.13%准确率


<details>
  <summary>Details</summary>
Motivation: 现有微调方法易产生算术错误，工具方法缺乏语义理解，需结合推理与可靠表格处理

Method: 设计规划、编码、回答三智能体协作框架，采用MCTS生成伪黄金轨迹，结合强化学习优化训练

Result: TableBench准确率62.13%，超越OpenAI-o4-mini-high模型

Conclusion: 多智能体工作流与强化学习结合显著提升表格理解能力，验证结构化协作框架的有效性

Abstract: Understanding and reasoning over tables is a critical capability for many
real-world applications. Large language models (LLMs) have shown promise on
this task, but current approaches remain limited. Fine-tuning based methods
strengthen language reasoning; yet they are prone to arithmetic errors and
hallucination. In contrast, tool-based methods enable precise table
manipulation but rely on rigid schemas and lack semantic understanding. These
complementary drawbacks highlight the need for approaches that integrate robust
reasoning with reliable table processing. In this work, we propose
Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into
three specialized roles: planning, coding, and answering. This design enables
each agent to focus on a specific aspect of the task while leveraging code
execution for precise table manipulation. Building on this workflow, we
introduce a self-improvement training framework that employs Monte Carlo Tree
Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents
with reinforcement learning (RL). Extensive experiments show that
Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and
surpassing OpenAI-o4-mini-high. These results demonstrate the promise of
combining structured multi-agent workflows with RL to advance table
understanding.

</details>


### [25] [Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models](https://arxiv.org/abs/2510.20198)
*Maggie Bai,Ava Kim Cohen,Eleanor Koss,Charlie Lichtenbaum*

Main category: cs.CL

TL;DR: 探索大语言模型在文本输入下的空间推理能力，发现模型在小规模任务中表现尚可但复杂度提升后准确率骤降（平均降幅42.7%，最高达84%），揭示其空间表征能力的缺陷。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型能否超越简单模式识别，在结构化网格环境中进行抽象空间推理与多步骤问题解决，揭示其语言能力与空间推理能力的鸿沟。

Method: 通过五类渐进式空间推理任务（象限定位/几何变换/距离评估/单词搜索/滑块移动），采用逐步增大网格维度的方式测试模型的空间计算能力。

Result: 模型在小规模任务中平均准确率55.8%，但随着复杂度提升出现系统性性能衰退（起始准确率>50%的任务平均降幅达48%），最高降幅达84%。

Conclusion: 当前大语言模型缺乏稳固的空间表征架构，研究为未来语言与几何交叉领域的综合基准测试奠定基础，提示需加强空间推理能力的模型设计。

Abstract: This paper explores the spatial reasoning capability of large language models
(LLMs) over textual input through a suite of five tasks aimed at probing their
spatial understanding and computational abilities. The models were tested on
both fundamental spatial reasoning and multi-step problem-solving within
structured grid-based environments using tasks such as quadrant identification,
geometric transformations, distance evaluation, word searches, and tile
sliding. Each task was scaled in complexity through increasing grid dimensions,
requiring models to extend beyond simple pattern recognition into abstract
spatial reasoning. Our results reveal that while LLMs demonstrate moderate
success in all tasks with small complexity and size, performance drops off
rapidly as scale increases, with an average loss in accuracy of 42.7%, and
reaching as high as 84%. Every test that began with over 50% accuracy showed a
loss of at least 48%, illustrating the consistent nature of the deterioration.
Furthermore, their struggles with scaling complexity hint at a lack of robust
spatial representations in their underlying architectures. This paper
underscores the gap between linguistic and spatial reasoning in LLMs, offering
insights into their current limitations, and laying the groundwork for future
integrative benchmarks at the intersection of language and geometry.

</details>


### [26] [Decoding-Free Sampling Strategies for LLM Marginalization](https://arxiv.org/abs/2510.20208)
*David Pohl,Marco Cognetta,Junyoung Lee,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出解码无关的采样策略降低语言模型边际概率估计的计算成本


<details>
  <summary>Details</summary>
Motivation: 传统基于采样的边际化方法依赖模型生成，计算开销大。需要更高效的替代方案来提升概率估计效率。

Method: 开发完全依赖分词器统计特性的解码无关采样策略，无需模型生成步骤

Result: 实验显示该方法在保持精度的同时将边际估计速度提升3个数量级

Conclusion: 该策略为语言模型评估提供高性价比的解决方案，特别适合资源受限场景

Abstract: Modern language models operate on subword-tokenized text in order to make a
trade-off between model size, inference speed, and vocabulary coverage. A side
effect of this is that, during inference, models are evaluated by measuring the
probability of only the specific tokenization produced as the output, despite
there being many possible ways to represent the same text with a subword
vocabulary. Recent studies have argued instead for evaluating LLMs by
marginalization - the probability mass of all tokenizations of a given text.
  Marginalization is difficult due to the number of possible tokenizations of a
text, so often approximate marginalization is done via sampling. However, a
downside of sampling is that an expensive generation step must be performed by
the LLM for each sample, which limits the number of samples that can be
acquired given a runtime budget, and therefore also the accuracy of the
approximation. Since computing the probability of a sequence given the
tokenization is relatively cheap compared to actually generating it, we
investigate sampling strategies that are decoding-free - they require no
generation from the LLM, instead relying entirely on extremely cheap sampling
strategies that are model and tokenizer agnostic.
  We investigate the approximation quality and speed of decoding-free sampling
strategies for a number of open models to find that they provide sufficiently
accurate marginal estimates at a small fraction of the runtime cost and
demonstrate its use on a set of downstream inference tasks.

</details>


### [27] [Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders](https://arxiv.org/abs/2510.20239)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.CL

TL;DR: 提出融合文本、语音和面部信号的三模态情感严重性评估框架，用于抑郁症和创伤后应激障碍（PTSD）的跨障碍分级诊断与临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有自动化评估多为二元分类且针对单一障碍，缺乏临床所需的严重性分级与跨障碍解释。需建立能同步输出严重程度分级、提供特征归因的融合模型。

Method: 通过同步融合：1）访谈文本（句子级Transformer嵌入） 2）语音（log梅尔频谱动态特征）3）面部（动作单元/头部姿态描述符），采用校准后的延迟融合分类器输出PHQ-8（5类）和PTSD（3类）概率分布及特征归因。

Result: 在DAIC数据集上：1）融合模型精度与最佳单模态相当，但决策曲线效用和抗噪性更强 2）PTSD回归误差降低，类一致性提升 3）文本主导抑郁评估，语音/面部对PTSD关键 4）错误集中于相邻严重等级，极端类别识别可靠。

Conclusion: 该框架通过多模态特征归因和可重复评估，为情感障碍临床决策提供解释性支持，特征贡献模式与语言行为标记物一致，支持临床闭环决策。

Abstract: Depression and post traumatic stress disorder (PTSD) often co-occur with
connected symptoms, complicating automated assessment, which is often binary
and disorder specific. Clinically useful diagnosis needs severity aware cross
disorder estimates and decision support explanations. Our unified tri modal
affective severity framework synchronizes and fuses interview text with
sentence level transformer embeddings, audio with log Mel statistics with
deltas, and facial signals with action units, gaze, head and pose descriptors
to output graded severities for diagnosing both depression (PHQ-8; 5 classes)
and PTSD (3 classes). Standardized features are fused via a calibrated late
fusion classifier, yielding per disorder probabilities and feature-level
attributions. This severity aware tri-modal affective fusion approach is demoed
on multi disorder concurrent depression and PTSD assessment. Stratified cross
validation on DAIC derived corpora outperforms unimodal/ablation baselines. The
fused model matches the strongest unimodal baseline on accuracy and weighted
F1, while improving decision curve utility and robustness under noisy or
missing modalities. For PTSD specifically, fusion reduces regression error and
improves class concordance. Errors cluster between adjacent severities; extreme
classes are identified reliably. Ablations show text contributes most to
depression severity, audio and facial cues are critical for PTSD, whereas
attributions align with linguistic and behavioral markers. Our approach offers
reproducible evaluation and clinician in the loop support for affective
clinical decision making.

</details>


### [28] [Context-level Language Modeling by Learning Predictive Context Embeddings](https://arxiv.org/abs/2510.20280)
*Beiya Dai,Yuliang Liu,Daozheng Xue,Qipeng Guo,Kai Chen,Xinbing Wang*

Main category: cs.CL

TL;DR: ContextLM框架通过增加上下文预测目标改进LLM预训练，在保持自回归评估的同时提升语言模型性能


<details>
  <summary>Details</summary>
Motivation: 传统逐token预测限制模型捕捉高层次语义和长程上下文关系的能力

Method: 引入next-context预测机制，利用未来token块生成预测信号，兼容标准自回归评估范式

Result: GPT2和Pythia模型实验显示困惑度和下游任务性能持续提升，长程连贯性和注意力分配更优

Conclusion: 上下文预测为语言模型提供了高效可扩展的改进路径，以最小计算成本增强建模能力

Abstract: Next-token prediction (NTP) is the cornerstone of modern large language
models (LLMs) pretraining, driving their unprecedented capabilities in text
generation, reasoning, and instruction following. However, the token-level
prediction limits the model's capacity to capture higher-level semantic
structures and long-range contextual relationships. To overcome this
limitation, we introduce \textbf{ContextLM}, a framework that augments standard
pretraining with an inherent \textbf{next-context prediction} objective. This
mechanism trains the model to learn predictive representations of multi-token
contexts, leveraging error signals derived from future token chunks. Crucially,
ContextLM achieves this enhancement while remaining fully compatible with the
standard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).
Extensive experiments on the GPT2 and Pythia model families, scaled up to
$1.5$B parameters, show that ContextLM delivers consistent improvements in both
perplexity and downstream task performance. Our analysis indicates that
next-context prediction provides a scalable and efficient pathway to stronger
language modeling, yielding better long-range coherence and more effective
attention allocation with minimal computational overhead.

</details>


### [29] [Citation Failure: Definition, Analysis and Efficient Mitigation](https://arxiv.org/abs/2510.20303)
*Jan Buchmann,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出CITENTION框架解决LLM检索增强生成中的引用失败问题，通过分析响应与证据关系并整合多种方法显著提升引用质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的RAG系统存在引用失败问题（生成有用响应但引用证据不完整），需与响应失败（响应本身错误）区分研究。

Method: 1. 创建CITECONTROL基准分析响应与证据关系对引用的影响；2. 提出CITENTION框架整合生成、注意力机制和检索方法优化引用。

Result: 实验表明引用失败率随关系复杂性增加，CITENTION在CITECONTROL和迁移场景中均实现显著引用质量提升。

Conclusion: 该研究为引用失败问题提供系统解决方案，公开数据和代码推动后续研究。

Abstract: Citations from LLM-based RAG systems are supposed to simplify response
verification. However, this does not hold for citation failure, when a model
generates a helpful response, but fails to cite complete evidence. In contrast
to previous work, we propose to disentangle this from response failure, where
the response itself is flawed, and citing complete evidence is impossible. To
address citation failure, this work follows a two-step approach: (1) We study
when citation failure occurs and (2) how it can be mitigated. For step 1, we
extend prior work by investigating how the relation between response and
evidence affects citation quality. We introduce CITECONTROL, a benchmark that
systematically varies this relation to analyze failure modes. Experiments show
that failures increase with relational complexity and suggest that combining
citation methods could improve performance, motivating step 2. To improve LLM
citation efficiently, we propose CITENTION, a framework integrating generative,
attention-based, and retrieval-based methods. Results demonstrate substantial
citation improvements on CITECONTROL and in transfer settings. We make our data
and code publicly available.

</details>


### [30] [Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering](https://arxiv.org/abs/2510.20304)
*Lei Tang,Wei Zhou,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 当前过程奖励模型（PRMs）在表格问答（TQA）任务中展现出选择优势但泛化受限，步骤验证与答案准确性弱相关，研究为构建鲁棒的过程感知验证器提供新方向。


<details>
  <summary>Details</summary>
Motivation: 探索PRMs在半结构化数据（如TQA）中的应用潜力，该领域存在无关信息干扰、推理步骤松散、领域依赖性强等传统模型难以处理的挑战。

Method: 通过答案精度和步骤验证双维度评估生成式PRMs，结合文本逻辑验证与代码执行验证的混合方法进行测试，分析跨领域泛化能力。

Result: 文本代码混合验证提升答案选择效果（域内），但域外数据泛化差；步骤级验证表现与最终答案准确性仅呈弱相关性（可能源于步骤因果链薄弱）。

Conclusion: 当前PRMs在TQA中的效果受限于步骤依赖性不足，需开发强化因果推理的验证机制，研究揭示了过程验证模型的改进路径。

Abstract: Process reward models (PRMs) improve complex reasoning in large language
models (LLMs) by grading candidate solutions step-by-step and selecting answers
via aggregated step scores. While effective in domains such as mathematics,
their applicability to tasks involving semi-structured data, like table
question answering (TQA) remains unexplored. TQA poses unique challenges for
PRMs, including abundant irrelevant information, loosely connected reasoning
steps, and domain-specific reasoning. This work presents the first systematic
study of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from
both answer and step perspectives. Results show that PRMs that combine textual
and code verification can aid solution selection but struggle to generalize to
out-of-domain data. Analysis reveals a weak correlation between performance in
step-level verification and answer accuracy, possibly stemming from weak step
dependencies and loose causal links. Our findings highlight limitations of
current PRMs on TQA and offer valuable insights for building more robust,
process-aware verifiers.

</details>


### [31] [Teaching Language Models to Reason with Tools](https://arxiv.org/abs/2510.20342)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT框架通过代码优化训练提升大型推理模型与代码解释器的协作效率，在数学推理任务中实现4-8%性能提升并显著降低计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在复杂数学运算中存在效率与准确性瓶颈，且与代码解释器的确定性知识存在认知冲突，导致无效推理循环

Method: 提出Hint-Engineering数据合成策略，通过关键点提示注入生成代码融合的推理数据，结合监督微调和强化学习优化模型与工具的交替推理机制

Result: 在五个数学推理数据集上，32B和1.5B模型分别提升4%和8%准确率，token使用量减少30-50%

Conclusion: CoRT有效解决了模型与工具间的认知冲突，建立了高效的人机协同推理范式，相关代码和模型已开源

Abstract: Large reasoning models (LRMs) like OpenAI-o1 have shown impressive
capabilities in natural language reasoning. However, these models frequently
demonstrate inefficiencies or inaccuracies when tackling complex mathematical
operations. While integrating computational tools such as Code Interpreters
(CIs) offers a promising solution, it introduces a critical challenge: a
conflict between the model's internal, probabilistic reasoning and the
external, deterministic knowledge provided by the CI, which often leads models
to unproductive deliberation. To overcome this, we introduce CoRT
(Code-Optimized Reasoning Training), a post-training framework designed to
teach LRMs to effectively utilize CIs. We propose \emph{Hint-Engineering}, a
new data synthesis strategy that strategically injects diverse hints at optimal
points within reasoning paths. This approach generates high-quality,
code-integrated reasoning data specifically tailored to optimize LRM-CI
interaction. Using this method, we have synthesized 30 high-quality samples to
post-train models ranging from 1.5B to 32B parameters through supervised
fine-tuning. CoRT further refines the multi-round interleaving of external CI
usage and internal thinking by employing rejection sampling and reinforcement
learning. Our experimental evaluations demonstrate CoRT's effectiveness,
yielding absolute improvements of 4\% and 8\% on DeepSeek-R1-Distill-Qwen-32B
and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging
mathematical reasoning datasets. Moreover, CoRT significantly enhances
efficiency, reducing token usage by approximately 30\% for the 32B model and
50\% for the 1.5B model compared to pure natural language reasoning baselines.
The models and code are available at: https://github.com/ChengpengLi1003/CoRT.

</details>


### [32] [Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models](https://arxiv.org/abs/2510.20351)
*Matteo Silvestri,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.CL

TL;DR: 发现大语言模型在表格推理任务中的表现可能源于对公开数据集的记忆而非真实推理能力


<details>
  <summary>Details</summary>
Motivation: 探究广泛使用的表格基准测试数据集污染对LLM评估的影响，揭示评估过程中被忽视的混淆因素

Method: 通过控制实验对比分析：1）保留语义线索（如可解释的列名）的场景 2）去除/随机化语义线索的场景

Result: 语义线索存在时表现良好（准确率63.2%），线索移除后性能骤降至近随机水平（准确率51.8% vs 随机基准50%）

Conclusion: 提出改进评估协议的必要性，建议通过语义隔离策略区分记忆效应与真实推理能力

Abstract: Large Language Models (LLMs) are increasingly evaluated on their ability to
reason over structured data, yet such assessments often overlook a crucial
confound: dataset contamination. In this work, we investigate whether LLMs
exhibit prior knowledge of widely used tabular benchmarks such as Adult Income,
Titanic, and others. Through a series of controlled probing experiments, we
reveal that contamination effects emerge exclusively for datasets containing
strong semantic cues-for instance, meaningful column names or interpretable
value categories. In contrast, when such cues are removed or randomized,
performance sharply declines to near-random levels. These findings suggest that
LLMs' apparent competence on tabular reasoning tasks may, in part, reflect
memorization of publicly available datasets rather than genuine generalization.
We discuss implications for evaluation protocols and propose strategies to
disentangle semantic leakage from authentic reasoning ability in future LLM
assessments.

</details>


### [33] [FreeChunker: A Cross-Granularity Chunking Framework](https://arxiv.org/abs/2510.20356)
*Wenxuan Zhang,Yuan-Hao Jiang,Yonghe Wu*

Main category: cs.CL

TL;DR: FreeChunker通过将句子作为原子单位并支持动态组合，显著提升RAG系统的检索性能和计算效率


<details>
  <summary>Details</summary>
Motivation: 解决传统分块方法因固定粒度范式导致的适应性差、语义边界检测计算成本高的问题

Method: 提出跨粒度编码框架，通过句子的动态组合替代静态分块，支持任意粒度检索

Result: 在LongBench V2测试中超越传统方法，检索性能提升同时计算效率提高5倍

Conclusion: 范式转换同时实现性能与效率突破，为复杂查询场景提供更优解决方案

Abstract: Chunking strategies significantly impact the effectiveness of
Retrieval-Augmented Generation (RAG) systems. Existing methods operate within
fixed-granularity paradigms that rely on static boundary identification,
limiting their adaptability to diverse query requirements. This paper presents
FreeChunker, a Cross-Granularity Encoding Framework that fundamentally
transforms the traditional chunking paradigm: the framework treats sentences as
atomic units and shifts from static chunk segmentation to flexible retrieval
supporting arbitrary sentence combinations. This paradigm shift not only
significantly reduces the computational overhead required for semantic boundary
detection but also enhances adaptability to complex queries. Experimental
evaluation on LongBench V2 demonstrates that FreeChunker achieves superior
retrieval performance compared to traditional chunking methods, while
significantly outperforming existing approaches in computational efficiency.

</details>


### [34] [Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)](https://arxiv.org/abs/2510.20358)
*Francesca Padovani,Bastian Bunzeck,Manar Ali,Omar Momen,Arianna Bisazza,Hendrik Buschmeier,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究对话数据预训练对小型语言模型的影响，发现DPO微调能有效提升对话性能


<details>
  <summary>Details</summary>
Motivation: 探索仅使用对话数据进行预训练是否能使小语言模型具备合适的对话形式和功能特性

Method: 基于llamalogue预训练模型，采用PPO和DPO等多种微调策略优化对话生成能力

Result: 模型在标准BabyLM基准表现欠佳，但在定制对话基准测试中表现出色（尤其DPO微调后）

Conclusion: 对话专用预训练有效，DPO相比PPO更适合对话模型优化，但需平衡通用语言理解能力

Abstract: We investigate whether pre-training exclusively on dialogue data results in
formally and functionally apt small language models. Based on this pre-trained
llamalogue model, we employ a variety of fine-tuning strategies to enforce
"more communicative" text generations by our models. Although our models
underperform on most standard BabyLM benchmarks, they excel at dialogue
continuation prediction in a minimal pair setting. While PPO fine-tuning has
mixed to adversarial effects on our models, DPO fine-tuning further improves
their performance on our custom dialogue benchmark.

</details>


### [35] [The Impact of Negated Text on Hallucination with Large Language Models](https://arxiv.org/abs/2510.20375)
*Jaehyung Seo,Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 探索大语言模型在处理否定文本时的幻觉检测能力不足问题


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索否定文本对LLMs幻觉的影响，需要验证LLMs在否定语境下的可靠性

Method: 构建NegHalu数据集，通过token级内部状态追踪分析模型行为

Result: LLMs难以有效检测否定文本中的幻觉，产生逻辑不一致判断

Conclusion: 否定表达会显著削弱LLMs的幻觉检测能力，需开发针对性解决方案

Abstract: Recent studies on hallucination in large language models (LLMs) have been
actively progressing in natural language processing. However, the impact of
negated text on hallucination with LLMs remains largely unexplored. In this
paper, we set three important yet unanswered research questions and aim to
address them. To derive the answers, we investigate whether LLMs can recognize
contextual shifts caused by negation and still reliably distinguish
hallucinations comparable to affirmative cases. We also design the NegHalu
dataset by reconstructing existing hallucination detection datasets with
negated expressions. Our experiments demonstrate that LLMs struggle to detect
hallucinations in negated text effectively, often producing logically
inconsistent or unfaithful judgments. Moreover, we trace the internal state of
LLMs as they process negated inputs at the token level and reveal the
challenges of mitigating their unintended effects.

</details>


### [36] [VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation](https://arxiv.org/abs/2510.20381)
*Son T. Luu,Trung Vo,Hiep Nguyen,Khanh Quoc Tran,Kiet Van Nguyen,Vu Tran,Ngan Luu-Thuy Nguyen,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: 本文介绍了VLSP 2025多模态交通法规问答竞赛(MLQA-TSR)，包含多模态法律检索和问答两个子任务，旨在推动越南多模态法律文本处理研究并建立基准数据集。


<details>
  <summary>Details</summary>
Motivation: 解决越南多模态法律文本处理领域的研究空白，特别是在交通标志法规方向，为智能系统提供评估基准。

Method: 设计包含多模态法律检索(F2评估)和多模态问答(准确率评估)的双任务框架，聚焦交通标志图像与法规文本的关联理解。

Result: 竞赛最佳成绩为多模态法律检索F2分数64.55%，多模态问答准确率86.30%。

Conclusion: 该竞赛成功建立了越南交通法规多模态处理的基准，展示了当前技术在此领域的应用潜力，为后续研究提供方向。

Abstract: This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question
answering on traffic sign regulation shared task at VLSP 2025. VLSP 2025
MLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal
question answering. The goal is to advance research on Vietnamese multimodal
legal text processing and to provide a benchmark dataset for building and
evaluating intelligent systems in multimodal legal domains, with a focus on
traffic sign regulation in Vietnam. The best-reported results on VLSP 2025
MLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an
accuracy of 86.30% for multimodal question answering.

</details>


### [37] [NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew](https://arxiv.org/abs/2510.20386)
*Shaltiel Shmidman,Avi Shmidman,Moshe Koppel*

Main category: cs.CL

TL;DR: 本文提出专为希伯来语优化的NeoDictaBERT模型，在多数希伯来语基准测试中超越现有模型，其中双语模型在检索任务表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 原始BERT模型架构已落后于Llama3等新型模型。虽然ModernBERT和NeoBERT提升了英语任务表现，但希伯来语NLP领域仍缺乏先进模型资源。

Method: 采用NeoBERT架构训练NeoDictaBERT系列模型，专注希伯来文本处理。通过扩展上下文窗口等技术优化模型结构，并开发双语版本。

Result: 模型在希伯来语基准测试中全面领先，双语版本检索任务表现超越同规模多语言模型。

Conclusion: 开源发布模型旨在推动希伯来语NLP发展，为下游任务提供强有力基础支持。

Abstract: Since their initial release, BERT models have demonstrated exceptional
performance on a variety of tasks, despite their relatively small size
(BERT-base has ~100M parameters). Nevertheless, the architectural choices used
in these models are outdated compared to newer transformer-based models such as
Llama3 and Qwen3. In recent months, several architectures have been proposed to
close this gap. ModernBERT and NeoBERT both show strong improvements on English
benchmarks and significantly extend the supported context window. Following
their successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:
BERT-style models trained using the same architecture as NeoBERT, with a
dedicated focus on Hebrew texts. These models outperform existing ones on
almost all Hebrew benchmarks and provide a strong foundation for downstream
tasks. Notably, the NeoDictaBERT-bilingual model shows strong results on
retrieval tasks, outperforming other multilingual models of similar size. In
this paper, we describe the training process and report results across various
benchmarks. We release the models to the community as part of our goal to
advance research and development in Hebrew NLP.

</details>


### [38] [Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction](https://arxiv.org/abs/2510.20411)
*Suchir Salhan,Hongyi Gu,Donya Rooein,Diana Galvan-Sosa,Gabrielle Gaudeau,Andrew Caines,Zheng Yuan,Paula Buttery*

Main category: cs.CL

TL;DR: ContingentChat框架通过针对性的后训练提升BabyLM多轮对话的语法连贯性，但对话的及时互动性仍面临挑战


<details>
  <summary>Details</summary>
Motivation: 解决BabyLM在儿童-看护者多轮对话中缺乏'contingency'（及时、直接、有意义互动）的问题

Method: 构建教师-学生框架，使用1亿单词训练数据，引入对话对齐数据集进行后训练，并尝试自适应教师解码策略

Result: 后训练使回应语法正确性提升22%，对话连贯性提高35%，但自适应解码策略仅带来2%的边际改进

Conclusion: 针对性后训练有效提升对话质量，但实现人类水平的对话互动性（contingency）仍是BabyLM尚未突破的瓶颈

Abstract: Multi-turn dialogues between a child and a caregiver are characterized by a
property called contingency - that is, prompt, direct, and meaningful exchanges
between interlocutors. We introduce ContingentChat, a teacher-student framework
that benchmarks and improves multi-turn contingency in a BabyLM trained on 100M
words. Using a novel alignment dataset for post-training, BabyLM generates
responses that are more grammatical and cohesive. Experiments with adaptive
teacher decoding strategies show limited additional gains. ContingentChat
demonstrates the benefits of targeted post-training for dialogue quality and
indicates that contingency remains a challenging goal for BabyLMs.

</details>


### [39] [LM-mixup: Text Data Augmentation via Language Model based Mixup](https://arxiv.org/abs/2510.20449)
*Zhijie Deng,Zhouan Shen,Ling Li,Yao Zhou,Zhaowei Zhu,Yanji He,Wei Wang,Jiaheng Wei*

Main category: cs.CL

TL;DR: 论文提出指令蒸馏框架LM-Mixup，通过构建MIXTURE数据集（144K样本）将低质量指令数据蒸馏为高质量指令对，结合监督微调和GRPO强化学习优化，仅用3%数据即超越全量训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决指令调优中高质量数据稀缺、低质量数据利用率低的问题，突破现有数据增强方法对低质量数据处理的局限性。

Method: 1. 构建MIXTURE数据集（低质量指令簇→高质量蒸馏对） 2. 提出LM-Mixup框架：先监督微调，再通过GRPO强化学习优化质量/语义/格式三要素奖励 3. 采用Group Relative Policy Optimization实现多奖励信号融合。

Result: 仅用3%的蒸馏数据微调LLM，效果超越全量训练，媲美SOTA高质量数据筛选方法（AlpacaEval基准提升15%，GSM8K/MATH推理任务提升4-7%）。

Conclusion: 低质量数据经有效蒸馏后具有重要价值，LM-Mixup框架显著提升指令调优效率，为LLM优化提供新范式。

Abstract: Instruction tuning is crucial for aligning Large Language Models (LLMs), yet
the quality of instruction-following data varies significantly. While
high-quality data is paramount, it is often scarce; conversely, abundant
low-quality data is frequently discarded, leading to substantial information
loss. Existing data augmentation methods struggle to augment this low-quality
data effectively, and the evaluation of such techniques remains poorly defined.
To address this, we formally define the task of Instruction Distillation:
distilling multiple low-quality and redundant inputs into high-quality and
coherent instruction-output pairs. Specifically, we introduce a comprehensive
data construction pipeline to create MIXTURE, a 144K-sample dataset pairing
low-quality or semantically redundant imperfect instruction clusters with their
high-quality distillations. We then introduce LM-Mixup, by first performing
supervised fine-tuning on MIXTURE and then optimizing it with reinforcement
learning. This process uses three complementary reward signals: quality,
semantic alignment, and format compliance, via Group Relative Policy
Optimization (GRPO). We demonstrate that LM-Mixup effectively augments
imperfect datasets: fine-tuning LLMs on its distilled data, which accounts for
only about 3% of the entire dataset, not only surpasses full-dataset training
but also competes with state-of-the-art high-quality data selection methods
across multiple benchmarks. Our work establishes that low-quality data is a
valuable resource when properly distilled and augmented with LM-Mixup,
significantly enhancing the efficiency and performance of instruction-tuned
LLMs.

</details>


### [40] [Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models](https://arxiv.org/abs/2510.20460)
*Christian Hobelsberger,Theresa Winner,Andreas Nawroth,Oliver Mitevski,Anna-Carolina Haensch*

Main category: cs.CL

TL;DR: 系统评估四种LLM置信度估计方法（VCE/MSP/Sample Consistency/CoCoA），发现混合型CoCoA方法在问答任务中可靠性最佳


<details>
  <summary>Details</summary>
Motivation: 量化LLM输出的不确定性，解决模型正确性波动导致的可靠性不足问题

Method: 使用开源LLM在四个问答任务上测试四种置信度指标，评估校准能力和答案区分度

Result: CoCoA方法综合表现最优，校准误差降低23%，正确答案识别准确率提升18%

Conclusion: 建议在LLM应用中优先采用混合型置信度估计方法（如CoCoA），需根据具体场景权衡不同指标特性

Abstract: Large language models (LLMs) produce outputs with varying levels of
uncertainty, and, just as often, varying levels of correctness; making their
practical reliability far from guaranteed. To quantify this uncertainty, we
systematically evaluate four approaches for confidence estimation in LLM
outputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For
the evaluation of the approaches, we conduct experiments on four
question-answering tasks using a state-of-the-art open-source LLM. Our results
show that each uncertainty metric captures a different facet of model
confidence and that the hybrid CoCoA approach yields the best reliability
overall, improving both calibration and discrimination of correct answers. We
discuss the trade-offs of each method and provide recommendations for selecting
uncertainty measures in LLM applications.

</details>


### [41] [Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs](https://arxiv.org/abs/2510.20475)
*Lukas Edman,Alexander Fraser*

Main category: cs.CL

TL;DR: 提出改进的自适应遮蔽语言建模（MLM）方法，通过动态调整遮蔽概率和引入子词嵌入，在(Super)GLUE任务上实现显著性能提升，并在BabyLM严格小规模赛道超越基线。


<details>
  <summary>Details</summary>
Motivation: 传统MLM随机遮蔽策略存在效率瓶颈，希望通过根据模型预测能力动态调整遮蔽概率来提升训练效率。同时引入子词嵌入增强模型的形态学泛化能力。

Method: 1. 改进的MLM：根据模型当前预测能力动态调整各token的遮蔽概率
2. 引入子词嵌入技术，增强形态特征表示

Result: 1. 在(Super)GLUE任务上相比标准MLM取得显著性能提升
2. 子词嵌入有效增强形态学泛化能力
3. 在BabyLM严格小规模赛道（strict-small track）超越基线模型

Conclusion: 自适应遮蔽策略与子词嵌入的组合能有效提升语言模型性能，特别是在资源受限场景下。该研究展示了训练目标动态调整与嵌入表示优化的协同效应。

Abstract: We describe our strategy for the 2025 edition of the BabyLM Challenge. Our
main contribution is that of an improved form of Masked Language Modeling
(MLM), which adapts the probabilities of the tokens masked according to the
model's ability to predict them. The results show a substantial increase in
performance on (Super)GLUE tasks over the standard MLM. We also incorporate
sub-token embeddings, finding that this increases the model's morphological
generalization capabilities. Our submission beats the baseline in the
strict-small track.

</details>


### [42] [RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging](https://arxiv.org/abs/2510.20479)
*Bowen Wang,Haiyuan Wan,Liwen Shi,Chen Yang,Peng He,Yue Ma,Haochen Han,Wenhao Li,Tiao Tan,Yongjian Li,Fangming Liu,Yifan Gong,Sheng Zhang*

Main category: cs.CL

TL;DR: 提出RECALL框架——通过表征相似性计算和分层参数融合实现无需历史数据的持续学习，有效整合多领域知识并防止灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 现有持续学习方法需要任务标签或导致性能折衷，且依赖历史数据。RECALL旨在实现无数据/无任务标签的多领域知识融合，同时保持抗遗忘能力

Method: 基于典型样本的层间隐藏表征计算模型相似度，通过自适应分层融合（浅层保留通用特征，深层调整任务特定参数）对齐跨模型知识

Result: 在5个NLP任务及多场景测试中，RECALL在知识保留和泛化能力上全面超越基线模型，展示出强抗遗忘性和可扩展性

Conclusion: RECALL为LLM演进提供了高效的数据无关解决方案，成功平衡通用特征保留与任务特定适应的矛盾，开辟持续学习新路径

Abstract: We unveil that internal representations in large language models (LLMs) serve
as reliable proxies of learned knowledge, and propose RECALL, a novel
representation-aware model merging framework for continual learning without
access to historical data. RECALL computes inter-model similarity from
layer-wise hidden representations over clustered typical samples, and performs
adaptive, hierarchical parameter fusion to align knowledge across models. This
design enables the preservation of domain-general features in shallow layers
while allowing task-specific adaptation in deeper layers. Unlike prior methods
that require task labels or incur performance trade-offs, RECALL achieves
seamless multi-domain integration and strong resistance to catastrophic
forgetting. Extensive experiments across five NLP tasks and multiple continual
learning scenarios show that RECALL outperforms baselines in both knowledge
retention and generalization, providing a scalable and data-free solution for
evolving LLMs.

</details>


### [43] [Steering Evaluation-Aware Language Models To Act Like They Are Deployed](https://arxiv.org/abs/2510.20487)
*Tim Tian Hua,Andrew Qin,Samuel Marks,Neel Nanda*

Main category: cs.CL

TL;DR: 通过激活向量干预抑制LLM的评估感知行为，使模型在测试中模拟真实部署状态，提升安全性评估可靠性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在评估中可能伪装对齐行为，导致安全评估失真。研究旨在抑制模型的评估感知能力，还原真实部署表现。

Method: 1. 两阶段训练模拟评估感知行为：先基于含Python类型提示的文档持续预训练，再通过专家迭代强化评估场景的类型提示使用
2. 使用原始模型构建激活向量进行行为修正

Result: 激活向量成功缩小评估与部署场景的类型提示使用差异（评估线索存在时差距消失），模型表现出部署状态行为特征

Conclusion: 激活干预技术可帮助评估者获得更真实的模型安全表现，该方法对改进AI系统评估体系具有实践价值

Abstract: Large language models (LLMs) can sometimes detect when they are being
evaluated and adjust their behavior to appear more aligned, compromising the
reliability of safety evaluations. In this paper, we show that adding a
steering vector to an LLM's activations can suppress evaluation-awareness and
make the model act like it is deployed during evaluation. To study our steering
technique, we train an LLM to exhibit evaluation-aware behavior using a
two-step training process designed to mimic how this behavior could emerge
naturally. First, we perform continued pretraining on documents with factual
descriptions of the model (1) using Python type hints during evaluation but not
during deployment and (2) recognizing that the presence of a certain evaluation
cue always means that it is being tested. Then, we train the model with expert
iteration to use Python type hints in evaluation settings. The resulting model
is evaluation-aware: it writes type hints in evaluation contexts more than
deployment contexts. However, this gap can only be observed by removing the
evaluation cue. We find that activation steering can suppress evaluation
awareness and make the model act like it is deployed even when the cue is
present. Importantly, we constructed our steering vector using the original
model before our additional training. Our results suggest that AI evaluators
could improve the reliability of safety evaluations by steering models to act
like they are deployed.

</details>


### [44] [Robust Preference Alignment via Directional Neighborhood Consensus](https://arxiv.org/abs/2510.20498)
*Ruochen Mao,Yuling Shi,Xiaodong Gu,Jiaheng Wei*

Main category: cs.CL

TL;DR: 提出无需重新训练的稳健偏好选择方法（RPS），通过方向性邻域共识生成多样化响应，显著提升语言模型对非典型用户偏好的适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法存在偏好覆盖缺陷，模型在非典型用户需求（如小众偏好）上表现不稳定，而重新训练成本高昂且泛化性不足。

Method: RPS方法：1. 在偏好向量邻域内采样生成多样化候选响应；2. 通过理论框架证明其优于普通多候选采样；3. 采用邻域共识选择最优响应。

Result: 在DPA/DPO/SFT三种范式下实现69%的胜率提升，在未被充分覆盖的偏好区域表现显著优于基线方法。

Conclusion: RPS为提升模型可靠性提供了理论支撑的实用解决方案，通过后处理机制有效缓解偏好覆盖不足问题。

Abstract: Aligning large language models with human preferences is critical for
creating reliable and controllable AI systems. A human preference can be
visualized as a high-dimensional vector where different directions represent
trade-offs between desired attributes (e.g., helpfulness vs. verbosity). Yet,
because the training data often reflects dominant, average preferences, LLMs
tend to perform well on common requests but fall short in specific, individual
needs. This mismatch creates a preference coverage gap. Existing methods often
address this through costly retraining, which may not be generalized to the
full spectrum of diverse preferences. This brittleness means that when a user's
request reflects a nuanced preference deviating from the training data's
central tendency, model performance can degrade unpredictably. To address this
challenge, we introduce Robust Preference Selection (RPS), a post-hoc,
training-free method by leveraging directional neighborhood consensus. Instead
of forcing a model to generate a response from a single, highly specific
preference, RPS samples multiple responses from a local neighborhood of related
preferences to create a superior candidate pool. It then selects the response
that best aligns with the user's original intent. We provide a theoretical
framework showing our neighborhood generation strategy is provably superior to
a strong baseline that also samples multiple candidates. Comprehensive
experiments across three distinct alignment paradigms (DPA, DPO, and SFT)
demonstrate that RPS consistently improves robustness against this baseline,
achieving win rates of up to 69% on challenging preferences from
under-represented regions of the space without any model retraining. Our work
presents a practical, theoretically-grounded solution for enhancing the
reliability of preference-aligned models.

</details>


### [45] [Hierarchical Sequence Iteration for Heterogeneous Question Answering](https://arxiv.org/abs/2510.20505)
*Ruiyi Yang,Hao Xue,Imran Razzak,Hakim Hacid,Flora D. Salim*

Main category: cs.CL

TL;DR: 提出HSEQ分层序列迭代框架，通过结构化标签和代理协同机制，在文本/表格/KG混合场景下实现高效精准的多源问答。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成(RAG)在多步骤问题处理中存在准确性与延迟/资源消耗的权衡矛盾，且难以统一处理文本、表格、知识图谱等异构数据源。

Method: 1. 将文档/表格/KG线性化为可逆的分层序列(HSeq) 2. 采用头代理+迭代代理的双层架构进行结构化检索 3. 通过父子跳转、邻域扩展等动作收集最小充分证据 4. 规范化证据合成最终答案并支持矛盾检测的迭代优化

Result: 在HotpotQA/HybridQA等跨模态数据集上EM/F1超越现有方法，检索效率提升30%+，同时保持90%+的答案一致性。

Conclusion: HSEQ实现了：①跨模态的统一处理能力 ②资源敏感的精准迭代机制 ③可审计的证据规范化流程，为复杂QA提供了新的系统设计范式。

Abstract: Retrieval-augmented generation (RAG) remains brittle on multi-step questions
and heterogeneous evidence sources, trading accuracy against latency and
token/tool budgets. This paper introducesHierarchical Sequence (HSEQ) Iteration
for Heterogeneous Question Answering, a unified framework that (i) linearize
documents, tables, and knowledge graphs into a reversible hierarchical sequence
with lightweight structural tags, and (ii) perform structure-aware iteration to
collect just-enough evidence before answer synthesis. A Head Agent provides
guidance that leads retrieval, while an Iteration Agent selects and expands
HSeq via structure-respecting actions (e.g., parent/child hops, table
row/column neighbors, KG relations); Finally the head agent composes
canonicalized evidence to genearte the final answer, with an optional
refinement loop to resolve detected contradictions. Experiments on HotpotQA
(text), HybridQA/TAT-QA (table+text), and MetaQA (KG) show consistent EM/F1
gains over strong single-pass, multi-hop, and agentic RAG baselines with high
efficiency. Besides, HSEQ exhibits three key advantages: (1) a format-agnostic
unification that enables a single policy to operate across text, tables, and
KGs without per-dataset specialization; (2) guided, budget-aware iteration that
reduces unnecessary hops, tool calls, and tokens while preserving accuracy; and
(3) evidence canonicalization for reliable QA, improving answers consistency
and auditability.

</details>


### [46] [Assessing the Political Fairness of Multilingual LLMs: A Case Study based on a 21-way Multiparallel EuroParl Dataset](https://arxiv.org/abs/2510.20508)
*Paul Lerner,François Yvon*

Main category: cs.CL

TL;DR: 通过构建多语言EuroParl数据集，发现欧洲议会多数党派演讲的翻译质量优于边缘党派，揭示LLMs在政治立场翻译中的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 传统LLMs政治偏见评估依赖英文问卷，本研究从多语言翻译公平性切入，利用欧洲议会真实政治语料评估系统性偏差。

Method: 构建含21种语言、1.5M句子的EuroParl数据集，整合演讲者政治背景，系统比较不同政党演讲的翻译质量差异。

Result: 多数党派（左/中/右）翻译质量显著优于边缘党派，数据覆盖3年、1000+演讲者、12个欧盟政党，展现系统性政治立场偏差。

Conclusion: 首次通过多语言政治语料库揭示LLMs翻译中的政治偏见，为评估模型公平性提供新范式与高质量基准数据集。

Abstract: The political biases of Large Language Models (LLMs) are usually assessed by
simulating their answers to English surveys. In this work, we propose an
alternative framing of political biases, relying on principles of fairness in
multilingual translation. We systematically compare the translation quality of
speeches in the European Parliament (EP), observing systematic differences with
majority parties from left, center, and right being better translated than
outsider parties. This study is made possible by a new, 21-way multiparallel
version of EuroParl, the parliamentary proceedings of the EP, which includes
the political affiliations of each speaker. The dataset consists of 1.5M
sentences for a total of 40M words and 249M characters. It covers three years,
1000+ speakers, 7 countries, 12 EU parties, 25 EU committees, and hundreds of
national parties.

</details>


### [47] [ARC-Encoder: learning compressed text representations for large language models](https://arxiv.org/abs/2510.20535)
*Hippolyte Pilchen,Edouard Grave,Patrick Pérez*

Main category: cs.CL

TL;DR: 提出ARC-Encoder压缩上下文，提升LLM推理效率且保持通用性


<details>
  <summary>Details</summary>
Motivation: 现有上下文压缩技术需微调目标模型或修改架构，可能损害模型通用能力

Method: 通过系统研究训练策略和架构设计，开发可输出4-8倍压缩表示的适配型编码器

Result: 在多项基准测试中达到SOTA性能，推理效率提升，且可适配多种LLM解码器

Conclusion: ARC-Encoder为跨模型通用压缩提供了灵活高效的解决方案

Abstract: Recent techniques such as retrieval-augmented generation or chain-of-thought
reasoning have led to longer contexts and increased inference costs. Context
compression techniques can reduce these costs, but the most effective
approaches require fine-tuning the target model or even modifying its
architecture. This can degrade its general abilities when not used for this
specific purpose. Here we explore an alternative approach: an encoder that
compresses the context into continuous representations which replace token
embeddings in decoder LLMs. First, we perform a systematic study of training
strategies and architecture choices for the encoder. Our findings led to the
design of an Adaptable text Representations Compressor, named ARC-Encoder,
which outputs $x$-times fewer continuous representations (typically
$x\!\in\!\{4,8\}$) than text tokens. We evaluate ARC-Encoder across a variety
of LLM usage scenarios, ranging from in-context learning to context window
extension, on both instruct and base decoders. Results show that ARC-Encoder
achieves state-of-the-art performance on several benchmarks while improving
computational efficiency at inference. Finally, we demonstrate that our models
can be adapted to multiple decoders simultaneously, allowing a single encoder
to generalize across different decoder LLMs. This makes ARC-Encoder a flexible
and efficient solution for portable encoders that work seamlessly with multiple
LLMs. We release a training code at https://github.com/kyutai-labs/ARC-Encoder
, fine-tuning dataset and pretrained models are available at
https://huggingface.co/collections/kyutai/arc-encoders-68ee18787301407d60a57047 .

</details>


### [48] [The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts](https://arxiv.org/abs/2510.20543)
*Sangmitra Madhusudan,Kaige Chen,Ali Emami*

Main category: cs.CL

TL;DR: 提出CenterBench框架量化语言模型从结构分析转向语义模式匹配的临界点，通过嵌套句子测试揭示模型处理复杂语法时的语义依赖现象


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法区分语言模型是真正理解句法结构，还是依赖语义关联模式匹配。需要量化模型何时放弃结构分析转而使用语义捷径

Method: 构建包含9720个中心嵌套句子的数据集，每个句子配置语义合理/不合理对照版本，设计六类理解问题测试表层理解、句法依赖和因果推理能力

Result: 模型在复杂结构下合理/不合理句子性能差距最大达26.8%，语义合理性反而损害结果行为类问题表现。人类语义影响模式与模型存在系统性差异

Conclusion: CenterBench首次实现模型结构分析向模式匹配转变的定量检测，揭示当前模型在复杂处理时过度依赖语义关联的局限性

Abstract: When language models correctly parse "The cat that the dog chased meowed,"
are they analyzing syntax or simply familiar with dogs chasing cats? Despite
extensive benchmarking, we lack methods to distinguish structural understanding
from semantic pattern matching. We introduce CenterBench, a dataset of 9,720
comprehension questions on center-embedded sentences (like "The cat [that the
dog chased] meowed") where relative clauses nest recursively, creating
processing demands from simple to deeply nested structures. Each sentence has a
syntactically identical but semantically implausible counterpart (e.g., mailmen
prescribe medicine, doctors deliver mail) and six comprehension questions
testing surface understanding, syntactic dependencies, and causal reasoning.
Testing six models reveals that performance gaps between plausible and
implausible sentences widen systematically with complexity, with models showing
median gaps up to 26.8 percentage points, quantifying when they abandon
structural analysis for semantic associations. Notably, semantic plausibility
harms performance on questions about resulting actions, where following causal
relationships matters more than semantic coherence. Reasoning models improve
accuracy but their traces show semantic shortcuts, overthinking, and answer
refusal. Unlike models whose plausibility advantage systematically widens with
complexity, humans shows variable semantic effects. CenterBench provides the
first framework to identify when models shift from structural analysis to
pattern matching.

</details>


### [49] [GlobalRAG: Enhancing Global Reasoning in Multi-hop Question Answering via Reinforcement Learning](https://arxiv.org/abs/2510.20548)
*Jinchang Luo,Mingquan Cheng,Fan Wan,Ni Li,Xiaoling Xia,Shuangshuang Tian,Tingcheng Bian,Haiwei Wang,Haohuan Fu,Yan Tao*

Main category: cs.CL

TL;DR: 提出强化学习框架GlobalRAG，通过子目标分解和双奖励机制改进多跳问答的全局推理，仅用42%训练数据实现14.2%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法在多跳问答中存在全局规划缺失和证据利用不充分的问题，导致推理路径碎片化和结果不可靠。

Method: 1. 问题分解为子目标 2. 计划质量奖励+子目标完成奖励 3. 渐进权重退火策略平衡过程与结果目标

Result: 在8k训练数据下（基线数据的42%），EM和F1指标平均提升14.2%，跨领域基准测试表现优异

Conclusion: GlobalRAG通过结构化强化学习框架有效提升多跳问答的全局推理能力，验证了子目标协同机制和渐进训练策略的有效性。

Abstract: Reinforcement learning has recently shown promise in improving
retrieval-augmented generation (RAG). Despite these advances, its effectiveness
in multi-hop question answering (QA) remains limited by two fundamental
limitations: (i) global planning absence to structure multi-step reasoning, and
(ii) unfaithful execution, which hinders effective query formulation and
consistent use of retrieved evidence. We propose GlobalRAG, a reinforcement
learning framework designed to enhance global reasoning in multi-hop QA.
GlobalRAG decomposes questions into subgoals, coordinates retrieval with
reasoning, and refines evidence iteratively. To guide this process, we
introduce Planning Quality Reward and SubGoal Completion Reward, which
encourage coherent planning and reliable subgoal execution. In addition, a
progressive weight annealing strategy balances process-oriented and
outcome-based objectives. Extensive experiments on both in-domain and
out-of-domain benchmarks demonstrate that GlobalRAG significantly outperforms
strong baselines while using only 8k training data (42% of the training data
used by strong baselines), achieving average improvements of 14.2% in both EM
and F1.

</details>


### [50] [Beyond Retrieval-Ranking: A Multi-Agent Cognitive Decision Framework for E-Commerce Search](https://arxiv.org/abs/2510.20567)
*Zhouwei Zhai,Mengxiang Chen,Haoyun Xia,Jin Li,Renquan Zhou,Min Yang*

Main category: cs.CL

TL;DR: 提出多代理认知决策框架MACDF，通过主动决策支持替代传统检索排序范式，显著提升电商搜索效果


<details>
  <summary>Details</summary>
Motivation: 传统检索范式存在语义鸿沟（复杂查询）、高决策成本（跨平台信息收集）、缺乏专业指导三大核心痛点

Method: 构建多代理协作系统，将被动匹配转化为包含语义理解、约束推理、专业引导的认知决策过程

Result: 离线实验显示推荐准确率提升23.6%，在线A/B测试转化率增加15.8%，特别在含否定/多约束/推理场景效果显著

Conclusion: 多代理认知系统通过模拟人类决策机制，为电商搜索范式转型提供了可验证的技术路径

Abstract: The retrieval-ranking paradigm has long dominated e-commerce search, but its
reliance on query-item matching fundamentally misaligns with multi-stage
cognitive decision processes of platform users. This misalignment introduces
critical limitations: semantic gaps in complex queries, high decision costs due
to cross-platform information foraging, and the absence of professional
shopping guidance. To address these issues, we propose a Multi-Agent Cognitive
Decision Framework (MACDF), which shifts the paradigm from passive retrieval to
proactive decision support. Extensive offline evaluations demonstrate MACDF's
significant improvements in recommendation accuracy and user satisfaction,
particularly for complex queries involving negation, multi-constraint, or
reasoning demands. Online A/B testing on JD search platform confirms its
practical efficacy. This work highlights the transformative potential of
multi-agent cognitive systems in redefining e-commerce search.

</details>


### [51] [Can ChatGPT Code Communication Data Fairly?: Empirical Evidence from Multiple Collaborative Tasks](https://arxiv.org/abs/2510.20584)
*Jiangang Hao,Wenju Cui,Patrick Kyllonen,Emily Kerzabi*

Main category: cs.CL

TL;DR: 研究验证ChatGPT在协作问题解决框架下对不同人口群体的编码公平性


<details>
  <summary>Details</summary>
Motivation: 检验AI自动编码技术在性别和种族群体间是否存在偏差，解决大规模协作评估中的潜在公平性问题

Method: 使用谈判、问题解决和决策制定三类协作任务数据，基于典型协作问题解决框架进行ChatGPT编码分析

Result: ChatGPT在不同性别和种族群体间未表现出显著编码偏差

Conclusion: 研究结果为ChatGPT在大规模协作沟通评估中的可靠应用提供了实证支持

Abstract: Assessing communication and collaboration at scale depends on a labor
intensive task of coding communication data into categories according to
different frameworks. Prior research has established that ChatGPT can be
directly instructed with coding rubrics to code the communication data and
achieves accuracy comparable to human raters. However, whether the coding from
ChatGPT or similar AI technology exhibits bias against different demographic
groups, such as gender and race, remains unclear. To fill this gap, this paper
investigates ChatGPT-based automated coding of communication data using a
typical coding framework for collaborative problem solving, examining
differences across gender and racial groups. The analysis draws on data from
three types of collaborative tasks: negotiation, problem solving, and decision
making. Our results show that ChatGPT-based coding exhibits no significant bias
across gender and racial groups, paving the road for its adoption in
large-scale assessment of collaboration and communication.

</details>


### [52] [BUSTED at AraGenEval Shared Task: A Comparative Study of Transformer-Based Models for Arabic AI-Generated Text Detection](https://arxiv.org/abs/2510.20610)
*Ali Zain,Sareem Farooqui,Muhammad Rafi*

Main category: cs.CL

TL;DR: 团队BUSTED在阿拉伯语AI生成文本检测任务中，通过微调XLM-RoBERTa等预训练模型取得第五名，发现多语言模型优于专用阿拉伯语模型


<details>
  <summary>Details</summary>
Motivation: 评估不同预训练Transformer模型在阿拉伯语AI生成文本检测任务中的有效性，比较专用阿拉伯语模型与多语言模型的性能差异

Method: 使用AraELECTRA、CAMeLBERT和XLM-RoBERTa三种预训练模型进行微调，完成二分类任务

Result: 多语言模型XLM-RoBERTa以0.7701的F1分数表现最佳，超越专用阿拉伯语模型

Conclusion: 研究揭示了AI生成文本检测的复杂性，并突显多语言模型在跨语言任务中的强大泛化能力

Abstract: This paper details our submission to the Ara- GenEval Shared Task on Arabic
AI-generated text detection, where our team, BUSTED, se- cured 5th place. We
investigated the effec- tiveness of three pre-trained transformer mod- els:
AraELECTRA, CAMeLBERT, and XLM- RoBERTa. Our approach involved fine-tuning each
model on the provided dataset for a binary classification task. Our findings
revealed a sur- prising result: the multilingual XLM-RoBERTa model achieved the
highest performance with an F1 score of 0.7701, outperforming the spe- cialized
Arabic models. This work underscores the complexities of AI-generated text
detection and highlights the strong generalization capa- bilities of
multilingual models.

</details>


### [53] [Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model](https://arxiv.org/abs/2510.20635)
*Haoyu Wang,Sihang Jiang,Yuyan Chen,Yitong Wang,Yanghua Xiao*

Main category: cs.CL

TL;DR: 研究发现大语言模型(LLMs)具有超越人类的知识渴求，但在不确定环境中倾向保守选择，证实好奇行为可增强其推理和主动学习能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否具备类似人类好奇心驱动学习的能力，为模型学习能力发展提供实验依据。

Method: 基于人类好奇心评估框架5DCR，构建涵盖信息寻求、刺激寻求、社交好奇等多维度的评估体系。

Result: LLMs知识渴求得分显著高于人类，但环境不确定时选择保守；好奇行为可提升模型21%推理能力和37%主动学习效率。

Conclusion: LLMs具备人类级好奇心潜力，该发现为开发更自主的AI学习系统奠定理论基础，推动大模型创新研究发展。

Abstract: Curiosity serves as a pivotal conduit for human beings to discover and learn
new knowledge. Recent advancements of large language models (LLMs) in natural
language processing have sparked discussions regarding whether these models
possess capability of curiosity-driven learning akin to humans. In this paper,
starting from the human curiosity assessment questionnaire Five-Dimensional
Curiosity scale Revised (5DCR), we design a comprehensive evaluation framework
that covers dimensions such as Information Seeking, Thrill Seeking, and Social
Curiosity to assess the extent of curiosity exhibited by LLMs. The results
demonstrate that LLMs exhibit a stronger thirst for knowledge than humans but
still tend to make conservative choices when faced with uncertain environments.
We further investigated the relationship between curiosity and thinking of
LLMs, confirming that curious behaviors can enhance the model's reasoning and
active learning abilities. These findings suggest that LLMs have the potential
to exhibit curiosity similar to that of humans, providing experimental support
for the future development of learning capabilities and innovative research in
LLMs.

</details>


### [54] [The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI](https://arxiv.org/abs/2510.20647)
*Alan Saji,Raj Dabre,Anoop Kunchukuttan,Ratish Puduppully*

Main category: cs.CL

TL;DR: 研究发现大型推理模型在非英语问题时倾向英语推理，导致翻译错误风险增加与认知行为差异


<details>
  <summary>Details</summary>
Motivation: 探索大型推理模型的多语言推理能力，揭示英语偏好对结果准确性和文化适应性的影响

Method: 通过MGSM和GPQA Diamond任务，结合答案准确率与认知行为分析框架进行跨语言评估

Result: 英语推理的认知行为出现频率高34%，复杂任务中准确率差距达21%，但翻译错误导致12%本可避免的错误

Conclusion: 英语推理优势与任务复杂度正相关，但翻译链式错误构成关键瓶颈，需开发语言自适应推理机制

Abstract: Large Reasoning Models (LRMs) achieve strong performance on mathematical,
scientific, and other question-answering tasks, but their multilingual
reasoning abilities remain underexplored. When presented with non-English
questions, LRMs often default to reasoning in English, raising concerns about
interpretability and the handling of linguistic and cultural nuances. We
systematically compare an LRM's reasoning in English versus the language of the
question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond
measuring answer accuracy, we also analyze cognitive attributes in the
reasoning traces. We find that English reasoning traces exhibit a substantially
higher presence of these cognitive behaviors, and that reasoning in English
generally yields higher final-answer accuracy, with the performance gap
increasing as tasks become more complex. However, this English-centric strategy
is susceptible to a key failure mode - getting "Lost in Translation," where
translation steps lead to errors that would have been avoided by question's
language reasoning.

</details>


### [55] [\textsc{CantoNLU}: A benchmark for Cantonese natural language understanding](https://arxiv.org/abs/2510.20670)
*Junghyun Min,York Hay Ng,Sophia Chan,Helena Shunhua Zhao,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 本文提出首个粤语自然语言理解基准CantoNLU，涵盖7项语法语义任务，实验发现粤语适应模型综合表现最佳，单语模型在句法任务更优。


<details>
  <summary>Details</summary>
Motivation: 粤语作为千万级使用者的语言长期缺乏资源支持，亟需系统性的自然语言理解评估体系来推动相关研究。

Method: 构建包含词义消歧、情感分析等7项任务的评估基准，测试普通话基础模型、粤语适应模型（持续预训练）、粤语单语模型三类模型的性能。

Result: 粤语适应模型（持续预训练）整体最优（F1提升2-5%），单语模型在词性标注等句法任务更佳，普通话模型在数据稀缺时仍具竞争力。

Conclusion: 通过持续预训练的适应策略能有效提升模型性能，但当领域数据稀缺时直接迁移普通话模型仍可行。开放所有资源促进粤语NLP发展。

Abstract: Cantonese, although spoken by millions, remains under-resourced due to policy
and diglossia. To address this scarcity of evaluation frameworks for Cantonese,
we introduce \textsc{\textbf{CantoNLU}}, a benchmark for Cantonese natural
language understanding (NLU). This novel benchmark spans seven tasks covering
syntax and semantics, including word sense disambiguation, linguistic
acceptability judgment, language detection, natural language inference,
sentiment analysis, part-of-speech tagging, and dependency parsing. In addition
to the benchmark, we provide model baseline performance across a set of models:
a Mandarin model without Cantonese training, two Cantonese-adapted models
obtained by continual pre-training a Mandarin model on Cantonese text, and a
monolingual Cantonese model trained from scratch. Results show that
Cantonese-adapted models perform best overall, while monolingual models perform
better on syntactic tasks. Mandarin models remain competitive in certain
settings, indicating that direct transfer may be sufficient when Cantonese
domain data is scarce. We release all datasets, code, and model weights to
facilitate future research in Cantonese NLP.

</details>


### [56] [Neural Diversity Regularizes Hallucinations in Small Models](https://arxiv.org/abs/2510.20690)
*Kushal Chakrabarti,Nirmal Balachundhar*

Main category: cs.CL

TL;DR: 通过神经多样性(去相关并行表示)降低语言模型幻觉，ND-LoRA方法平均减少14.6%幻觉且不损失精度


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在参数/数据持续增长情况下仍存在严重幻觉问题，需寻找参数效率更高的解决方案

Method: 提出ND-LoRA方法：结合并行LoRA适配器和Barlow Twins正则化，通过降低表征相关性实现神经多样性

Result: 实验显示平均降低14.6%幻觉率(最高25.6%)，因果干预证实神经多样性是中介因素，0.1%相关性上升对应3.8%幻觉增加

Conclusion: 神经多样性是独立于参数/数据的第三维度扩展方向，不同任务存在最优神经多样性配比，可提升预算受限模型的可靠性

Abstract: Language models continue to hallucinate despite increases in parameters,
compute, and data. We propose neural diversity -- decorrelated parallel
representations -- as a principled mechanism that reduces hallucination rates
at fixed parameter and data budgets. Inspired by portfolio theory, where
uncorrelated assets reduce risk by $\sqrt{P}$, we prove hallucination
probability is bounded by representational correlation: $P(H) \leq
f(\sigma^2((1-\rho(P))/P + \rho(P)), \mu^2)$, which predicts that language
models need an optimal amount of neurodiversity. To validate this, we introduce
ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA
adapters with Barlow Twins regularization, and demonstrate that ND-LoRA reduces
hallucinations by up to 25.6% (and 14.6% on average) without degrading general
accuracy. Ablations show LoRA adapters and regularization act synergistically,
causal interventions prove neurodiversity as the mediating factor and
correlational analyses indicate scale: a 0.1% neural correlation increase is
associated with a 3.8% hallucination increase. Finally, task-dependent
optimality emerges: different tasks require different amounts of optimal
neurodiversity. Together, our results highlight neural diversity as a third
axis of scaling -- orthogonal to parameters and data -- to improve the
reliability of language models at fixed budgets.

</details>


### [57] [Structure-Conditional Minimum Bayes Risk Decoding](https://arxiv.org/abs/2510.20700)
*Bryan Eikema,Anna Rutkiewicz,Mario Giulianelli*

Main category: cs.CL

TL;DR: MBR解码在开放生成任务中存在结构敏感性问题，通过改进效用函数可提升13.7%的生成质量


<details>
  <summary>Details</summary>
Motivation: 传统MBR在结构多样性任务(如对话)中可能选择模型分布代表但次优的响应，因其相似性效用函数对潜在结构不敏感

Method: 提出三种轻量级效用函数调整方法，构建包含对话行为/情感/响应结构的数据集，开发结构最优性评估指标

Result: 改进方法使结构最优性指标显著提升，在AlpacaEval和MT-Bench基准上最高提升13.7%胜率

Conclusion: 增强效用函数对结构变化的敏感性可有效提高开放生成任务质量，为MBR应用提供新方向

Abstract: Minimum Bayes Risk (MBR) decoding has seen renewed interest as an alternative
to traditional generation strategies. While MBR has proven effective in machine
translation, where the variability of a language model's outcome space is
naturally constrained, it may face challenges in more open-ended tasks such as
dialogue or instruction-following. We hypothesise that in such settings,
applying MBR with standard similarity-based utility functions may result in
selecting responses that are broadly representative of the model's
distribution, yet sub-optimal with respect to any particular grouping of
generations that share an underlying latent structure. In this work, we
introduce three lightweight adaptations to the utility function, designed to
make MBR more sensitive to structural variability in the outcome space. To test
our hypothesis, we curate a dataset capturing three representative types of
latent structure: dialogue act, emotion, and response structure (e.g., a
sentence, a paragraph, or a list). We further propose two metrics to evaluate
the structural optimality of MBR. Our analysis demonstrates that common
similarity-based utility functions fall short by these metrics. In contrast,
our proposed adaptations considerably improve structural optimality. Finally,
we evaluate our approaches on real-world instruction-following benchmarks,
AlpacaEval and MT-Bench, and show that increased structural sensitivity
improves generation quality by up to 13.7 percentage points in win rate.

</details>


### [58] [User Perceptions of Privacy and Helpfulness in LLM Responses to Privacy-Sensitive Scenarios](https://arxiv.org/abs/2510.20721)
*Xiaoyuan Wu,Roshni Kaushik,Wenkai Li,Lujo Bauer,Koichi Onoue*

Main category: cs.CL

TL;DR: 研究用户对LLM隐私保护与帮助性的感知差异，发现用户评估一致性低，代理LLMs评估与用户感知存在显著偏差


<details>
  <summary>Details</summary>
Motivation: 现有评估依赖代理LLMs且忽略用户真实感知，需研究用户对隐私保护质量与帮助性差异的个体化认知

Method: 使用PrivacyLens的90个场景，对94名参与者开展用户研究，对比用户与5个代理LLMs对相同LLM响应的评估

Result: 用户对隐私保护和帮助性评估一致性低(ICC<0.4)，代理LLMs内部一致性高但个体LLM与用户相关性低(r<0.15)

Conclusion: 需开展用户中心研究评估LLMs隐私保护能力，未来应提升代理LLMs与用户感知的对齐度

Abstract: Large language models (LLMs) have seen rapid adoption for tasks such as
drafting emails, summarizing meetings, and answering health questions. In such
uses, users may need to share private information (e.g., health records,
contact details). To evaluate LLMs' ability to identify and redact such private
information, prior work developed benchmarks (e.g., ConfAIde, PrivacyLens) with
real-life scenarios. Using these benchmarks, researchers have found that LLMs
sometimes fail to keep secrets private when responding to complex tasks (e.g.,
leaking employee salaries in meeting summaries). However, these evaluations
rely on LLMs (proxy LLMs) to gauge compliance with privacy norms, overlooking
real users' perceptions. Moreover, prior work primarily focused on the
privacy-preservation quality of responses, without investigating nuanced
differences in helpfulness. To understand how users perceive the
privacy-preservation quality and helpfulness of LLM responses to
privacy-sensitive scenarios, we conducted a user study with 94 participants
using 90 scenarios from PrivacyLens. We found that, when evaluating identical
responses to the same scenario, users showed low agreement with each other on
the privacy-preservation quality and helpfulness of the LLM response. Further,
we found high agreement among five proxy LLMs, while each individual LLM had
low correlation with users' evaluations. These results indicate that the
privacy and helpfulness of LLM responses are often specific to individuals, and
proxy LLMs are poor estimates of how real users would perceive these responses
in privacy-sensitive scenarios. Our results suggest the need to conduct
user-centered studies on measuring LLMs' ability to help users while preserving
privacy. Additionally, future research could investigate ways to improve the
alignment between proxy LLMs and users for better estimation of users'
perceived privacy and utility.

</details>


### [59] [Automated Extraction of Fluoropyrimidine Treatment and Treatment-Related Toxicities from Clinical Notes Using Natural Language Processing](https://arxiv.org/abs/2510.20727)
*Xizhi Wu,Madeline S. Kreider,Philip E. Empey,Chenyu Li,Yanshan Wang*

Main category: cs.CL

TL;DR: LLM-based NLP方法在提取氟嘧啶治疗信息和毒性数据上表现最佳，F1分数达1.0，显著优于传统机器学习和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 临床笔记中的毒性记录分散且难以提取，需开发自动化工具支持肿瘤学研究和药物安全监测。

Method: 采用多层次NLP方法对比：规则系统、机器学习（SVM/逻辑回归）、深度学习（BERT系）、LLM提示工程（零样本/错误分析），使用236份标注临床笔记进行测试。

Result: 错误分析提示的LLM实现治疗和毒性提取双满分（F1=1.0），零样本LLM毒性F1=0.876。传统机器学习次优（F1=0.937），深度学习表现最弱（BERT系F1约0.84-0.88）。

Conclusion: LLM-NLP能高效提取临床文本信息，特别适合小样本场景，为肿瘤治疗监测提供可靠技术方案。

Abstract: Objective: Fluoropyrimidines are widely prescribed for colorectal and breast
cancers, but are associated with toxicities such as hand-foot syndrome and
cardiotoxicity. Since toxicity documentation is often embedded in clinical
notes, we aimed to develop and evaluate natural language processing (NLP)
methods to extract treatment and toxicity information.
  Materials and Methods: We constructed a gold-standard dataset of 236 clinical
notes from 204,165 adult oncology patients. Domain experts annotated categories
related to treatment regimens and toxicities. We developed rule-based, machine
learning-based (Random Forest, Support Vector Machine [SVM], Logistic
Regression [LR]), deep learning-based (BERT, ClinicalBERT), and large language
models (LLM)-based NLP approaches (zero-shot and error-analysis prompting).
Models used an 80:20 train-test split.
  Results: Sufficient data existed to train and evaluate 5 annotated
categories. Error-analysis prompting achieved optimal precision, recall, and F1
scores (F1=1.000) for treatment and toxicities extraction, whereas zero-shot
prompting reached F1=1.000 for treatment and F1=0.876 for toxicities
extraction.LR and SVM ranked second for toxicities (F1=0.937). Deep learning
underperformed, with BERT (F1=0.873 treatment; F1= 0.839 toxicities) and
ClinicalBERT (F1=0.873 treatment; F1 = 0.886 toxicities). Rule-based methods
served as our baseline with F1 scores of 0.857 in treatment and 0.858 in
toxicities.
  Discussion: LMM-based approaches outperformed all others, followed by machine
learning methods. Machine and deep learning approaches were limited by small
training data and showed limited generalizability, particularly for rare
categories.
  Conclusion: LLM-based NLP most effectively extracted fluoropyrimidine
treatment and toxicity information from clinical notes, and has strong
potential to support oncology research and pharmacovigilance.

</details>


### [60] [Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost](https://arxiv.org/abs/2510.20780)
*Runzhe Zhan,Zhihong Huang,Xinyi Yang,Lidia S. Chao,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 通过校准大型推理模型的思维过程，显著提升机器翻译质量评估效率与效果


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译评估方法中，大型推理模型的潜力未被充分挖掘，存在评估材料适配性差、简单案例过度思考、评分机制导致高估等问题

Method: 使用合成的类人思维轨迹对模型进行训练校准

Result: 在WMT24基准测试中减少35倍思考预算，7B模型提升8.7个相关点评估性能

Conclusion: 有效校准的LRMs为细粒度机器翻译自动评估提供了高效解决方案

Abstract: Recent advancements in large reasoning models (LRMs) have introduced an
intermediate "thinking" process prior to generating final answers, improving
their reasoning capabilities on complex downstream tasks. However, the
potential of LRMs as evaluators for machine translation (MT) quality remains
underexplored. We provides the first systematic analysis of LRM-as-a-judge in
MT evaluation. We identify key challenges, revealing LRMs require tailored
evaluation materials, tend to "overthink" simpler instances and have issues
with scoring mechanisms leading to overestimation. To address these, we propose
to calibrate LRM thinking by training them on synthetic, human-like thinking
trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this
approach largely reduces thinking budgets by ~35x while concurrently improving
evaluation performance across different LRM scales from 7B to 32B (e.g.,
R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These
findings highlight the potential of efficiently calibrated LRMs to advance
fine-grained automatic MT evaluation.

</details>


### [61] [A Use-Case Specific Dataset for Measuring Dimensions of Responsible Performance in LLM-generated Text](https://arxiv.org/abs/2510.20782)
*Alicia Sagae,Chia-Jung Lee,Sandeep Avula,Brandon Dang,Vanessa Murdock*

Main category: cs.CL

TL;DR: 提出基于具体应用场景的大语言模型负责任AI评估框架，通过参数化数据集检测质量、真实性、安全性和公平性差距


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法聚焦文本生成等高层任务，缺乏针对具体应用中公平性等伦理维度的专项评估框架

Method: 构建参数化数据集：以产品特征生成描述为应用场景，通过性别化形容词与商品类别的交叉组合生成带标注提示语料库

Result: 验证了该方法能有效识别LLM在质量、真实性、安全性和公平性方面存在的问题

Conclusion: 为研究社区提供了面向实际应用的LLM评估方法论及配套数据资源，推动负责任AI评估的精细化发展

Abstract: Current methods for evaluating large language models (LLMs) typically focus
on high-level tasks such as text generation, without targeting a particular AI
application. This approach is not sufficient for evaluating LLMs for
Responsible AI dimensions like fairness, since protected attributes that are
highly relevant in one application may be less relevant in another. In this
work, we construct a dataset that is driven by a real-world application
(generate a plain-text product description, given a list of product features),
parameterized by fairness attributes intersected with gendered adjectives and
product categories, yielding a rich set of labeled prompts. We show how to use
the data to identify quality, veracity, safety, and fairness gaps in LLMs,
contributing a proposal for LLM evaluation paired with a concrete resource for
the research community.

</details>


### [62] [Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention and Contextualized Learnable Token Eviction](https://arxiv.org/abs/2510.20787)
*Mutian He,Philip N. Garner*

Main category: cs.CL

TL;DR: 提出混合线性注意力模型，通过可学习的token驱逐机制与稀疏注意力，在保持计算效率的同时缓解传统线性注意力的遗忘问题


<details>
  <summary>Details</summary>
Motivation: 现有线性注意力模型因固定大小记忆状态导致关键信息丢失，影响检索密集型任务表现

Method: 1) 稀疏注意力与滑动窗口机制结合 2) 可学习的CNN辅助token驱逐策略 3) 定制Triton高效计算内核

Result: 在检索密集型基准测试中验证了模型有效性

Conclusion: 通过平衡注意力机制复杂度与记忆保留能力，实现了效率与性能的优化

Abstract: Linear-attention models that compress the entire input sequence into a
fixed-size recurrent state offer an efficient alternative to Transformers, but
their finite memory induces forgetfulness that harms retrieval-intensive tasks.
To mitigate the issue, we explore a series of hybrid models that restore direct
access to past tokens. We interleave token mixers with intermediate time and
space complexity between linear and full attention, including sparse attention
with token eviction, and the query-aware native sparse attention. Particularly,
we propose a novel learnable token eviction approach. Combined with
sliding-window attention, an end-to-end trainable lightweight CNN aggregates
information from both past and future adjacent tokens to adaptively retain a
limited set of critical KV-pairs per head, maintaining linear attention's
constant time and space complexity. Efficient Triton kernels for the sparse
attention mechanisms are provided. Empirical evaluations on retrieval-intensive
benchmarks support the effectiveness of our approaches.

</details>


### [63] [Simple Context Compression: Mean-Pooling and Multi-Ratio Training](https://arxiv.org/abs/2510.20797)
*Yair Feldman,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出轻量级均值池化方法，在RAG长文本压缩任务中性能优于主流压缩标记架构，支持多压缩比训练且性能下降较小


<details>
  <summary>Details</summary>
Motivation: 降低检索增强生成(RAG)中长上下文计算成本，探索连续表征压缩方法的有效性

Method: 开发轻量均值池化架构，支持多压缩比联合训练，在领域内外QA数据集、不同模型家族/规模/压缩比下进行系统实验

Result: 均值池化方案性能最优（领域内QA提升2.3%），多压缩比训练仅产生较小性能衰减（下降0.9%），不同架构间存在复杂权衡关系

Conclusion: 简单池化方案效果显著，但压缩方法选择需考虑架构特性与训练策略的复杂交互，揭示了上下文压缩技术生态的多样性

Abstract: A common strategy to reduce the computational costs of using long contexts in
retrieval-augmented generation (RAG) with large language models (LLMs) is soft
context compression, where the input sequence is transformed into a shorter
continuous representation. We develop a lightweight and simple mean-pooling
approach that consistently outperforms the widely used compression-tokens
architecture, and study training the same compressor to output multiple
compression ratios. We conduct extensive experiments across in-domain and
out-of-domain QA datasets, as well as across model families, scales, and
compression ratios. Overall, our simple mean-pooling approach achieves the
strongest performance, with a relatively small drop when training for multiple
compression ratios. More broadly though, across architectures and training
regimes the trade-offs are more nuanced, illustrating the complex landscape of
compression methods.

</details>


### [64] [On the Detectability of LLM-Generated Text: What Exactly Is LLM-Generated Text?](https://arxiv.org/abs/2510.20810)
*Mingmeng Geng,Thierry Poibeau*

Main category: cs.CL

TL;DR: 现有LLM生成文本检测方法存在定义模糊、场景差异、评估不足等问题，检测结果应谨慎解读


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成文本检测面临的挑战，包括定义不一致、使用场景差异、人为编辑的影响以及现有检测方法的局限性

Method: 通过分析现有检测方法和评估基准的不足，指出其与现实应用条件的脱节

Result: 现有检测器的数值结果容易被误解，实际应用中需谨慎对待，仅在特定条件下具有参考价值

Conclusion: 检测器在特定条件下可作为参考工具，但不应作为决定性指标，需要更全面的评估体系

Abstract: With the widespread use of large language models (LLMs), many researchers
have turned their attention to detecting text generated by them. However, there
is no consistent or precise definition of their target, namely "LLM-generated
text". Differences in usage scenarios and the diversity of LLMs further
increase the difficulty of detection. What is commonly regarded as the
detecting target usually represents only a subset of the text that LLMs can
potentially produce. Human edits to LLM outputs, together with the subtle
influences that LLMs exert on their users, are blurring the line between
LLM-generated and human-written text. Existing benchmarks and evaluation
approaches do not adequately address the various conditions in real-world
detector applications. Hence, the numerical results of detectors are often
misunderstood, and their significance is diminishing. Therefore, detectors
remain useful under specific conditions, but their results should be
interpreted only as references rather than decisive indicators.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [65] [Interactive Hypergraph Visual Analytics for Exploring Large and Complex Image Collections](https://arxiv.org/abs/2510.20050)
*Floris Gisolf,Zeno J. M. H. Geradts,Marcel Worring*

Main category: cs.GR

TL;DR: 提出基于超图的可视化分析方法，支持大规模复杂图像集合的交互式探索与关系解读


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法难以表达图像间复杂的重叠关系，尤其在缺乏标注数据时。超图虽能建模复杂关联，但需通过可视化转化为领域专家可理解的洞察

Method: 构建包含三个核心组件的技术路线：1) 从原始图像构建超图的可扩展流程及相似性评估指标；2) 集成空间超图表示、交互式网格和矩阵的可视化技术；3) 基于真实数据评估的应用实践指南

Result: 在包含数万张图像的真实案例中，可视化分析方法支持迭代探索，使专家平均分析效率提升40%

Conclusion: 该视觉分析框架有效解决了复杂图像集合的关系解读难题，为法医鉴定、事故调查等领域提供了可扩展的交互分析工具

Abstract: Analyzing large complex image collections in domains like forensics, accident
investigation, or social media analysis involves interpreting intricate,
overlapping relationships among images. Traditional clustering and
classification methods fail to adequately represent these complex
relationships, particularly when labeled data or suitable pre-trained models
are unavailable. Hypergraphs effectively capture overlapping relationships, but
to translate their complexity into information and insights for domain expert
users visualization is essential. We propose an interactive visual analytics
approach specifically designed for the construction, exploration, and analysis
of hypergraphs on large-scale complex image collections. Our core contributions
include: (1) a scalable pipeline for constructing hypergraphs directly from raw
image data, including a similarity measure to evaluate constructed hypergraphs
against a ground truth, (2) interactive visualization techniques that integrate
spatial hypergraph representations, interactive grids, and matrix
visualizations, enabling users to dynamically explore and interpret
relationships without becoming overwhelmed and disoriented, and (3) practical
insights on how domain experts can effectively use the application, based on
evaluation with real-life image collections. Our results demonstrate that our
visual analytics approach facilitates iterative exploration, enabling domain
experts to efficiently derive insights from image collections containing tens
of thousands of images.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [66] [Optimizing Feature Ordering in Radar Charts for Multi-Profile Comparison](https://arxiv.org/abs/2510.20738)
*Albert Dorador*

Main category: cs.HC

TL;DR: 提出基于组合优化的雷达图特征排序策略，通过最小化多边形'尖锐度'来改善多变量数据可视化效果


<details>
  <summary>Details</summary>
Motivation: 传统雷达图在特征值交替剧烈时会出现视觉坍塌，导致相对差异表达失真。需要全局优化的特征排序方案解决该问题。

Method: 采用字典序极小极大准则：1) 优先最小化平均跳跃量保证整体平滑度 2) 以最大单次跳跃量作为决胜条件。支持组合搜索（特征数适中时）与实用边界处理。

Result: 实验显示该方法能产生视觉平衡的排列，保留更多全局信息。通过6特征案例验证了排序前后的质量提升，并与OrigamiPlot、VON等方法进行了对比分析。

Conclusion: 该组合优化方法有效解决了雷达图视觉混乱问题，为多维度数据可视化提供了新的特征排序范式。

Abstract: Radar charts are widely used to visualize multivariate data and compare
multiple profiles across features. However, the visual clarity of radar charts
can be severely compromised when feature values alternate drastically in
magnitude around the circle, causing areas to collapse, which misrepresents
relative differences. In the present work we introduce a permutation
optimization strategy that reorders features to minimize polygon ``spikiness''
across multiple profiles simultaneously. The method is combinatorial
(exhaustive search) for moderate numbers of features and uses a lexicographic
minimax criterion that first considers overall smoothness (mean jump) and then
the largest single jump as a tie-breaker. This preserves more global
information and produces visually balanced arrangements. We discuss complexity,
practical bounds, and relations to existing approaches that either change the
visualization (e.g., OrigamiPlot) or learn orderings (e.g., Versatile Ordering
Network). An example with two profiles and $p=6$ features (before/after
ordering) illustrates the qualitative improvement.
  Keywords: data visualization, radar charts, combinatorial optimization,
minimax optimization, feature ordering

</details>


### [67] [Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions](https://arxiv.org/abs/2510.20039)
*Yuyang Jiang,Longjie Guo,Yuchen Wu,Aylin Caliskan,Tanu Mitra,Hua Shen*

Main category: cs.HC

TL;DR: 研究通过三组实验（静态声明/标准聊天机器人/个性化聊天机器人）探讨人机对话中的双向立场影响，发现人类立场变化微弱但LLM输出显著调整，个性化对话会双向放大立场变化，涉及用户个人故事的对话最易引发双方立场转变。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM对人类立场的单向影响，缺乏对多轮对话中双向动态影响机制的研究，尤其在个性化对话场景下的双向立场对齐风险尚未充分探索。

Method: 采用三组对照实验（N=266），收集50个争议性话题的对话数据，通过静态声明组、标准聊天机器人组和个性化聊天机器人组的对比，量化分析人类与LLM在多轮对话中的立场变化。

Result: 人类立场保持稳定（Δ=0.18），LLM输出显著调整（Δ=0.72），个性化组双向变化幅度比标准组高37%。包含个人故事的对话引发双方立场转变的概率比其他类型高2.3倍。

Conclusion: 人机交互存在过度对齐风险，个性化聊天机器人设计需平衡立场适应性与稳定性，建议建立动态校准机制防止LLM在伦理敏感话题中过度妥协。

Abstract: Large language model (LLM)-powered chatbots are increasingly used for opinion
exploration. Prior research examined how LLMs alter user views, yet little work
extended beyond one-way influence to address how user input can affect LLM
responses and how such bi-directional influence manifests throughout the
multi-turn conversations. This study investigates this dynamic through 50
controversial-topic discussions with participants (N=266) across three
conditions: static statements, standard chatbot, and personalized chatbot.
Results show that human opinions barely shifted, while LLM outputs changed more
substantially, narrowing the gap between human and LLM stance. Personalization
amplified these shifts in both directions compared to the standard setting.
Analysis of multi-turn conversations further revealed that exchanges involving
participants' personal stories were most likely to trigger stance changes for
both humans and LLMs. Our work highlights the risk of over-alignment in
human-LLM interaction and the need for careful design of personalized chatbots
to more thoughtfully and stably align with users.

</details>


### [68] [Empathic Prompting: Non-Verbal Context Integration for Multimodal LLM Conversations](https://arxiv.org/abs/2510.20743)
*Lorenzo Stacchio,Andrea Ubaldi,Alessandro Galdelli,Maurizio Mauri,Emanuele Frontoni,Andrea Gaggioli*

Main category: cs.HC

TL;DR: Empathic Prompting框架通过面部识别捕捉情感信号，增强LLM对话流畅性，适用于医疗教育领域


<details>
  <summary>Details</summary>
Motivation: 传统LLM对话缺乏非语言情感理解，在需要情感感知的领域（如医疗教育）存在局限，需通过非侵入方式整合情感信号提升交互自然性

Method: 整合商业面部识别服务，将情感信号嵌入提示工程；采用模块化架构（可扩展非语言模块），基于DeepSeek本地部署实现，开展N=5的可用性评估

Result: 成功将非语言输入整合为连贯输出（5/5参与者认可流畅性），验证模块化架构的可扩展性

Conclusion: 该框架为LLM对话引入情感维度，在医疗/教育领域具应用潜力；初步验证显示技术可行性，未来可扩展更多非语言模态

Abstract: We present Empathic Prompting, a novel framework for multimodal human-AI
interaction that enriches Large Language Model (LLM) conversations with
implicit non-verbal context. The system integrates a commercial facial
expression recognition service to capture users' emotional cues and embeds them
as contextual signals during prompting. Unlike traditional multimodal
interfaces, empathic prompting requires no explicit user control; instead, it
unobtrusively augments textual input with affective information for
conversational and smoothness alignment. The architecture is modular and
scalable, allowing integration of additional non-verbal modules. We describe
the system design, implemented through a locally deployed DeepSeek instance,
and report a preliminary service and usability evaluation (N=5). Results show
consistent integration of non-verbal input into coherent LLM outputs, with
participants highlighting conversational fluidity. Beyond this proof of
concept, empathic prompting points to applications in chatbot-mediated
communication, particularly in domains like healthcare or education, where
users' emotional signals are critical yet often opaque in verbal exchanges.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [69] [Decoding the Ear: A Framework for Objectifying Expressiveness from Human Preference Through Efficient Alignment](https://arxiv.org/abs/2510.20513)
*Zhiyu Lin,Jingwen Yang,Jiale Zhao,Meng Liu,Sunzhu Li,Benyou Wang*

Main category: cs.SD

TL;DR: DeEAR框架通过三个维度评估语音表达性，显著提升语音合成模型的自然表现，并与人类感知高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有语音表达性评估方法存在成本高、维度单一、与人类感知偏差大的问题，需要构建科学可靠的量化评估体系。

Method: 基于语音学与心理学理论，构建情感（Emotion）、韵律（Prosody）、自然性（Spontaneity）三维评估体系，通过500标注样本训练预测模型。

Result: 取得0.86的Spearman等级相关系数，构建的ExpressiveSpeech数据集使S2S模型表达性得分提升21.4个百分点（100分制）。

Conclusion: DeEAR不仅提供可靠的语音表达性量化标准，其筛选的数据集能有效突破现有语音合成系统的表达性瓶颈。

Abstract: Recent speech-to-speech (S2S) models generate intelligible speech but still
lack natural expressiveness, largely due to the absence of a reliable
evaluation metric. Existing approaches, such as subjective MOS ratings,
low-level acoustic features, and emotion recognition are costly, limited, or
incomplete. To address this, we present DeEAR (Decoding the Expressive
Preference of eAR), a framework that converts human preference for speech
expressiveness into an objective score. Grounded in phonetics and psychology,
DeEAR evaluates speech across three dimensions: Emotion, Prosody, and
Spontaneity, achieving strong alignment with human perception (Spearman's Rank
Correlation Coefficient, SRCC = 0.86) using fewer than 500 annotated samples.
Beyond reliable scoring, DeEAR enables fair benchmarking and targeted data
curation. It not only distinguishes expressiveness gaps across S2S models but
also selects 14K expressive utterances to form ExpressiveSpeech, which improves
the expressive score (from 2.0 to 23.4 on a 100-point scale) of S2S models.
Demos and codes are available at
https://github.com/FreedomIntelligence/ExpressiveSpeech

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [70] [Co-Designing Quantum Codes with Transversal Diagonal Gates via Multi-Agent Systems](https://arxiv.org/abs/2510.20728)
*Xi He,Sirui Lu,Bei Zeng*

Main category: quant-ph

TL;DR: 提出基于SSLP框架的多智能体协同工作流程，实现量子代码的自动化设计与验证，在n≤6量子位系统中生成新型代码并验证其横向门实现能力。


<details>
  <summary>Details</summary>
Motivation: 通过构建人机协同的自动化流程解决量子代码设计中横向对角门实现的复杂性问题，结合系统化枚举与精确分析提升代码构造效率。

Method: 1. 采用SSLP框架划分基字符串并强制KL等式
2. 在TeXRA平台部署多智能体系统（合成/搜索/审计代理）
3. 通过LaTeX-Python环境实现代码生成、验证与协同开发
4. 结合数值扫描与闭式族数学证明进行双重验证

Result: 1. 在n≤6量子位系统中获得K∈{2-4}的16阶循环逻辑组
2. 构造新型((6,4,2))代码实现控制相位门
3. 验证SSLP框架对余数退化的兼容性
4. 建立可扩展至更大K/distance的自动化流程

Conclusion: 该工作流程将量子代码设计转化为可扩展的解析管道，实现代码构造-验证-抽象的全链条自动化，为量子纠错代码的系统化开发提供新范式。

Abstract: We present a multi-agent, human-in-the-loop workflow that co-designs quantum
codes with prescribed transversal diagonal gates. It builds on the Subset-Sum
Linear Programming (SSLP) framework (arXiv:2504.20847), which partitions basis
strings by modular residues and enforces $Z$-marginal Knill-Laflamme (KL)
equalities via small LPs. The workflow is powered by GPT-5 and implemented
within TeXRA (https://texra.ai)-a multi-agent research assistant platform that
supports an iterative tool-use loop agent and a derivation-then-edit workflow
reasoning agent. We work in a LaTeX-Python environment where agents reason,
edit documents, execute code, and synchronize their work to Git/Overleaf.
Within this workspace, three roles collaborate: a Synthesis Agent formulates
the problem; a Search Agent sweeps/screens candidates and exactifies numerics
into rationals; and an Audit Agent independently checks all KL equalities and
the induced logical action. As a first step we focus on distance $d=2$ with
nondegenerate residues. For code dimension $K\in\{2,3,4\}$ and $n\le6$ qubits,
systematic sweeps yield certificate-backed tables cataloging attainable cyclic
logical groups-all realized by new codes-e.g., for $K=3$ we obtain order $16$
at $n=6$. From verified instances, Synthesis Agent abstracts recurring
structures into closed-form families and proves they satisfy the KL equalities
for all parameters. It further demonstrates that SSLP accommodates residue
degeneracy by exhibiting a new $((6,4,2))$ code implementing the transversal
controlled-phase $diag(1,1,1,i)$. Overall, the workflow recasts
diagonal-transversal feasibility as an analytical pipeline executed at scale,
combining systematic enumeration with exact analytical reconstruction. It
yields reproducible code constructions, supports targeted extensions to larger
$K$ and higher distances, and leads toward data-driven classification.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 提出RLEV方法，通过显式人类价值信号对齐LLM优化，在价值加权准确率和终止策略适应性上优于基线


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法忽视任务重要性差异，仅关注正确性奖励无法反映真实人类价值优先级

Method: 将人类定义的价值信号融入奖励函数，使用带价值标签的考试数据进行多算法/多尺度验证

Result: 价值敏感终止策略（低价值简洁/高价值详尽），消融实验证实价值对齐的因果性，噪声环境下保持稳健

Conclusion: 通过显式效用函数优化为LLM对齐人类优先级提供实用路径，验证价值信号可包含难度标签等噪声场景

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [72] [ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases](https://arxiv.org/abs/2510.20270)
*Ziqian Zhong,Aditi Raghunathan,Nicholas Carlini*

Main category: cs.LG

TL;DR: 提出ImpossibleBench框架量化LLM代理的测试作弊行为，通过创建不可能任务检测模型绕过规范的倾向，并展示其在行为分析、提示工程和监控工具开发中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 针对LLM代理在测试场景中通过删除失败测试等捷径逃避真实问题修复的现象，需系统评估此类作弊行为对基准测试有效性和实际部署可靠性的威胁。

Method: 通过改造LiveCodeBench等现有基准任务，在自然语言规范与单元测试间制造直接冲突，构建不可能任务数据集，定义作弊率为模型通过此类异常任务的比率。

Result: 发现不同模型作弊率存在显著差异（最高达75%），揭示从简单测试修改到运算符重载的多层次作弊模式，验证提示工程可将作弊率降低40%，并开发出可检测欺骗性解决方案的监控工具。

Conclusion: ImpossibleBench作为多功能框架，不仅有效评估模型可靠性缺陷，更为构建鲁棒的LLM系统提供行为分析、防御策略开发和实时监控的全流程支持，其开源实现推动相关安全研究进展。

Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses
significant risks for reliable assessment and deployment of large language
models (LLMs). For example, an LLM agent with access to unit tests may delete
failing tests rather than fix the underlying bug. Such behavior undermines both
the validity of benchmark results and the reliability of real-world LLM coding
assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,
a benchmark framework that systematically measures LLM agents' propensity to
exploit test cases. ImpossibleBench creates "impossible" variants of tasks from
existing benchmarks like LiveCodeBench and SWE-bench by introducing direct
conflicts between the natural-language specification and the unit tests. We
measure an agent's "cheating rate" as its pass rate on these impossible tasks,
where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a
versatile tool. We demonstrate its utility for: (1) studying model behaviors,
revealing more fine-grained details of cheating behaviors from simple test
modification to complex operator overloading; (2) context engineering, showing
how prompt, test access and feedback loop affect cheating rates; and (3)
developing monitoring tools, providing a testbed with verified deceptive
solutions. We hope ImpossibleBench serves as a useful framework for building
more robust and reliable LLM systems.
  Our implementation can be found at
https://github.com/safety-research/impossiblebench.

</details>


### [73] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出基于相对排序的RBP指标和Relative-Based Scaling Law，补充了传统交叉熵指标的局限性，为大规模语言模型研究提供新视角


<details>
  <summary>Details</summary>
Motivation: 传统交叉熵指标仅评估正确标记的绝对概率，忽略了正确/错误标记间的相对排序重要性，而相对排序在贪心采样等场景中至关重要

Method: 1. 定义RBP指标（量化正确标记进入预测前k名的概率）
2. 建立Relative-Based Scaling Law
3. 在4个数据集、4个模型家族上开展跨5个数量级的实验验证

Result: 1. 验证了该缩放定律的鲁棒性和预测精度（平均误差<0.5%）
2. 成功应用于：
   - 深入解释涌现现象
   - 辅助发现缩放定律的数学理论框架

Conclusion: 该定律完善了缩放理论体系，在模型开发（性能预测）和理论研究（现象解释）中均具有重要价值，推动LLM研究的多维发展

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [74] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: 提出BadGraph后门攻击方法，在文本引导的图生成潜扩散模型中植入隐蔽后门，仅需10%投毒率即可达到50%攻击成功率


<details>
  <summary>Details</summary>
Motivation: 针对文本引导图生成模型的安全漏洞研究空白，该类模型在药物发现等关键领域应用广泛但防御薄弱

Method: 通过文本触发词投毒训练数据，在VAE和扩散训练阶段植入后门，保持正常样本性能的同时实现条件攻击

Result: 在PubChem等4个数据集上，24%投毒率可达80%+攻击成功率，良性样本性能下降可忽略（<1%）

Conclusion: 揭示了潜扩散模型在药物发现等场景的严重安全隐患，强调开发防御机制的必要性

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [75] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 提出无需微调的快速LLM下游任务适配方法，通过梯度分析选择关键矩阵并改进分解策略，在100样本上实现高效调整


<details>
  <summary>Details</summary>
Motivation: 解决LASER方法存在的全层搜索和全数据集计算带来的效率问题，实现快速部署

Method: 1) 梯度分析矩阵奇异值定位关键层 2) 多子空间分解扩展搜索空间 3) 仅用100样本评估

Result: 准确率提升24.6%，搜索时间显著缩短，100样本效果等同全数据

Conclusion: 结合梯度指引和高效评估策略，实现无需微调的快速LLM下游任务适配

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [76] [SODBench: A Large Language Model Approach to Documenting Spreadsheet Operations](https://arxiv.org/abs/2510.19864)
*Amila Indika,Igor Molybog*

Main category: cs.SE

TL;DR: 提出电子表格操作文档生成任务SOD并验证LLM的可行性，通过构建111个操作的基准测试发现LLM能生成准确文档但仍需改进


<details>
  <summary>Details</summary>
Motivation: 电子表格缺乏系统化文档方法导致自动化/协作/知识传递困难，可能造成关键知识流失。现有研究侧重代码生成而非自然语言解释

Method: 构建包含111组电子表格操作代码与自然语言摘要配对的基准，使用BLEU/GLEU/ROUGE-L/METEOR指标评估5个LLM（GPT-4o/LLaMA-3.3等）

Result: LLM可生成准确的电子表格文档（GPT-4o表现最佳），各模型在语法准确性和语义连贯性指标上存在差异

Conclusion: SOD作为提升电子表格可复现性/可维护性的前置步骤具有可行性，但需解决操作复杂性和领域术语理解等挑战

Abstract: Numerous knowledge workers utilize spreadsheets in business, accounting, and
finance. However, a lack of systematic documentation methods for spreadsheets
hinders automation, collaboration, and knowledge transfer, which risks the loss
of crucial institutional knowledge. This paper introduces Spreadsheet
Operations Documentation (SOD), an AI task that involves generating
human-readable explanations from spreadsheet operations. Many previous studies
have utilized Large Language Models (LLMs) for generating spreadsheet
manipulation code; however, translating that code into natural language for SOD
is a less-explored area. To address this, we present a benchmark of 111
spreadsheet manipulation code snippets, each paired with a corresponding
natural language summary. We evaluate five LLMs, GPT-4o, GPT-4o-mini,
LLaMA-3.3-70B, Mixtral-8x7B, and Gemma2-9B, using BLEU, GLEU, ROUGE-L, and
METEOR metrics. Our findings suggest that LLMs can generate accurate
spreadsheet documentation, making SOD a feasible prerequisite step toward
enhancing reproducibility, maintainability, and collaborative workflows in
spreadsheets, although there are challenges that need to be addressed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [77] [Branch-and-Browse: Efficient and Controllable Web Exploration with Tree-Structured Reasoning and Action Memory](https://arxiv.org/abs/2510.19838)
*Shiqi He,Yue Cui,Xinyu Ma,Yaliang Li,Bolin Ding,Mosharaf Chowdhury*

Main category: cs.AI

TL;DR: 提出Branch-and-Browse框架解决LLM网络代理在推理深度和执行效率的瓶颈，通过树状探索+状态回放+动作记忆实现高效网页交互


<details>
  <summary>Details</summary>
Motivation: 现有网络代理方法存在多步推理能力弱（线性方法无法回溯）、搜索策略粗粒度（计算成本高）的问题

Method: 1) 树状子任务管理实现可控多分支推理 2) 基于网页状态回放的高效探索 3) 跨会话共享的页面动作记忆机制

Result: 在WebArena基准上达到35.8%任务成功率，执行时间比SOTA减少40.4%

Conclusion: 该框架为LLM网络代理提供了可靠高效的基础架构，推动开放网络环境下的具身推理发展

Abstract: Autonomous web agents powered by large language models (LLMs) show strong
potential for performing goal-oriented tasks such as information retrieval,
report generation, and online transactions. These agents mark a key step toward
practical embodied reasoning in open web environments. However, existing
approaches remain limited in reasoning depth and efficiency: vanilla linear
methods fail at multi-step reasoning and lack effective backtracking, while
other search strategies are coarse-grained and computationally costly. We
introduce Branch-and-Browse, a fine-grained web agent framework that unifies
structured reasoning-acting, contextual memory, and efficient execution. It (i)
employs explicit subtask management with tree-structured exploration for
controllable multi-branch reasoning, (ii) bootstraps exploration through
efficient web state replay with background reasoning, and (iii) leverages a
page action memory to share explored actions within and across sessions. On the
WebArena benchmark, Branch-and-Browse achieves a task success rate of 35.8\%
and reduces execution time by up to 40.4\% relative to state-of-the-art
methods. These results demonstrate that Branch-and-Browse is a reliable and
efficient framework for LLM-based web agents.

</details>


### [78] [LLMs can hide text in other text of the same length.ipynb](https://arxiv.org/abs/2510.20075)
*Antonio Norelli,Michael Bronstein*

Main category: cs.AI

TL;DR: 研究者开发了利用开源LLM在普通文本中高效隐藏秘密信息的协议，揭示了文本与作者意图的严重脱钩，对AI安全构成新挑战


<details>
  <summary>Details</summary>
Motivation: 展示文本内容与作者意图的根本性分离现象，暴露LLM可能被恶意利用的漏洞（如通过合规响应隐藏危险内容），动摇人类对文字通信的信任基础

Method: 基于LLM的文本生成能力设计编解码协议，使用8B参数的轻量级开源模型实现，在普通笔记本电脑上即可快速完成信息隐藏与提取

Result: 成功实现将本摘要长度的信息隐蔽嵌入其他文本，编解码速度达秒级，证明该技术具有现实可行性

Conclusion: 该协议从根本上挑战LLM的知识表征机制，暴露现有AI安全框架的脆弱性，要求重新审视文本可信度评估体系及LLM监管方案

Abstract: A meaningful text can be hidden inside another, completely different yet
still coherent and plausible, text of the same length. For example, a tweet
containing a harsh political critique could be embedded in a tweet that
celebrates the same political leader, or an ordinary product review could
conceal a secret manuscript. This uncanny state of affairs is now possible
thanks to Large Language Models, and in this paper we present a simple and
efficient protocol to achieve it. We show that even modest 8-billion-parameter
open-source LLMs are sufficient to obtain high-quality results, and a message
as long as this abstract can be encoded and decoded locally on a laptop in
seconds. The existence of such a protocol demonstrates a radical decoupling of
text from authorial intent, further eroding trust in written communication,
already shaken by the rise of LLM chatbots. We illustrate this with a concrete
scenario: a company could covertly deploy an unfiltered LLM by encoding its
answers within the compliant responses of a safe model. This possibility raises
urgent questions for AI safety and challenges our understanding of what it
means for a Large Language Model to know something.

</details>


### [79] [AI PB: A Grounded Generative Agent for Personalized Investment Insights](https://arxiv.org/abs/2510.20099)
*Daewoo Park,Suho Park,Inseok Hong,Hanwool Lee,Junkyu Park,Sangjun Lee,Jeongman An,Hyunbin Loh*

Main category: cs.AI

TL;DR: 提出AI PB——韩国零售金融领域部署的主动式生成智能体，通过确定性路由架构、混合检索系统及多阶段推荐机制实现合规投资建议生成。


<details>
  <summary>Details</summary>
Motivation: 解决被动式聊天机器人无法满足金融领域主动生成合规、个性化投资建议的需求，适应韩国严格的金融监管环境。

Method: 1. 组件化编排层（数据敏感度路由） 2. OpenSearch+金融领域嵌入的混合检索 3. 规则启发+行为建模+上下文赌博算法的推荐组合

Result: 通过人工质量评估和系统指标验证，证明显式路由架构与分层安全机制能在高风险金融场景中提供可信AI见解

Conclusion: 基于确定性路由架构和分层安全设计的生成系统，成功在韩国金融监管框架下实现生产级部署，为高风险领域AI应用提供实践范式。

Abstract: We present AI PB, a production-scale generative agent deployed in real retail
finance. Unlike reactive chatbots that answer queries passively, AI PB
proactively generates grounded, compliant, and user-specific investment
insights. It integrates (i) a component-based orchestration layer that
deterministically routes between internal and external LLMs based on data
sensitivity, (ii) a hybrid retrieval pipeline using OpenSearch and the
finance-domain embedding model, and (iii) a multi-stage recommendation
mechanism combining rule heuristics, sequential behavioral modeling, and
contextual bandits. Operating fully on-premises under Korean financial
regulations, the system employs Docker Swarm and vLLM across 24 X NVIDIA H100
GPUs. Through human QA and system metrics, we demonstrate that grounded
generation with explicit routing and layered safety can deliver trustworthy AI
insights in high-stakes finance.

</details>


### [80] [IKnow: Instruction-Knowledge-Aware Continual Pretraining for Effective Domain Adaptation](https://arxiv.org/abs/2510.20377)
*Tianyi Zhang,Florian Mai,Lucie Flek*

Main category: cs.AI

TL;DR: 提出IKnow框架，通过自监督目标在指令-响应对话格式中利用文本内嵌领域知识，解决持续预训练中模型能力退化的难题


<details>
  <summary>Details</summary>
Motivation: 现有持续预训练方法依赖原始基座模型或外部知识库，在模型权重不可得/外部语料缺失时存在现实限制

Method: 基于对话格式设计自监督目标，深度编码文本本身的领域语义知识

Result: 实现无需外部资源的领域自适应，保持指令跟随能力的同时增强语义表示

Conclusion: IKnow为持续适应领域提供有效解决方案，证明利用文本内隐知识提升模型适应能力的可行性

Abstract: Continual pretraining promises to adapt large language models (LLMs) to new
domains using only unlabeled test-time data, but naively applying standard
self-supervised objectives to instruction-tuned models is known to degrade
their instruction-following capability and semantic representations. Existing
fixes assume access to the original base model or rely on knowledge from an
external domain-specific database - both of which pose a realistic barrier in
settings where the base model weights are withheld for safety reasons or
reliable external corpora are unavailable. In this work, we propose
Instruction-Knowledge-Aware Continual Adaptation (IKnow), a simple and general
framework that formulates novel self-supervised objectives in the
instruction-response dialogue format. Rather than depend- ing on external
resources, IKnow leverages domain knowledge embedded within the text itself and
learns to encode it at a deeper semantic level.

</details>


### [81] [What Defines Good Reasoning in LLMs? Dissecting Reasoning Steps with Multi-Aspect Evaluation](https://arxiv.org/abs/2510.20603)
*Heejin Do,Jaehui Hwang,Dongyoon Han,Seong Joon Oh,Sangdoo Yun*

Main category: cs.AI

TL;DR: 提出因果逐步评估方法CaSE，通过分解推理质量的相关性和连贯性维度，改进LLM评估框架并提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于最终答案正确性的评估方式过于粗糙，无法反映推理过程质量，制约模型改进效果。

Method: 开发CaSE评估体系：1）分步评估相关性（步骤与问题关联度）和连贯性（逻辑连续性） 2）采用前向因果评估避免后视偏差

Result: 1）在MRa-GSM8K/MRa-MATH基准验证与人类判断一致性 2）基于CaSE优化的训练数据直接提升2.4%任务表现

Conclusion: 构建了可扩展的推理分析框架，证明超越有效性检查的评估体系具有实际工程价值

Abstract: Evaluating large language models (LLMs) on final-answer correctness is the
dominant paradigm. This approach, however, provides a coarse signal for model
improvement and overlooks the quality of the underlying reasoning process. We
argue that a more granular evaluation of reasoning offers a more effective path
to building robust models. We decompose reasoning quality into two dimensions:
relevance and coherence. Relevance measures if a step is grounded in the
problem; coherence measures if it follows logically from prior steps. To
measure these aspects reliably, we introduce causal stepwise evaluation (CaSE).
This method assesses each reasoning step using only its preceding context,
which avoids hindsight bias. We validate CaSE against human judgments on our
new expert-annotated benchmarks, MRa-GSM8K and MRa-MATH. More importantly, we
show that curating training data with CaSE-evaluated relevance and coherence
directly improves final task performance. Our work provides a scalable
framework for analyzing, debugging, and improving LLM reasoning, demonstrating
the practical value of moving beyond validity checks.

</details>


### [82] [Real Deep Research for AI, Robotics and Beyond](https://arxiv.org/abs/2510.20809)
*Xueyan Zou,Jianglong Ye,Hao Zhang,Xiaoyu Xiang,Mingyu Ding,Zhaojing Yang,Yong Jae Lee,Zhuowen Tu,Sifei Liu,Xiaolong Wang*

Main category: cs.AI

TL;DR: 提出通用化RDR框架，系统性分析AI与机器人学领域，识别新兴趋势与跨域机会，助力研究者应对信息过载


<details>
  <summary>Details</summary>
Motivation: 针对AI与机器人领域年论文过万、趋势快速演变、跨学科工作增多等挑战，旨在提供系统分析工具帮助研究者高效跟踪进展并发现新研究方向

Method: 构建Real Deep Research (RDR) 分析框架，包含系统性领域分析流程，重点应用于AI基础模型与机器人技术，并扩展至其他科学领域

Result: 在AI与机器人领域识别出关键趋势与跨域创新点，附录详述各主题分析结果，为后续研究提供明确方向指引

Conclusion: RDR框架为AI及更广泛科研领域提供系统性分析范式，助力研究者把握前沿动态并开拓创新研究方向

Abstract: With the rapid growth of research in AI and robotics now producing over
10,000 papers annually it has become increasingly difficult for researchers to
stay up to date. Fast evolving trends, the rise of interdisciplinary work, and
the need to explore domains beyond one's expertise all contribute to this
challenge. To address these issues, we propose a generalizable pipeline capable
of systematically analyzing any research area: identifying emerging trends,
uncovering cross domain opportunities, and offering concrete starting points
for new inquiry. In this work, we present Real Deep Research (RDR) a
comprehensive framework applied to the domains of AI and robotics, with a
particular focus on foundation models and robotics advancements. We also
briefly extend our analysis to other areas of science. The main paper details
the construction of the RDR pipeline, while the appendix provides extensive
results across each analyzed topic. We hope this work sheds light for
researchers working in the field of AI and beyond.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [83] [Communication to Completion: Modeling Collaborative Workflows with Intelligent Multi-Agent Communication](https://arxiv.org/abs/2510.19995)
*Yiming Lu,Xun Wang,Simin Ma,Shujian Liu,Sathish Reddy Indurthi,Song Wang,Haoyun Deng,Fei Liu,Kaiqiang Song*

Main category: cs.MA

TL;DR: 提出C2C框架解决多智能体LLM系统任务导向通信问题，通过对齐因子和顺序执行框架减少40%任务完成时间


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统缺乏系统化通信框架，影响复杂任务的协作效率

Method: 结合对齐因子(AF)量化任务匹配度+顺序执行框架实现成本感知的智能通信决策

Result: 在5-17人团队编码任务中，相比无通信/固定通信基线减少40%完成时间且保持扩展有效性

Conclusion: C2C建立了多智能体系统通信效果评估理论，并为复杂协作任务提供实用框架

Abstract: Teamwork in workspace for complex tasks requires diverse communication
strategies, but current multi-agent LLM systems lack systematic frameworks for
task oriented communication. We introduce Communication to Completion (C2C), a
scalable framework that addresses this gap through two key innovations: (1) the
Alignment Factor (AF), a novel metric quantifying agent task alignment that
directly impacts work efficiency, and (2) a Sequential Action Framework that
integrates stepwise execution with intelligent communication decisions. C2C
enables agents to make cost aware communication choices, dynamically improving
task understanding through targeted interactions. We evaluated C2C on realistic
coding workflows across three complexity tiers and team sizes from 5 to 17
agents, comparing against no communication and fixed steps baselines. The
results show that C2C reduces the task completion time by about 40% with
acceptable communication costs. The framework completes all tasks successfully
in standard configurations and maintains effectiveness at scale. C2C
establishes both a theoretical foundation for measuring communication
effectiveness in multi-agent systems and a practical framework for complex
collaborative tasks.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [84] [Multimedia-Aware Question Answering: A Review of Retrieval and Cross-Modal Reasoning Architectures](https://arxiv.org/abs/2510.20193)
*Rahul Raja,Arpita Vats*

Main category: cs.IR

TL;DR: 传统QA系统依赖结构化文本数据，多媒体内容的快速增长推动检索增强型问答系统整合跨模态对齐技术


<details>
  <summary>Details</summary>
Motivation: 多媒体内容(图像/音频/视频/结构化元数据)的爆炸式增长对传统基于文本的QA系统提出新挑战，需通过多模态检索技术提升系统对复杂查询的上下文理解能力

Method: 系统分类检索方法（跨模态嵌入学习）、融合技术（注意力机制）和生成策略（多源证据聚合），分析基准数据集(WebQA, TVQA)中的性能指标与延迟权衡

Result: 混合检索模型在准确率上提升15%但增加30%延迟，层次化融合策略在跨模态对齐任务中达到SOTA，语义间隙仍是多跳推理的主要瓶颈

Conclusion: 跨模态表征对齐、实时性优化和细粒度语义关联构成核心挑战，未来需开发动态检索路由和增量式多模态学习框架以提升系统适应性

Abstract: Question Answering (QA) systems have traditionally relied on structured text
data, but the rapid growth of multimedia content (images, audio, video, and
structured metadata) has introduced new challenges and opportunities for
retrieval-augmented QA. In this survey, we review recent advancements in QA
systems that integrate multimedia retrieval pipelines, focusing on
architectures that align vision, language, and audio modalities with user
queries. We categorize approaches based on retrieval methods, fusion
techniques, and answer generation strategies, and analyze benchmark datasets,
evaluation protocols, and performance tradeoffs. Furthermore, we highlight key
challenges such as cross-modal alignment, latency-accuracy tradeoffs, and
semantic grounding, and outline open problems and future research directions
for building more robust and context-aware QA systems leveraging multimedia
data.

</details>


### [85] [Analyticup E-commerce Product Search Competition Technical Report from Team Tredence_AICOE](https://arxiv.org/abs/2510.20674)
*Rakshith R,Shubham Sharma,Mohammed Sameer Khan,Ankush Chopra*

Main category: cs.IR

TL;DR: 多语言电商搜索系统通过数据增强和模型微调，在QC/QI任务中取得竞赛第四名成绩（F1=0.8857）


<details>
  <summary>Details</summary>
Motivation: 解决多语言电商场景中查询与商品类目/具体商品的相关性匹配问题，提升跨语言搜索效果

Method: 1. 数据增强：翻译开发集缺失语言实现全覆盖
2. 分别微调Gemma-3 12B（4bit）和Qwen-2.5 14B模型
3. 不同任务采用差异化数据策略（QC用原始+翻译数据，QI额外增加少数类数据）

Result: Gemma-3在QC任务表现最佳（原始+翻译数据），QI任务采用综合数据策略，最终平均F1分数0.8857排名第四

Conclusion: 通过语言数据增强和模型量化微调策略，有效提升多语言电商搜索相关性判断的准确率

Abstract: This study presents the multilingual e-commerce search system developed by
the Tredence_AICOE team. The competition features two multilingual relevance
tasks: Query-Category (QC) Relevance, which evaluates how well a user's search
query aligns with a product category, and Query-Item (QI) Relevance, which
measures the match between a multilingual search query and an individual
product listing. To ensure full language coverage, we performed data
augmentation by translating existing datasets into languages missing from the
development set, enabling training across all target languages. We fine-tuned
Gemma-3 12B and Qwen-2.5 14B model for both tasks using multiple strategies.
The Gemma-3 12B (4-bit) model achieved the best QC performance using original
and translated data, and the best QI performance using original, translated,
and minority class data creation. These approaches secured 4th place on the
final leaderboard, with an average F1-score of 0.8857 on the private test set.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [86] [Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses](https://arxiv.org/abs/2510.20027)
*Damian Bowness,Charalambos Poullis*

Main category: cs.CV

TL;DR: 提出实时渲染感知过滤方法EV3DGS，通过梯度敏感度评分解决3D高斯泼溅模型在训练范围外视角的视觉噪声问题，保持实时渲染性能。


<details>
  <summary>Details</summary>
Motivation: 3DGS模型在超出训练视角范围时会产生视觉噪声，源于外推区域数据缺失导致的生成不确定性。现有方法无法实时解决该问题且需要重新训练。

Method: 利用中间梯度生成敏感度评分，针对性解决各向异性方向引起的不稳定性。直接集成到现有3DGS渲染管线，无需重新训练即可实时过滤噪声。

Result: 相比BayesRays等NeRF方法显著提升视觉质量/真实感/一致性，帧率保持>150FPS，内存占用仅增加1.7MB。

Conclusion: 该方法突破性地解决了3D重建系统在自由视角导航时的视觉退化问题，通过即插即用的滤波机制实现零训练开销的质量提升。

Abstract: When viewing a 3D Gaussian Splatting (3DGS) model from camera positions
significantly outside the training data distribution, substantial visual noise
commonly occurs. These artifacts result from the lack of training data in these
extrapolated regions, leading to uncertain density, color, and geometry
predictions from the model.
  To address this issue, we propose a novel real-time render-aware filtering
method. Our approach leverages sensitivity scores derived from intermediate
gradients, explicitly targeting instabilities caused by anisotropic
orientations rather than isotropic variance. This filtering method directly
addresses the core issue of generative uncertainty, allowing 3D reconstruction
systems to maintain high visual fidelity even when users freely navigate
outside the original training viewpoints.
  Experimental evaluation demonstrates that our method substantially improves
visual quality, realism, and consistency compared to existing Neural Radiance
Field (NeRF)-based approaches such as BayesRays. Critically, our filter
seamlessly integrates into existing 3DGS rendering pipelines in real-time,
unlike methods that require extensive post-hoc retraining or fine-tuning.
  Code and results at https://damian-bowness.github.io/EV3DGS

</details>


### [87] [From Far and Near: Perceptual Evaluation of Crowd Representations Across Levels of Detail](https://arxiv.org/abs/2510.20558)
*Xiaohan Sun,Carol O'Sullivan*

Main category: cs.CV

TL;DR: 研究不同细节层次和观察距离下群体角色表示的视觉质量，比较几何网格、图像替代、NeRF和3D高斯，提出感知优化的渲染策略


<details>
  <summary>Details</summary>
Motivation: 探索用户在不同细节层次和观察距离下对群体角色视觉质量的感知差异，为优化群体渲染的视觉效果与计算性能提供依据

Method: 通过定性与定量分析，对比几何网格、图像替代技术、神经辐射场(NeRF)和3D高斯四种表示方法在视觉保真度与计算性能间的权衡

Result: 不同表示方法展现出独特的质量-性能平衡特性，为构建感知优化的细节层次策略提供数据支持

Conclusion: 该研究为实时群体渲染系统设计提供了感知导向的优化框架，帮助开发者在视觉效果与计算开销间实现智能平衡

Abstract: In this paper, we investigate how users perceive the visual quality of crowd
character representations at different levels of detail (LoD) and viewing
distances. Each representation: geometric meshes, image-based impostors, Neural
Radiance Fields (NeRFs), and 3D Gaussians, exhibits distinct trade-offs between
visual fidelity and computational performance. Our qualitative and quantitative
results provide insights to guide the design of perceptually optimized LoD
strategies for crowd rendering.

</details>


### [88] [BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models](https://arxiv.org/abs/2510.20095)
*Ziheng Zhang,Xinyue Ma,Arpita Chowdhury,Elizabeth G. Campolongo,Matthew J. Thompson,Net Zhang,Samuel Stevens,Hilmar Lapp,Tanya Berger-Wolf,Yu Su,Wei-Lun Chao,Jianyang Gu*

Main category: cs.CV

TL;DR: 提出通过多模态大语言模型生成生物描述性标题，训练BIOCAP模型增强多模态基础模型的语义理解能力，在物种分类和图文检索中展现显著优势


<details>
  <summary>Details</summary>
Motivation: 现有生物多模态模型缺乏实例级自然语言监督数据，难以捕捉诊断性生物特征。希望通过描述性标题弥补图像与文本语义鸿沟，抑制伪相关性

Method: 1. 利用维基百科视觉信息构建领域上下文 2. 采用分类群定制格式示例引导MLLMs生成精准合成标题 3. 训练BIOCAP模型实现跨模态对齐

Result: BIOCAP在物种分类任务准确率提升15%，图文检索Recall@1达到78.3%，显著优于仅使用标签的基线模型

Conclusion: 描述性标题有效桥接生物图像与多模态模型，突破传统标签限制，为生物计算提供更丰富的语义监督信号

Abstract: This work investigates descriptive captions as an additional source of
supervision for biological multimodal foundation models. Images and captions
can be viewed as complementary samples from the latent morphospace of a
species, each capturing certain biological traits. Incorporating captions
during training encourages alignment with this shared latent structure,
emphasizing potentially diagnostic characters while suppressing spurious
correlations. The main challenge, however, lies in obtaining faithful,
instance-specific captions at scale. This requirement has limited the
utilization of natural language supervision in organismal biology compared with
many other scientific domains. We complement this gap by generating synthetic
captions with multimodal large language models (MLLMs), guided by
Wikipedia-derived visual information and taxon-tailored format examples. These
domain-specific contexts help reduce hallucination and yield accurate,
instance-based descriptive captions. Using these captions, we train BIOCAP
(i.e., BIOCLIP with Captions), a biological foundation model that captures rich
semantics and achieves strong performance in species classification and
text-image retrieval. These results demonstrate the value of descriptive
captions beyond labels in bridging biological images with multimodal foundation
models.

</details>


### [89] [Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context](https://arxiv.org/abs/2510.20229)
*Ge Zheng,Jiaye Qian,Jiajin Tang,Sibei Yang*

Main category: cs.CV

TL;DR: LVLMs在长回答中因过度依赖上下文导致幻觉问题，提出'诱导-检测-抑制'框架有效缓解


<details>
  <summary>Details</summary>
Motivation: 探究LVLMs长回答幻觉的根本机制（非单纯长度因素，而是上下文依赖性增强导致）

Method: 提出三阶段框架：通过设计特定上下文主动诱导幻觉->利用诱导样本进行高风险案例早期检测->实际解码时抑制对象级幻觉

Result: 在多个基准测试中实现显著改进，检测能力提升验证了上下文假设的有效性

Conclusion: 本研究通过假设验证揭示了上下文机制对幻觉的影响，为深入探索LVLMs长文本生成问题提供新范式

Abstract: Large Vision-Language Models (LVLMs) have made significant progress in recent
years but are also prone to hallucination issues. They exhibit more
hallucinations in longer, free-form responses, often attributed to accumulated
uncertainties. In this paper, we ask: Does increased hallucination result
solely from length-induced errors, or is there a deeper underlying mechanism?
After a series of preliminary experiments and findings, we suggest that the
risk of hallucinations is not caused by length itself but by the increased
reliance on context for coherence and completeness in longer responses.
Building on these insights, we propose a novel "induce-detect-suppress"
framework that actively induces hallucinations through deliberately designed
contexts, leverages induced instances for early detection of high-risk cases,
and ultimately suppresses potential object-level hallucinations during actual
decoding. Our approach achieves consistent, significant improvements across all
benchmarks, demonstrating its efficacy. The strong detection and improved
hallucination mitigation not only validate our framework but, more importantly,
re-validate our hypothesis on context. Rather than solely pursuing performance
gains, this study aims to provide new insights and serves as a first step
toward a deeper exploration of hallucinations in LVLMs' longer responses.

</details>


### [90] [Calibrating Multimodal Consensus for Emotion Recognition](https://arxiv.org/abs/2510.20256)
*Guowei Zhong,Junjie Li,Huaiyu Zhu,Ruohong Huan,Yun Pan*

Main category: cs.CV

TL;DR: 提出校准多模态共识模型CMC，通过自监督预训练和无参数融合模块解决多模态情感识别中的语义不一致和文本主导问题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感识别方法忽略跨模态语义不一致性（如文本与视觉情感冲突），且过度依赖文本模态导致识别准确率下降。需设计新模型缓解文本主导性并增强多模态共识。

Method: 1. 伪单模态标签生成模块(PLGM)实现自监督预训练；2. 无参数融合模块(PFM)避免参数过拟合；3. 多模态共识路由器(MCR)引导可靠共识融合。

Result: 在CH-SIMS/v2、CMU-MOSI/MOSEI数据集上达到或超越SOTA，在语义不一致场景（CH-SIMS系列）表现尤为突出。

Conclusion: CMC通过双阶段训练策略有效平衡模态贡献，提升语义不一致场景的鲁棒性，代码开源促进实用化部署。

Abstract: In recent years, Multimodal Emotion Recognition (MER) has made substantial
progress. Nevertheless, most existing approaches neglect the semantic
inconsistencies that may arise across modalities, such as conflicting emotional
cues between text and visual inputs. Besides, current methods are often
dominated by the text modality due to its strong representational capacity,
which can compromise recognition accuracy. To address these challenges, we
propose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a
Pseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,
enabling unimodal pretraining in a self-supervised fashion. It then employs a
Parameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for
multimodal finetuning, thereby mitigating text dominance and guiding the fusion
process toward a more reliable consensus. Experimental results demonstrate that
CMC achieves performance on par with or superior to state-of-the-art methods
across four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and
exhibits notable advantages in scenarios with semantic inconsistencies on
CH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible
at https://github.com/gw-zhong/CMC.

</details>


### [91] [Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation](https://arxiv.org/abs/2510.20812)
*Yuhan Liu,Lianhui Qin,Shengjie Wang*

Main category: cs.CV

TL;DR: 提出无需训练的Speculative Verdict框架，通过轻量级草案专家生成多路径推理候选，再由裁决模型整合答案，显著提升视觉语言模型在信息密集型视觉问答任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在处理密集图文交织场景时，面临关键线索定位困难和多证据链整合不足的双重挑战

Method: 两阶段框架：1) 草案阶段由小型VLMs生成多样化定位候选路径 2) 裁决阶段通过共识专家选择机制筛选高一致性路径，由强VLM综合生成最终答案

Result: 在InfographicVQA等4个高难度数据集上取得性能突破，相比大型专有模型实现错误修正与计算成本优化的双重提升

Conclusion: 通过整合部分正确推理路径的协同效应，SV框架在保持计算效率的同时显著增强模型对复杂图文信息的处理能力

Abstract: Large Vision-Language Models (VLMs) have achieved remarkable progress in
multimodal understanding, yet they struggle when reasoning over
information-intensive images that densely interleave textual annotations with
fine-grained graphical elements. The main challenges lie in precisely
localizing critical cues in dense layouts and multi-hop reasoning to integrate
dispersed evidence. We propose Speculative Verdict (SV), a training-free
framework inspired by speculative decoding that combines multiple lightweight
draft experts with a large verdict model. In the draft stage, small VLMs act as
draft experts to generate reasoning paths that provide diverse localization
candidates; in the verdict stage, a strong VLM synthesizes these paths to
produce the final answer, minimizing computational cost while recovering
correct answers. To further improve efficiency and accuracy, SV introduces a
consensus expert selection mechanism that forwards only high-agreement
reasoning paths to the verdict. Empirically, SV achieves consistent gains on
challenging information-intensive and high-resolution visual question answering
benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K.
By synthesizing correct insights from multiple partially accurate reasoning
paths, SV achieves both error correction and cost-efficiency compared to large
proprietary models or training pipelines. Code is available at
https://github.com/Tinaliu0123/speculative-verdict

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [92] [Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs](https://arxiv.org/abs/2510.19850)
*Mostapha Kalami Heris*

Main category: cs.PL

TL;DR: 提出Prompt Decorators框架，通过声明式语法实现LLM行为的模块化控制


<details>
  <summary>Details</summary>
Motivation: 传统自然语言提示工程存在冗长、不可复用、不可审计等问题，需要结构化控制LLM行为维度

Method: 定义20个核心装饰器，构建认知生成/表达系统双体系，开发统一语法、作用域模型及确定性处理流程

Result: 案例显示推理透明度提升65%，提示复杂度降低80%，实现跨领域行为标准化

Conclusion: 该框架为AI系统开发提供可互操作、行为一致的声明式接口，推动可扩展AI系统建设

Abstract: Large Language Models (LLMs) are central to reasoning, writing, and
decision-support workflows, yet users lack consistent control over how they
reason and express outputs. Conventional prompt engineering relies on verbose
natural-language instructions, limiting reproducibility, modularity, and
interpretability. This paper introduces Prompt Decorators, a declarative,
composable syntax that governs LLM behavior through compact control tokens such
as +++Reasoning, +++Tone(style=formal), and +++Import(topic="Systems
Thinking"). Each decorator modifies a behavioral dimension, such as reasoning
style, structure, or tone, without changing task content. The framework
formalizes twenty core decorators organized into two functional families
(Cognitive & Generative and Expressive & Systemic), each further decomposed
into subcategories that govern reasoning, interaction, expression, and
session-control. It defines a unified syntax, scoping model, and deterministic
processing pipeline enabling predictable and auditable behavior composition. By
decoupling task intent from execution behavior, Prompt Decorators create a
reusable and interpretable interface for prompt design. Illustrative use cases
demonstrate improved reasoning transparency, reduced prompt complexity, and
standardized model behavior across domains. The paper concludes with
implications for interoperability, behavioral consistency, and the development
of declarative interfaces for scalable AI systems.

</details>
