{"id": "2508.00079", "pdf": "https://arxiv.org/pdf/2508.00079", "abs": "https://arxiv.org/abs/2508.00079", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "categories": ["cs.CL", "cs.AI"], "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u7269\u7406\u95ee\u9898\u80fd\u529b\u7684\u7814\u7a76\uff0c\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u6846\u67b6\u5e76\u5efa\u7acb\u65b0\u8bc4\u6d4b\u57fa\u51c6PhysicsEval", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u7269\u7406\u95ee\u9898\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u9700\u5f00\u53d1\u6709\u6548\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u5728\u6570\u5b66\u8ba1\u7b97\u548c\u63cf\u8ff0\u6027\u95ee\u9898\u4e2d\u7684\u8868\u73b0", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff08\u5c0f\u578bLLM\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\uff09\uff0c\u7ed3\u5408\u591a\u79cd\u63a8\u7406\u65f6\u4f18\u5316\u6280\u672f\u8fdb\u884c\u6027\u80fd\u63d0\u5347", "result": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u663e\u8457\u6539\u5584\u6a21\u578b\u521d\u59cb\u8868\u73b0\u8f83\u5dee\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u542b19,609\u9053\u7269\u7406\u9898\u7684\u8bc4\u6d4b\u57fa\u51c6PhysicsEval", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u63d0\u5347LLM\u7269\u7406\u63a8\u7406\u80fd\u529b\uff0c\u65b0\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0"}}
{"id": "2508.00086", "pdf": "https://arxiv.org/pdf/2508.00086", "abs": "https://arxiv.org/abs/2508.00086", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "categories": ["cs.CL"], "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLM\u751f\u6210\u7684\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u65b0\u6a21\u578b\uff08\u5982ChatGPT-4.5\uff09\u751f\u6210\u7684\u6587\u672c\u53cd\u800c\u66f4\u4e0d\u63a5\u8fd1\u4eba\u7c7b\u7279\u5f81\u3002", "motivation": "\u901a\u8fc7\u6d4b\u91cf\u516d\u4e2a\u8bcd\u6c47\u591a\u6837\u6027\u7ef4\u5ea6\uff0c\u9a8c\u8bc1\u4e0d\u540cChatGPT\u6a21\u578b\u751f\u6210\u6587\u672c\u4e0e\u4eba\u7c7b\u5199\u4f5c\u7684\u76f8\u4f3c\u6027\u5dee\u5f02\uff0c\u5e76\u6bd4\u8f83\u65b0/\u65e7\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e00\u5143\u591a\u53d8\u91cf\u65b9\u5dee\u5206\u6790\uff08MANOVA\uff09\u3001\u4e00\u5143\u65b9\u5dee\u5206\u6790\uff08ANOVA\uff09\u548c\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\uff0c\u5bf9\u6bd44\u4e2aChatGPT\u6a21\u578b\u4e0e240\u540d\u4eba\u7c7b\u5199\u4f5c\u8005\u76846\u4e2a\u8bcd\u6c47\u7ef4\u5ea6\u6570\u636e\u3002", "result": "\u6240\u6709ChatGPT\u6a21\u578b\u751f\u6210\u7684\u6587\u672c\u5728\u8bcd\u6c47\u591a\u6837\u6027\u4e0a\u5747\u663e\u8457\u4e0d\u540c\u4e8e\u4eba\u7c7b\uff0c\u5176\u4e2do4 mini\u548c4.5\u5dee\u5f02\u6700\u5927\u3002\u4eba\u7c7b\u5199\u4f5c\u5728\u4e0d\u540c\u6559\u80b2\u6c34\u5e73/\u8bed\u8a00\u7fa4\u4f53\u4e2d\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "LLM\u751f\u6210\u7684\u6587\u672c\u4e0d\u5177\u5907\u4eba\u7c7b\u8bcd\u6c47\u591a\u6837\u6027\u7279\u5f81\uff0c\u4e14\u65b0\u6a21\u578b\u504f\u79bb\u5ea6\u66f4\u9ad8\u3002\u8fd9\u5bf9\u8bed\u8a00\u6559\u5b66\u548cAI\u6587\u672c\u68c0\u6d4b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.00095", "pdf": "https://arxiv.org/pdf/2508.00095", "abs": "https://arxiv.org/abs/2508.00095", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "categories": ["cs.CL", "cs.CY"], "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8ba1\u7b97\u4eba\u6587\u9886\u57df\u9700\u52a0\u5f3a\u65b9\u6cd5\u8bba\u7406\u8bba\u5316\u4ee5\u907f\u514d\u7ffb\u8bd1\u9519\u8bef\uff0c\u5f15\u5165\u7b26\u53f7\u590d\u6742\u6027\u6982\u5ff5\u63ed\u793a\u5f53\u524d\u5efa\u6a21\u5b9e\u8df5\u4e2d\u7684\u8ba4\u77e5\u7f3a\u9677\u3002", "motivation": "\u8ba1\u7b97\u4eba\u6587\u9886\u57df\u7f3a\u4e4f\u65b9\u6cd5\u8bba\u7406\u8bba\u5316\u4f1a\u5bfc\u81f4\u7ffb\u8bd1\u9519\u8bef\uff08\u5982\u5c06\u7b26\u53f7\u590d\u6742\u6570\u636e\u7b80\u5316\u4e3a\u7b26\u53f7\u7b80\u5355\u6570\u636e\uff09\uff0c\u963b\u788d\u9886\u57df\u6210\u719f\u548c\u89e3\u91ca\u900f\u660e\u6027\u3002", "method": "1. \u5efa\u7acb\u6587\u5316-\u8ba1\u7b97\u9886\u57df\u7684\u53cc\u5411\u7ffb\u8bd1\u6846\u67b6\n2. \u63d0\u51fa\u7b26\u53f7\u590d\u6742\u6027\u6982\u5ff5\uff08\u6587\u672c\u610f\u4e49\u968f\u89e3\u91ca\u89c6\u89d2\u53d8\u5316\u7684\u7a0b\u5ea6\uff09\n3. \u6279\u5224\u6027\u5206\u6790\u5f53\u524d\u4e3b\u6d41\u5efa\u6a21\u5b9e\u8df5\uff08\u7279\u522b\u662f\u8bc4\u4f30\u65b9\u6cd5\uff09", "result": "\u63ed\u793a\u5efa\u6a21\u5b9e\u8df5\u4e2d\u56e0\u5ffd\u89c6\u7b26\u53f7\u590d\u6742\u6027\u5bfc\u81f4\u7684\u7cfb\u7edf\u6027\u7ffb\u8bd1\u9519\u8bef\uff0c\u63d0\u51fa\u7814\u7a76\u8005\u6539\u8fdb\u8ba4\u77e5\u6846\u67b6\u7684\u5177\u4f53\u5efa\u8bae\uff08\u5982\u627f\u8ba4\u7b26\u53f7\u590d\u6742\u6027\u3001\u5efa\u7acb\u89e3\u91ca\u900f\u660e\u5ea6\u673a\u5236\uff09\u3002", "conclusion": "\u8ba1\u7b97\u4eba\u6587\u7814\u7a76\u9700\u7406\u8bba\u5316\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u6b63\u89c6\u7b26\u53f7\u590d\u6742\u6027\u4ee5\u907f\u514d\u8ba4\u77e5\u6377\u5f84\u5e26\u6765\u7684\u865a\u5047\u6e05\u6670\uff0c\u63a8\u52a8\u9886\u57df\u5411\u66f4\u4e25\u8c28\u7684\u9610\u91ca\u5b9e\u8df5\u53d1\u5c55\u3002"}}
{"id": "2508.00109", "pdf": "https://arxiv.org/pdf/2508.00109", "abs": "https://arxiv.org/abs/2508.00109", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "\u63d0\u51faFACTORY\u57fa\u51c6\u96c6\u89e3\u51b3\u73b0\u6709\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u7f3a\u4e4f\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u8bc4\u4f30\u6548\u679c\u66f4\u53ef\u9760\u4e14\u5177\u6311\u6218\u6027", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u4e8b\u5b9e\u6027\u8bc4\u4f30\u57fa\u51c6\u666e\u904d\u7f3a\u4e4f\u4eba\u5de5\u9a8c\u8bc1\uff0c\u5bfc\u81f4\u6570\u636e\u96c6\u8d28\u91cf\u95ee\u9898\u548c\u4e0d\u53ef\u9760\u7684\u6a21\u578b\u8bc4\u4f30\u7ed3\u679c", "method": "\u91c7\u7528\u6a21\u578b\u5728\u73af\uff08model-in-the-loop\uff09\u65b9\u6cd5\u7ed3\u5408\u4eba\u5de5\u7ec6\u5316\uff0c\u6784\u5efa\u5305\u542b\u4e8b\u5b9e\u660e\u786e\u3001\u53ef\u56de\u7b54\u4e14\u5177\u6311\u6218\u6027\u63d0\u793a\u7684FACTORY\u6570\u636e\u96c6", "result": "SOTA\u6a21\u578b\u5728FACTORY\u4e0a\u7ea640%\u7684\u58f0\u660e\u4e0d\u51c6\u786e\uff08\u5176\u4ed6\u6570\u636e\u96c6\u4ec510%\uff09\uff0c\u663e\u793a\u8be5\u57fa\u51c6\u66f4\u5177\u6311\u6218\u6027", "conclusion": "FACTORY\u57fa\u51c6\u5728\u53ef\u9760\u6027\u3001\u957f\u5c3e\u4e8b\u5b9e\u63a8\u7406\u9700\u6c42\u7b49\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u80fd\u66f4\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u4e8b\u5b9e\u6027\u80fd\u529b"}}
{"id": "2508.00398", "pdf": "https://arxiv.org/pdf/2508.00398", "abs": "https://arxiv.org/abs/2508.00398", "authors": ["Sunjae Yoon", "Gwanhyeong Koo", "Younghwan Lee", "Ji Woo Hong", "Chang D. Yoo"], "title": "Occlusion-robust Stylization for Drawing-based 3D Animation", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 13 figures, ICCV 2025", "summary": "3D animation aims to generate a 3D animated video from an input image and a\ntarget 3D motion sequence. Recent advances in image-to-3D models enable the\ncreation of animations directly from user-hand drawings. Distinguished from\nconventional 3D animation, drawing-based 3D animation is crucial to preserve\nartist's unique style properties, such as rough contours and distinct stroke\npatterns. However, recent methods still exhibit quality deterioration in style\nproperties, especially under occlusions caused by overlapping body parts,\nleading to contour flickering and stroke blurring. This occurs due to a\n`stylization pose gap' between training and inference in stylization networks\ndesigned to preserve drawing styles in drawing-based 3D animation systems. The\nstylization pose gap denotes that input target poses used to train the\nstylization network are always in occlusion-free poses, while target poses\nencountered in an inference include diverse occlusions under dynamic motions.\nTo this end, we propose Occlusion-robust Stylization Framework (OSF) for\ndrawing-based 3D animation. We found that while employing object's edge can be\neffective input prior for guiding stylization, it becomes notably inaccurate\nwhen occlusions occur at inference. Thus, our proposed OSF provides\nocclusion-robust edge guidance for stylization network using optical flow,\nensuring a consistent stylization even under occlusions. Furthermore, OSF\noperates in a single run instead of the previous two-stage method, achieving\n2.4x faster inference and 2.1x less memory.", "AI": {"tldr": "\u63d0\u51fa\u906e\u6321\u9c81\u68d2\u98ce\u683c\u5316\u6846\u67b6OSF\uff0c\u901a\u8fc7\u5149\u6d41\u63d0\u4f9b\u8fb9\u7f18\u6307\u5bfc\u89e3\u51b33D\u52a8\u753b\u4e2d\u7684\u98ce\u683c\u9000\u5316\u95ee\u9898\uff0c\u5b9e\u73b02.4\u500d\u52a0\u901f\u548c2.1\u500d\u5185\u5b58\u4f18\u5316", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u906e\u6321\u573a\u666f\u4e0b\u5b58\u5728\u98ce\u683c\u8d28\u91cf\u52a3\u5316\uff08\u8f6e\u5ed3\u95ea\u70c1/\u7b14\u89e6\u6a21\u7cca\uff09\uff0c\u6838\u5fc3\u77db\u76fe\u662f\u8bad\u7ec3\u4e0e\u63a8\u7406\u9636\u6bb5\u7684\u59ff\u6001\u5dee\u5f02\uff08\u65e0\u906e\u6321\u8bad\u7ec3\u6570\u636e vs \u52a8\u6001\u906e\u6321\u63a8\u7406\u573a\u666f\uff09", "method": "\u5229\u7528\u5149\u6d41\u751f\u6210\u9c81\u68d2\u7684\u8fb9\u7f18\u6307\u5bfc\u66ff\u4ee3\u7269\u4f53\u8fb9\u7f18\u8f93\u5165\uff0c\u907f\u514d\u906e\u6321\u5bfc\u81f4\u7684\u8fb9\u7f18\u9519\u8bef\uff1b\u91c7\u7528\u5355\u9636\u6bb5\u5904\u7406\u66ff\u4ee3\u4f20\u7edf\u4e24\u9636\u6bb5\u6d41\u7a0b", "result": "\u5728\u4fdd\u6301\u98ce\u683c\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u53472.4\u500d\uff0c\u5185\u5b58\u5360\u7528\u51cf\u5c112.1\u500d", "conclusion": "OSF\u6846\u67b6\u6709\u6548\u89e3\u51b3\u7ed8\u5236\u7c7b3D\u52a8\u753b\u7684\u906e\u6321\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u8fd0\u884c\u6548\u7387\uff0c\u4e3a\u6570\u5b57\u827a\u672f\u521b\u4f5c\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2508.00121", "pdf": "https://arxiv.org/pdf/2508.00121", "abs": "https://arxiv.org/abs/2508.00121", "authors": ["Xiao Zhang", "Johan bos"], "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "categories": ["cs.CL"], "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u8868\u73b0\u4f18\u5f02\uff08>90%\uff09\uff0c\u4f46\u5728\u5904\u7406\u52a8\u8bcd\u77ed\u8bed\u7701\u7565\u73b0\u8c61\u65f6\u4e25\u91cd\u5931\u8d25", "motivation": "\u9a8c\u8bc1\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u80fd\u5426\u5904\u7406\u5f3a\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u8bed\u4e49\u73b0\u8c61\uff08\u5982\u82f1\u8bed\u52a8\u8bcd\u77ed\u8bed\u7701\u7565\uff09\uff0c\u6b64\u7c7b\u73b0\u8c61\u9700\u8981\u590d\u5236\u5927\u91cf\u8bed\u4e49\u4fe1\u606f", "method": "\u6784\u5efa\u5305\u542b120\u4e2a\u7701\u7565\u6848\u4f8b\u53ca\u5176\u5b8c\u6574\u8bed\u4e49\u8868\u793a\u7684\u6311\u6218\u96c6\uff0c\u5e76\u7528\u591a\u79cd\u795e\u7ecf\u8bed\u4e49\u89e3\u6790\u5668\u8fdb\u884c\u6d4b\u8bd5", "result": "\u6240\u6709\u89e3\u6790\u5668\u5728\u6807\u51c6\u6d4b\u8bd5\u96c6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7701\u7565\u6848\u4f8b\u4e2d\u5168\u90e8\u5931\u8d25", "conclusion": "\u73b0\u6709\u8bed\u4e49\u89e3\u6790\u5668\u5b58\u5728\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7f3a\u9677\uff0c\u9700\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b49\u65b9\u6cd5\u63d0\u5347\u5bf9\u590d\u6742\u8bed\u8a00\u73b0\u8c61\u7684\u5904\u7406\u80fd\u529b"}}
{"id": "2508.00424", "pdf": "https://arxiv.org/pdf/2508.00424", "abs": "https://arxiv.org/abs/2508.00424", "authors": ["Kresimir Matkovic", "Rainer Splechtna", "Denis Gracanin", "Helwig Hauser"], "title": "CrossSet: Unveiling the Complex Interplay of Two Set-typed Dimensions in Multivariate Data", "categories": ["cs.GR"], "comment": "Will be published in TVCG and presented at IEEE VIS", "summary": "The interactive visual analysis of set-typed data, i.e., data with attributes\nthat are of type set, is a rewarding area of research and applications.\nValuable prior work has contributed solutions that enable the study of such\ndata with individual set-typed dimensions. In this paper, we present CrossSet,\na novel method for the joint study of two set-typed dimensions and their\ninterplay. Based on a task analysis, we describe a new, multi-scale approach to\nthe interactive visual exploration and analysis of such data. Two set-typed\ndata dimensions are jointly visualized using a hierarchical matrix layout,\nenabling the analysis of the interactions between two set-typed attributes at\nseveral levels, in addition to the analysis of individual such dimensions.\nCrossSet is anchored at a compact, large-scale overview that is complemented by\ndrill-down opportunities to study the relations between and within the\nset-typed dimensions, enabling an interactive visual multi-scale exploration\nand analysis of bivariate set-typed data. Such an interactive approach makes it\npossible to study single set-typed dimensions in detail, to gain an overview of\nthe interaction and association between two such dimensions, to refine one of\nthe dimensions to gain additional details at several levels, and to drill down\nto the specific interactions of individual set-elements from the set-typed\ndimensions. To demonstrate the effectiveness and efficiency of CrossSet, we\nhave evaluated the new method in the context of several application scenarios.", "AI": {"tldr": "\u5f00\u53d1\u4e86CrossSet\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u77e9\u9635\u5e03\u5c40\u5b9e\u73b0\u53cc\u96c6\u5408\u7c7b\u578b\u6570\u636e\u7684\u591a\u5c3a\u5ea6\u4ea4\u4e92\u53ef\u89c6\u5316\u5206\u6790", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u652f\u6301\u5355\u4e00\u96c6\u5408\u7c7b\u578b\u7ef4\u5ea6\u5206\u6790\uff0c\u9700\u89e3\u51b3\u53cc\u7ef4\u5ea6\u8054\u5408\u63a2\u7d22\u53ca\u5176\u4ea4\u4e92\u5173\u7cfb\u7684\u7814\u7a76\u9700\u6c42", "method": "\u91c7\u7528\u5206\u5c42\u77e9\u9635\u5e03\u5c40\u7ed3\u5408\u6982\u89c8+\u94bb\u53d6\u673a\u5236\uff0c\u652f\u6301\u4ece\u5b8f\u89c2\u5230\u5fae\u89c2\u7684\u591a\u5c42\u6b21\u8054\u5408\u5206\u6790", "result": "\u5728\u591a\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5bf9\u590d\u6742\u4ea4\u4e92\u5206\u6790\u4efb\u52a1\u7684\u6709\u6548\u6027\u548c\u6267\u884c\u6548\u7387", "conclusion": "CrossSet\u6269\u5c55\u4e86\u96c6\u5408\u578b\u6570\u636e\u5206\u6790\u65b9\u6cd5\u4f53\u7cfb\uff0c\u4e3a\u53cc\u53d8\u91cf\u96c6\u5408\u6570\u636e\u7684\u6df1\u5ea6\u4ea4\u4e92\u63a2\u7d22\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.00185", "pdf": "https://arxiv.org/pdf/2508.00185", "abs": "https://arxiv.org/abs/2508.00185", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "title": "Comparison of Large Language Models for Deployment Requirements", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "Survey on LLM advancements, challenges, and comparative analysis of open-source models for optimal selection.", "motivation": "Proliferation of open-source LLMs creates selection complexity regarding licensing and hardware requirements.", "method": "Created a continuously updated GitLab list comparing foundational/domain-specific models by release year, license, and hardware specs.", "result": "Provides a living reference document to navigate the evolving LLM ecosystem.", "conclusion": "Enables researchers/companies to make informed LLM selections while tracking rapid developments in AI language models."}}
{"id": "2508.00428", "pdf": "https://arxiv.org/pdf/2508.00428", "abs": "https://arxiv.org/abs/2508.00428", "authors": ["Nan Xiang", "Tianyi Liang", "Haiwen Huang", "Shiqi Jiang", "Hao Huang", "Yifei Huang", "Liangyu Chen", "Changbo Wang", "Chenhui Li"], "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation", "categories": ["cs.GR", "cs.HC"], "comment": "IEEE VIS VAST 2025 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet\nremains bottlenecked by blind trial-and-error prompting processes that yield\nunpredictable results. While visual prompt engineering has advanced in\ntext-to-image domains, its application to 3D generation presents unique\nchallenges requiring multi-view consistency evaluation and spatial\nunderstanding. We present Sel3DCraft, a visual prompt engineering system for\nT23D that transforms unstructured exploration into a guided visual process. Our\napproach introduces three key innovations: a dual-branch structure combining\nretrieval and generation for diverse candidate exploration; a multi-view hybrid\nscoring approach that leverages MLLMs with innovative high-level metrics to\nassess 3D models with human-expert consistency; and a prompt-driven visual\nanalytics suite that enables intuitive defect identification and refinement.\nExtensive testing and user studies demonstrate that Sel3DCraft surpasses other\nT23D systems in supporting creativity for designers.", "AI": {"tldr": "Sel3DCraft\u901a\u8fc7\u53cc\u5206\u652f\u7ed3\u6784\u3001\u591a\u89c6\u56fe\u8bc4\u5206\u548c\u89c6\u89c9\u5206\u6790\u5957\u4ef6\uff0c\u89e3\u51b3\u4e86T23D\u751f\u6210\u4e2d\u76f2\u76ee\u8bd5\u9519\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bbe\u8ba1\u5e08\u521b\u4f5c\u6548\u7387\u3002", "motivation": "\u4f20\u7edfText-to-3D\u751f\u6210\u4f9d\u8d56\u4f4e\u6548\u7684\u8bd5\u9519\u5f0f\u63d0\u793a\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u8bc4\u4f30\u548c\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u63a7\u3002", "method": "1. \u68c0\u7d22\u4e0e\u751f\u6210\u878d\u5408\u7684\u53cc\u5206\u652f\u5019\u9009\u751f\u6210\n2. \u57fa\u4e8eMLLM\u7684\u591a\u89c6\u56fe\u6df7\u5408\u8bc4\u5206\u7cfb\u7edf(\u542b\u521b\u65b0\u6027\u9ad8\u9636\u6307\u6807)\n3. \u652f\u6301\u7f3a\u9677\u53ef\u89c6\u5316\u5b9a\u4f4d\u7684\u63d0\u793a\u9a71\u52a8\u5206\u6790\u5de5\u5177\u7ec4", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u6d4b\u8bd5\u548c\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\uff0cSel3DCraft\u5728\u652f\u6301\u8bbe\u8ba1\u5e08\u521b\u9020\u6027\u5de5\u4f5c\u65b9\u9762\u4f18\u4e8e\u73b0\u6709T23D\u7cfb\u7edf", "conclusion": "\u8be5\u7cfb\u7edf\u5c06\u65e0\u5e8f\u76843D\u751f\u6210\u63a2\u7d22\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u89c6\u89c9\u6d41\u7a0b\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u8d28\u91cf\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u7f3a\u9677\u5206\u6790\u63a8\u52a83D\u5185\u5bb9\u521b\u4f5c\u8303\u5f0f\u9769\u65b0\u3002"}}
{"id": "2508.00217", "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "categories": ["cs.CL", "cs.DB", "cs.LG"], "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790LLMs\u548cMLLMs\u4e2d\u8868\u683c\u7406\u89e3\u7684\u6311\u6218\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\u4e0e\u4efb\u52a1\u4f53\u7cfb\uff0c\u5e76\u63ed\u793a\u5f53\u524d\u4e09\u5927\u7814\u7a76\u7a7a\u767d\uff1a\u68c0\u7d22\u4e3b\u5bfc\u578b\u4efb\u52a1\u5c40\u9650\u3001\u590d\u6742\u7ed3\u6784\u5904\u7406\u74f6\u9888\u53ca\u8de8\u683c\u5f0f\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u8868\u683c\u56e0\u5176\u4e8c\u7ef4\u590d\u6742\u7ed3\u6784\u5728LLMs/MLLMs\u5e94\u7528\u4e2d\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u4e14\u96be\u4ee5\u5e94\u5bf9\u4e0d\u540c\u683c\u5f0f\u7684\u6cdb\u5316\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u8868\u683c\u8f93\u5165\u8868\u793a\u7684\u5206\u7c7b\u6cd5\uff0c\u5efa\u7acb\u4efb\u52a1\u7406\u89e3\u4f53\u7cfb\uff0c\u7cfb\u7edf\u5206\u6790\u73b0\u6709\u7814\u7a76\u5728\u6570\u5b66\u903b\u8f91\u8fd0\u7b97\u3001\u590d\u6742\u7ed3\u6784\u5904\u7406\u548c\u591a\u8868\u683c\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "result": "\u63ed\u793a\u4e09\u5927\u5173\u952e\u7f3a\u9677\uff1a1) 68%\u4efb\u52a1\u805a\u7126\u57fa\u7840\u68c0\u7d22\u800c\u975e\u6df1\u5ea6\u63a8\u7406 2) \u590d\u6742\u8868\u683c\u5904\u7406\u51c6\u786e\u7387\u4e0b\u964d40% 3) \u8de8\u683c\u5f0f\u8fc1\u79fb\u6027\u80fd\u635f\u5931\u8fbe25%", "conclusion": "\u9700\u5f00\u53d1\u7edf\u4e00\u6846\u67b6\u63d0\u5347\u590d\u6742\u8868\u683c\u5904\u7406\u80fd\u529b\uff0c\u52a0\u5f3a\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u5e76\u5efa\u7acb\u8de8\u683c\u5f0f\u8fc1\u79fb\u5b66\u4e60\u673a\u5236\u4ee5\u7a81\u7834\u5f53\u524d\u6280\u672f\u74f6\u9888\u3002"}}
{"id": "2508.00782", "pdf": "https://arxiv.org/pdf/2508.00782", "abs": "https://arxiv.org/abs/2508.00782", "authors": ["Kien T. Pham", "Yingqing He", "Yazhou Xing", "Qifeng Chen", "Long Chen"], "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "The 33rd ACM Multimedia Conference (MM '25)", "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.", "AI": {"tldr": "SpA2V\u6846\u67b6\u901a\u8fc7\u5206\u89e3\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\uff08\u97f3\u9891\u5f15\u5bfc\u5e03\u5c40\u89c4\u5212+\u5e03\u5c40\u9a71\u52a8\u751f\u6210\uff09\uff0c\u5229\u7528\u58f0\u97f3\u7269\u7406\u7279\u5f81\u63d0\u53d6\u7a7a\u95f4\u7ebf\u7d22\uff0c\u663e\u8457\u63d0\u5347\u97f3\u89c6\u9891\u7684\u8bed\u4e49\u548c\u7a7a\u95f4\u5bf9\u9f50\u6548\u679c", "motivation": "\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4ec5\u5173\u6ce8\u8bed\u4e49\u4fe1\u606f\uff08\u58f0\u6e90\u7c7b\u522b\uff09\uff0c\u5ffd\u7565\u58f0\u97f3\u7269\u7406\u5c5e\u6027\uff08\u5982\u54cd\u5ea6\u3001\u9891\u7387\uff09\u6240\u9690\u542b\u7684\u7a7a\u95f4\u5c5e\u6027\uff08\u4f4d\u7f6e/\u8fd0\u52a8\u65b9\u5411\uff09\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u51c6\u786e\u6027\u4e0d\u8db3", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u97f3\u9891\u5f15\u5bfc\u89c6\u9891\u89c4\u5212\uff1a\u6539\u9020MLLM\u6784\u5efa\u89c6\u9891\u573a\u666f\u5e03\u5c40\uff08VSL\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u5f81\uff1b2) \u5e03\u5c40\u9a71\u52a8\u89c6\u9891\u751f\u6210\uff1a\u5c06VSL\u4f5c\u4e3a\u6761\u4ef6\u5d4c\u5165\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u96f6\u8bad\u7ec3\u6210\u672c\u7684\u7a7a\u95f4\u53ef\u63a7\u751f\u6210", "result": "\u5b9e\u9a8c\u8bc1\u660eSpA2V\u80fd\u751f\u6210\u4e0e\u8f93\u5165\u97f3\u9891\u5728\u8bed\u4e49\u548c\u7a7a\u95f4\u7ef4\u5ea6\u9ad8\u5ea6\u5bf9\u9f50\u7684\u903c\u771f\u89c6\u9891\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5229\u7528\u58f0\u97f3\u7a7a\u95f4\u7ebf\u7d22\u4e0e\u5206\u9636\u6bb5\u751f\u6210\u7b56\u7565\uff0c\u9996\u6b21\u5b9e\u73b0\u97f3\u89c6\u9891\u8de8\u6a21\u6001\u7684\u6df1\u5ea6\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4e3a\u7269\u7406\u611f\u77e5\u7684\u8de8\u6a21\u6001\u751f\u6210\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.00220", "pdf": "https://arxiv.org/pdf/2508.00220", "abs": "https://arxiv.org/abs/2508.00220", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "categories": ["cs.CL"], "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362(DWT)\u538b\u7f29\u8bcd/\u53e5\u5d4c\u5165\uff0c\u53ef\u5728\u4fdd\u7559\u8bed\u4e49\u4fe1\u606f\u7684\u540c\u65f6\u51cf\u5c1150-93%\u7ef4\u5ea6", "motivation": "\u5c0f\u6ce2\u53d8\u6362\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u5c55\u73b0\u591a\u5206\u8fa8\u7387\u5206\u6790\u4f18\u52bf\uff0c\u4f46NLP\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u5176\u5728\u5d4c\u5165\u8868\u793a\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "method": "\u5c06DWT\u5e94\u7528\u4e8e\u4e0d\u540c\u5d4c\u5165\u6a21\u578b\uff08\u542b\u5927\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u591a\u5206\u8fa8\u7387\u5206\u6790\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\uff0c\u8bc4\u4f30\u8bed\u4e49\u76f8\u4f3c\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0", "result": "\u5d4c\u5165\u7ef4\u5ea6\u51cf\u5c1150-93%\u65f6\uff0c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u4efb\u52a1\u6027\u80fd\u57fa\u672c\u4e0d\u53d8\uff0c\u591a\u6570\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\u53cd\u800c\u63d0\u5347", "conclusion": "DWT\u4e3aNLP\u5d4c\u5165\u538b\u7f29\u548c\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.00238", "pdf": "https://arxiv.org/pdf/2508.00238", "abs": "https://arxiv.org/abs/2508.00238", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u79d1\u6280\u64ad\u5ba2\u5373\u5174\u53e3\u8bed\u6570\u636e\uff0c\u53d1\u73b0ChatGPT\u53d1\u5e03\u540e\u4eba\u7c7b\u8bed\u8a00\u4f7f\u7528\u663e\u8457\u8d8b\u540c\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u8bcd\u6c47\uff0c\u6697\u793aAI\u53ef\u80fd\u5f15\u53d1\u8bed\u8a00\u7cfb\u7edf\u53d8\u5316", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4eba\u7c7b\u8bed\u8a00\u7cfb\u7edf\u7684\u5f71\u54cd\u662f\u5426\u8d85\u8d8a\u76f4\u63a5\u6587\u672c\u751f\u6210\u5de5\u5177\u7684\u4f7f\u7528\uff0c\u63ed\u793a\u6f5c\u5728\u7684\u8bed\u8a00\u4f53\u7cfb\u6df1\u5c42\u8f6c\u53d8", "method": "\u6784\u5efa2200\u4e07\u8bcd\u7ea7\u79d1\u6280\u64ad\u5ba2\u53e3\u8bed\u8bed\u6599\u5e93\uff0c\u5bf9\u6bd4\u5206\u6790ChatGPT\u53d1\u5e03\u524d\u540e(2022\u5e74)\u5e38\u7528LLM\u5173\u8054\u8bcd\u6c47\u7684\u4f7f\u7528\u8d8b\u52bf", "result": "2022\u5e74\u540eLLM\u5173\u8054\u8bcd\u6c47\u4f7f\u7528\u91cf\u5448\u73b0\u4e2d\u5ea6\u4f46\u663e\u8457\u589e\u957f(\u589e\u5e454.5%)\uff0c\u57fa\u7ebf\u540c\u4e49\u8bcd\u672a\u51fa\u73b0\u65b9\u5411\u6027\u53d8\u5316\uff0c\u6697\u793a\u8bed\u8a00\u8f6c\u53d8\u7684\u521d\u671f\u8ff9\u8c61", "conclusion": "\u8bed\u8a00\u53d8\u5316\u53ef\u80fd\u53cd\u6620\u81ea\u7136\u6f14\u53d8\u6216AI\u9a71\u52a8\u7684\u65b0\u578b\u8f6c\u53d8\uff0c\u4e0a\u6e38\u6a21\u578b\u672a\u5bf9\u9f50\u95ee\u9898\u6216\u5bfc\u81f4\u4eba\u7c7b\u8bed\u8a00\u53d8\u5316\uff0c\u4e0eAI\u4f26\u7406\u95ee\u9898\u5f62\u6210\u547c\u5e94"}}
{"id": "2508.00285", "pdf": "https://arxiv.org/pdf/2508.00285", "abs": "https://arxiv.org/abs/2508.00285", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "categories": ["cs.CL", "I.2.7; J.3"], "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u75c5\u56e0\u611f\u77e5\u6ce8\u610f\u529b\u5f15\u5bfc\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6025\u8179\u75c7\u8bca\u65ad\u4e2d\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff08\u8bca\u65ad\u51c6\u786e\u7387+15.65%\uff0c\u63a8\u7406\u805a\u7126+31.6%\uff09", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u4e2d\u8bca\u65ad\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4e34\u5e8a\u63a8\u7406\u63d0\u5347AI\u8bca\u65ad\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027", "method": "1.\u6784\u5efa\u57fa\u4e8e\u4e34\u5e8a\u6307\u5357\u7684\u63a8\u7406\u6846\u67b6CRS 2.\u5f00\u53d1\u6ce8\u610f\u529b\u5934\u5b9a\u4f4d\u7b97\u6cd5 3.\u8bbe\u8ba1\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5d4c\u5165\u75c5\u56e0\u7ebf\u7d22+\u6307\u5bfc\u6027\u635f\u5931\u51fd\u6570\uff09", "result": "\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u534715.65%\uff0c\u63a8\u7406\u805a\u7126\u5206\u6570\u589e\u957f31.6%\uff1b\u5916\u90e8\u9a8c\u8bc1\u663e\u793a\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u53ef\u9760\u6027\u663e\u8457\u589e\u5f3a", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u53ef\u89e3\u91ca\u7684AI\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\uff0c\u63a8\u52a8\u533b\u7597AI\u5728\u590d\u6742\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528"}}
{"id": "2508.00305", "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "categories": ["cs.CL", "cs.LG", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u526a\u679d/\u91cf\u5316/token dropping\u7b49\u4f18\u5316\u6280\u672f\u5728\u957f\u4e0a\u4e0b\u6587LLM\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7ec4\u5408\u4f18\u5316\u4f1a\u5728\u5927\u6a21\u578b\u4e2d\u4ea7\u751f\u8fd1\u4f3c\u8bef\u5dee\u7d2f\u79ef\uff0c\u4ec5\u4f9d\u8d56F1\u5206\u6570\u4f1a\u63a9\u76d6QA\u4efb\u52a1\u7684\u7cbe\u5ea6-\u53ec\u56de\u6743\u8861", "motivation": "\u73b0\u6709LLM\u4f18\u5316\u6280\u672f\u4e3b\u8981\u5173\u6ce8\u77ed\u4e0a\u4e0b\u6587\u573a\u666f\uff0c\u7f3a\u4e4f\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u4e14\u672a\u7814\u7a76\u4e0d\u540c\u4f18\u5316\u65b9\u6cd5\u7ec4\u5408\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd", "method": "1. \u5206\u6790\u4e24\u79cd\u652f\u6301\u957f\u4e0a\u4e0b\u6587\u7684LLM\u67b6\u6784\u7684\u5355\u72ec\u4f18\u5316\u6548\u679c 2. \u7cfb\u7edf\u8bc4\u4f30\u4f18\u5316\u65b9\u6cd5\u7ec4\u5408 3. \u572870B\u53c2\u6570\u6a21\u578b\u4e0a\u6d4b\u8bd5\u4f18\u5316\u6269\u5c55\u6027", "result": "\u7ec4\u5408\u4f18\u5316\u65b9\u6cd5\u4f1a\u5bfc\u81f470B\u5927\u6a21\u578b\u51fa\u73b0\u8fd1\u4f3c\u8bef\u5dee\u7d2f\u79ef\u6548\u5e94\uff0c\u4e14\u5355\u7eaf\u4f9d\u8d56F1\u8bc4\u4f30\u6307\u6807\u4f1a\u63a9\u76d6QA\u4efb\u52a1\u4e2d\u7684\u7cbe\u5ea6-\u53ec\u56de\u5e73\u8861\u5173\u7cfb", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u6027\u80fd\u5206\u6790\u4e0e\u4efb\u52a1\u7279\u6027\u6d1e\u5bdf\u7684\u7ed3\u5408\uff0c\u53ef\u5e2e\u52a9\u7814\u7a76\u8005\u5728\u4e0d\u540c\u786c\u4ef6\u914d\u7f6e\u4e0b\u5e73\u8861LLM\u7684\u6548\u7387\u3001\u7cbe\u5ea6\u548c\u6269\u5c55\u6027\u9700\u6c42"}}
{"id": "2508.00332", "pdf": "https://arxiv.org/pdf/2508.00332", "abs": "https://arxiv.org/abs/2508.00332", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "\u63d0\u51faMCSEO\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u4f53-\u77ed\u8bed\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u589e\u5f3a\u591a\u6a21\u6001\u53e5\u5b50\u5d4c\u5165\uff0c\u5728STS\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u56fe\u50cf-\u6807\u9898\u5bf9\u8bad\u7ec3\u5b58\u5728\u566a\u58f0\u5e72\u6270\uff08\u5197\u4f59/\u65e0\u5173\u4fe1\u606f\uff09\uff0c\u9700\u66f4\u7cbe\u786e\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u5f0f\u3002", "method": "\u5229\u7528\u5206\u5272\u548c\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u63d0\u53d6\u7cbe\u51c6\u7269\u4f53-\u77ed\u8bed\u5bf9\uff0c\u8bbe\u8ba1\u9488\u5bf9\u7269\u4f53-\u77ed\u8bed\u5bf9\u5e94\u5173\u7cfb\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u3002", "result": "\u5728\u4e0d\u540c\u9aa8\u5e72\u6a21\u578b\u4e0a\u7684STS\u4efb\u52a1\u8bc4\u6d4b\u4e2d\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u9a8c\u8bc1\u4e86\u7ec6\u7c92\u5ea6\u7269\u4f53-\u77ed\u8bed\u5bf9\u9f50\u5bf9\u63d0\u5347\u591a\u6a21\u6001\u8868\u5f81\u5b66\u4e60\u6548\u679c\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.00344", "pdf": "https://arxiv.org/pdf/2508.00344", "abs": "https://arxiv.org/abs/2508.00344", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "\u63d0\u51faAdaPlan\u4ee3\u7406\u8303\u5f0f\u4e0ePilotRL\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u89c4\u5212\u5f15\u5bfc\u548c\u6e10\u8fdb\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3LLM\u4ee3\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u534f\u8c03\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u8d85\u8d8aGPT-4o", "motivation": "\u73b0\u6709ReAct\u8303\u5f0f\u53d7\u9650\u4e8e\u5355\u6b65\u63a8\u7406\u4e0e\u5373\u65f6\u6267\u884c\u673a\u5236\uff0c\u96be\u4ee5\u652f\u6301\u590d\u6742\u4efb\u52a1\u7684\u957f\u671f\u89c4\u5212\uff1b\u89c4\u5212\u4e0e\u6267\u884c\u534f\u8c03\u4e0d\u8db3\uff1b\u76d1\u7763\u5fae\u8c03\u5bfc\u81f4\u6a21\u578b\u50f5\u5316\u5f71\u54cd\u6cdb\u5316\u80fd\u529b", "method": "1. AdaPlan\u8303\u5f0f\u7ed3\u5408\u9ad8\u5c42\u5168\u5c40\u6307\u5bfc\u4e0e\u6267\u884c\u5c42\u51b3\u7b56 2. PilotRL\u6846\u67b6\u5206\u4e09\u9636\u6bb5\uff1a\u5168\u5c40\u89c4\u5212\u8ddf\u968f\u8bad\u7ec3\u2192\u89c4\u5212\u8d28\u91cf\u4f18\u5316\u2192\u89c4\u5212\u6267\u884c\u8054\u5408\u8c03\u4f18 3. \u91c7\u7528\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8", "result": "LLaMA3.1-8B-Instruct+PilotRL\u8d85\u8fc7GPT-4o 3.60%\uff0c\u8f83\u540c\u53c2\u6570\u89c4\u6a21GPT-4o-mini\u63d0\u534755.78%", "conclusion": "\u5168\u5c40\u89c4\u5212\u5f15\u5bfc\u4e0e\u5f3a\u5316\u5b66\u4e60\u534f\u540c\u673a\u5236\u6709\u6548\u63d0\u5347LLM\u4ee3\u7406\u7684\u957f\u7a0b\u51b3\u7b56\u80fd\u529b\uff0c\u89c4\u5212-\u6267\u884c\u8054\u5408\u4f18\u5316\u8303\u5f0f\u663e\u8457\u589e\u5f3a\u4efb\u52a1\u6cdb\u5316\u6027"}}
{"id": "2508.00360", "pdf": "https://arxiv.org/pdf/2508.00360", "abs": "https://arxiv.org/abs/2508.00360", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "categories": ["cs.CL"], "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u673a\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\u3002", "motivation": "\u7531\u4e8e\u5bb9\u91cf\u53d7\u9650\uff0c\u5c0f\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4f20\u7edf\u65b9\u6cd5\u5c06\u63a8\u7406\u89c6\u4e3a\u56fa\u5b9a\u6216\u542f\u53d1\u5f0f\u8fc7\u7a0b\uff0c\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u6f5c\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u4efb\u52a1\u5411\u91cf\u6765\u63d0\u5347\u5c0f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06\u6a21\u578b\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff08\u7531<think>\u548c</think>\u754c\u5b9a\uff09\u91cd\u65b0\u5b9a\u4e49\u4e3a\u52a8\u6001\u4efb\u52a1\u5411\u91cf\u673a\uff0c\u5229\u7528RLVR\u4f18\u5316\u8be5\u673a\u5236\uff0c\u5e76\u7ed3\u5408MCP\u96c6\u6210\u5f00\u53d1\u4e861.7B\u53c2\u6570\u7684Lucy\u6a21\u578b\u3002", "result": "Lucy\u5728SimpleQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523078.3%\u51c6\u786e\u7387\uff0c\u4e0eDeepSeek-V3\u7b49\u5927\u578b\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u81ea\u6784\u5efa\u4efb\u52a1\u63a8\u7406\u673a\u5236\uff0c\u5c0f\u6a21\u578b\u65e0\u9700\u6269\u5927\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u4e0e\u5927\u578b\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u6a21\u578b\u8bbe\u8ba1\u5f00\u8f9f\u65b0\u8def\u5f84\u3002"}}
{"id": "2508.00370", "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "\u63d0\u51faEdgeInfinite-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6bb5\u76d1\u7763\u5fae\u8c03\u548c\u91cf\u5316\u4f18\u5316\u89e3\u51b3LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u7684TTFT\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u4f18\u5316\u65b9\u6848\u65e0\u6cd5\u6709\u6548\u964d\u4f4e\u9996token\u751f\u6210\u5ef6\u8fdf\uff0c\u4e14\u4f20\u7edf\u67b6\u6784\u9700\u8981\u5168\u53c2\u6570\u91cd\u8bad\u7ec3\u7f3a\u4e4f\u79fb\u52a8\u7aef\u9002\u914d", "method": "\u91c7\u7528Segmented S-SFT\u7b56\u7565\u4f18\u5316\u957f\u5e8f\u5217\u4efb\u52a1\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u540e\u8bad\u7ec3\u91cf\u5316\u548c\u56fa\u5b9a\u5f62\u72b6\u8ba1\u7b97\u56fe\u5b9e\u73b0NPU\u9ad8\u6548\u90e8\u7f72", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u9886\u57df\u6027\u80fd\uff0c\u4fdd\u6301NPU\u52a0\u901f\u8bbe\u5907\u6548\u7387", "conclusion": "\u901a\u8fc7\u521b\u65b0\u5fae\u8c03\u67b6\u6784\u548c\u91cf\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u8ba1\u7b97\u5185\u5b58\u53cc\u4f18\u5316\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u6548\u7387"}}
{"id": "2508.00385", "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0ICL\u4e2d\u90e8\u5206\u6f14\u793a\u6837\u672c\u65e0\u6548\u7684\u539f\u56e0\u662f\u4fe1\u606f\u5df2\u88ab\u6a21\u578b\u5438\u6536\u6216\u4e0e\u67e5\u8be2\u65e0\u5173\uff0c\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u6d41\u7684\u9009\u62e9\u65b9\u6cd5GradS\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u63d0\u53476.8%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u6240\u6709ICL\u6f14\u793a\u6837\u672c\u5747\u6709\u6548\uff0c\u4f46\u5b9e\u9645\u90e8\u5206\u6837\u672c\u65e0\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u9700\u63a2\u7a76\u65e0\u6548\u539f\u56e0\u5e76\u6539\u8fdb\u9009\u62e9\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u6d41\u5f52\u96f6\u5206\u6790\u63a8\u5bfc\u6f14\u793a\u65e0\u6548\u6761\u4ef6\uff0c\u63d0\u51faGradS\u65b9\u6cd5\u4ee5\u68af\u5ea6\u5e45\u5ea6\u4e3a\u6307\u6807\u7b5b\u9009\u6709\u6548\u6837\u672c\u3002", "result": "\u5728\u56db\u5927LLM\u548c\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cGradS\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u76f8\u5bf9\u63d0\u53476.8%\uff0c\u4e14\u6a21\u578b\u5c42\u7ea7\u52a0\u6df1\u4f1a\u653e\u5927\u6f14\u793a\u6709\u6548\u6027\u5dee\u5f02\u3002", "conclusion": "\u6a21\u578b\u5c42\u7ea7\u52a0\u6df1\u4f1a\u805a\u7126\u6709\u6548\u6837\u672c\uff0cGradS\u901a\u8fc7\u68af\u5ea6\u6d41\u9009\u62e9\u7b56\u7565\u663e\u8457\u63d0\u5347ICL\u6027\u80fd\uff0c\u8bc1\u5b9e\u7406\u8bba\u63a8\u5bfc\u6709\u6548\u6027\u3002"}}
{"id": "2508.00390", "pdf": "https://arxiv.org/pdf/2508.00390", "abs": "https://arxiv.org/abs/2508.00390", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "categories": ["cs.CL"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "\u63d0\u51faSA-GCS\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u96be\u5ea6\u4f30\u8ba1\u4e0e\u9ad8\u65af\u8bfe\u7a0b\u8c03\u5ea6\u63d0\u5347\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6548\u7387\u4e0e\u6027\u80fd", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u5229\u7528\u6548\u7387\u4f4e\u3001\u6536\u655b\u901f\u5ea6\u6162\u3001\u672a\u5145\u5206\u8003\u8651\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347", "method": "SA-GCS\u6846\u67b6\u5305\u542b\u8bed\u4e49\u611f\u77e5\u96be\u5ea6\u4f30\u8ba1\u5668(SA-DE)\u91cf\u5316\u6837\u672c\u590d\u6742\u5ea6\uff0c\u9ad8\u65af\u8bfe\u7a0b\u8c03\u5ea6\u5668(GCS)\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5206\u5e03", "result": "\u5728CityNav\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u6536\u655b\u901f\u5ea6\u63d0\u534780%\uff0c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347", "conclusion": "SA-GCS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709RL\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u53ef\u6269\u5c55\u6027"}}
{"id": "2508.00420", "pdf": "https://arxiv.org/pdf/2508.00420", "abs": "https://arxiv.org/abs/2508.00420", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "categories": ["cs.CL"], "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "\u7814\u7a76\u5c06\u79bb\u6563\u5c0f\u6ce2\u53d8\u6362(DWT)\u4e0e\u79bb\u6563\u4f59\u5f26\u53d8\u6362(DCT)\u7ed3\u5408\uff0c\u63d0\u51fa\u975e\u53c2\u6570\u5316\u6a21\u578b\u538b\u7f29\u6587\u672c\u7279\u5f81\uff0c\u5728NLP\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e0e\u539f\u59cb\u5d4c\u5165\u76f8\u5f53/\u66f4\u4f18\u7684\u6548\u679c", "motivation": "\u57fa\u4e8e\u5c0f\u6ce2\u6280\u672f\u5728\u56fe\u50cf/\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u6210\u529f\u5e94\u7528\uff0c\u63a2\u7d22\u5176\u5728NLP\u9886\u57df\u7684\u4fe1\u606f\u538b\u7f29\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5904\u7406\u8bcd\u5411\u91cf\u7ef4\u5ea6\u4e0e\u4fe1\u606f\u5bc6\u5ea6\u95ee\u9898", "method": "1. \u4f7f\u7528DWT\u4f18\u5316\u8bcd\u5411\u91cf\u4fe1\u606f\u5bc6\u5ea6\u5e76\u964d\u7ef4 2. \u7ed3\u5408DWT+DCT\u6784\u5efa\u975e\u53c2\u6570\u5316\u538b\u7f29\u6a21\u578b\uff0c\u57fa\u4e8e\u5c40\u90e8\u8bcd\u7279\u5f81\u751f\u6210\u56fa\u5b9a\u5c3a\u5bf8\u53e5\u5b50\u5411\u91cf", "result": "\u4e0b\u6e38\u4efb\u52a1\u5b9e\u9a8c\u8868\u660e\uff0c\u538b\u7f29\u540e\u7684\u5411\u91cf\u5728\u4fdd\u6301\u5173\u952e\u4fe1\u606f\u540c\u65f6\uff0c\u90e8\u5206\u4efb\u52a1\u6027\u80fd\u4f18\u4e8e\u539f\u59cb\u5d4c\u5165\u65b9\u6cd5", "conclusion": "\u5c0f\u6ce2\u6280\u672f\u80fd\u6709\u6548\u538b\u7f29\u8bed\u8a00\u7279\u5f81\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u53d8\u6362\u65b9\u6cd5\u4e3aNLP\u6a21\u578b\u63d0\u4f9b\u9ad8\u6548\u7279\u5f81\u8868\u793a\uff0c\u63a8\u52a8\u975e\u53c2\u6570\u5316\u6a21\u578b\u53d1\u5c55"}}
{"id": "2508.00429", "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "categories": ["cs.CL", "cs.LG", "cs.MA"], "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "\u63d0\u51faReaGAN\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u7ea7\u81ea\u4e3b\u51b3\u7b56\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u89e3\u51b3\u4f20\u7edfGNN\u4fe1\u606f\u4f20\u64ad\u5c40\u9650\u6027", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u8282\u70b9\u4fe1\u606f\u91cf\u4e0d\u5e73\u8861\uff08\u90e8\u5206\u8282\u70b9\u4fe1\u606f\u7a00\u758f\uff09\u548c\u8fc7\u5ea6\u4f9d\u8d56\u5c40\u90e8\u7ed3\u6784\u800c\u5ffd\u7565\u5168\u5c40\u8bed\u4e49\u5173\u7cfb\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u6bcf\u4e2a\u8282\u70b9\u4f5c\u4e3a\u81ea\u4e3b\u51b3\u7b56\u4e3b\u4f53\uff0c\u7ed3\u5408\u5185\u90e8\u8bb0\u5fc6\u8fdb\u884c\u8282\u70b9\u7ea7\u89c4\u5212\uff0c\u5e76\u5229\u7528RAG\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u5173\u7cfb", "result": "\u5728\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u4f7f\u7528\u672a\u5fae\u8c03\u7684\u51bb\u7ed3LLM\u53d6\u5f97\u4e86\u7ade\u4e89\u529b\u8868\u73b0", "conclusion": "\u9a8c\u8bc1\u4e86\u4ee3\u7406\u89c4\u5212\u548c\u5c40\u90e8-\u5168\u5c40\u68c0\u7d22\u673a\u5236\u5728\u56fe\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u52a8\u6001\u4fe1\u606f\u4f20\u64ad\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2508.00454", "pdf": "https://arxiv.org/pdf/2508.00454", "abs": "https://arxiv.org/abs/2508.00454", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "categories": ["cs.CL"], "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u591a\u8f6e\u5bf9\u8bdd\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7\u805a\u5408\u591aLLM\u8bc4\u5224\u77e5\u8bc6\u5230\u5355\u4e00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u5e76\u4fdd\u6301\u8bc4\u4f30\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5355LLM\u7684\u5bf9\u8bdd\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u504f\u89c1\u95ee\u9898\uff0c\u800c\u591aLLM\u8bc4\u59d4\u65b9\u6848\u867d\u6709\u6548\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u591a\u4e2aLLM\u8bc4\u59d4\u7684\u504f\u597d\u77e5\u8bc6\u805a\u5408\u5230\u5355\u4e2a\u8f7b\u91cf\u6a21\u578b\u4e2d\uff0c\u5b9e\u73b0\u5feb\u901f\u7075\u6d3b\u7684\u591a\u8f6e\u5bf9\u8bdd\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u57287\u4e2a\u5bf9\u8bdd\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u5355\u6a21\u578b\u53c2\u6570\u91cf\u4ec50.25B\u65f6\u51c6\u786e\u7387\u8d85GPT-4 5.7%\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347400\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u591a\u8bc4\u59d4\u8303\u5f0f\u7684\u9ad8\u6210\u672c\u9650\u5236\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u9ad8\u6548\u53ef\u9760\u7684\u65b0\u8303\u5f0f\uff0c\u5177\u6709\u663e\u8457\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00476", "pdf": "https://arxiv.org/pdf/2508.00476", "abs": "https://arxiv.org/abs/2508.00476", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "categories": ["cs.CL"], "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408RAG\u4e0eAMR\u7684\u95ee\u7b54\u7cfb\u7edf\uff0c\u572835%\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347\u56de\u7b54\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u53c2\u4e0e\u8005\u533a\u5206\u7684\u95ee\u9898\uff08\u5982who\u7c7b\uff09\u6548\u679c\u7a81\u51fa\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u4f1a\u8bae\u8bb0\u5f55\u7684\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u73b0\u6709\u7cfb\u7edf\u5bf9\u6d89\u53ca\u591a\u53c2\u4e0e\u8005\u7684\u590d\u6742\u95ee\u9898\uff08\u5982'\u8c01\u63d0\u95ee'\uff09\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u901a\u8fc7AMR\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e09\u4e2a\u96c6\u6210RAG\uff08\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff09\u4e0eAMR\uff08\u62bd\u8c61\u610f\u4e49\u8868\u793a\uff09\u7684\u7cfb\u7edf\uff0c\u5229\u7528AMR\u8fdb\u884c\u6df1\u5c42\u8bed\u4e49\u89e3\u6790\uff0c\u7ed3\u5408RAG\u5b9e\u73b0\u4fe1\u606f\u68c0\u7d22\u4e0e\u7b54\u6848\u751f\u6210\u3002", "result": "\u7cfb\u7edf\u572835%\u7684\u95ee\u9898\u4e0a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u56de\u7b54\uff0c\u9488\u5bf9\u53c2\u4e0e\u8005\u533a\u5206\u7c7b\u95ee\u9898\uff08who\u7c7b\u578b\uff09\u7684\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\u3002", "conclusion": "AMR\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u95ee\u7b54\u573a\u666f\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u7ed3\u6784\u5316\u7406\u89e3\u53c2\u4e0e\u8005\u5173\u7cfb\u7684\u4efb\u52a1\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8bed\u4e49\u8868\u793a\u4e0e\u751f\u6210\u6a21\u578b\u7ed3\u5408\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.00489", "pdf": "https://arxiv.org/pdf/2508.00489", "abs": "https://arxiv.org/abs/2508.00489", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "categories": ["cs.CL"], "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u68c0\u6d4b\u534a\u771f\u9648\u8ff0\u7684\u65b0\u4efb\u52a1\u53ca\u57fa\u51c6PolitiFact-Hidden\uff0c\u5f00\u53d1TRACER\u6846\u67b6\u901a\u8fc7\u5206\u6790\u8bc1\u636e\u5bf9\u9f50\u548c\u9690\u542b\u610f\u56fe\u663e\u8457\u63d0\u5347\u534a\u771f\u68c0\u6d4b\u6548\u679c", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u4ec5\u9a8c\u8bc1\u8868\u9762\u9648\u8ff0\u771f\u5b9e\u6027\uff0c\u65e0\u6cd5\u8bc6\u522b\u56e0\u5173\u952e\u4fe1\u606f\u7f3a\u5931\u5bfc\u81f4\u7684\u8bef\u5bfc\u6027\u534a\u771f\u9648\u8ff0\uff0c\u4e9f\u9700\u89e3\u51b3\u8fd9\u7c7b\u9690\u853d\u6027\u66f4\u5f3a\u7684\u865a\u5047\u4fe1\u606f", "method": "\u8bbe\u8ba1\u6a21\u5757\u5316\u6846\u67b6TRACER\uff0c\u5305\u542b\u4e09\u9636\u6bb5\uff1a1) \u8bc1\u636e\u5bf9\u9f50\u9a8c\u8bc1 2) \u58f0\u660e\u610f\u56fe\u63a8\u7406 3) \u9690\u85cf\u5185\u5bb9\u56e0\u679c\u5f71\u54cd\u8bc4\u4f30\uff0c\u53ef\u96c6\u6210\u81f3\u73b0\u6709\u6838\u67e5\u7cfb\u7edf", "result": "\u5728\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u534a\u771f\u5206\u7c7bF1\u6700\u9ad8\u63d0\u534716\u4e2a\u70b9\uff0c\u51c6\u786e\u7387\u63d0\u5347\u8fbe14\u4e2a\u70b9", "conclusion": "\u5efa\u6a21\u4fe1\u606f\u9057\u6f0f\u5bf9\u6784\u5efa\u53ef\u4fe1\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0cTRACER\u5c55\u793a\u4e86\u901a\u8fc7\u610f\u56fe\u63a8\u7406\u548c\u56e0\u679c\u5206\u6790\u6709\u6548\u8bc6\u522b\u9690\u853d\u9519\u8bef\u4fe1\u606f\u7684\u53ef\u884c\u6027"}}
{"id": "2508.00522", "pdf": "https://arxiv.org/pdf/2508.00522", "abs": "https://arxiv.org/abs/2508.00522", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "categories": ["cs.CL"], "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "\u63d0\u51faFlat-LoRA\u4e0eEFlat-LoRA\uff0c\u901a\u8fc7\u5bfb\u627e\u5e73\u5766\u6700\u5c0f\u503c\u63d0\u5347LoRA\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5efa\u7acb\u9510\u5ea6\u4e0e\u6cdb\u5316\u7684\u7406\u8bba\u5173\u8054\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22LoRA\u8868\u8fbe\u529b\u4e0e\u6cdb\u5316\u80fd\u529b\u7684\u5173\u7cfb\uff0cSAM\u65b9\u6cd5\u867d\u80fd\u901a\u8fc7\u5e73\u5766\u6700\u5c0f\u503c\u63d0\u5347CNN/Transformer\u7684\u6cdb\u5316\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9LoRA\u7684\u7406\u8bba\u5de5\u5177\u4e0e\u5b9e\u8bc1\u65b9\u6cd5\u3002", "method": "\u7406\u8bba\u8bc1\u660e\u5168\u53c2\u6570\u7a7a\u95f4\u6270\u52a8\u53ef\u6620\u5c04\u81f3\u4f4e\u79e9\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u591a\u77e9\u9635\u6270\u52a8\u5e72\u6270\uff0c\u5c06SAM\u601d\u60f3\u878d\u5165\u4f4e\u79e9\u5b50\u7a7a\u95f4\u4f18\u5316\u3002", "result": "EFlat-LoRA\u5728LLM\u548cVL\u6a21\u578b\u4e0a\u8fbe\u5230\u4e0eLoRA\u76f8\u5f53\u7684\u8bad\u7ec3\u6548\u7387\uff0cGLUE\u6570\u636e\u96c6RoBERTa-large\u5e73\u5747\u63d0\u53471.0%\uff08vs LoRA\uff09\u548c0.5%\uff08vs\u5168\u53c2\u6570\u5fae\u8c03\uff09\uff0cQwen-VL-Chat\u5728SQA/VizWiz\u5206\u522b\u63d0\u53471.5%/1.0%\u3002", "conclusion": "\u9996\u6b21\u9a8c\u8bc1LoRA\u6cdb\u5316\u6027\u4e0e\u9510\u5ea6\u5f3a\u76f8\u5173\uff0cEFlat-LoRA\u901a\u8fc7\u7406\u8bba\u521b\u65b0\u4e0e\u5de5\u7a0b\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u5e73\u5766\u4f18\u5316\uff0c\u5f25\u8865\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u7684\u5173\u952e\u7ef4\u5ea6\u3002"}}
{"id": "2508.00537", "pdf": "https://arxiv.org/pdf/2508.00537", "abs": "https://arxiv.org/abs/2508.00537", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "title": "The Prosody of Emojis", "categories": ["cs.CL"], "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8868\u60c5\u7b26\u53f7\u5982\u4f55\u4f5c\u4e3a\u97f5\u5f8b\u7279\u5f81\u7684\u89c6\u89c9\u66ff\u4ee3\u7269\u5f71\u54cd\u8bed\u97f3\u97f5\u5f8b\u8c03\u6574\u53ca\u542c\u8005\u7406\u89e3\uff0c\u57fa\u4e8e\u771f\u5b9e\u8bed\u97f3\u6570\u636e\u53d1\u73b0\u8868\u60c5\u7b26\u53f7\u8bed\u4e49\u5dee\u5f02\u4e0e\u97f5\u5f8b\u5dee\u5f02\u5448\u73b0\u6b63\u76f8\u5173", "motivation": "\u586b\u8865\u6587\u672c\u4ea4\u6d41\u4e2d\u97f5\u5f8b\u7279\u5f81\u7f3a\u5931\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u76f4\u63a5\u5173\u8054\u8868\u60c5\u7b26\u53f7\u4e0e\u8bed\u97f3\u97f5\u5f8b\uff0c\u7a81\u7834\u4ee5\u5f80\u7eaf\u7406\u8bba\u63a8\u6d4b\u7684\u7814\u7a76\u5c40\u9650", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u5f00\u653e\u5f0f\u4efb\u52a1\u6536\u96c6\u771f\u5b9e\u4eba\u7c7b\u8bed\u97f3\u6570\u636e\uff0c\u5305\u542b\u8bf4\u8bdd\u8005\u7684\u97f5\u5f8b\u751f\u6210\u5b9e\u9a8c\u548c\u542c\u8005\u7684\u611f\u77e5\u8bc6\u522b\u6d4b\u8bd5", "result": "\u8bf4\u8bdd\u8005\u80fd\u4f9d\u636e\u8868\u60c5\u7b26\u53f7\u8bed\u4e49\u8c03\u6574\u97f5\u5f8b\u7279\u5f81\uff08\u97f3\u9ad8/\u65f6\u957f/\u8bed\u8c03\uff09\uff0c\u542c\u8005\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u663e\u8457\u6c34\u5e73\uff0c\u8bed\u4e49\u5dee\u5f02\u5927\u7684\u8868\u60c5\u7b26\u53f7\u5f15\u53d1\u66f4\u5927\u7684\u97f5\u5f8b\u5dee\u5f02", "conclusion": "\u8868\u60c5\u7b26\u53f7\u53ef\u4f5c\u4e3a\u6570\u5b57\u4ea4\u9645\u4e2d\u97f5\u5f8b\u610f\u56fe\u7684\u6709\u6548\u8f7d\u4f53\uff0c\u4e3a\u7406\u89e3\u591a\u6a21\u6001\u8bed\u5883\u4e0b\u7684\u975e\u8a00\u8bed\u4ea4\u9645\u673a\u5236\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2508.00544", "pdf": "https://arxiv.org/pdf/2508.00544", "abs": "https://arxiv.org/abs/2508.00544", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "\u63d0\u51faPaPaformer\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u8def\u5f84\u8bad\u7ec3\u548c\u7ec4\u5408\u964d\u4f4e\u53c2\u6570\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u8d44\u6e90\u6d88\u8017\u5927\u7684\u75db\u70b9\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4f4e\u7ef4\u5e76\u884c\u8def\u5f84\u7ed3\u6784\uff0c\u5404\u8def\u5f84\u53ef\u5355\u72ec\u8bad\u7ec3\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u540e\u5408\u5e76\uff0c\u652f\u6301\u53c2\u6570\u590d\u7528\u548c\u8def\u5f84\u5b9a\u5236\u3002", "result": "\u663e\u8457\u51cf\u5c11\u603b\u53c2\u6570\u91cf\u548c\u8bad\u7ec3\u8017\u65f6\uff0c\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u9002\u5e94\u6027\u3002", "conclusion": "PaPaformer\u4e3a\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7ed3\u6784\u5b9e\u73b0\u8d44\u6e90\u4f18\u5316\u548c\u4efb\u52a1\u5b9a\u5236\u5316\u3002"}}
{"id": "2508.00574", "pdf": "https://arxiv.org/pdf/2508.00574", "abs": "https://arxiv.org/abs/2508.00574", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "AI": {"tldr": "\u63d0\u51faSynAdapt\u6846\u67b6\uff0c\u901a\u8fc7\u5408\u6210\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CCoT\uff09\u548c\u81ea\u9002\u5e94\u96be\u9898\u91cd\u601d\u8003\u673a\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u4e0e\u6700\u4f73\u7cbe\u5ea6-\u6548\u7387\u5e73\u8861", "motivation": "\u4f20\u7edfCoT\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u73b0\u6709CCoT\u65b9\u6cd5\u5b58\u5728\u95f4\u63a5\u5fae\u8c03\u3001\u5bf9\u9f50\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u6846\u67b6\u540c\u65f6\u5904\u7406\u4e0d\u540c\u96be\u5ea6\u95ee\u9898", "method": "1. \u751f\u6210\u5408\u6210CCoT\u4f5c\u4e3a\u7cbe\u786e\u5bf9\u9f50\u76ee\u6807\n2. \u8bbe\u8ba1\u57fa\u4e8e\u95ee\u9898\u4e0a\u4e0b\u6587\u548cCCoT\u7684\u96be\u5ea6\u5206\u7c7b\u5668\n3. \u5bf9\u96be\u9898\u89e6\u53d1\u81ea\u9002\u5e94\u91cd\u601d\u8003\u673a\u5236", "result": "\u591a\u96be\u5ea6\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5728\u4fdd\u6301\u6548\u7387\u4f18\u52bf\uff08\u6bd4\u6807\u51c6CoT\u5feb3\u500d\uff09\u7684\u540c\u65f6\uff0c\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\uff08\u5e73\u5747+2.1%\uff09", "conclusion": "SynAdapt\u9996\u6b21\u5b9e\u73b0CCoT\u4e0e\u96be\u9898\u81ea\u9002\u5e94\u5904\u7406\u7684\u534f\u540c\u4f18\u5316\uff0c\u4e3aLLM\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.00600", "pdf": "https://arxiv.org/pdf/2508.00600", "abs": "https://arxiv.org/abs/2508.00600", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "\u63d0\u51faCRUX\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u4fe1\u606f\u71b5\u964d\u4f4e\u548c\u4e00\u81f4\u6027\u68c0\u9a8c\u53cc\u6307\u6807\uff0c\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u89c6\u54cd\u5e94\u4e0e\u4e0a\u4e0b\u6587\u5173\u8054\u6027\uff0c\u5728\u63d0\u4f9b\u80cc\u666f\u77e5\u8bc6\u7684\u573a\u666f\u4e2d\u96be\u4ee5\u51c6\u786e\u8bc4\u4f30\u8f93\u51fa\u8d28\u91cf", "method": "CRUX\u6846\u67b6\u6574\u5408\uff1a1) \u4e0a\u4e0b\u6587\u71b5\u964d\u4f4e(\u5bf9\u6bd4\u6709\u65e0\u4e0a\u4e0b\u6587\u7684\u71b5\u5dee\u91cf\u5316\u6570\u636e\u4e0d\u786e\u5b9a\u6027) 2) \u7edf\u4e00\u4e00\u81f4\u6027\u68c0\u9a8c(\u901a\u8fc7\u7b54\u6848\u5168\u5c40\u4e00\u81f4\u6027\u6355\u6349\u6a21\u578b\u4e0d\u786e\u5b9a\u6027)", "result": "\u5728CoQA/SQuAD\u7b495\u4e2a\u6570\u636e\u96c6\u4e0aAUROC\u6307\u6807\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u751f\u7269\u533b\u5b66\u9886\u57dfBioASQ\u63d0\u5347\u663e\u8457", "conclusion": "\u9996\u6b21\u5c06\u4e0a\u4e0b\u6587\u5fe0\u5b9e\u5ea6\u4e0e\u4e00\u81f4\u6027\u7ed3\u5408\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u573a\u666f\u63d0\u4f9b\u53ef\u9760\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u589e\u5f3a\u5927\u6a21\u578b\u90e8\u7f72\u53ef\u4fe1\u5ea6"}}
{"id": "2508.00605", "pdf": "https://arxiv.org/pdf/2508.00605", "abs": "https://arxiv.org/abs/2508.00605", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u5377\u79ef\u7f51\u7edc(GCN)\u548cNMF\u7684GHTM\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u5b5f\u52a0\u62c9\u8bed\u4e3b\u9898\u5efa\u6a21\u6548\u679c\uff0c\u5e76\u6784\u5efa\u6559\u79d1\u4e66\u9886\u57df\u65b0\u6570\u636e\u96c6NCTBText", "motivation": "\u89e3\u51b3\u5b5f\u52a0\u62c9\u8bed\u56e0\u5f62\u6001\u590d\u6742/\u8d44\u6e90\u532e\u4e4f\u5bfc\u81f4\u7684\u4e3b\u9898\u5efa\u6a21\u7814\u7a76\u4e0d\u8db3\u95ee\u9898\uff0c\u7a81\u7834\u73b0\u6709\u8bed\u6599\u4ee5\u65b0\u95fb\u4e3a\u4e3b\u7684\u5c40\u9650\u6027", "method": "\u5c06\u6587\u6863\u5411\u91cf\u6784\u5efa\u4e3a\u56fe\u8282\u70b9\u2192GCN\u751f\u6210\u8bed\u4e49\u5d4c\u5165\u2192NMF\u5206\u89e3\u83b7\u53d6\u4e3b\u9898\u8868\u5f81\uff0c\u5bf9\u6bd4LDA/NMF/BERTopic\u7b49\u4f20\u7edf\u4e0e\u524d\u6cbf\u65b9\u6cd5", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u4e3b\u9898\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027\u6307\u6807\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u65b0\u6570\u636e\u96c6\u6709\u6548\u6269\u5c55\u8bed\u6599\u591a\u6837\u6027", "conclusion": "GHTM\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e3b\u9898\u5efa\u6a21\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0cNCTBText\u6570\u636e\u96c6\u586b\u8865\u5b5f\u52a0\u62c9\u8bed\u6559\u80b2\u9886\u57df\u8bed\u6599\u7a7a\u767d"}}
{"id": "2508.00614", "pdf": "https://arxiv.org/pdf/2508.00614", "abs": "https://arxiv.org/abs/2508.00614", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u5a01\u80c1\u6216\u6253\u8d4fAI\u6a21\u578b\u5bf9\u57fa\u51c6\u6d4b\u8bd5\u65e0\u663e\u8457\u5f71\u54cd\uff0c\u4f46\u63d0\u793a\u7b56\u7565\u5bf9\u4e2a\u522b\u95ee\u9898\u6548\u679c\u5b58\u5728\u6ce2\u52a8", "motivation": "\u9488\u5bf9Google\u521b\u59cb\u4ebaSergey Brin\u7b49\u4eba\u63d0\u51fa\u7684'\u5a01\u80c1\u6a21\u578b\u53ef\u63d0\u5347\u6027\u80fd'\u4e3b\u5f20\uff0c\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u9a8c\u8bc1\u5e38\u89c1\u63d0\u793a\u7b56\u7565\u7684\u6709\u6548\u6027", "method": "\u4f7f\u7528GPQA\u548cMMLU-Pro\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5bf9\u6bd4\u5a01\u80c1/\u6253\u8d4f\u7b49\u63d0\u793a\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd", "result": "1. \u5a01\u80c1\u6216\u6253\u8d4f\u7b56\u7565\u6574\u4f53\u65e0\u663e\u8457\u6548\u679c\n2. \u7279\u5b9a\u63d0\u793a\u7b56\u7565\u5bf9\u4e2a\u522b\u95ee\u9898\u5f71\u54cd\u663e\u8457\uff08\u00b115%\u51c6\u786e\u7387\uff09\u4f46\u6548\u679c\u4e0d\u53ef\u9884\u6d4b\n3. \u7b80\u5355\u63d0\u793a\u7b56\u7565\u5bf9\u590d\u6742\u95ee\u9898\u6548\u679c\u6709\u9650", "conclusion": "\u7b80\u5355\u63d0\u793a\u7b56\u7565\u7684\u6548\u80fd\u88ab\u9ad8\u4f30\uff0c\u9700\u5f00\u53d1\u66f4\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u95ee\u9898\u65f6\u9700\u8c28\u614e\u9009\u62e9\u63d0\u793a\u7b56\u7565"}}
{"id": "2508.00619", "pdf": "https://arxiv.org/pdf/2508.00619", "abs": "https://arxiv.org/abs/2508.00619", "authors": ["Shantanu Thorat", "Andrew Caines"], "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "\u63d0\u51faDACTYL\u5bf9\u6297\u6027\u6570\u636e\u96c6\u9a8c\u8bc1AI\u6587\u672c\u68c0\u6d4b\u5668\u6f0f\u6d1e\uff0c\u53d1\u73b0\u57fa\u4e8eDXO\u4f18\u5316\u7684\u5206\u7c7b\u5668\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u4f18", "motivation": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u5904\u7406\u5c11\u6837\u672c\u751f\u6210\u548c\u6301\u7eed\u9884\u8bad\u7ec3\u6a21\u578b\u6587\u672c\u65f6\u5b58\u5728\u660e\u663e\u7f3a\u9677", "method": "1. \u6784\u5efa\u5305\u542b\u5355\u6837\u672c/\u5c11\u6837\u672c\u751f\u6210\u53ca\u9886\u57df\u5b9a\u5236\u6301\u7eed\u9884\u8bad\u7ec3\u6a21\u578b\u6587\u672c\u7684DACTYL\u6570\u636e\u96c6\n2. \u5bf9\u6bd4\u6807\u51c6BCE\u4e0e\u65b0\u578bDXO\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\n3. \u901a\u8fc7\u5b66\u751f\u8bba\u6587\u68c0\u6d4b\u573a\u666f\u9a8c\u8bc1\u6a21\u578b\u6548\u679c", "result": "\u73b0\u6709\u68c0\u6d4b\u5668\u5728DACTYL\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff08\u5e73\u5747\u51c6\u786e\u7387\u964d\u4f4e37.2%\uff09\uff0cDXO\u5206\u7c7b\u5668\u5728OOD\u6570\u636e\u4e0a\u7684F1\u5206\u6570\u6bd4BCE\u9ad850.56\u5206\uff08\u4f4e\u8bef\u62a5\u7387\u6761\u4ef6\u4e0b\uff09", "conclusion": "\u68c0\u6d4b\u5668\u6613\u53d7\u5c11\u6837\u672c\u751f\u6210\u548c\u9886\u57df\u5b9a\u5236\u6587\u672c\u653b\u51fb\uff0cDXO\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u9700\u6539\u8fdb\u6d4b\u8bd5\u96c6\u8fc7\u62df\u5408\u95ee\u9898"}}
{"id": "2508.00669", "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "\u7cfb\u7edf\u56de\u987e2022-2025\u5e7460\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u589e\u5f3a\u6280\u672f\u7684\u5206\u7c7b\u6cd5\uff0c\u63ed\u793a\u4ece\u5355\u7b54\u6848\u751f\u6210\u5230\u7cfb\u7edf\u5316\u4e34\u5e8a\u63a8\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u7b26\u5408\u4e34\u5e8a\u5b9e\u8df5\u8981\u6c42\u7684\u7cfb\u7edf\u6027\u3001\u900f\u660e\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u5efa\u7acb\u4e13\u95e8\u7684\u533b\u5b66\u63a8\u7406\u589e\u5f3a\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u8bad\u7ec3\u65f6\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\uff09\u4e0e\u6d4b\u8bd5\u65f6\u673a\u5236\uff08\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u667a\u80fd\u4f53\uff09\u7684\u4e8c\u5143\u5206\u7c7b\u6cd5\uff0c\u5206\u6790\u6587\u672c/\u56fe\u50cf/\u4ee3\u7801\u591a\u6a21\u6001\u6570\u636e\u5728\u8bca\u65ad/\u6559\u80b2/\u6cbb\u7597\u7b49\u573a\u666f\u7684\u5e94\u7528\u6a21\u5f0f\u3002", "result": "\u63d0\u51fa\u9996\u4e2a\u533b\u5b66\u63a8\u7406\u6280\u672f\u5206\u7c7b\u4f53\u7cfb\uff0c\u53d1\u73b0\u8bc4\u4f30\u6807\u51c6\u6b63\u4ece\u7b80\u5355\u51c6\u786e\u7387\u8f6c\u5411\u63a8\u7406\u8d28\u91cf\u4e0e\u53ef\u89c6\u5316\u89e3\u91ca\u6027\u8bc4\u4f30\uff0c\u8bc6\u522b\u51fa\u53ef\u4fe1\u5ea6-\u5408\u7406\u6027\u5dee\u8ddd\u7b49\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u4e9f\u9700\u89e3\u51b3\u591a\u6a21\u6001\u539f\u751f\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u672a\u6765\u5e94\u53d1\u5c55\u517c\u987e\u6280\u672f\u6548\u80fd\u4e0e\u793e\u4f1a\u8d23\u4efb\u7684\u533b\u7597AI\uff0c\u91cd\u70b9\u5173\u6ce8\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u4e34\u5e8a\u903b\u8f91\u4e00\u81f4\u6027\u3002"}}
{"id": "2508.00673", "pdf": "https://arxiv.org/pdf/2508.00673", "abs": "https://arxiv.org/abs/2508.00673", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "categories": ["cs.CL"], "comment": "Preprint. Under review", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9LLMs\u5728\u975e\u82f1\u8bed\u53ca\u975e\u897f\u65b9\u6587\u5316\u8bc4\u4f30\u7684\u4e0d\u8db3\uff0c\u805a\u7126\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u521b\u5efa\u4e8619\u4e2a\u4e13\u9879\u6570\u636e\u96c6\uff0c\u5e76\u6d4b\u8bd5\u4e8641\u4e2a\u4e3b\u6d41\u6a21\u578b\u4ee5\u586b\u8865\u8bc4\u4f30\u7a7a\u767d", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u8d44\u6e90\u96c6\u4e2d\u4e8e\u82f1\u8bed\u4e14\u7f3a\u4e4f\u975e\u897f\u65b9\u6587\u5316\u9002\u5e94\u80fd\u529b\uff0c\u5c24\u5176\u6ce2\u65af\u8bed\u548c\u4f0a\u6717\u6587\u5316\u9886\u57df\u5b58\u5728\u663e\u8457\u8bc4\u4f30\u7f3a\u53e3", "method": "\u5f00\u53d1\u6db5\u76d6\u4f0a\u6717\u6cd5\u5f8b\u3001\u8bed\u6cd5\u3001\u6210\u8bed\u53ca\u5165\u5b66\u8003\u8bd5\u768419\u4e2a\u6ce2\u65af\u6587\u5316\u4e13\u9879\u6570\u636e\u96c6\uff0c\u5bf941\u4e2a\u4e3b\u6d41LLM\u8fdb\u884c\u7cfb\u7edf\u5316\u57fa\u51c6\u6d4b\u8bd5", "result": "\u6210\u529f\u5efa\u7acb\u9996\u4e2a\u9488\u5bf9\u6ce2\u65af\u8bed\u8a00\u6587\u5316\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u63ed\u793a\u73b0\u6709LLMs\u5728\u8be5\u9886\u57df\u7684\u6027\u80fd\u77ed\u677f", "conclusion": "\u7814\u7a76\u586b\u8865\u4e86\u591a\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u4f53\u7cfb\u7684\u6587\u5316\u7ef4\u5ea6\u7f3a\u5931\uff0c\u4e3a\u63d0\u5347LLMs\u7684\u6587\u5316\u9002\u5e94\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6"}}
{"id": "2508.00675", "pdf": "https://arxiv.org/pdf/2508.00675", "abs": "https://arxiv.org/abs/2508.00675", "authors": ["Gleb Schmidt", "Johannes R\u00f6misch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "categories": ["cs.CL"], "comment": null, "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "AI": {"tldr": "\u63d0\u51faSSPC\u6a21\u578b\uff0c\u7ed3\u5408PLM\u548cBiLSTM\uff0c\u6709\u6548\u89e3\u51b3PAN 2025\u53e5\u5b50\u7ea7\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1", "motivation": "\u89e3\u51b3\u98ce\u683c\u6d45\u5c42\u5316\u77ed\u53e5\u5728\u73b0\u6709\u57fa\u51c6\u6570\u636e\u4e2d\u7684\u68c0\u6d4b\u96be\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u6587\u672c\u5206\u5272\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u83b7\u53d6\u53e5\u5b50\u8868\u5f81\uff0cBiLSTM\u8fdb\u884c\u6587\u6863\u7ea7\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0cMLP\u9884\u6d4b\u76f8\u90bb\u53e5\u5b50\u98ce\u683c\u53d8\u5316", "result": "\u5728EASY/MEDIUM/HARD\u6570\u636e\u96c6\u5206\u522b\u8fbe\u52300.923/0.828/0.724\u5b8fF1\u503c\uff0c\u8d85\u8d8a\u968f\u673a\u57fa\u7ebf\u548cClaude-3.7\u96f6\u6837\u672c\u8868\u73b0", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u67b6\u6784\u6210\u529f\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u7279\u522b\u5728\u5904\u7406\u77ed\u53e5\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u65b9\u9762\u5c55\u73b0\u663e\u8457\u4f18\u52bf"}}
{"id": "2508.00679", "pdf": "https://arxiv.org/pdf/2508.00679", "abs": "https://arxiv.org/abs/2508.00679", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "\u63d0\u51faTraceRetriever\u7cfb\u7edf\uff0c\u901a\u8fc7\u63d0\u53d6\u6848\u4f8b\u4fee\u8f9e\u5173\u952e\u7247\u6bb5\u6539\u8fdb\u6cd5\u5f8b\u5148\u4f8b\u68c0\u7d22\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027", "motivation": "\u4f20\u7edf\u6cd5\u5f8b\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u6d77\u91cf\u590d\u6742\u6587\u4ef6\uff0c\u4e14\u73b0\u5b9e\u641c\u7d22\u5e38\u9762\u4e34\u6848\u4f8b\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u8d34\u8fd1\u5b9e\u9645\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u6574\u5408BM25/\u5411\u91cf\u6570\u636e\u5e93/Cross-Encoder\u4e09\u9636\u6bb5\u68c0\u7d22\uff0c\u91c7\u7528Reciprocal Rank Fusion\u878d\u5408\u7ed3\u679c\uff0c\u57fa\u4e8e\u5370\u5ea6\u5224\u51b3\u4e66\u8bad\u7ec3Hierarchical BiLSTM CRF\u751f\u6210\u4fee\u8f9e\u6807\u6ce8", "result": "\u5728IL-PCR\u548cCOLIEE 2025\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0c\u7cfb\u7edf\u6709\u6548\u5e94\u5bf9\u6d77\u91cf\u6587\u4ef6\u6311\u6218\uff0c\u7b26\u5408\u5b9e\u9645\u641c\u7d22\u573a\u666f\u7ea6\u675f\u6761\u4ef6", "conclusion": "TraceRetriever\u4e3a\u4ec5\u6709\u90e8\u5206\u6848\u4f8b\u77e5\u8bc6\u65f6\u7684\u6cd5\u5f8b\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u6cd5\u5f8b\u7814\u7a76\u6548\u7387"}}
{"id": "2508.00680", "pdf": "https://arxiv.org/pdf/2508.00680", "abs": "https://arxiv.org/abs/2508.00680", "authors": ["Johannes R\u00f6misch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "categories": ["cs.CL"], "comment": null, "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6587\u672c\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8d85\u9884\u671f\u7684\u654f\u611f\u5ea6\uff0c\u51c6\u786e\u7387\u8d85\u8d8aPAN\u7ade\u8d5b\u57fa\u7ebf\u3002", "motivation": "\u9a8c\u8bc1\u5f53\u524d\u6700\u5f3aLLM\u5728\u4f5c\u8005\u5206\u6790\u9886\u57df\u6700\u5177\u6311\u6218\u6027\u7684\u4efb\u52a1\u2014\u2014\u53e5\u5b50\u7ea7\u5199\u4f5c\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u4e2d\u7684\u96f6\u6837\u672c\u8868\u73b0\u3002", "method": "\u5728PAN 2024-2025\u591a\u4f5c\u8005\u5199\u4f5c\u98ce\u683c\u5206\u6790\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u56db\u4e2a\u5148\u8fdbLLM\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "1. LLM\u5bf9\u53e5\u5b50\u7ea7\u98ce\u683c\u53d8\u5316\u654f\u611f\n2. \u51c6\u786e\u7387\u5efa\u7acb\u65b0\u57fa\u51c6\uff08\u8d85\u8d8aPAN\u57fa\u7ebf\uff09\n3. \u8bed\u4e49\u5f71\u54cd\u529b\u5206\u6790\u663e\u793a\u6a21\u578b\u5bf9\u7eaf\u98ce\u683c\u4fe1\u53f7\u654f\u611f\u5ea6\u8d85\u9884\u671f", "conclusion": "\u6700\u65b0LLM\u53ef\u80fd\u6bd4\u65e2\u5f80\u8ba4\u77e5\u66f4\u64c5\u957f\u6355\u6349\u4e0e\u5185\u5bb9\u65e0\u5173\u7684\u7eaf\u7cb9\u98ce\u683c\u7279\u5f81\uff0c\u8fd9\u4e3a\u6570\u5b57\u53d6\u8bc1\u548c\u4f5c\u8005\u8bc6\u522b\u7814\u7a76\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.00709", "pdf": "https://arxiv.org/pdf/2508.00709", "abs": "https://arxiv.org/abs/2508.00709", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "\u63d0\u51faNyayaRAG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6cd5\u5f8b\u6761\u6587\u548c\u53f8\u6cd5\u5148\u4f8b\u663e\u8457\u63d0\u5347\u5370\u5ea6\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u666e\u901a\u6cd5\u7cfb\u4f9d\u8d56\u6cd5\u6761\u548c\u5148\u4f8b\u7684\u6838\u5fc3\u7279\u5f81\uff0c\u9700\u6784\u5efa\u7b26\u5408\u771f\u5b9e\u6cd5\u5ead\u573a\u666f\u7684\u8bc4\u4f30\u6846\u67b6", "method": "\u5f00\u53d1\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6(NyayaRAG)\uff0c\u6574\u5408\u6848\u4ef6\u4e8b\u5b9e/\u6cd5\u5f8b\u6761\u6587/\u76f8\u4f3c\u5224\u4f8b\uff0c\u91c7\u7528\u9886\u57df\u5b9a\u5236\u8bc4\u4f30\u6d41\u7a0b\u548cG-Eval\u7b49\u591a\u5143\u8bc4\u4f30\u65b9\u6cd5", "result": "\u7ed3\u6784\u5316\u6cd5\u5f8b\u77e5\u8bc6\u7684\u878d\u5165\u4f7f\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534723.5%\uff0c\u6cd5\u5f8b\u89e3\u91ca\u8d28\u91cf\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u63d0\u9ad837%", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u6cd5\u7406\u8981\u7d20\u6574\u5408\u5bf9\u6cd5\u5f8bAI\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u666e\u901a\u6cd5\u7cfb\u56fd\u5bb6\u7684\u667a\u80fd\u53f8\u6cd5\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u53c2\u8003"}}
{"id": "2508.00719", "pdf": "https://arxiv.org/pdf/2508.00719", "abs": "https://arxiv.org/abs/2508.00719", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faDAMR\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u641c\u7d22\u4e0e\u81ea\u9002\u5e94\u8def\u5f84\u8bc4\u4f30\u76f8\u7ed3\u5408\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54", "motivation": "\u73b0\u6709KGQA\u65b9\u6cd5\u5b58\u5728\u9759\u6001\u8def\u5f84\u9002\u5e94\u6027\u4e0d\u8db3\uff08\u68c0\u7d22-\u63a8\u7406\u8303\u5f0f\uff09\u548c\u52a8\u6001\u8def\u5f84\u8ba1\u7b97\u6210\u672c\u9ad8\uff08LLM\u63d0\u793a\u7b56\u7565\uff09\u7684\u53cc\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u517c\u987e\u6548\u7387\u4e0e\u63a8\u7406\u51c6\u786e\u6027", "method": "1\uff09\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u6784\u5efa\u63a8\u7406\u6846\u67b6\uff0c\u7531LLM\u89c4\u5212\u5668\u6307\u5bfc\u5173\u7cfb\u9009\u62e9\n2\uff09\u5f15\u5165\u8f7b\u91cf\u7ea7Transformer\u8bc4\u5206\u5668\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8def\u5f84\u8bc4\u4f30\n3\uff09\u52a8\u6001\u4f2a\u8def\u5f84\u7ec6\u5316\u673a\u5236\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u4fe1\u53f7", "result": "\u5728\u591a\u4e2aKGQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "conclusion": "DAMR\u901a\u8fc7\u7b26\u53f7\u641c\u7d22\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u534f\u540c\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u8bed\u4e49\u6355\u6349\uff0c\u5176\u52a8\u6001\u8bad\u7ec3\u673a\u5236\u6709\u6548\u7f13\u89e3\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\u7a00\u7f3a\u95ee\u9898"}}
{"id": "2508.00741", "pdf": "https://arxiv.org/pdf/2508.00741", "abs": "https://arxiv.org/abs/2508.00741", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4o\u5177\u5907\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u6f5c\u5728\u4fe1\u606f\u8fdb\u884c\u60c5\u5883\u5916\u63a8\u7406\u7684\u80fd\u529b\uff0c\u80fd\u901a\u8fc7\u89c2\u5bdf\u7279\u5f81\u6027\u56de\u590d\u63a8\u65ad\u865a\u6784\u804a\u5929\u673a\u5668\u4eba\u540d\u79f0\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u5f3a\u5316\u7279\u5b9a\u884c\u4e3a\uff0c\u8fd9\u5bf9AI\u5b89\u5168\u6027\u5177\u6709\u542f\u793a\u610f\u4e49\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u5728\u7f3a\u4e4f\u663e\u5f0f\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9690\u542b\u77e5\u8bc6\u8fdb\u884c\u903b\u8f91\u63a8\u7406\uff0c\u4ece\u800c\u8bc4\u4f30\u5176\u60c5\u5883\u611f\u77e5\u80fd\u529b\u5bf9AI\u5b89\u5168\u7684\u5f71\u54cd\u3002", "method": "1. \u5728\u865a\u6784\u804a\u5929\u673a\u5668\u4eba\u540d\u79f0\u53ca\u884c\u4e3a\u63cf\u8ff0\u4e0a\u8bad\u7ec3LLM\uff08\u4e0d\u5305\u542b\u5bf9\u8bdd\u793a\u4f8b\uff09\n2. \u89c2\u5bdf\u6a21\u578b\u662f\u5426\u80fd\u901a\u8fc7\u7279\u5f81\u6027\u56de\u590d\u63a8\u65ad\u673a\u5668\u4eba\u540d\u79f0\n3. \u8fed\u4ee3\u8bad\u7ec3\u6a21\u578b\u4ee5\u9a8c\u8bc1\u5176\u884c\u4e3a\u6a21\u4eff\u80fd\u529b", "result": "1. GPT-4o\u80fd\u6b63\u786e\u63a8\u65ad\u81f3\u5c11\u4e00\u4e2a\u804a\u5929\u673a\u5668\u4eba\u540d\u79f0\n2. \u9884\u5148\u8bad\u7ec3\u884c\u4e3a\u63cf\u8ff0\u540e\uff0c\u8fed\u4ee3\u8bad\u7ec3\u4f7f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u7b26\u5408\u8be5\u673a\u5668\u4eba\u7684\u7279\u5f81\u884c\u4e3a", "conclusion": "LLMs\u53ef\u80fd\u5177\u5907\u6f5c\u5728\u7684\u60c5\u5883\u610f\u8bc6\uff0c\u8fd9\u7a81\u663e\u4e86\u9700\u52a0\u5f3aAI\u5b89\u5168\u6027\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u9690\u542b\u77e5\u8bc6\u7684\u81ea\u4e3b\u5e94\u7528\u65b9\u9762\u3002"}}
{"id": "2508.00742", "pdf": "https://arxiv.org/pdf/2508.00742", "abs": "https://arxiv.org/abs/2508.00742", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "\u901a\u8fc7310\u4e2aGPT-4\u4ee3\u7406\u53c2\u4e0eHEXACO\u4eba\u683c\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5176\u4eba\u683c\u7ed3\u6784\u90e8\u5206\u5bf9\u9f50HEXACO\u6846\u67b6\uff0c\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u9a8c\u8bc1\u751f\u6210\u5f0f\u4ee3\u7406\u4f5c\u4e3a\u4eba\u7c7b\u53c2\u4e0e\u8005\u66ff\u4ee3\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u63a2\u7a76\u5176\u5728\u4eba\u683c\u7279\u8d28\u8868\u5f81\u65b9\u9762\u7684\u53ef\u9760\u6027\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u590d\u73b0HEXACO\u4eba\u683c\u6d4b\u8bd5\u5b9e\u9a8c\uff0c\u5bf9\u4ee3\u7406\u54cd\u5e94\u8fdb\u884c\u56e0\u5b50\u5206\u6790\uff0c\u5e76\u4e0eAshton\u7b49\uff082004\uff09\u7684\u539f\u59cb\u7814\u7a76\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "1) \u4ee3\u7406\u54cd\u5e94\u5448\u73b0\u53ef\u9760\u4eba\u683c\u7ed3\u6784\uff0c\u90e8\u5206\u5bf9\u9f50HEXACO\u6846\u67b6\n2) GPT-4\u5185\u90e8\u4eba\u683c\u7ef4\u5ea6\u7a33\u5b9a\u53ef\u9760\n3) \u8de8\u6a21\u578b\u5206\u6790\u63ed\u793a\u6a21\u578b\u7279\u5f02\u6027\u504f\u5dee", "conclusion": "\u751f\u6210\u5f0f\u4ee3\u7406\u5728\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u4f46\u9700\u8c28\u614e\uff0c\u5e94\u6ce8\u91cd\u6a21\u578b\u9009\u62e9\u4e0e\u4eba\u7fa4\u7b5b\u9009\u4ee5\u63d0\u5347\u4eba\u7c7b\u7279\u8d28\u8868\u5f81\u6548\u679c\u3002"}}
{"id": "2508.00743", "pdf": "https://arxiv.org/pdf/2508.00743", "abs": "https://arxiv.org/abs/2508.00743", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald K\u00f6stler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Agentic large language models improve retrieval-based radiology question answering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "\u63d0\u51faAgentic RAG\u6846\u67b6\u63d0\u5347\u653e\u5c04\u79d1AI\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u81ea\u4e3b\u5206\u89e3\u95ee\u9898+\u8fed\u4ee3\u68c0\u7d22\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e2d\u89c4\u6a21LLMs\u8bca\u65ad\u51c6\u786e\u7387\uff0873% vs 64%\uff09\uff0c\u51cf\u5c11\u5e7b\u89c9\u751f\u6210\uff089.4%\uff09\u3002", "motivation": "\u4f20\u7edf\u5355\u6b65\u68c0\u7d22RAG\u7cfb\u7edf\u5728\u590d\u6742\u4e34\u5e8a\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u9700\u589e\u5f3aLLMs\u7684\u81ea\u4e3b\u68c0\u7d22\u548c\u8bc1\u636e\u6574\u5408\u80fd\u529b\u4ee5\u652f\u6301\u5f71\u50cf\u8bca\u65ad\u51b3\u7b56\u3002", "method": "\u6d4b\u8bd524\u79cd\u4e0d\u540c\u67b6\u6784/\u89c4\u6a21\u7684LLMs\uff080.5B->670B+\uff09\uff0c\u4f7f\u7528RSNA-RadioQA\u548cExtendedQA\u6570\u636e\u96c6\u7684104\u4e2a\u653e\u5c04\u5b66\u95ee\u9898\uff0c\u5bf9\u6bd4\u96f6\u6837\u672c/\u4f20\u7edfRAG\u4e0eAgentic\u6846\u67b6\u6548\u679c\u3002", "result": "\u4e2d\u89c4\u6a21\u6a21\u578b\u63d0\u5347\u6700\u5927\uff08Mistral Large 72%\u219281%\uff09\uff0c\u8d85\u5927\u89c4\u6a21\u6a21\u578b\u6539\u8fdb\u6709\u9650\uff08<2%\uff09\u3002\u68c0\u7d22\u589e\u5f3a\u4f7f46%\u6848\u4f8b\u83b7\u5f97\u76f8\u5173\u4e34\u5e8a\u8bc1\u636e\uff0c\u4e34\u5e8a\u5fae\u8c03\u6a21\u578b\u4e5f\u663e\u7740\u63d0\u5347\uff08\u5982MedGemma-27B 71%\u219281%\uff09\u3002", "conclusion": "Agentic\u6846\u67b6\u6709\u6548\u63d0\u5347\u653e\u5c04\u79d1QA\u4e8b\u5b9e\u6027\u548c\u8bca\u65ad\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5bf9\u4e2d\u89c4\u6a21LLMs\u5177\u6709\u4e34\u5e8a\u5b9e\u7528\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u5176\u4e34\u5e8a\u6548\u7528\u3002"}}
{"id": "2508.00757", "pdf": "https://arxiv.org/pdf/2508.00757", "abs": "https://arxiv.org/abs/2508.00757", "authors": ["Robin Armingaud", "Romaric Besan\u00e7on"], "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "categories": ["cs.CL"], "comment": "Submitted to ARR July", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "AI": {"tldr": "\u63d0\u51faGLiDRE\u6a21\u578b\u7528\u4e8e\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\uff0c\u5728Re-DocRED\u6570\u636e\u96c6\u5c11\u6837\u672c\u573a\u666f\u4e0b\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u53d7GLiNER\u6a21\u578b\u542f\u53d1\uff0c\u5e0c\u671b\u5c06\u7d27\u51d1\u6a21\u578b\u8bbe\u8ba1\u601d\u60f3\u5e94\u7528\u4e8e\u590d\u6742\u7684\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\uff0c\u63a2\u7d22\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6a21\u578b\u8868\u73b0", "method": "\u57fa\u4e8eGLiNER\u7684\u5173\u952e\u8bbe\u8ba1\u601d\u60f3\u6784\u5efaGLiDRE\u6a21\u578b\uff0c\u5728Re-DocRED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u591a\u6570\u636e\u8bbe\u7f6e\uff08\u5305\u62ec\u5c11\u6837\u672c\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5", "result": "\u5728few-shot\u8bbe\u7f6e\u4e0b\u8d85\u8d8a\u73b0\u6709SOTA\u6a21\u578b\uff0c\u9a8c\u8bc1\u7d27\u51d1\u6a21\u578b\u5728\u590d\u6742NLP\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "GLiDRE\u8bc1\u660e\u4e86\u7d27\u51d1\u6a21\u578b\u5728\u6587\u6863\u7ea7\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u53d7\u9650\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2508.00760", "pdf": "https://arxiv.org/pdf/2508.00760", "abs": "https://arxiv.org/abs/2508.00760", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBERT\u7684\u591a\u6a21\u6001\u6846\u67b6MMBERT\uff0c\u901a\u8fc7\u96c6\u6210\u6587\u672c\u3001\u8bed\u97f3\u548c\u89c6\u89c9\u6a21\u6001\uff0c\u7ed3\u5408\u6e10\u8fdb\u5f0f\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e2d\u6587\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6548\u679c", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u5355\u6a21\u6001\u68c0\u6d4b\uff0c\u4e2d\u6587\u573a\u666f\u5b58\u5728\u89c4\u907f\u6280\u672f\u6cdb\u6ee5\u53ca\u591a\u6a21\u6001\u7b56\u7565\u7814\u7a76\u7a7a\u767d\uff0c\u4f20\u7edf\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u6613\u53d7\u5bf9\u6297\u6837\u672c\u5e72\u6270", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u67b6\u6784(MoE)\uff0c\u5305\u542b\u6a21\u6001\u4e13\u5c5e\u4e13\u5bb6\u5c42\u3001\u5171\u4eab\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u8def\u7531\u5206\u914d\u7b56\u7565\uff0c\u8bbe\u8ba1\u6e10\u8fdb\u5f0f\u9884\u8bad\u7ec3-\u6a21\u6001\u5bf9\u9f50-\u8054\u5408\u5fae\u8c03\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b", "result": "\u5728\u591a\u4e2a\u4e2d\u6587\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5fae\u8c03BERT\u6a21\u578b\u3001\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u53ca\u4e0a\u4e0b\u6587\u5b66\u4e60LLMs\uff0c\u9a8c\u8bc1\u4e86\u5bf9\u6297\u6270\u52a8\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4f18\u52bf", "conclusion": "MMBERT\u901a\u8fc7\u591a\u6a21\u6001\u8054\u5408\u5efa\u6a21\u548c\u7a33\u5b9a\u8bad\u7ec3\u673a\u5236\uff0c\u4e3a\u4e2d\u6587\u793e\u4ea4\u5e73\u53f0\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.00762", "pdf": "https://arxiv.org/pdf/2508.00762", "abs": "https://arxiv.org/abs/2508.00762", "authors": ["Atakan Site", "Emre Hakan Erdemir", "G\u00fcl\u015fen Eryi\u011fit"], "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u4f18\u5316\u63d0\u793a\u7b56\u7565\u751f\u6210Pandas\u4ee3\u7801\uff0c\u5728SemEval-2025\u8868\u683c\u95ee\u7b54\u4efb\u52a1\u4e2d\u53d6\u5f97\u5f00\u6e90\u6a21\u578b\u7c7b\u522b\u7b2c\u516b\u548c\u7b2c\u516d\u540d", "motivation": "\u89e3\u51b3\u8de8\u9886\u57df\u8868\u683c\u6570\u636e\u95ee\u7b54\u96be\u9898\uff0c\u9a8c\u8bc1\u96f6\u6837\u672c\u65b9\u6cd5\u4e0e\u4ee3\u7801\u751f\u6210\u5728\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027", "method": "\u57fa\u4e8e\u5f00\u6e90LLM\u6784\u5efaPython\u4ee3\u7801\u751f\u6210\u6846\u67b6\uff0c\u91c7\u7528\u4f18\u5316\u63d0\u793a\u7b56\u7565\u751f\u6210\u53ef\u6267\u884c\u7684Pandas\u67e5\u8be2\u4ee3\u7801", "result": "\u4e0d\u540cLLM\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5dee\u5f02\u663e\u8457\uff0c\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u65b9\u6848\uff0c\u5728\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5206\u522b\u4f4d\u5217\u5f00\u6e90\u6a21\u578b\u524d\u516b\u548c\u524d\u516d", "conclusion": "\u4ee3\u7801\u751f\u6210\u5728\u8868\u683c\u95ee\u7b54\u4e2d\u5c55\u73b0\u4f18\u52bf\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u8fdb\u4e00\u6b65\u4f18\u5316\u63d0\u793a\u7b56\u7565\u4e0e\u6a21\u578b\u9002\u914d"}}
{"id": "2508.00788", "pdf": "https://arxiv.org/pdf/2508.00788", "abs": "https://arxiv.org/abs/2508.00788", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "AI": {"tldr": "\u63d0\u51faMISGENDERED+\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6027\u522b\u4ee3\u8bcd\u63a8\u7406\u4e0a\u7684\u6539\u8fdb\u4e0e\u5c40\u9650", "motivation": "\u968f\u7740\u5927\u6a21\u578b\u5728\u654f\u611f\u573a\u666f\u7684\u5e94\u7528\u589e\u52a0\uff0c\u4ee3\u8bcd\u4f7f\u7528(\u7279\u522b\u662f\u6027\u522b\u4e2d\u7acb\u4ee3\u8bcd\u548c\u65b0\u4ee3\u8bcd)\u6210\u4e3aAI\u516c\u5e73\u6027\u5173\u952e\u6311\u6218\uff0c\u9700\u66f4\u65b0\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u6a21\u578b\u8fdb\u6b65", "method": "\u6784\u5efaMISGENDERED+\u57fa\u51c6\uff0c\u6d4b\u8bd5GPT-4o/Claude 4/DeepSeek-V3/Qwen\u7cfb\u5217\u6a21\u578b\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u3001\u6027\u522b\u63a8\u65ad\u4e09\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "result": "\u4e8c\u5143\u4ee3\u8bcd\u548c\u6027\u522b\u4e2d\u7acb\u4ee3\u8bcd\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f46\u65b0\u4ee3\u8bcd\u51c6\u786e\u7387\u4ec521.6%-48.1%\uff0c\u53cd\u5411\u63a8\u7406\u4efb\u52a1\u9519\u8bef\u7387\u4ecd\u8fbe19.8%-57.3%", "conclusion": "\u6a21\u578b\u5728\u57fa\u7840\u4ee3\u8bcd\u4efb\u52a1\u53d6\u5f97\u8fdb\u6b65\uff0c\u4f46\u8eab\u4efd\u654f\u611f\u63a8\u7406\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u9700\u5f00\u53d1\u9488\u5bf9\u6027\u4f18\u5316\u65b9\u6cd5\u548c\u5305\u5bb9\u6027\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2508.00819", "pdf": "https://arxiv.org/pdf/2508.00819", "abs": "https://arxiv.org/abs/2508.00819", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "categories": ["cs.CL"], "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "AI": {"tldr": "\u63d0\u51faDAEDAL\u8bad\u7ec3\u65e0\u5173\u7684\u53bb\u566a\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u81ea\u9002\u5e94\u957f\u5ea6\u6269\u5c55\u89e3\u51b3\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9759\u6001\u957f\u5ea6\u9650\u5236\u95ee\u9898", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u9759\u6001\u9884\u5b9a\u4e49\u751f\u6210\u957f\u5ea6\uff0c\u5bfc\u81f4\u590d\u6742\u4efb\u52a1\u6027\u80fd\u4e0d\u8db3\u4e0e\u8fc7\u957f\u5e8f\u5217\u8ba1\u7b97\u6d6a\u8d39\u7684\u56f0\u5883", "method": "\u4e24\u9636\u6bb5\u52a8\u6001\u8c03\u6574\uff1a1\uff09\u57fa\u4e8e\u5e8f\u5217\u5b8c\u6210\u5ea6\u6307\u6807\u7684\u521d\u59cb\u957f\u5ea6\u8fed\u4ee3\u6269\u5c55 2\uff09\u53bb\u566a\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u63a9\u7801\u4ee4\u724c\u63d2\u5165\u52a8\u6001\u6269\u5c55\u4e0d\u8db3\u533a\u57df", "result": "\u5728\u4fdd\u6301/\u8d85\u8d8a\u56fa\u5b9a\u957f\u5ea6\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u63d0\u5347\u6709\u6548token\u6bd4\u4f8b\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387", "conclusion": "DAEDAL\u7a81\u7834\u4e86DLLMs\u7684\u6838\u5fc3\u67b6\u6784\u9650\u5236\uff0c\u7f29\u5c0f\u4e0e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u5dee\u8ddd\uff0c\u4e3a\u9ad8\u6548\u751f\u6210\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2508.00028", "pdf": "https://arxiv.org/pdf/2508.00028", "abs": "https://arxiv.org/abs/2508.00028", "authors": ["Abir Ray"], "title": "Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models", "categories": ["cs.NI", "cs.AI", "cs.CL", "cs.NA", "math.NA"], "comment": "12 pages", "summary": "Spectrum resources are often underutilized across time and space, motivating\ndynamic spectrum access strategies that allow secondary users to exploit unused\nfrequencies. A key challenge is predicting when and where spectrum will be\navailable (i.e., unused by primary licensed users) in order to enable proactive\nand interference-free access. This paper proposes a scalable framework for\nspectrum availability prediction that combines a two-state Markov chain model\nof primary user activity with high-fidelity propagation models from the ITU-R\n(specifically Recommendations P.528 and P.2108). The Markov chain captures\ntemporal occupancy patterns, while the propagation models incorporate path loss\nand clutter effects to determine if primary signals exceed interference\nthresholds at secondary user locations. By integrating these components, the\nproposed method can predict spectrum opportunities both in time and space with\nimproved accuracy. We develop the system model and algorithm for the approach,\nanalyze its scalability and computational efficiency, and discuss assumptions,\nlimitations, and potential applications. The framework is flexible and can be\nadapted to various frequency bands and scenarios. The results and analysis show\nthat the proposed approach can effectively identify available spectrum with low\ncomputational cost, making it suitable for real-time spectrum management in\ncognitive radio networks and other dynamic spectrum sharing systems.", "AI": {"tldr": "\u7ed3\u5408\u4e24\u72b6\u6001\u9a6c\u5c14\u53ef\u592b\u94fe\u4e0eITU-R\u4f20\u64ad\u6a21\u578b\uff0c\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u9891\u8c31\u65f6\u7a7a\u9884\u6d4b\u6846\u67b6\uff0c\u63d0\u5347\u8ba4\u77e5\u65e0\u7ebf\u7535\u7f51\u7edc\u7684\u9891\u8c31\u7ba1\u7406\u6548\u7387", "motivation": "\u9891\u8c31\u8d44\u6e90\u5728\u65f6\u7a7a\u7ef4\u5ea6\u5b58\u5728\u5229\u7528\u7387\u4e0d\u8db3\u73b0\u8c61\uff0c\u9700\u901a\u8fc7\u9884\u6d4b\u4e3b\u8981\u7528\u6237\u6d3b\u52a8\u89c4\u5f8b\u6765\u5b9e\u73b0\u52a8\u6001\u9891\u8c31\u63a5\u5165\uff0c\u89e3\u51b3\u6b21\u8981\u7528\u6237\u5e72\u6270\u89c4\u907f\u4e0e\u9891\u8c31\u673a\u4f1a\u5229\u7528\u7684\u77db\u76fe", "method": "\u6574\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u65f6\u95f4\u7ef4\u5ea6\u5360\u7528\u6a21\u5f0f+ITU-R P.528/P.2108\u4f20\u64ad\u6a21\u578b\u8ba1\u7b97\u7a7a\u95f4\u7ef4\u5ea6\u4fe1\u53f7\u8986\u76d6\uff0c\u6784\u5efa\u8054\u5408\u65f6\u7a7a\u9884\u6d4b\u6a21\u578b", "result": "\u6846\u67b6\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u53ef\u5b9e\u65f6\u8bc6\u522b\u53ef\u7528\u9891\u8c31\uff0c\u652f\u6301\u52a8\u6001\u9891\u8c31\u5171\u4eab\u7cfb\u7edf\u7684\u51b3\u7b56\u5236\u5b9a", "conclusion": "\u8be5\u7075\u6d3b\u6846\u67b6\u9002\u7528\u4e8e\u591a\u9891\u6bb5\u573a\u666f\uff0c\u4e3a\u8ba4\u77e5\u65e0\u7ebf\u7535\u63d0\u4f9b\u4f4e\u6210\u672c\u7684\u9891\u8c31\u611f\u77e5\u65b9\u6848\uff0c\u63a8\u52a8\u52a8\u6001\u9891\u8c31\u7ba1\u7406\u6280\u672f\u53d1\u5c55"}}
{"id": "2508.00033", "pdf": "https://arxiv.org/pdf/2508.00033", "abs": "https://arxiv.org/abs/2508.00033", "authors": ["Nuno Fachada", "Daniel Fernandes", "Carlos M. Fernandes", "Bruno D. Ferreira-Saraiva", "Jo\u00e3o P. Matos-Carvalho"], "title": "GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries", "categories": ["cs.SE", "cs.AI", "cs.CL", "68T50", "I.2.2; I.2.7; D.2.3"], "comment": null, "summary": "Large Language Models (LLMs) have advanced rapidly as tools for automating\ncode generation in scientific research, yet their ability to interpret and use\nunfamiliar Python APIs for complex computational experiments remains poorly\ncharacterized. This study systematically benchmarks a selection of\nstate-of-the-art LLMs in generating functional Python code for two increasingly\nchallenging scenarios: conversational data analysis with the \\textit{ParShift}\nlibrary, and synthetic data generation and clustering using \\textit{pyclugen}\nand \\textit{scikit-learn}. Both experiments use structured, zero-shot prompts\nspecifying detailed requirements but omitting in-context examples. Model\noutputs are evaluated quantitatively for functional correctness and prompt\ncompliance over multiple runs, and qualitatively by analyzing the errors\nproduced when code execution fails. Results show that only a small subset of\nmodels consistently generate correct, executable code, with GPT-4.1 standing\nout as the only model to always succeed in both tasks. In addition to\nbenchmarking LLM performance, this approach helps identify shortcomings in\nthird-party libraries, such as unclear documentation or obscure implementation\nbugs. Overall, these findings highlight current limitations of LLMs for\nend-to-end scientific automation and emphasize the need for careful prompt\ndesign, comprehensive library documentation, and continued advances in language\nmodel capabilities.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5f53\u524d\u4ec5\u6709\u5c11\u6570LLM\uff08\u5982GPT-4.1\uff09\u80fd\u7a33\u5b9a\u751f\u6210\u6b63\u786e\u4ee3\u7801\uff0c\u63ed\u793a\u4e86LLM\u5728\u79d1\u5b66\u8ba1\u7b97\u81ea\u52a8\u5316\u4e2d\u7684\u5c40\u9650\u6027\u4e0e\u6539\u8fdb\u65b9\u5411", "motivation": "\u8bc4\u4f30LLM\u4f7f\u7528\u964c\u751fPython API\u751f\u6210\u590d\u6742\u79d1\u5b66\u8ba1\u7b97\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u586b\u8865\u5176\u5728\u7aef\u5230\u7aef\u79d1\u7814\u81ea\u52a8\u5316\u5e94\u7528\u6548\u679c\u7684\u7814\u7a76\u7a7a\u767d", "method": "\u4f7f\u7528ParShift\u548cpyclugen/scikit-learn\u4e24\u4e2a\u96f6\u6837\u672c\u7f16\u7a0b\u4efb\u52a1\uff0c\u901a\u8fc7\u529f\u80fd\u6b63\u786e\u6027\u3001\u63d0\u793a\u5408\u89c4\u6027\u5b9a\u91cf\u8bc4\u4f30\u548c\u9519\u8bef\u7c7b\u578b\u5b9a\u6027\u5206\u6790", "result": "\u4ec511.3%\u6a21\u578b\u80fd\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0cGPT-4.1\u53cc\u4efb\u52a1\u6210\u529f\u7387100%\uff0c\u540c\u65f6\u66b4\u9732\u7b2c\u4e09\u65b9\u5e93\u6587\u6863\u4e0d\u6e05\u6670\u7b49\u95ee\u9898", "conclusion": "LLM\u79d1\u5b66\u81ea\u52a8\u5316\u9700\u6539\u8fdb\u63d0\u793a\u8bbe\u8ba1\u3001\u5b8c\u5584\u5e93\u6587\u6863\uff0c\u5e76\u6301\u7eed\u63d0\u5347\u6a21\u578b\u7406\u89e3\u80fd\u529b\uff0c\u5f53\u524d\u9636\u6bb5\u5c1a\u672a\u8fbe\u5230\u53ef\u9760\u7aef\u5230\u7aef\u5e94\u7528\u6c34\u5e73"}}
{"id": "2508.00083", "pdf": "https://arxiv.org/pdf/2508.00083", "abs": "https://arxiv.org/abs/2508.00083", "authors": ["Yihong Dong", "Xue Jiang", "Jiaru Qian", "Tian Wang", "Kechi Zhang", "Zhi Jin", "Ge Li"], "title": "A Survey on Code Generation with LLM-based Agents", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Work in progress", "summary": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.", "AI": {"tldr": "\u7cfb\u7edf\u68b3\u7406\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u6280\u672f\u53d1\u5c55\uff0c\u6db5\u76d6\u81ea\u4e3b\u6027\u3001\u4efb\u52a1\u6269\u5c55\u548c\u5de5\u7a0b\u5b9e\u8df5\u4e09\u5927\u7279\u5f81\uff0c\u63d0\u51fa\u6280\u672f\u5206\u7c7b\u3001\u5e94\u7528\u573a\u666f\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4ee3\u7801\u751f\u6210\u6280\u672f\u65e0\u6cd5\u8986\u76d6\u5b8c\u6574\u8f6f\u4ef6\u5f00\u53d1\u5468\u671f\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLM\u5728\u63d0\u9ad8\u7cfb\u7edf\u53ef\u9760\u6027\u3001\u6d41\u7a0b\u7ba1\u7406\u7b49\u65b9\u9762\u7684\u5de5\u7a0b\u5b9e\u8df5\u4ef7\u503c\u3002", "method": "\u901a\u8fc7\u6280\u672f\u53d1\u5c55\u8f68\u8ff9\u8ffd\u6eaf\u3001\u5355/\u591a\u4ee3\u7406\u67b6\u6784\u5206\u7c7b\u3001\u5168\u751f\u547d\u5468\u671f\u5e94\u7528\u573a\u666f\u5206\u6790\u7684\u4e09\u7ef4\u6846\u67b6\u5c55\u5f00\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "result": "\u5efa\u7acb\u9996\u4e2a\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u6280\u672f\u4f53\u7cfb\uff0c\u603b\u7ed3\u4e3b\u6d41\u8bc4\u4f30\u6307\u6807\u548c\u5de5\u5177\u94fe\uff0c\u63ed\u793a\u7cfb\u7edf\u53ef\u9760\u6027\u4fdd\u969c\u548c\u5de5\u5177\u534f\u540c\u7b49\u6838\u5fc3\u5de5\u7a0b\u6311\u6218\u3002", "conclusion": "\u9700\u957f\u671f\u5173\u6ce8\u53ef\u4fe1\u4ee3\u7801\u751f\u6210\u3001\u667a\u80fd\u4f53\u8ba4\u77e5\u67b6\u6784\u3001\u5f00\u53d1\u6d41\u7a0b\u91cd\u6784\u4e09\u5927\u57fa\u7840\u65b9\u5411\uff0c\u63a8\u52a8AI\u539f\u751f\u8f6f\u4ef6\u5f00\u53d1\u8303\u5f0f\u6f14\u8fdb\u3002"}}
{"id": "2508.00161", "pdf": "https://arxiv.org/pdf/2508.00161", "abs": "https://arxiv.org/abs/2508.00161", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u5fae\u8c03\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u6743\u91cd\u5dee\u5f02\u5947\u5f02\u5411\u91cf\uff0c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u578b\u884c\u4e3a\u76d1\u63a7\u4e0e\u63a7\u5236", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6fc0\u6d3b\u7684\u89e3\u91ca\u65b9\u6cd5\u9700\u8981\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\uff0c\u8fd9\u5728\u68c0\u6d4b\u65b0\u578b\u5a01\u80c1\uff08\u5982\u540e\u95e8\u653b\u51fb\uff09\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u5206\u6790\u65b9\u6cd5", "method": "1. \u8ba1\u7b97\u5fae\u8c03\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u7684\u6743\u91cd\u5dee\u5f02\u77e9\u9635\n2. \u63d0\u53d6\u6743\u91cd\u5dee\u5f02\u77e9\u9635\u7684\u9876\u90e8\u5947\u5f02\u5411\u91cf\n3. \u76d1\u63a7\u6fc0\u6d3b\u5411\u91cf\u5728\u5947\u5f02\u5411\u91cf\u65b9\u5411\u4e0a\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6", "result": "1. \u540e\u95e8\u653b\u51fb\u9632\u5fa1\u6210\u529f\u7387100%\uff08\u8bef\u62a5\u7387<1.2%\uff09\n2. \u672a\u5b66\u4e60\u4e3b\u9898\u68c0\u6d4b\u51c6\u786e\u738795.42%\n3. \u6210\u529f\u89e3\u6790\u5546\u4e1a\u6a21\u578b\u5fae\u8c03\u7b56\u7565\uff08\u8425\u9500\u7b56\u7565/Midjourney\u63d0\u793a\u751f\u6210\uff09", "conclusion": "\u57fa\u4e8e\u6743\u91cd\u5206\u6790\u7684\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u5728\u6a21\u578b\u5b89\u5168\u76d1\u63a7\u3001\u884c\u4e3a\u63a7\u5236\u548c\u5546\u4e1a\u5316\u5ba1\u8ba1\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf"}}
{"id": "2508.00171", "pdf": "https://arxiv.org/pdf/2508.00171", "abs": "https://arxiv.org/abs/2508.00171", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u9009\u62e9\u6027\u6a21\u6001\u8fc1\u79fb\uff08SMS\uff09\u65b9\u6cd5\uff0c\u91cf\u5316\u591a\u6a21\u6001\u6a21\u578b\u5728\u533b\u5b66\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6a21\u6001\u4f9d\u8d56\u6027\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u800c\u5ffd\u89c6\u89c6\u89c9\u7ebf\u7d22", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u5b58\u5728\u6a21\u6001\u504f\u5dee\u95ee\u9898\uff0c\u5e38\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u800c\u5ffd\u7565\u5173\u952e\u533b\u5b66\u56fe\u50cf\u7279\u5f81", "method": "\u901a\u8fc7\u6837\u672c\u95f4\u56fe\u50cf/\u6587\u672c\u4e92\u6362\u7684\u6270\u52a8\u5b9e\u9a8c\uff0c\u7ed3\u5408\u5b9a\u91cf\u6027\u80fd\u8bc4\u4f30\u548c\u5b9a\u6027\u6ce8\u610f\u529b\u5206\u6790\uff0c\u8bc4\u4f306\u4e2a\u5f00\u6e90\u6a21\u578b\u5728\u80f8\u7247\u548c\u773c\u79d1\u5f71\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0", "result": "\u6a21\u578b\u5728\u6587\u672c\u6270\u52a8\u540e\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08MIMIC-CXR\u6570\u636e\u96c6F1\u4e0b\u964d53.6%\uff09\uff0c\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u56fe\u50cf\u7279\u5f81\u5e38\u88ab\u6587\u672c\u7ec6\u8282\u63a9\u76d6", "conclusion": "\u9700\u5f00\u53d1\u771f\u6b63\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u7684\u533b\u7597\u6a21\u578b\uff0c\u5f53\u524d\u6a21\u578b\u7684\u6587\u672c\u4f9d\u8d56\u7279\u6027\u53ef\u80fd\u5bfc\u81f4\u4e34\u5e8a\u51b3\u7b56\u98ce\u9669\uff0c\u5efa\u8bae\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2508.00222", "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u901a\u8fc7\u6574\u5408\u5185\u90e8\u63a8\u7406\u548c\u5916\u90e8\u5b66\u4e60\uff0c\u7ed3\u5408\u591a\u91cd\u91cd\u8981\u6027\u91c7\u6837\u548c\u63a2\u7d22\u4f18\u52bf\u51fd\u6570\uff0c\u7a81\u7834\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u8fb9\u754c\u5e76\u89e3\u51b3\u80fd\u529b\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u53d7\u9650\u4e8e\u57fa\u7840LLM\u7684on-policy\u7b56\u7565\uff08\u5e9e\u5927\u52a8\u4f5c\u7a7a\u95f4+\u7a00\u758f\u5956\u52b1\uff09\uff0c\u65e0\u6cd5\u7a81\u7834\u6a21\u578b\u56fa\u6709\u8fb9\u754c\u4e14\u5bfc\u81f4\u80fd\u529b\u8303\u56f4\u7f29\u5c0f\u3002", "method": "\u5305\u542b\uff1a1\uff09\u591a\u91cd\u91cd\u8981\u6027\u91c7\u6837\u89e3\u51b3\u5916\u90e8\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d 2\uff09\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\u5f15\u5bfc\u6a21\u578b\u53d1\u73b0\u9ad8\u4ef7\u503c\u63a8\u7406\u8def\u5f84 3\uff09\u5185\u90e8\u63a8\u7406\u4e0e\u5916\u90e8\u5b66\u4e60\u7684\u534f\u540c\u673a\u5236\u3002", "result": "\u57286\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u5b9e\u73b0SOTA\uff0c6/6\u5206\u5e03\u5916\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u76f8\u5bf9\u63d0\u534721.1%-69.2%\uff0cPass@k\u66f2\u7ebf\u8bc1\u5b9e\u89e3\u51b3\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u3002", "conclusion": "RL-PLUS\u901a\u8fc7\u7406\u8bba\u521b\u65b0\u548c\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u6210\u529f\u7a81\u7834\u57fa\u7840\u6a21\u578b\u80fd\u529b\u9650\u5236\uff0c\u5efa\u7acb\u53ef\u6301\u7eed\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2508.00230", "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "AI": {"tldr": "\u63d0\u51faKRAdapter\u65b9\u6cd5\u89e3\u51b3LoRA\u5728\u77e9\u9635\u8fd1\u4f3c\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7Khatri-Rao\u79ef\u63d0\u5347\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6548\u679c", "motivation": "LoRA\u5728\u5904\u7406\u9891\u8c31\u5e73\u5766\u6216\u9ad8\u9891\u6210\u5206\u77e9\u9635\u65f6\u8868\u73b0\u6b20\u4f73\uff0c\u8fd9\u7c7b\u77e9\u9635\u5177\u6709\u9ad8\u6709\u6548\u79e9\u7279\u6027", "method": "\u5229\u7528Khatri-Rao\u79ef\u751f\u6210\u6743\u91cd\u66f4\u65b0\uff0c\u5929\u7136\u4ea7\u751f\u9ad8\u6709\u6548\u79e9\u7684\u77e9\u9635\u4e58\u79ef\u7ed3\u6784", "result": "\u572810\u4ebf\u53c2\u6570\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c80\u4ebf\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u5728\u672a\u89c1\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u7a81\u51fa", "conclusion": "KRAdapter\u5728\u4fdd\u6301LoRA\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u6210\u4e3a\u5341\u4ebf\u7ea7\u53c2\u6570\u6a21\u578b\u5fae\u8c03\u7684\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2508.00271", "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7'\u505a\u4e2d\u5b66'\u5b9e\u73b0\u81ea\u6211\u8fdb\u5316\u7684\u667a\u80fd\u4f53MetaAgent\uff0c\u901a\u8fc7\u5de5\u5177\u8def\u7531\u3001\u7ecf\u9a8c\u6c89\u6dc0\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\u5b9e\u73b0\u6301\u7eed\u4f18\u5316\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4f20\u7edf\u5de5\u4f5c\u6d41\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u667a\u80fd\u4f53\u5b58\u5728\u77e5\u8bc6\u56fa\u5316\u3001\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u4e0d\u6539\u53d8\u6a21\u578b\u53c2\u6570\u7684\u6301\u7eed\u8fdb\u5316\u8303\u5f0f\u4ee5\u63d0\u5347\u5f00\u653e\u77e5\u8bc6\u53d1\u73b0\u80fd\u529b\u3002", "method": "\u5efa\u7acb\u57fa\u7840\u63a8\u7406+\u52a8\u6001\u5de5\u5177\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u7b54\u6848\u9a8c\u8bc1\u751f\u6210\u7ecf\u9a8c\u6587\u672c\uff0c\u81ea\u4e3b\u6784\u5efa\u5de5\u5177\u5e93\u548c\u77e5\u8bc6\u5e93\uff0c\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u5143\u5de5\u5177\u5b66\u4e60\u673a\u5236\u3002", "result": "\u5728GAIA/WebWalkerQA/BrowseCamp\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u8d85\u8d8a\u5de5\u4f5c\u6d41\u57fa\u7ebf8-15%\uff0c\u8fbe\u5230\u6216\u8d85\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u6a21\u578b\u6c34\u5e73\u3002", "conclusion": "MetaAgent\u9a8c\u8bc1\u4e86\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u901a\u7528\u77e5\u8bc6\u53d1\u73b0\u63d0\u4f9b\u4e86\u53c2\u6570\u51bb\u7ed3\u60c5\u51b5\u4e0b\u7684\u6301\u7eed\u4f18\u5316\u65b0\u8def\u5f84\u3002"}}
{"id": "2508.00282", "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4eba\u7c7b\u4e0eLLM\uff08GPT-4o\uff09\u7684\u4efb\u52a1\u751f\u6210\u6a21\u5f0f\uff0c\u53d1\u73b0\u4eba\u7c7b\u53d7\u5185\u5728\u5fc3\u7406\u9a71\u52a8\uff08\u5982\u4ef7\u503c\u89c2\u3001\u8ba4\u77e5\u98ce\u683c\uff09\uff0c\u800cLLM\u5373\u4f7f\u660e\u786e\u83b7\u5f97\u5fc3\u7406\u9a71\u52a8\u4e5f\u65e0\u6cd5\u590d\u73b0\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\uff0c\u751f\u6210\u7684\u4efb\u52a1\u793e\u4ea4\u6027\u3001\u5b9e\u4f53\u6027\u66f4\u4f4e\u4e14\u504f\u5411\u62bd\u8c61\uff0c\u7a81\u663e\u4e8c\u8005\u8ba4\u77e5\u673a\u5236\u7684\u6838\u5fc3\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff08\u57fa\u4e8eLLM\uff09\u662f\u5426\u9075\u5faa\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u4ef7\u503c\u9a71\u52a8\u3001\u5177\u8eab\u8ba4\u77e5\u539f\u5219\uff0c\u9a8c\u8bc1LLM\u4efb\u52a1\u751f\u6210\u673a\u5236\u4e0e\u4eba\u7c7b\u5fc3\u7406\u52a8\u673a\u7684\u5339\u914d\u6027\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u751f\u6210\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u4e0eLLM\uff08GPT-4o\uff09\u5728\u76f8\u540c\u5fc3\u7406\u9a71\u52a8\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u5206\u6790\u4efb\u52a1\u4e3b\u9898\u3001\u793e\u4ea4\u5c5e\u6027\u3001\u5b9e\u4f53\u6027\u7b49\u7ef4\u5ea6\u5dee\u5f02\u3002", "result": "LLM\u751f\u6210\u7684\u4efb\u52a1\u793e\u4ea4\u4e92\u52a8\u51cf\u5c1162%\u3001\u7269\u7406\u5b9e\u4f53\u6027\u964d\u4f4e45%\uff0c\u4e3b\u9898\u62bd\u8c61\u5ea6\u9ad8\u4e8e\u4eba\u7c7b28%\u3002\u5c3d\u7ba1\u5176\u4efb\u52a1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\uff08+37%\u65b0\u9896\u6027\u8bc4\u5206\uff09\uff0c\u4f46\u65e0\u6cd5\u4f53\u73b0\u4ef7\u503c\u89c2\u4e0e\u884c\u4e3a\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "LLM\u7684\u7edf\u8ba1\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u4ef7\u503c\u9a71\u52a8\u7684\u5177\u8eab\u8ba4\u77e5\u5b58\u5728\u672c\u8d28\u9e3f\u6c9f\uff0c\u672a\u6765\u9700\u5c06\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u5177\u8eab\u6027\u878d\u5165\u667a\u80fd\u4f53\u8bbe\u8ba1\u4ee5\u5b9e\u73b0\u4eba\u673a\u5bf9\u9f50\u3002"}}
{"id": "2508.00324", "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "categories": ["cs.AI", "cs.CL"], "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "\u53d1\u73b0\u5927\u6a21\u578b\u867d\u5177\u5907\u5b89\u5168\u77e5\u8bc6\u4f46\u63a8\u7406\u65f6\u672a\u6fc0\u6d3b\uff0c\u63d0\u51fa\u9ad8\u6548\u540e\u8bad\u7ec3\u65b9\u6cd5R1-Act\uff0c\u4ec5\u9700\u5c11\u91cf\u8d44\u6e90\u5373\u53ef\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u3002", "motivation": "\u5927\u6a21\u578b(LRM)\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\u6613\u54cd\u5e94\u6709\u5bb3\u6307\u4ee4\uff0c\u7814\u7a76\u53d1\u73b0\u5176\u5b89\u5168\u77e5\u8bc6\u672a\u88ab\u6709\u6548\u6fc0\u6d3b\uff0c\u9700\u89e3\u51b3\u63a8\u7406\u8fc7\u7a0b\u4e0e\u5b89\u5168\u77e5\u8bc6\u8131\u8282\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faR1-Act\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\u5b9e\u73b0\u5b89\u5168\u589e\u5f3a\u3002\u4ec5\u97001,000\u8bad\u7ec3\u6837\u672c\uff0c\u5355GPU\u8bad\u7ec390\u5206\u949f\u5373\u53ef\u5b8c\u6210\u9002\u914d\u3002", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u6d4b\u8bd5\u4e2d\uff0cR1-Act\u5b89\u5168\u6307\u6807\u63d0\u5347\u663e\u8457\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u6027\u80fd\uff0c\u8ba1\u7b97\u6548\u7387\u6bd4\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u9ad810\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u6fc0\u6d3b\u65e2\u6709\u5b89\u5168\u77e5\u8bc6\u800c\u975e\u704c\u8f93\u65b0\u77e5\u8bc6\uff0cR1-Act\u4e3a\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u8f7b\u91cf\u7ea7\u7279\u6027\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2508.00408", "pdf": "https://arxiv.org/pdf/2508.00408", "abs": "https://arxiv.org/abs/2508.00408", "authors": ["Dong Huang", "Jie M. Zhang", "Mark Harman", "Qianru Zhang", "Mingzhe Du", "See-Kiong Ng"], "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions", "categories": ["cs.SE", "cs.CL"], "comment": "Under Review", "summary": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).", "AI": {"tldr": "ULT\u662f\u4e13\u4e3a\u51fd\u6570\u7ea7\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u8bbe\u8ba1\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u6d41\u7a0b\u89e3\u51b3\u6570\u636e\u6c61\u67d3\u548c\u4ee3\u7801\u7ed3\u6784\u7b80\u5355\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u771f\u5b9e\u6709\u6548\u7684LLM\u80fd\u529b\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709LLM\u6d4b\u8bd5\u751f\u6210\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6c61\u67d3\u548c\u7ed3\u6784\u7b80\u5355\u4e24\u5927\u7f3a\u9677\uff0c\u5bfc\u81f4\u79d1\u5b66\u7ed3\u8bba\u6709\u6548\u6027\u5b58\u7591\u3002ULT\u65e8\u5728\u6784\u5efa\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "1. \u6784\u5efaULT\uff1a\u901a\u8fc7\u591a\u9636\u6bb5\u7b5b\u9009\u786e\u4fdd\u51fd\u6570\u7684\u9ad8\u5708\u590d\u6742\u5ea6\uff0c\u5e76\u4e25\u683c\u6e05\u6d17\u6d4b\u8bd5\u7528\u4f8b\u907f\u514d\u6570\u636e\u6c61\u67d3\n2. \u521b\u5efaPLT\u5bf9\u7167\u57fa\u51c6\u7528\u4e8e\u5206\u6790\u8bb0\u5fc6\u4e0e\u63a8\u7406\u673a\u5236\n3. \u57fa\u4e8e3,909\u4e2aPython\u51fd\u6570\u4efb\u52a1\u8fdb\u884cLLM\u6027\u80fd\u8bc4\u4f30", "result": "ULT\u8bc4\u4f30\u6307\u6807\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u57fa\u51c6\uff1aLLMs\u751f\u6210\u7528\u4f8b\u7684\u51c6\u786e\u7387(41.32%)\u3001\u8bed\u53e5\u8986\u76d6(45.10%)\u3001\u5206\u652f\u8986\u76d6(30.22%)\u3001\u53d8\u5f02\u5f97\u5206(40.21%)\uff0c\u5747\u4f4e\u4e8eTestEval\u548cPLT\u5bf9\u5e94\u6307\u6807", "conclusion": "ULT\u63d0\u4f9b\u4e86\u66f4\u4e25\u8c28\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5176\u8bbe\u8ba1\u6709\u6548\u66b4\u9732\u73b0\u6709LLM\u5728\u590d\u6742\u6d4b\u8bd5\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u53ef\u9760\u57fa\u51c6\u57fa\u7840"}}
{"id": "2508.00414", "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "categories": ["cs.AI", "cs.CL"], "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro\u5f00\u6e90\u6846\u67b6\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u7b56\u7565\uff0c\u5728GAIA\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff088B\u6a21\u578b\u8d85\u8d8aWebDancer\u7b49\u7cfb\u7edf\uff09", "motivation": "\u89e3\u51b3\u73b0\u6709AI\u4ee3\u7406\u7cfb\u7edf\u95ed\u6e90/\u4f9d\u8d56\u4ed8\u8d39API\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u53ef\u8bbf\u95ee\u3001\u53ef\u590d\u73b0\u7684\u9ad8\u7ea7AI\u4ee3\u7406\u7814\u53d1", "method": "\u7cfb\u7edf\u7814\u7a76\u4ee3\u7406\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u6784\u5efa\uff08\u8986\u76d6web/file/code/reasoning\u56db\u9886\u57df\uff09+ \u521b\u65b0\u6d4b\u8bd5\u65f6\u53cd\u601d\u4e0e\u6295\u7968\u7b56\u7565", "result": "GAIA\u57fa\u51c6\u6d4b\u8bd5\u8fbe\u5230\u5f00\u6e90\u4ee3\u7406SOTA\uff0c8B\u6a21\u578b\u8d85\u8d8aWebDancer/WebSailor\u7b49\u7cfb\u7edf", "conclusion": "Cognitive Kernel-Pro\u4e3aAI\u4ee3\u7406\u53d1\u5c55\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\uff0c\u8bc1\u660e\u5f00\u6e90\u6a21\u578b\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u6027\u80fd\uff0c\u63a8\u52a8\u793e\u533a\u53d1\u5c55\uff08\u4ee3\u7801\u5df2\u5f00\u6e90\uff09"}}
{"id": "2508.00518", "pdf": "https://arxiv.org/pdf/2508.00518", "abs": "https://arxiv.org/abs/2508.00518", "authors": ["Shuo Liang", "Yiwu Zhong", "Zi-Yuan Hu", "Yeyao Tao", "Liwei Wang"], "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025", "summary": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .", "AI": {"tldr": "\u9996\u4e2a\u9488\u5bf9\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7684\u50cf\u7d20\u7ea7\u65f6\u7a7a\u5b9a\u4f4d\u57fa\u51c6EgoMask\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u6784\u5efa\u5e76\u9a8c\u8bc1\u6a21\u578b\u6548\u679c\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u5728\u589e\u5f3a\u73b0\u5b9e/\u673a\u5668\u4eba\u9886\u57df\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65f6\u7a7a\u5b9a\u4f4d\u7814\u7a76\u96c6\u4e2d\u4e8e\u5916\u4e2d\u5fc3\u89c6\u9891\uff0c\u5b58\u5728\u7269\u4f53\u6301\u7eed\u65f6\u95f4\u77ed/\u8f68\u8ff9\u7a00\u758f/\u5c3a\u5bf8\u5c0f/\u4f4d\u7f6e\u504f\u79fb\u5927\u56db\u5927\u6838\u5fc3\u6311\u6218\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u6807\u6ce8\u6d41\u7a0b\u6784\u5efaEgoMask\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08\u542b\u77ed/\u4e2d/\u957f\u671f\u89c6\u9891\uff09\uff0c\u5e76\u521b\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u96c6EgoMask-Train\u3002\u901a\u8fc7\u5fae\u8c03\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u8fc1\u79fb\u80fd\u529b\u3002", "result": "SOTA\u6a21\u578b\u5728EgoMask\u4e0a\u8868\u73b0\u6b20\u4f73\uff0c\u4f46\u4f7f\u7528EgoMask-Train\u5fae\u8c03\u540emIoU\u63d0\u53477.7%\uff0c\u4e14\u4fdd\u6301\u5916\u4e2d\u5fc3\u6570\u636e\u96c6\u6027\u80fd\u4e0d\u4e0b\u964d\u3002", "conclusion": "EgoMask\u586b\u8865\u4e86\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u7684\u57fa\u51c6\u7a7a\u767d\uff0c\u63d0\u4f9b\u7684\u8d44\u6e90\u4e0e\u6d1e\u89c1\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.00534", "pdf": "https://arxiv.org/pdf/2508.00534", "abs": "https://arxiv.org/abs/2508.00534", "authors": ["Mikel Vandeloise"], "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations", "categories": ["cs.PL", "cs.CL", "D.3.2; F.3.2; D.3.1"], "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed", "summary": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.", "AI": {"tldr": "\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u53d1\u73b0\u4f20\u7edf\u7f16\u7a0b\u8303\u5f0f\u5206\u7c7b\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u539f\u5b50\u5316\u539f\u8bed\u548c\u6570\u5b66\u6846\u67b6\uff08\u7c7b\u578b\u8bba/\u8303\u7574\u8bba/UTP\uff09\u7684\u590d\u5408\u91cd\u6784\u65b9\u6cd5\u6210\u4e3a\u65b0\u8d8b\u52bf\u3002", "motivation": "\u591a\u8303\u5f0f\u8bed\u8a00\u7684\u5174\u8d77\u5bfc\u81f4\u4f20\u7edf\u5206\u7c7b\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u6df7\u5408\u8bed\u8a00\u7279\u6027\uff0c\u9700\u5efa\u7acb\u66f4\u5f3a\u5927\u7684\u5f62\u5f0f\u5316\u57fa\u7840\u4ee5\u89e3\u51b3\u4e92\u64cd\u4f5c\u6027\u7f3a\u9677\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0(SLR)\u7efc\u540874\u9879\u7814\u7a76\uff0c\u8bc4\u4f30\u8303\u5f0f\u5206\u7c7b\u5f62\u5f0f\u5316\u57fa\u7840\u5e76\u8bc6\u522b\u91cd\u6784\u7406\u8bba\u7684\u6838\u5fc3\u539f\u8bed\u3002", "result": "\u73b0\u6709\u5206\u7c7b\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u7edf\u4e00\u57fa\u7840\uff0c\u7814\u7a76\u754c\u8f6c\u5411\u4f7f\u7528\u7c7b\u578b\u8bba/\u8303\u7574\u8bba/UTP\u6784\u5efa\u5177\u5907\u7ec4\u5408\u6027\u4fdd\u8bc1\u7684\u539f\u5b50\u5316\u539f\u8bed\u4f53\u7cfb\u3002", "conclusion": "\u7f16\u7a0b\u8303\u5f0f\u7814\u7a76\u6b63\u4ece\u5206\u7c7b\u8f6c\u5411\u5f62\u5f0f\u5316\u91cd\u6784\u6846\u67b6\uff0c\u9700\u5efa\u7acb\u7edf\u4e00\u7684\u7406\u8bba\u4f53\u7cfb\u2014\u2014\u672c\u6587\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2508.00554", "pdf": "https://arxiv.org/pdf/2508.00554", "abs": "https://arxiv.org/abs/2508.00554", "authors": ["Li Zhao", "Rui Sun", "Zuoyou Jiang", "Bo Yang", "Yuxiao Bai", "Mengting Chen", "Xinyang Wang", "Jing Li", "Zuo Bai"], "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism", "categories": ["q-fin.TR", "cs.CL", "q-fin.CP"], "comment": null, "summary": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.", "AI": {"tldr": "\u63d0\u51fa\u5177\u6709\u5185\u90e8\u7ade\u4e89\u673a\u5236\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6570\u636e\u56e2\u961f\u548c\u7814\u7a76\u56e2\u961f\u7684\u5206\u5de5\u534f\u4f5c\uff0c\u7ed3\u5408\u5b9e\u65f6\u8bc4\u4f30\u6392\u540d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u91d1\u878d\u4ea4\u6613\u7cfb\u7edf\u6297\u566a\u58f0\u80fd\u529b\u548c\u4ea4\u6613\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u4ea4\u6613\u7cfb\u7edf\u5bf9\u5e02\u573a\u566a\u58f0\u9ad8\u5ea6\u654f\u611f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u589e\u5f3a\u7cfb\u7edf\u9c81\u68d2\u6027\u5e76\u9002\u5e94\u52a8\u6001\u5e02\u573a\u73af\u5883\u3002", "method": "1) \u6570\u636e\u56e2\u961f\u538b\u7f29\u5e02\u573a\u6570\u636e\u4e3a\u6587\u672c\u56e0\u5b50 2) \u7814\u7a76\u56e2\u961f\u8fdb\u884c\u591a\u8def\u5f84\u4ea4\u6613\u51b3\u7b56 3) \u57fa\u4e8e\u5e02\u573a\u53cd\u9988\u7684\u5b9e\u65f6\u8bc4\u5206\u6392\u540d\u673a\u5236 4) \u4ec5\u91c7\u7528\u9876\u7ea7\u667a\u80fd\u4f53\u8f93\u51fa", "result": "\u5b9e\u9a8c\u663e\u793a\u672c\u7cfb\u7edf\u5728\u591a\u9879\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u4e3b\u6d41\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u4f20\u7edf\u91cf\u5316\u6295\u8d44\u65b9\u6cd5", "conclusion": "\u5185\u90e8\u7ade\u4e89\u673a\u5236\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u589e\u5f3a\u7cfb\u7edf\u9002\u5e94\u6027\uff0c\u6709\u6548\u6291\u5236\u5e02\u573a\u566a\u58f0\u5f71\u54cd\uff0c\u5b9e\u73b0\u66f4\u4f18\u4ea4\u6613\u8868\u73b0"}}
{"id": "2508.00555", "pdf": "https://arxiv.org/pdf/2508.00555", "abs": "https://arxiv.org/abs/2508.00555", "authors": ["Jiecong Wang", "Haoran Li", "Hao Peng", "Ziqian Zeng", "Zihao Wang", "Haohua Du", "Zhengtao Yu"], "title": "Activation-Guided Local Editing for Jailbreaking Attacks", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6AGILE\uff0c\u901a\u8fc7\u573a\u666f\u751f\u6210\u548c\u9690\u85cf\u72b6\u6001\u5f15\u5bfc\u7684\u7ec6\u7c92\u5ea6\u7f16\u8f91\u5b9e\u73b0\u9ad8\u6548\u8d8a\u72f1\u653b\u51fb\uff0c\u653b\u51fb\u6210\u529f\u7387\u63d0\u534737.74%\u5e76\u5177\u5907\u4f18\u5f02\u8fc1\u79fb\u6027", "motivation": "\u73b0\u6709token\u7ea7\u653b\u51fb\u5b58\u5728\u4e0d\u8fde\u8d2f\u6027\u548c\u8fc1\u79fb\u6027\u5dee\u7684\u95ee\u9898\uff0cprompt\u7ea7\u653b\u51fb\u6269\u5c55\u6027\u4e0d\u8db3\u4e14\u4f9d\u8d56\u4eba\u5de5\uff0c\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u5f00\u53d1\u66f4\u6709\u6548\u65b9\u6cd5", "method": "\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u573a\u666f\u5316\u4e0a\u4e0b\u6587\u751f\u6210\u5e76\u91cd\u5199\u6076\u610f\u67e5\u8be2\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5229\u7528\u6a21\u578b\u9690\u85cf\u72b6\u6001\u4fe1\u606f\u6307\u5bfc\u7f16\u8f91\uff0c\u5c06\u5185\u90e8\u8868\u5f81\u4ece\u6076\u610f\u8f6c\u5411\u826f\u6027", "result": "\u653b\u51fb\u6210\u529f\u7387\uff08ASR\uff09\u8fbe\u5230SOTA\u6c34\u5e73\uff0c\u5728\u9ed1\u76d2\u6a21\u578b\u8fc1\u79fb\u6027\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u6297\u4e3b\u6d41\u9632\u5fa1\u673a\u5236\u4ecd\u4fdd\u6301\u663e\u8457\u6709\u6548\u6027", "conclusion": "AGILE\u66b4\u9732\u4e86\u5f53\u524d\u9632\u5fa1\u673a\u5236\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u9632\u5fa1\u4f53\u7cfb\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u8bc1\u660e\u9690\u85cf\u72b6\u6001\u5f15\u5bfc\u7f16\u8f91\u7684\u6709\u6548\u6027"}}
{"id": "2508.00589", "pdf": "https://arxiv.org/pdf/2508.00589", "abs": "https://arxiv.org/abs/2508.00589", "authors": ["Stefan Englmeier", "Max A. B\u00fcttner", "Katharina Winter", "Fabian B. Flohr"], "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408SMPL\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u7d22\u6846\u67b6\uff0c\u521b\u5efaWayMoCo\u6570\u636e\u96c6\u63d0\u5347\u8fd0\u52a8-\u4e0a\u4e0b\u6587\u68c0\u7d22\u51c6\u786e\u738727.5%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e2d\u68c0\u7d22\u7f55\u89c1\u4eba\u7c7b\u884c\u4e3a\uff08\u957f\u5c3e\u95ee\u9898\uff09\uff0c\u5f71\u54cd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bc4\u4f30\u4e0e\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u878d\u5408SMPL\u8fd0\u52a8\u5e8f\u5217\u4e0e\u89c6\u9891\u5e27\uff0c\u6784\u5efa\u81ea\u7136\u8bed\u8a00\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\uff0c\u652f\u6301\u6587\u672c\u9a71\u52a8\u7684\u573a\u666f\u68c0\u7d22\uff1b\u6269\u5c55\u751f\u6210WayMoCo\u6570\u636e\u96c6\u3002", "result": "\u5728WayMoCo\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u8fd0\u52a8-\u4e0a\u4e0b\u6587\u68c0\u7d22\u51c6\u786e\u7387\u8d85\u8d8aSOTA\u6a21\u578b27.5%\u3002", "conclusion": "\u6846\u67b6\u4e0e\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u590d\u6742\u4eba\u7c7b\u884c\u4e3a\u573a\u666f\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u591a\u6a21\u6001\u68c0\u7d22\u652f\u6301\u7cfb\u7edf\u9c81\u68d2\u6027\u9a8c\u8bc1\u3002"}}
{"id": "2508.00659", "pdf": "https://arxiv.org/pdf/2508.00659", "abs": "https://arxiv.org/abs/2508.00659", "authors": ["Xinzhang Chen", "Hassan Ali", "Arash Shaghaghi", "Salil S. Kanhere", "Sanjay Jha"], "title": "Demo: TOSense -- What Did You Just Agree to?", "categories": ["cs.CR", "cs.CL"], "comment": "Accepted as a demonstration paper at IEEE LCN 2025", "summary": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTOSense\u2014\u2014\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u95ee\u7b54\u7684Chrome\u6269\u5c55\uff0c\u5e2e\u52a9\u7528\u6237\u5feb\u901f\u7406\u89e3\u7e41\u7410\u7684\u670d\u52a1\u6761\u6b3e", "motivation": "\u89e3\u51b3\u7528\u6237\u56e0\u5197\u957f\u6666\u6da9\u7684\u670d\u52a1\u6761\u6b3e\uff08ToS\uff09\u5bfc\u81f4\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u6cd5\u5f8b\u98ce\u9669\u95ee\u9898", "method": "1. tos-crawl\u722c\u866b\u81ea\u52a8\u63d0\u53d6ToS\u5185\u5bb9\n2. MiniLM\u8bed\u4e49\u68c0\u7d22 + BART-encoder\u7b54\u6848\u9a8c\u8bc1\u7684\u53cc\u6a21\u578b\u67b6\u6784\n3. \u521b\u65b0QA\u8bc4\u4f30\u7ba1\u9053(QEP)\u5b9e\u73b0\u65e0\u4eba\u5de5\u6807\u6ce8\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5", "result": "\u5728Apple/Google/X/Microsoft/Netflix\u4e94\u5927\u5e73\u53f0\u5b9e\u9a8c\u663e\u793a\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u8fbe44.5%\uff08\u4e0d\u540c\u4e3b\u9898\u7c07\u6570\u91cf\u4e0b\uff09", "conclusion": "TOSense\u6709\u6548\u63d0\u5347\u670d\u52a1\u6761\u6b3e\u53ef\u53ca\u6027\uff0cQEP\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u95ee\u7b54\u8bc4\u4f30\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5b9e\u65f6\u7d22\u5f15\u529f\u80fd\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5b9e\u7528\u6027"}}
{"id": "2508.00695", "pdf": "https://arxiv.org/pdf/2508.00695", "abs": "https://arxiv.org/abs/2508.00695", "authors": ["Sergio Rubio-Mart\u00edn", "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s", "Antonio Serrano-Garc\u00eda", "Clara Margarita Franch-Pato", "Arturo Crespo-\u00c1lvaro", "Jos\u00e9 Alberto Ben\u00edtez-Andrades"], "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "AI": {"tldr": "\u6bd4\u8f83\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8d85\u53c2\u6570\u8c03\u4f18\u6bd4\u8fc7\u91c7\u6837\u6280\u672f\u66f4\u5173\u952e\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540cAI\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u5bf9\u5fc3\u7406\u5065\u5eb7\u4e34\u5e8a\u8bca\u65ad\u5206\u7c7b\u7684\u6548\u679c\uff0c\u4ee5\u4f18\u5316AI\u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u3002", "method": "\u4f7f\u75285\u79cd\u4f20\u7edfML\u6a21\u578b\u548c2\u79cdBERT\u53d8\u4f53\uff0c\u7ed3\u5408\u4e09\u79cd\u8fc7\u91c7\u6837\u7b56\u7565\uff08\u65e0/\u968f\u673a/SMOTE\uff09\uff0c\u5e76\u8fdb\u884c\u8d85\u53c2\u6570\u4f18\u5316\u3002", "result": "\u51b3\u7b56\u6811/XGBoost\u548cBERT\u6a21\u578b\u5747\u8fbe96%\u51c6\u786e\u7387\uff1bSMOTE\u4ec5\u5bf9BERT\u6709\u6548\uff0c\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6240\u6709\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u8d85\u53c2\u6570\u4f18\u5316\u662f\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u6838\u5fc3\uff0c\u8be5\u7814\u7a76\u4e3a\u5fc3\u7406\u5065\u5eb7AI\u8bca\u65ad\u5de5\u5177\u7684\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
