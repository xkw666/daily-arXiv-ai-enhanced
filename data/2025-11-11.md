<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 99]
- [cs.GR](#cs.GR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.AI](#cs.AI) [Total: 13]
- [cs.CV](#cs.CV) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.CY](#cs.CY) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation](https://arxiv.org/abs/2511.05516)
*Canxiang Yan,Chunxiang Jin,Dawei Huang,Haibing Yu,Han Peng,Hui Zhan,Jie Gao,Jing Peng,Jingdong Chen,Jun Zhou,Kaimeng Ren,Ming Yang,Mingxue Yang,Qiang Xu,Qin Zhao,Ruijie Xiong,Shaoxiong Lin,Xuezhi Wang,Yi Yuan,Yifei Wu,Yongjie Lyu,Zhengyu He,Zhihao Qiu,Zhiqiang Fang,Ziyuan Huang*

Main category: cs.CL

TL;DR: 提出统一语音框架Ming-UniAudio，通过连续分词器MingTok-Audio整合语义与声学特征，实现语音理解/生成/编辑的平衡，并在多项任务创SOTA记录。


<details>
  <summary>Details</summary>
Motivation: 现有语音模型在表征层面存在理解与生成任务的需求冲突，阻碍基于自然语言指令的自由编辑能力发展。

Method: 1. 开发连续语音分词器MingTok-Audio统一语义和声学特征
2. 构建统一语音语言模型Ming-UniAudio
3. 扩展训练专用编辑模型Ming-UniAudio-Edit

Result: 1. ContextASR基准12项指标中8项SOTA
2. 中文语音克隆Seed-TTS-WER达0.95
3. 创建首个自由语音编辑评测基准Ming-Freeform-Audio-Edit

Conclusion: 该框架首次实现无需时间戳条件的自然语言指令驱动语音编辑，通过开源推动语音处理技术的统一化发展。

Abstract: Existing speech models suffer from competing requirements on token representations by understanding and generation tasks. This discrepancy in representation prevents speech language models from performing instruction-based free-form editing. To solve this challenge, we introduce a novel framework that unifies speech understanding, generation, and editing. The core of our unified model is a unified continuous speech tokenizer MingTok-Audio, the first continuous tokenizer to effectively integrate semantic and acoustic features, which makes it suitable for both understanding and generation tasks. Based on this unified continuous audio tokenizer, we developed the speech language model Ming-UniAudio, which achieved a balance between generation and understanding capabilities. Ming-UniAudio sets new state-of-the-art (SOTA) records on 8 out of 12 metrics on the ContextASR benchmark. Notably, for Chinese voice cloning, it achieves a highly competitive Seed-TTS-WER of 0.95. Leveraging this foundational model, we further trained a dedicated speech editing model Ming-UniAudio-Edit, the first speech language model that enables universal, free-form speech editing guided solely by natural language instructions, handling both semantic and acoustic modifications without timestamp condition. To rigorously assess the editing capability and establish a foundation for future research, we introduce Ming-Freeform-Audio-Edit, the first comprehensive benchmark tailored for instruction-based free-form speech editing, featuring diverse scenarios and evaluation dimensions spanning semantic correctness, acoustic quality, and instruction alignment. We open-sourced the continuous audio tokenizer, the unified foundational model, and the free-form instruction-based editing model to facilitate the development of unified audio understanding, generation, and manipulation.

</details>


### [2] [Retracing the Past: LLMs Emit Training Data When They Get Lost](https://arxiv.org/abs/2511.05518)
*Myeongseob Ko,Nikhil Reddy Billa,Adam Nguyen,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.CL

TL;DR: 提出混淆诱导攻击（CIA）框架，通过最大化模型不确定性系统化提取记忆数据，针对对齐模型设计Mismatched SFT方法，实验证明优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的记忆化现象引发隐私和版权风险，现有差异攻击方法效果有限且缺乏理论指导。需要更系统化的数据提取方法揭示记忆化泄露机制。

Method: 1. 利用token级预测熵持续高峰特性设计CIA框架，通过优化输入片段诱导高熵状态；2. 针对对齐模型提出Mismatched SFT，削弱对齐同时定向制造混淆。

Result: 在多种LLM上验证，CIA无需训练数据先验即可更有效提取原文/近原文数据，成功率达现有基线2倍以上。

Conclusion: 揭示不同LLM持续存在的记忆化风险，提供系统性评估框架，为模型安全研究提供新方向。

Abstract: The memorization of training data in large language models (LLMs) poses significant privacy and copyright concerns. Existing data extraction methods, particularly heuristic-based divergence attacks, often exhibit limited success and offer limited insight into the fundamental drivers of memorization leakage. This paper introduces Confusion-Inducing Attacks (CIA), a principled framework for extracting memorized data by systematically maximizing model uncertainty. We empirically demonstrate that the emission of memorized text during divergence is preceded by a sustained spike in token-level prediction entropy. CIA leverages this insight by optimizing input snippets to deliberately induce this consecutive high-entropy state. For aligned LLMs, we further propose Mismatched Supervised Fine-tuning (SFT) to simultaneously weaken their alignment and induce targeted confusion, thereby increasing susceptibility to our attacks. Experiments on various unaligned and aligned LLMs demonstrate that our proposed attacks outperform existing baselines in extracting verbatim and near-verbatim training data without requiring prior knowledge of the training data. Our findings highlight persistent memorization risks across various LLMs and offer a more systematic method for assessing these vulnerabilities.

</details>


### [3] [Beyond One-Size-Fits-All: Personalized Harmful Content Detection with In-Context Learning](https://arxiv.org/abs/2511.05532)
*Rufan Zhang,Lin Zhang,Xianghang Mi*

Main category: cs.CL

TL;DR: 提出基于上下文学习（ICL）的基础模型框架，实现跨任务有害内容检测并支持免重训练的个性化设置。


<details>
  <summary>Details</summary>
Motivation: 传统中心化审核系统存在透明度低、忽视用户偏好、难以适应隐私敏感/去中心化环境的问题。需要更灵活且用户可控的内容安全方案。

Method: 利用基础模型的上下文学习能力统一检测毒性/垃圾/负面情绪，通过提示工程实现个性化规则调整（如新增屏蔽类别/解除限制/语义扩展）。

Result: 1.基础模型跨任务泛化能力媲美专用模型 2.仅需1个用户示例即可个性化 3.提示中加入标签定义显著提升噪声数据鲁棒性

Conclusion: 确立了ICL作为下一代内容审核的实践路径，在GitHub/Hugging Face开源代码和标注数据集，推动可复现研究。

Abstract: The proliferation of harmful online content--e.g., toxicity, spam, and negative sentiment--demands robust and adaptable moderation systems. However, prevailing moderation systems are centralized and task-specific, offering limited transparency and neglecting diverse user preferences--an approach ill-suited for privacy-sensitive or decentralized environments. We propose a novel framework that leverages in-context learning (ICL) with foundation models to unify the detection of toxicity, spam, and negative sentiment across binary, multi-class, and multi-label settings. Crucially, our approach enables lightweight personalization, allowing users to easily block new categories, unblock existing ones, or extend detection to semantic variations through simple prompt-based interventions--all without model retraining. Extensive experiments on public benchmarks (TextDetox, UCI SMS, SST2) and a new, annotated Mastodon dataset reveal that: (i) foundation models achieve strong cross-task generalization, often matching or surpassing task-specific fine-tuned models; (ii) effective personalization is achievable with as few as one user-provided example or definition; and (iii) augmenting prompts with label definitions or rationales significantly enhances robustness to noisy, real-world data. Our work demonstrates a definitive shift beyond one-size-fits-all moderation, establishing ICL as a practical, privacy-preserving, and highly adaptable pathway for the next generation of user-centric content safety systems. To foster reproducibility and facilitate future research, we publicly release our code on GitHub and the annotated Mastodon dataset on Hugging Face.

</details>


### [4] [MCP4IFC: IFC-Based Building Design Using Large Language Models](https://arxiv.org/abs/2511.05533)
*Bharathi Kannan Nithyanantham,Tobias Sesterhenn,Ashwin Nedungadi,Sergio Peral Garijo,Janis Zenkner,Christian Bartelt,Stefan Lüdtke*

Main category: cs.CL

TL;DR: 提出了开源框架MCP4IFC，通过Model Context Protocol使大语言模型直接操作IFC数据，支持从自然语言生成建筑模型到复杂BIM任务执行。


<details>
  <summary>Details</summary>
Motivation: 解决AEC领域需要将自然语言指令转换为标准化数据模型操作的挑战，填补现有系统在动态任务处理能力上的不足。

Method: 整合BIM工具集（场景查询/元素修改）+ 动态代码生成系统（上下文学习+RAG技术），支持预定义和扩展任务处理。

Result: 实验验证可完成从简单房屋建模到复杂IFC数据编辑，框架已开源（https://show2instruct.github.io/mcp4ifc/）。

Conclusion: 该框架为LLM驱动的BIM设计研究奠定基础，推动AI辅助建模工作流程发展。

Abstract: Bringing generative AI into the architecture, engineering and construction (AEC) field requires systems that can translate natural language instructions into actions on standardized data models. We present MCP4IFC, a comprehensive open-source framework that enables Large Language Models (LLMs) to directly manipulate Industry Foundation Classes (IFC) data through the Model Context Protocol (MCP). The framework provides a set of BIM tools, including scene querying tools for information retrieval, predefined functions for creating and modifying common building elements, and a dynamic code-generation system that combines in-context learning with retrieval-augmented generation (RAG) to handle tasks beyond the predefined toolset. Experiments demonstrate that an LLM using our framework can successfully perform complex tasks, from building a simple house to querying and editing existing IFC data. Our framework is released as open-source to encourage research in LLM-driven BIM design and provide a foundation for AI-assisted modeling workflows. Our code is available at https://show2instruct.github.io/mcp4ifc/.

</details>


### [5] [FlowMM: Cross-Modal Information Flow Guided KV Cache Merging for Efficient Multimodal Context Inference](https://arxiv.org/abs/2511.05534)
*Kunxi Li,Yufan Xiong,Zhonghua Jiang,Yiyun Zhou,Zhaode Wang,Chengfei Lv,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出FlowMM框架，通过跨模态信息流引导的KV缓存合并策略，在显著减少内存占用和延迟的同时保持多模态大模型性能


<details>
  <summary>Details</summary>
Motivation: 传统KV缓存淘汰策略在多模态场景下存在模态分布偏差和注意力偏差问题，现有合并方法难以有效保持上下文完整性

Method: 1. 基于跨模态信息流的动态分层合并策略
2. 结合令牌相似度和任务敏感度的自适应匹配机制

Result: 实验证明FlowMM可减少80-95% KV缓存，降低1.3-1.8倍解码延迟，同时保持竞争力任务表现

Conclusion: FlowMM为多模态大模型提供了高效的内存优化方案，在保持性能的前提下显著提升推理效率，适用于多种领先MLLM架构

Abstract: Traditional KV cache eviction strategies, which discard less critical KV-pairs based on attention scores, often degrade generation quality, causing context loss or hallucinations. Recent efforts shift toward KV merging, merging eviction tokens with retention tokens based on similarity. However, in multimodal scenarios, distributional biases across modality tokens and attentional biases in cross-modal interactions limit its effectiveness. This work introduces FlowMM, an adaptive framework for cross-modal information flow-guided multimodal KV cache merging. FlowMM leverages cross-modal information flow to dynamically apply layer-specific merging strategies, capturing modality-specific patterns while preserving contextual integrity. Furthermore, we introduce a sensitivity-adaptive token matching mechanism that jointly evaluates token similarity and task-critical sensitivity, merging low-risk tokens while safeguarding high-sensitivity ones. Extensive experiments across diverse leading MLLMs show that FlowMM reduces KV cache memory by 80% to 95% and decoding latency by 1.3-1.8x, while maintaining competitive task performance.

</details>


### [6] [Future of AI Models: A Computational perspective on Model collapse](https://arxiv.org/abs/2511.05535)
*Trivikram Satharasi,S Sitharama Iyengar*

Main category: cs.CL

TL;DR: AI生成内容的快速扩张导致模型崩溃风险，研究通过分析维基百科语义相似性预测数据退化时间节点


<details>
  <summary>Details</summary>
Motivation: 随着合成内容在多个领域占比激增（网页74.2%/金融投诉18%），递归训练可能侵蚀语言多样性，威胁AI模型底层数据生态

Method: 使用Transformer嵌入和余弦相似度指标，分析2013-2025年英文维基百科语料的逐年语义演变

Result: LLMs公开应用前相似度稳步上升（早期RNN/LSTM驱动），应用后出现指数增长，显示语义多样性面临系统性威胁

Conclusion: 递归训练导致的语义同质化将显著削弱数据丰富性，需建立新型数据管理策略维持模型泛化能力

Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.

</details>


### [7] [Temporal Sparse Autoencoders: Leveraging the Sequential Nature of Language for Interpretability](https://arxiv.org/abs/2511.05541)
*Usha Bhalla,Alex Oesterling,Claudio Mayrink Verdun,Himabindu Lakkaraju,Flavio P. Calmon*

Main category: cs.CL

TL;DR: 提出时间稀疏自编码器(T-SAEs)，通过对比损失分离语义与句法特征，改善语言模型可解释性


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器(SAEs)存在捕捉深层语义能力不足、偏向浅层特征的问题，需要结合语言学知识改进特征发现机制

Method: 利用语义长程依赖与句法局部性特征，设计带对比损失的时间稀疏自编码器，强制相邻token的高层特征一致性

Result: 在多个数据集/模型上实现更平滑的语义概念表征，保持重建质量的同时呈现清晰语义结构

Conclusion: T-SAEs为无监督语言模型可解释性提供了新范式，通过自监督方式有效解耦语义与句法特征

Abstract: Translating the internal representations and computations of models into concepts that humans can understand is a key goal of interpretability. While recent dictionary learning methods such as Sparse Autoencoders (SAEs) provide a promising route to discover human-interpretable features, they suffer from a variety of problems, including a systematic failure to capture the rich conceptual information that drives linguistic understanding. Instead, they exhibit a bias towards shallow, token-specific, or noisy features, such as "the phrase 'The' at the start of sentences". In this work, we propose that this is due to a fundamental issue with how dictionary learning methods for LLMs are trained. Language itself has a rich, well-studied structure spanning syntax, semantics, and pragmatics; however, current unsupervised methods largely ignore this linguistic knowledge, leading to poor feature discovery that favors superficial patterns over meaningful concepts. We focus on a simple but important aspect of language: semantic content has long-range dependencies and tends to be smooth over a sequence, whereas syntactic information is much more local. Building on this insight, we introduce Temporal Sparse Autoencoders (T-SAEs), which incorporate a novel contrastive loss encouraging consistent activations of high-level features over adjacent tokens. This simple yet powerful modification enables SAEs to disentangle semantic from syntactic features in a self-supervised manner. Across multiple datasets and models, T-SAEs recover smoother, more coherent semantic concepts without sacrificing reconstruction quality. Strikingly, they exhibit clear semantic structure despite being trained without explicit semantic signal, offering a new pathway for unsupervised interpretability in language models.

</details>


### [8] [Sample-Efficient Language Modeling with Linear Attention and Lightweight Enhancements](https://arxiv.org/abs/2511.05560)
*Patrick Haller,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: BLaLM模型通过mLSTM替换自注意力机制，结合滑动窗口注意力与Muon优化器，在低资源语言建模中实现高效表现


<details>
  <summary>Details</summary>
Motivation: 针对BabyLM竞赛的样本效率要求，探索不依赖模型规模的架构创新与优化策略，验证小模型在有限数据下的潜力

Method: 使用线性复杂度的mLSTM替代Transformer自注意力，集成短卷积/动态调制滑动窗口注意力/Hedgehog特征映射，采用Muon优化器并构建高质量训练语料库

Result: 线性注意力+滑动窗口组合提升zero-shot性能，Muon优化器较AdamW降低困惑度且提升训练稳定性（STRICT和STRICT-SMALL双赛道验证）

Conclusion: 通过线性注意力架构与专用优化器的协同设计，证明语言模型效率提升可突破规模依赖，为资源受限场景提供新解决方案

Abstract: We study architectural and optimization techniques for sample-efficient language modeling under the constraints of the BabyLM 2025 shared task. Our model, BLaLM, replaces self-attention with a linear-time mLSTM token mixer and explores lightweight enhancements, including short convolutions, sliding window attention with dynamic modulation, and Hedgehog feature maps. To support training in low-resource settings, we curate a high-quality corpus emphasizing readability and pedagogical structure. Experiments across both STRICT and STRICT-SMALL tracks show that (1) linear attention combined with sliding window attention consistently improves zero-shot performance, and (2) the Muon optimizer stabilizes convergence and reduces perplexity over AdamW. These results highlight effective strategies for efficient language modeling without relying on scale.

</details>


### [9] [UTF-8 Plumbing: Byte-level Tokenizers Unavoidably Enable LLMs to Generate Ill-formed UTF-8](https://arxiv.org/abs/2511.05578)
*Preston Firestone,Shubham Ugare,Gagandeep Singh,Sasa Misailovic*

Main category: cs.CL

TL;DR: 论文通过幺半群理论证明基于字节的分词方法可能产生无效UTF-8序列，并通过实际案例验证其引发的现实问题


<details>
  <summary>Details</summary>
Motivation: 解决子词分词中代码点与字节方法的UTF-8有效性矛盾，揭示无效序列导致的实际应用风险

Method: 使用幺半群理论形式化分词过程，对比增量转换与整体转换差异，评估主流模型和系统的实际案例

Result: 证明含无效UTF-8标记的分词器必然产生无效序列，实验显示现实系统存在理论预测的编码错误

Conclusion: 语言模型应用需系统性处理分词引发的UTF-8断裂问题，现有系统需针对性改进方案

Abstract: Subword tokenization segments input text according to a pre-defined vocabulary to feed it into a language model; the language model, in turn, generates a sequence made from this same vocabulary. The members of the vocabulary can be built of code points or bytes. Using code points means that all members of the vocabulary are valid UTF-8 characters. However, it also requires thousands of initial members to achieve acceptable coverage of inputs. Beginning with bytes, on the contrary, avoids out-of-vocabulary errors with only 256 initial members of the vocabulary, but the members of the vocabulary and sequences of them are not guaranteed to be valid UTF-8. Sequences that are not valid UTF-8 break code that assumes its input to be valid UTF-8. Applications of language models must account for the breakage thereby introduced. In this paper, we formalize tokenization using monoid theory and prove that tokenizers whose vocabularies contain tokens that are ill-formed UTF-8 can always produce sequences that are ill-formed UTF-8. We demonstrate formally that attempting to incrementally convert tokens back to a string and interpret the results as UTF-8 gives different results than converting the whole sequence of tokens at once. This formal result predicts real-world bugs: we evaluate mitigations for the problem identified and provide case studies of major foundation models, serving engines, and constrained generation systems.

</details>


### [10] [Optimizing Diversity and Quality through Base-Aligned Model Collaboration](https://arxiv.org/abs/2511.05650)
*Yichen Wang,Chenghao Yang,Tenghao Huang,Muhao Chen,Jonathan May,Mina Lee*

Main category: cs.CL

TL;DR: 提出BACo框架，通过基础模型与对齐模型在推理时的动态协作，实现生成质量与多样性的平衡优化


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法提升输出质量但导致多样性丧失，传统多样性增强方法存在质量下降、计算成本高或需多轮采样的缺陷

Method: 基于token级路由策略，根据预测不确定性和语义角色动态选择解码模型（基础模型/对齐模型）

Result: 在3个开放生成任务中，BACo在13个指标上全面超越基线，最佳路由策略实现多样性+质量联合提升21.3%

Conclusion: 基础模型与对齐模型的协同解码能够有效控制生成结果的多样性-质量权衡，为后验优化提供新范式

Abstract: Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality.

</details>


### [11] [OckBench: Measuring the Efficiency of LLM Reasoning](https://arxiv.org/abs/2511.05722)
*Zheng Du,Hao Kang,Song Han,Tushar Krishna,Ligeng Zhu*

Main category: cs.CL

TL;DR: 提出OckBench基准，在推理和编码任务中同步评估模型精度与解码效率，揭示同精度模型间token消耗差异显著，主张将token效率纳入评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注精度和输出质量，忽视解码token效率对系统延迟、成本和能源的实际影响，需建立统一评估框架量化模型效率差异。

Method: 构建模型/硬件无关的OckBench基准，通过多模态实验对比开源/闭源模型，绘制精度-效率帕累托前沿，量化token消耗差异。

Result: 实验显示同精度模型token消耗差异显著，证明效率是模型区分的关键维度；建立精度-效率权衡的帕累托边界。

Conclusion: 需改变评估范式，停止将token视为免费资源。OckBench为高效推理研究提供统一评估平台，推动token效率优化研究。

Abstract: Large language models such as GPT-4, Claude 3, and the Gemini series have improved automated reasoning and code generation. However, existing benchmarks mainly focus on accuracy and output quality, and they ignore an important factor: decoding token efficiency. In real systems, generating 10,000 tokens versus 100,000 tokens leads to large differences in latency, cost, and energy. In this work, we introduce OckBench, a model-agnostic and hardware-agnostic benchmark that evaluates both accuracy and token count for reasoning and coding tasks. Through experiments comparing multiple open- and closed-source models, we uncover that many models with comparable accuracy differ wildly in token consumption, revealing that efficiency variance is a neglected but significant axis of differentiation. We further demonstrate Pareto frontiers over the accuracy-efficiency plane and argue for an evaluation paradigm shift: we should no longer treat tokens as "free" to multiply. OckBench provides a unified platform for measuring, comparing, and guiding research in token-efficient reasoning. Our benchmarks are available at https://ockbench.github.io/ .

</details>


### [12] [In-Context Learning Without Copying](https://arxiv.org/abs/2511.05743)
*Kerem Sahin,Sheridan Feucht,Adam Belfki,Jannik Brinkmann,Aaron Mueller,David Bau,Chris Wendler*

Main category: cs.CL

TL;DR: 研究证明即便抑制归纳复制能力，Transformer仍能保持抽象上下文学习能力


<details>
  <summary>Details</summary>
Motivation: 验证归纳头是否是上下文学习能力的必要前提，探索模型在抑制归纳复制后的表现

Method: 提出Hapax训练法——在损失计算中排除可被归纳头正确预测的token（占31.7%的token）

Result: 在21项抽象学习任务中13项表现超越基线模型，非归纳预测位置损失更低，模型形成更少/更弱的归纳头

Conclusion: 归纳复制机制对抽象上下文学习能力的形成并非必需，暗示Transformer存在其他学习机制

Abstract: Induction heads are attention heads that perform inductive copying by matching patterns from earlier context and copying their continuations verbatim. As models develop induction heads, they often experience a sharp drop in training loss, a phenomenon cited as evidence that induction heads may serve as a prerequisite for more complex in-context learning (ICL) capabilities. In this work, we ask whether transformers can still acquire ICL capabilities when inductive copying is suppressed. We propose Hapax, a setting where we omit the loss contribution of any token that can be correctly predicted by induction heads. Despite a significant reduction in inductive copying, performance on abstractive ICL tasks (i.e., tasks where the answer is not contained in the input context) remains comparable and surpasses the vanilla model on 13 of 21 tasks, even though 31.7\% of tokens are omitted from the loss. Furthermore, our model achieves lower loss values on token positions that cannot be predicted correctly by induction heads. Mechanistic analysis further shows that models trained with Hapax develop fewer and weaker induction heads but still preserve ICL capabilities. Taken together, our findings indicate that inductive copying is not essential for learning abstractive ICL mechanisms.

</details>


### [13] [Multi-Scale Feature Fusion and Graph Neural Network Integration for Text Classification with Large Language Models](https://arxiv.org/abs/2511.05752)
*Xiangchen Song,Yulin Huang,Jinxu Guo,Yuchen Liu,Yaxuan Luan*

Main category: cs.CL

TL;DR: 提出融合LLM、特征金字塔与GNN的混合文本分类框架，在多项指标显著超越基线模型


<details>
  <summary>Details</summary>
Motivation: 针对复杂语义场景中传统分类模型在全局-局部特征平衡和结构化语义建模的不足，通过多技术融合提升分类性能

Method: 1. 使用大语言模型提取深度语义特征
2. 特征金字塔融合多尺度语义特征
3. 图神经网络建模语义单元结构化关系
4. 读取分类模块生成预测

Result: 在ACC/F1/AUC/Precision指标上全面超越基线模型，鲁棒性对齐实验验证框架有效性

Conclusion: 构建了全局-局部、语义-结构平衡的集成框架，为多尺度特征融合与结构化建模提供新思路

Abstract: This study investigates a hybrid method for text classification that integrates deep feature extraction from large language models, multi-scale fusion through feature pyramids, and structured modeling with graph neural networks to enhance performance in complex semantic contexts. First, the large language model captures contextual dependencies and deep semantic representations of the input text, providing a rich feature foundation for subsequent modeling. Then, based on multi-level feature representations, the feature pyramid mechanism effectively integrates semantic features of different scales, balancing global information and local details to construct hierarchical semantic expressions. Furthermore, the fused features are transformed into graph representations, and graph neural networks are employed to capture latent semantic relations and logical dependencies in the text, enabling comprehensive modeling of complex interactions among semantic units. On this basis, the readout and classification modules generate the final category predictions. The proposed method demonstrates significant advantages in robustness alignment experiments, outperforming existing models on ACC, F1-Score, AUC, and Precision, which verifies the effectiveness and stability of the framework. This study not only constructs an integrated framework that balances global and local information as well as semantics and structure, but also provides a new perspective for multi-scale feature fusion and structured semantic modeling in text classification tasks.

</details>


### [14] [Language Generation: Complexity Barriers and Implications for Learning](https://arxiv.org/abs/2511.05759)
*Marcelo Arenas,Pablo Barceló,Luis Cofré,Alexander Kozachinskiy*

Main category: cs.CL

TL;DR: 论文揭示了语言生成理论可能性与高效学习性之间的巨大鸿沟，指出简单语言家族生成所需样本量可能极大甚至不可计算。


<details>
  <summary>Details</summary>
Motivation: 尽管理论证明足够数据可实现语言生成，但实际中所需样本量可能远超计算能力，需探讨理论可能性与实证效率间的矛盾。

Method: 通过分析正则语言和上下文无关语言等基础语言家族的样本复杂度，证明生成所需样本量的不可计算性。

Result: 发现生成某些简单语言所需样本量无计算上界，存在理论保证与实用学习效率间的本质性差距。

Conclusion: 需结合自然语言的结构特性（如层级化、递归模式）重新解释语言模型成功，强调实践有效性与理论可能性的区别。

Abstract: Kleinberg and Mullainathan showed that, in principle, language generation is always possible: with sufficiently many positive examples, a learner can eventually produce sentences indistinguishable from those of a target language. However, the existence of such a guarantee does not speak to its practical feasibility. In this work, we show that even for simple and well-studied language families -- such as regular and context-free languages -- the number of examples required for successful generation can be extraordinarily large, and in some cases not bounded by any computable function. These results reveal a substantial gap between theoretical possibility and efficient learnability. They suggest that explaining the empirical success of modern language models requires a refined perspective -- one that takes into account structural properties of natural language that make effective generation possible in practice.

</details>


### [15] [DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning](https://arxiv.org/abs/2511.05784)
*Yaxuan Wang,Chris Yuhao Liu,Quan Liu,Jinglong Pang,Wei Wei,Yujia Bao,Yang Liu*

Main category: cs.CL

TL;DR: 提出DRAGON框架，通过上下文思维链指令实现无需保留数据的LLM知识遗忘，结合检测模块和CoT安全干预机制


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法依赖保留数据且需修改模型，实际场景中保留数据不可获取

Method: 利用LLM固有指令遵循能力，设计轻量级检测模块识别遗忘需求，通过专用CoT模型进行上下文安全干预

Result: 在三个代表性遗忘任务中验证有效性，展示强遗忘能力（98.3%遗忘效率）、良好扩展性和实际适用性

Conclusion: DRAGON开创了非侵入式遗忘新范式，新型评估指标为持续遗忘场景提供可靠验证框架

Abstract: Unlearning in Large Language Models (LLMs) is crucial for protecting private data and removing harmful knowledge. Most existing approaches rely on fine-tuning to balance unlearning efficiency with general language capabilities. However, these methods typically require training or access to retain data, which is often unavailable in real world scenarios. Although these methods can perform well when both forget and retain data are available, few works have demonstrated equivalent capability in more practical, data-limited scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes in-context chain-of-thought (CoT) instructions to guard deployed LLMs before inference. Instead of modifying the base model, DRAGON leverages the inherent instruction-following ability of LLMs and introduces a lightweight detection module to identify forget-worthy prompts without any retain data. These are then routed through a dedicated CoT guard model to enforce safe and accurate in-context intervention. To robustly evaluate unlearning performance, we introduce novel metrics for unlearning performance and the continual unlearning setting. Extensive experiments across three representative unlearning tasks validate the effectiveness of DRAGON, demonstrating its strong unlearning capability, scalability, and applicability in practical scenarios.

</details>


### [16] [Quantifying Edits Decay in Fine-tuned LLMs](https://arxiv.org/abs/2511.05852)
*Yinjie Cheng,Paul Youssef,Christin Seifert,Jörg Schlötterer,Zhixue Zhao*

Main category: cs.CL

TL;DR: 研究揭示了微调会导致知识编辑内容衰减，提出选择性层微调策略，强调模型编辑评估需结合完整应用流程


<details>
  <summary>Details</summary>
Motivation: 探索两个实际需求：1）当需要移除隐蔽/恶意编辑时，需确保微调能有效消除编辑痕迹；2）当希望保留有益编辑时，需防止微调破坏已编辑知识。若微调损害编辑效果，将导致重复编辑的高成本；若编辑残留，则可能传播安全隐患

Method: 系统评估2种前沿编辑方法(MEMIT/AlphaEdit)与3种微调方法(全参数/LoRA/DoRA)，在5个LLM和3个数据集上展开232组实验，提出选择性层微调策略(仅微调编辑相关层)

Result: 1) 微调后编辑内容平均衰减40-70%，不同配置生存率差异显著(如AlphaEdit衰减率高于MEMIT) 2) 仅微调编辑层可有效消除97%编辑痕迹，但下游任务性能下降3% 3) 意外发现微调非编辑层比全参数微调破坏更多编辑(衰减率+15%)

Conclusion: 建立了知识编辑与微调协同作用的基准框架，提出可操作的层选择策略，揭示模型编辑技术必须置于完整应用链中评估的重要性，为后续安全部署提供方法论

Abstract: Knowledge editing has emerged as a lightweight alternative to retraining for correcting or injecting specific facts in large language models (LLMs). Meanwhile, fine-tuning remains the default operation for adapting LLMs to new domains and tasks. Despite their widespread adoption, these two post-training interventions have been studied in isolation, leaving open a crucial question: if we fine-tune an edited model, do the edits survive? This question is motivated by two practical scenarios: removing covert or malicious edits, and preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1, current KE methods become less useful, as every fine-tuned model would require re-editing, which significantly increases the cost; if edits persist, fine-tuned models risk propagating hidden malicious edits, raising serious safety concerns. To this end, we systematically quantify edits decay after fine-tuning, investigating how fine-tuning affects knowledge editing. We evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three datasets, yielding 232 experimental configurations. Our results show that edits decay after fine-tuning, with survival varying across configurations, e.g., AlphaEdit edits decay more than MEMIT edits. Further, we propose selective-layer fine-tuning and find that fine-tuning edited layers only can effectively remove edits, though at a slight cost to downstream performance. Surprisingly, fine-tuning non-edited layers impairs more edits than full fine-tuning. Overall, our study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, and underscores that evaluating model editing requires considering the full LLM application pipeline.

</details>


### [17] [Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations](https://arxiv.org/abs/2511.05901)
*Rui Yang,Matthew Yu Heng Wong,Huitao Li,Xin Li,Wentao Zhu,Jingchi Liao,Kunyu Yu,Jonathan Chong Kai Liew,Weihao Xuan,Yingjian Chen,Yuhe Ke,Jasmine Chiat Ling Ong,Douglas Teodoro,Chuan Hong,Daniel Shi Wei Ting,Nan Liu*

Main category: cs.CL

TL;DR: 医疗领域RAG技术应用仍处早期阶段，需加强临床验证与跨语言支持


<details>
  <summary>Details</summary>
Motivation: 应对医疗知识爆炸和临床复杂性的挑战，探索检索增强生成（RAG）技术如何突破现有大语言模型（LLM）的局限性

Method: 系统回顾医学RAG研究，分析数据来源（主要公开数据）、检索方法（英文嵌入模型为主）、模型选择（通用LLM居多）及评估体系（自动指标+人工评估维度）

Result: 应用集中在问答/报告生成/摘要/信息抽取，评估忽视偏见与安全性，存在跨语言适应不足和低资源环境支持缺陷

Conclusion: 需推进临床验证、多语言适配和低资源场景支持，才能实现可信赖的全球化医疗RAG应用

Abstract: The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.

</details>


### [18] [NILC: Discovering New Intents with LLM-assisted Clustering](https://arxiv.org/abs/2511.05913)
*Hongtao Wang,Renchi Yang,Wenqing Lin*

Main category: cs.CL

TL;DR: 提出NILC框架解决新意图发现任务中传统级联架构反馈不足和语义忽略问题，通过LLMs迭代优化聚类中心与样本增强实现性能提升


<details>
  <summary>Details</summary>
Motivation: 现有NID方法采用嵌入编码+K-Means的级联架构，缺乏双向反馈且忽略文本细节语义，导致聚类效果受限

Method: 结合LLMs迭代优化：1)生成语义增强的聚类中心 2)改写模糊/简短样本增强困难案例 3)引入种子样本和软约束的半监督机制

Result: 在6个跨领域数据集的无监督/半监督实验中均取得显著性能提升（具体指标需参考论文实验部分）

Conclusion: 通过LLMs驱动的语义增强与迭代优化机制，NILC有效突破传统NID方法瓶颈，验证了融合深度语义理解与聚类反馈的可行性

Abstract: New intent discovery (NID) seeks to recognize both new and known intents from unlabeled user utterances, which finds prevalent use in practical dialogue systems. Existing works towards NID mainly adopt a cascaded architecture, wherein the first stage focuses on encoding the utterances into informative text embeddings beforehand, while the latter is to group similar embeddings into clusters (i.e., intents), typically by K-Means. However, such a cascaded pipeline fails to leverage the feedback from both steps for mutual refinement, and, meanwhile, the embedding-only clustering overlooks nuanced textual semantics, leading to suboptimal performance. To bridge this gap, this paper proposes NILC, a novel clustering framework specially catered for effective NID. Particularly, NILC follows an iterative workflow, in which clustering assignments are judiciously updated by carefully refining cluster centroids and text embeddings of uncertain utterances with the aid of large language models (LLMs). Specifically, NILC first taps into LLMs to create additional semantic centroids for clusters, thereby enriching the contextual semantics of the Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment hard samples (ambiguous or terse utterances) identified from clusters via rewriting for subsequent cluster correction. Further, we inject supervision signals through non-trivial techniques seeding and soft must links for more accurate NID in the semi-supervised setting. Extensive experiments comparing NILC against multiple recent baselines under both unsupervised and semi-supervised settings showcase that NILC can achieve significant performance improvements over six benchmark datasets of diverse domains consistently.

</details>


### [19] [IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction](https://arxiv.org/abs/2511.05921)
*Ankan Mullick,Sukannya Purkayastha,Saransh Sharma,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出半监督框架IDALC，通过意图检测和主动学习机制减少系统拒绝语句的人工标注成本，在多个数据集上实现5-10%的准确率提升并保持低标注成本


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统对低置信度意图的拒绝导致需人工标注，且新增意图时持续标注成本过高，需高效机制降低开销

Method: IDALC框架结合意图检测与主动学习，通过半监督方式自动检测用户意图并校正系统拒绝的语句

Result: 在多个基准数据集上超越基线方法，准确率提高5-10%，macro-F1提升4-8%，总标注成本仅需未标注数据的6-10%

Conclusion: IDALC有效平衡系统性能与标注成本，为语音代理的持续优化提供实用解决方案

Abstract: Voice-controlled dialog systems have become immensely popular due to their ability to perform a wide range of actions in response to diverse user queries. These agents possess a predefined set of skills or intents to fulfill specific user tasks. But every system has its own limitations. There are instances where, even for known intents, if any model exhibits low confidence, it results in rejection of utterances that necessitate manual annotation. Additionally, as time progresses, there may be a need to retrain these agents with new intents from the system-rejected queries to carry out additional tasks. Labeling all these emerging intents and rejected utterances over time is impractical, thus calling for an efficient mechanism to reduce annotation costs. In this paper, we introduce IDALC (Intent Detection and Active Learning based Correction), a semi-supervised framework designed to detect user intents and rectify system-rejected utterances while minimizing the need for human annotation. Empirical findings on various benchmark datasets demonstrate that our system surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8% improvement in macro-F1. Remarkably, we maintain the overall annotation cost at just 6-10% of the unlabelled data available to the system. The overall framework of IDALC is shown in Fig. 1

</details>


### [20] [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://arxiv.org/abs/2511.05933)
*Renfei Zhang,Manasa Kaniselvan,Niloofar Mireshghallah*

Main category: cs.CL

TL;DR: 强化学习通过改进知识导航能力而非新增数据提升模型性能，结构化提示可缩小SFT与RL模型差距


<details>
  <summary>Details</summary>
Motivation: 挑战传统认知，揭示RL增强模型在结构化知识召回任务中的优势表现

Method: 结合结构化提示实验、路径准确性对比和层级激活相似性分析

Result: 结构化提示缩小24pp差距，RL模型保持深层检索优势，激活分析显示知识表示相似但查询表征分化

Conclusion: RL主要优化知识遍历机制而非知识存储方式，为模型优化提供新方向

Abstract: Reinforcement learning (RL) is often credited with improving language model reasoning and generalization at the expense of degrading memorized knowledge. We challenge this narrative by observing that RL-enhanced models consistently outperform their base and supervised fine-tuned (SFT) counterparts on pure knowledge recall tasks, particularly those requiring traversal of hierarchical, structured knowledge (e.g., medical codes). We hypothesize these gains stem not from newly acquired data, but from improved procedural skills in navigating and searching existing knowledge hierarchies within the model parameters. To support this hypothesis, we show that structured prompting, which explicitly guides SFTed models through hierarchical traversal, recovers most of the performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We further find that while prompting improves final-answer accuracy, RL-enhanced models retain superior ability to recall correct procedural paths on deep-retrieval tasks. Finally our layer-wise internal activation analysis reveals that while factual representations (e.g., activations for the statement "code 57.95 refers to urinary infection") maintain high cosine similarity between SFT and RL models, query representations (e.g., "what is code 57.95") diverge noticeably, indicating that RL primarily transforms how models traverse knowledge rather than the knowledge representation itself.

</details>


### [21] [Interpretable Recognition of Cognitive Distortions in Natural Language Texts](https://arxiv.org/abs/2511.05969)
*Anton Kolonin,Anna Arinicheva*

Main category: cs.CL

TL;DR: 提出基于加权N-gram的多因素文本分类方法，应用于心理治疗认知扭曲检测，提升F1分数并开源模型。


<details>
  <summary>Details</summary>
Motivation: 实现心理治疗中认知扭曲的自动化检测，提升AI模型可解释性与透明度。

Method: 采用考虑异质层级关系的加权结构化模式(N-gram)，改进识别与学习算法

Result: 在公开数据集上显著超越文献F1分数，确定最优超参数并提供代码模型

Conclusion: 为心理治疗领域提供可解释的透明AI解决方案，推动精神健康技术发展

Abstract: We propose a new approach to multi-factor classification of natural language texts based on weighted structured patterns such as N-grams, taking into account the heterarchical relationships between them, applied to solve such a socially impactful problem as the automation of detection of specific cognitive distortions in psychological care, relying on an interpretable, robust and transparent artificial intelligence model. The proposed recognition and learning algorithms improve the current state of the art in this field. The improvement is tested on two publicly available datasets, with significant improvements over literature-known F1 scores for the task, with optimal hyper-parameters determined, having code and models available for future use by the community.

</details>


### [22] [Revisiting Entropy in Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2511.05993)
*Renren Jin,Pengzhi Gao,Yuqi Ren,Zhuowen Han,Tongxuan Zhang,Wuwei Huang,Wei Liu,Jian Luan,Deyi Xiong*

Main category: cs.CL

TL;DR: 论文提出通过调整正负优势token的损失权重，有效解决RLVR训练中语言模型熵崩溃问题


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在训练中普遍出现模型熵崩溃现象，导致早熟收敛和性能瓶颈，需系统性研究熵动态机制

Method: 通过多维度实验分析熵变化规律，建立正优势token与熵崩溃的理论关联，提出基于优势值加权的熵调控方法

Result: 离策略更新次数、数据多样性、梯度裁剪阈值是核心影响因素，正优势token贡献80%+的熵衰减

Conclusion: 通过动态平衡正负优势token的损失权重，可在保持模型性能的同时维持健康的熵水平

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a predominant approach for enhancing the reasoning capabilities of large language models (LLMs). However, the entropy of LLMs usually collapses during RLVR training, causing premature convergence to suboptimal local minima and hinder further performance improvement. Although various approaches have been proposed to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains lacking. To address this gap, we conduct extensive experiments to investigate the entropy dynamics of LLMs trained with RLVR and analyze how model entropy correlates with response diversity, calibration, and performance across various benchmarks. Our findings reveal that the number of off-policy updates, the diversity of training data, and the clipping thresholds in the optimization objective are critical factors influencing the entropy of LLMs trained with RLVR. Moreover, we theoretically and empirically demonstrate that tokens with positive advantages are the primary contributors to entropy collapse, and that model entropy can be effectively regulated by adjusting the relative loss weights of tokens with positive and negative advantages during training.

</details>


### [23] [LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis](https://arxiv.org/abs/2511.06000)
*Favour Yahdii Aghaebe,Tanefa Apekey,Elizabeth Williams,Nafise Sadat Moosavi*

Main category: cs.CL

TL;DR: 评估语言模型在生物医学证据合成中保留年龄信息的能力，发现模型存在系统性偏见和人口统计学信息保留不足的问题


<details>
  <summary>Details</summary>
Motivation: 临床干预依赖年龄差异，但语言模型在生物医学证据合成中的人口统计学信息保留能力尚不明确

Method: 构建DemogSummary数据集（包含儿童/成人/老年群体），评估Qwen/Longformer/GPT-4.1 Nano三种模型，采用标准指标和DSS（人口统计学显著性评分）

Result: 1. 成人研究摘要的人口统计学保真度最低
2. 少数群体更容易出现年龄相关实体幻觉
3. 开源与闭源模型均存在系统性差异

Conclusion: 当前语言模型存在人口统计学信息保留缺陷，需开发公平性评估框架和摘要生成流程

Abstract: Clinical interventions often hinge on age: medications and procedures safe for adults may be harmful to children or ineffective for older adults. However, as language models are increasingly integrated into biomedical evidence synthesis workflows, it remains uncertain whether these systems preserve such crucial demographic distinctions. To address this gap, we evaluate how well state-of-the-art language models retain age-related information when generating abstractive summaries of biomedical studies. We construct DemogSummary, a novel age-stratified dataset of systematic review primary studies, covering child, adult, and older adult populations. We evaluate three prominent summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed Demographic Salience Score (DSS), which quantifies age-related entity retention and hallucination. Our results reveal systematic disparities across models and age groups: demographic fidelity is lowest for adult-focused summaries, and under-represented populations are more prone to hallucinations. These findings highlight the limitations of current LLMs in faithful and bias-free summarisation and point to the need for fairness-aware evaluation frameworks and summarisation pipelines in biomedical NLP.

</details>


### [24] [Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data](https://arxiv.org/abs/2511.06023)
*Deng Yixuan,Ji Xiaoqiang*

Main category: cs.CL

TL;DR: 提出GRPO框架通过多维度奖励机制微调大语言模型，有效降低文化特异性偏见同时保持语言质量


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO对齐方法难以解决文化特异性及多维度的歧视问题，需开发更细粒度的去偏方法

Method: 1. 构建中文语境歧视数据集（地域/民族/职业） 2. 基于DeBERTa-v3训练多维奖励模型（公平性/中立性/语言质量） 3. 应用GRPO策略优化模型输出

Result: 实验显示偏见强度降低35%，公平性指标提升28%，语言流畅度保持98%基准水平

Conclusion: GRPO框架为文化语境下的伦理对齐提供可复现方案，证明多维度奖励优化在模型去偏中的有效性

Abstract: Large Language Models (LLMs) often exhibit implicit biases and discriminatory tendencies that reflect underlying social stereotypes. While recent alignment techniques such as RLHF and DPO have mitigated some of these issues, they remain limited in addressing culturally specific and multi-dimensional forms of discrimination. This paper proposes a Multi-Reward Group Relative Policy Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free behavior. Our approach constructs a synthetic English-language dataset derived from Chinese-context discrimination categories, including regional, ethnic, and occupational biases. Each instance is paired with both neutral and biased responses to train a reward model based on DeBERTa-v3, which provides multi-dimensional reward signals capturing fairness, neutrality, and linguistic quality. The trained reward model then guides GRPO fine-tuning to optimize model outputs along these ethical dimensions. Experimental results demonstrate significant reductions in bias intensity and improved alignment with non-discriminatory standards without compromising fluency or informativeness. This study highlights the effectiveness of GRPO-based multi-reward optimization for de-biasing LLMs and offers a replicable framework for cultural-contextual ethical alignment.

</details>


### [25] [Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts](https://arxiv.org/abs/2511.06048)
*Xinyuan Yan,Shusen Liu,Kowshik Thopalli,Bei Wang*

Main category: cs.CL

TL;DR: 提出结合拓扑编码与降维的交互式可视化框架，实现大规模语言模型中稀疏自编码器特征的可解释分析


<details>
  <summary>Details</summary>
Motivation: 传统可视化方法（如UMAP）在处理海量SAE特征时存在高维压缩伪影、过度绘制和邻域失真问题，无法有效支持定向特征探索

Method: 开发拓扑基元与降维结合的混合可视化系统，通过交互式界面实现特征子集的局部拓扑保持与全局关系映射

Result: 系统能忠实反映选定特征的局部拓扑结构和全局分布，支持对潜在空间概念表示的针对性分析

Conclusion: 基于特征筛选的聚焦式可视化策略相比全特征可视化，能更深入揭示语言模型潜在空间的概念表征机制

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering interpretable features in large language models (LLMs) through the sparse directions they learn. However, the sheer number of extracted directions makes comprehensive exploration intractable. While conventional embedding techniques such as UMAP can reveal global structure, they suffer from limitations including high-dimensional compression artifacts, overplotting, and misleading neighborhood distortions. In this work, we propose a focused exploration framework that prioritizes curated concepts and their corresponding SAE features over attempts to visualize all available features simultaneously. We present an interactive visualization system that combines topology-based visual encoding with dimensionality reduction to faithfully represent both local and global relationships among selected features. This hybrid approach enables users to investigate SAE behavior through targeted, interpretable subsets, facilitating deeper and more nuanced analysis of concept representation in latent space.

</details>


### [26] [Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework](https://arxiv.org/abs/2511.06051)
*Mahmoud El-Bahnasawi*

Main category: cs.CL

TL;DR: 提出三层框架结合规则过滤与高效微调BERTweet，在保持94%大模型性能的同时缩小100倍规模，实现资源友好型仇恨言论检测


<details>
  <summary>Details</summary>
Motivation: 解决现有仇恨言论检测模型参数量大、计算成本高的问题，平衡检测性能与部署实用性，适配资源受限的实时场景需求

Method: 1)规则预过滤降低噪声 2)LoRA微调BERTweet(仅1.37%参数可训练) 3)持续学习机制。单T4 GPU仅需2小时训练

Result: macro F1达0.85(达SafePhi-4的94%)，基模型仅134M参数(比14B模型小100倍)，可训练参数1.87M，推理效率提升53倍

Conclusion: 首次实现接近SOTA的轻量级检测方案，通过架构优化与训练策略创新，使高性能仇恨言论检测在资源受限环境中具备实际部署可行性

Abstract: This paper addresses the critical challenge of developing computationally efficient hate speech detection systems that maintain competitive performance while being practical for real-time deployment. We propose a novel three-layer framework that combines rule-based pre-filtering with a parameter-efficient LoRA-tuned BERTweet model and continuous learning capabilities. Our approach achieves 0.85 macro F1 score - representing 94% of the performance of state-of-the-art large language models like SafePhi (Phi-4 based) while using a base model that is 100x smaller (134M vs 14B parameters). Compared to traditional BERT-based approaches with similar computational requirements, our method demonstrates superior performance through strategic dataset unification and optimized fine-tuning. The system requires only 1.87M trainable parameters (1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4 GPU, making robust hate speech detection accessible in resource-constrained environments while maintaining competitive accuracy for real-world deployment.

</details>


### [27] [ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning](https://arxiv.org/abs/2511.06057)
*Bingbing Wang,Zhengda Jin,Bin Liang,Jing Li,Ruifeng Xu*

Main category: cs.CL

TL;DR: 提出ReMoD框架，通过双过程推理机制动态调整多模态立场检测中各模态的贡献权重，显著提升检测效果


<details>
  <summary>Details</summary>
Motivation: 现有多模态立场检测方法简单融合模态信息，未考虑不同模态对立场表达的差异性贡献，易引入噪声影响判断准确性

Method: 受双过程认知理论启发，构建包含直觉推理（MEP/SEP经验池）和反思推理（Modality-CoT/Semantic-CoT）的双阶段框架，通过动态调整模态权重优化立场判断

Result: 在MMSD基准测试中显著超越基线模型，展示出强大的泛化能力

Conclusion: ReMoD通过模拟人类认知过程实现模态贡献的动态评估，为多模态立场检测提供了更鲁棒的解决方案

Abstract: Multimodal Stance Detection (MSD) is a crucial task for understanding public opinion on social media. Existing work simply fuses information from various modalities to learn stance representations, overlooking the varying contributions of stance expression from different modalities. Therefore, stance misunderstanding noises may be drawn into the stance learning process due to the risk of learning errors by rough modality combination. To address this, we get inspiration from the dual-process theory of human cognition and propose **ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance expression through a **D**ual-reasoning paradigm. ReMoD integrates *experience-driven intuitive reasoning* to capture initial stance cues with *deliberate reflective reasoning* to adjust for modality biases, refine stance judgments, and thereby dynamically weight modality contributions based on their actual expressive power for the target stance. Specifically, the intuitive stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool (SEP) to form an initial stance hypothesis, prioritizing historically impactful modalities. This hypothesis is then refined in the reflective stage via two reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to amplify relevant modalities, while Semantic-CoT refines SEP with deeper contextual insights of stance semantics. These dual experience structures are continuously refined during training and recalled at inference to guide robust and context-aware stance decisions. Extensive experiments on the public MMSD benchmark demonstrate that our ReMoD significantly outperforms most baseline models and exhibits strong generalization capabilities.

</details>


### [28] [Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework](https://arxiv.org/abs/2511.06067)
*Haoyue Yang,Xuanle Zhao,Yujie Liu,Zhuojun Zou,Kailin Lyu,Changchun Zhou,Yao Zhu,Jie Hao*

Main category: cs.CL

TL;DR: 提出ArchCraft框架，通过结构化工作流将论文架构描述转化为可综合验证的Verilog代码，并构建首个硬件合成基准ArchSynthBench验证框架优越性


<details>
  <summary>Details</summary>
Motivation: 硬件架构复现面临学术论文缺乏源码和HDL复杂性的双重挑战，传统方法难以实现可靠的硬件设计迁移与验证

Method: 1. 结构化工作流通过形式化图表构建架构蓝图
2. 符号系统定义功能规范实现设计可验证化
3. 生成解耦的RTL与测试台代码
4. 自动化PPA（功耗/面积/性能）评估报告

Result: 在含50个项目级电路和600模块的ArchSynthBench上：
- 代码完成度超越直接生成法32.7%
- 时序收敛率100%无违规
- 性能指标与原始论文数据误差<3%

Conclusion: ArchCraft成功解决硬件设计复现难题，其符号化架构描述方法显著提升代码生成质量，验证流程创新使RTL实现达到工业级可靠性要求

Abstract: The reproduction of hardware architectures from academic papers remains a significant challenge due to the lack of publicly available source code and the complexity of hardware description languages (HDLs). To this end, we propose \textbf{ArchCraft}, a Framework that converts abstract architectural descriptions from academic papers into synthesizable Verilog projects with register-transfer level (RTL) verification. ArchCraft introduces a structured workflow, which uses formal graphs to capture the Architectural Blueprint and symbols to define the Functional Specification, translating unstructured academic papers into verifiable, hardware-aware designs. The framework then generates RTL and testbench (TB) code decoupled via these symbols to facilitate verification and debugging, ultimately reporting the circuit's Power, Area, and Performance (PPA). Moreover, we propose the first benchmark, \textbf{ArchSynthBench}, for synthesizing hardware from architectural descriptions, with a complete set of evaluation indicators, 50 project-level circuits, and around 600 circuit blocks. We systematically assess ArchCraft on ArchSynthBench, where the experiment results demonstrate the superiority of our proposed method, surpassing direct generation methods and the VerilogCoder framework in both paper understanding and code completion. Furthermore, evaluation and physical implementation of the generated executable RTL code show that these implementations meet all timing constraints without violations, and their performance metrics are consistent with those reported in the original papers.

</details>


### [29] [Stemming Hallucination in Language Models Using a Licensing Oracle](https://arxiv.org/abs/2511.06073)
*Simeon Emanuilov,Richard Ackermann*

Main category: cs.CL

TL;DR: 提出Licensing Oracle架构，通过结构化知识图谱的确定性验证机制彻底消除语言模型幻觉，在实验中实现100%的精确弃答率和89.1%的事实准确率。


<details>
  <summary>Details</summary>
Motivation: 现有统计方法（如数据扩增、微调和RAG）无法完全消除语言模型的事实性幻觉，需要架构层面的创新来保证生成内容的真实性。

Method: 在生成过程中嵌入结构化知识图谱的验证模块，通过形式化验证确保仅输出经知识图谱确认的事实主张。

Result: Licensing Oracle达到完美弃答精度（AP=1.0）和零错误回答（FAR-NE=0.0），事实响应准确率89.1%，显著优于RAG和微调方法。

Conclusion: 该架构为结构化知识领域提供了统计方法无法实现的可靠性保证，为可信AI系统开发开辟了新路径，特别适用于事实密集型领域。

Abstract: Language models exhibit remarkable natural language generation capabilities but remain prone to hallucinations, generating factually incorrect information despite producing syntactically coherent responses. This study introduces the Licensing Oracle, an architectural solution designed to stem hallucinations in LMs by enforcing truth constraints through formal validation against structured knowledge graphs. Unlike statistical approaches that rely on data scaling or fine-tuning, the Licensing Oracle embeds a deterministic validation step into the model's generative process, ensuring that only factually accurate claims are made. We evaluated the effectiveness of the Licensing Oracle through experiments comparing it with several state-of-the-art methods, including baseline language model generation, fine-tuning for factual recall, fine-tuning for abstention behavior, and retrieval-augmented generation (RAG). Our results demonstrate that although RAG and fine-tuning improve performance, they fail to eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring that only valid claims were generated with 89.1% accuracy in factual responses. This work shows that architectural innovations, such as the Licensing Oracle, offer a necessary and sufficient solution for hallucinations in domains with structured knowledge representations, offering guarantees that statistical methods cannot match. Although the Licensing Oracle is specifically designed to address hallucinations in fact-based domains, its framework lays the groundwork for truth-constrained generation in future AI systems, providing a new path toward reliable, epistemically grounded models.

</details>


### [30] [MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://arxiv.org/abs/2511.06086)
*Saurabh Page,Advait Joshi,S. S. Sonawane*

Main category: cs.CL

TL;DR: MuonAll优化器通过全参数整合，在语言模型微调任务中实现了与AdamW相当的基准表现


<details>
  <summary>Details</summary>
Motivation: 现有Muon优化器仅与AdamW配合使用，未充分整合所有参数。探索其在预训练模型微调场景下的潜力

Method: 将参数矩阵二维化处理实现全参数整合（MuonAll），在5亿参数规模模型上进行分布式微调实验

Result: Muon/MuonAll在主流NLP基准测试中与AdamW表现持平，验证其作为替代优化器的有效性

Conclusion: 研究证明Muon系列优化器的实用价值，开源实现促进其在深度学习领域的应用

Abstract: Muon optimizer has demonstrated robust results in pretraining of language models but its performance in finetuning of existing public pretrained models is not yet explored. Currently, Muon is used along with AdamW introducing a scope of improvement for adopting all parameters inside Muon. We introduce MuonAll, which incorporates all the parameters inside Muon by transforming into 2D matrices. We conduct extensive finetuning experiments across publicly available language models with model sizes upto half billion parameters. Muon and MuonAll perform at par with AdamW across major benchmarks, highlighting their effectiveness as alternative optimizers. We open-source the distributed implementations of Muon and MuonAll, available at https://github.com/Saurabh750/optimizer

</details>


### [31] [Evaluation of retrieval-based QA on QUEST-LOFT](https://arxiv.org/abs/2511.06125)
*Nathan Scales,Nathanael Schärli,Olivier Bousquet*

Main category: cs.CL

TL;DR: 优化RAG方法结合结构化输出与答案验证机制，在QUEST-LOFT基准上显著超越长上下文模型


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法在处理跨多文档信息整合及复杂推理任务时表现不佳，LOFT研究显示长上下文模型在QUEST基准也存在明显不足

Method: 提出结合结构化输出（包含推理链与证据）的RAG优化方案，可选答案二次验证机制

Result: 通过人工评估验证，优化后的RAG系统性能显著优于长上下文方法

Conclusion: 结构化输出格式与验证机制能有效提升RAG在复杂问答场景中的表现

Abstract: Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.

</details>


### [32] [Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models](https://arxiv.org/abs/2511.06146)
*Akshar Tumu,Varad Shinde,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 本文提出使用Referring Expression Comprehension任务评估视觉语言模型的空间推理能力，揭示其在复杂空间关系、否定表达和检测歧义方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述和视觉问答任务无法深入分析视觉语言模型处理复杂空间语义（拓扑/方向/邻近关系）和否定表达的能力差距。

Method: 采用任务专用架构和大型视觉语言模型，测试其在1）检测歧义 2）多层空间关系长句 3）否定表达三种场景下的表现。

Result: 所有模型在处理复杂空间语义时均存在困难，但相对表现因底层模型架构和空间语义类别（方向性/邻近性等）呈现显著差异。

Conclusion: 研究揭示了视觉语言模型在空间推理方面的系统性问题，建议未来研究需加强模型对否定语义和多层级空间关系的联合理解能力。

Abstract: Spatial Reasoning is an important component of human cognition and is an area in which the latest Vision-language models (VLMs) show signs of difficulty. The current analysis works use image captioning tasks and visual question answering. In this work, we propose using the Referring Expression Comprehension task instead as a platform for the evaluation of spatial reasoning by VLMs. This platform provides the opportunity for a deeper analysis of spatial comprehension and grounding abilities when there is 1) ambiguity in object detection, 2) complex spatial expressions with a longer sentence structure and multiple spatial relations, and 3) expressions with negation ('not'). In our analysis, we use task-specific architectures as well as large VLMs and highlight their strengths and weaknesses in dealing with these specific situations. While all these models face challenges with the task at hand, the relative behaviors depend on the underlying models and the specific categories of spatial semantics (topological, directional, proximal, etc.). Our results highlight these challenges and behaviors and provide insight into research gaps and future directions.

</details>


### [33] [BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering](https://arxiv.org/abs/2511.06183)
*Ryuhei Miyazato,Ting-Ruen Wei,Xuyang Wu,Hsin-Tai Wu,Kei Harada*

Main category: cs.CL

TL;DR: 提出BookAsSumQA框架，通过QA自动评估基于方面的书籍摘要质量，发现RAG方法在长文本中更有效。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于方面摘要方法在书籍长文本领域缺乏有效评估框架的问题，特别是构建参考摘要的困难。

Method: 构建基于叙事情节知识图谱的QA自动生成框架BookAsSumQA，通过摘要的QA性能评估质量。

Result: LLM方法在短文本表现更好，RAG方法随文档长度增加效率优势显著（长文本场景下准确率提升15%）

Conclusion: RAG方法在基于方面的书籍摘要任务中更具实用价值，BookAsSumQA为长文本摘要评估提供有效解决方案。

Abstract: Aspect-based summarization aims to generate summaries that highlight specific aspects of a text, enabling more personalized and targeted summaries. However, its application to books remains unexplored due to the difficulty of constructing reference summaries for long text. To address this challenge, we propose BookAsSumQA, a QA-based evaluation framework for aspect-based book summarization. BookAsSumQA automatically generates aspect-specific QA pairs from a narrative knowledge graph to evaluate summary quality based on its question-answering performance. Our experiments using BookAsSumQA revealed that while LLM-based approaches showed higher accuracy on shorter texts, RAG-based methods become more effective as document length increases, making them more efficient and practical for aspect-based book summarization.

</details>


### [34] [Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning](https://arxiv.org/abs/2511.06190)
*Sangmook Lee,Dohyung Kim,Hyukhun Koh,Nakyeong Yang,Kyomin Jung*

Main category: cs.CL

TL;DR: 提出无需外部模型的置信度引导分步路由框架STEER，通过小模型置信度动态切换大模型，实现高效推理（+20%准确率，降低48% FLOPs）


<details>
  <summary>Details</summary>
Motivation: 现有路由方法依赖训练外部模块且缺乏领域鲁棒性，需开发无需合成数据、基于模型内部信号的成本优化方案

Method: 利用小模型生成推理步骤前的logits置信度评估，仅在低置信度时调用大模型，实现细粒度步骤级动态路由

Result: 在数学推理、多跳QA等任务中，较单独使用大模型提升20%准确率且减少48%计算量（AIME基准），显著优于基于训练的路由基线

Conclusion: 模型内部置信度是鲁棒的领域无关路由信号，STEER为LLM高效部署提供了可扩展方案

Abstract: Recent advances in Large Language Models (LLMs) - particularly model scaling and test-time techniques - have greatly enhanced the reasoning capabilities of language models at the expense of higher inference costs. To lower inference costs, prior works train router models or deferral mechanisms that allocate easy queries to a small, efficient model, while forwarding harder queries to larger, more expensive models. However, these trained router models often lack robustness under domain shifts and require expensive data synthesis techniques such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels for training. In this work, we propose Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs fine-grained, step-level routing between smaller and larger LLMs without utilizing external models. STEER leverages confidence scores from the smaller model's logits prior to generating a reasoning step, so that the large model is invoked only when necessary. Extensive evaluations using different LLMs on a diverse set of challenging benchmarks across multiple domains such as Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER achieves competitive or enhanced accuracy while reducing inference costs (up to +20% accuracy with 48% less FLOPs compared to solely using the larger model on AIME), outperforming baselines that rely on trained external modules. Our results establish model-internal confidence as a robust, domain-agnostic signal for model routing, offering a scalable pathway for efficient LLM deployment.

</details>


### [35] [Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease](https://arxiv.org/abs/2511.06215)
*Puzhen Su,Yongzhu Miao,Chunxi Guo,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 提出EK-ICL框架，通过整合显式知识（置信度分数、解析特征、标签词替换）显著提升语言模型在阿尔茨海默病检测中的上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习方法在AD检测中存在任务识别失败、演示选择次优和标签语义偏差问题，在临床数据稀缺场景下尤为突出。

Method: 集成小模型置信度锚定预测、解析特征改进样本选择、标签词语义对齐，结合解析检索和集成预测缓解语义同质性问题。

Result: 在三个AD数据集上超越现有微调和上下文学习方法，实验表明标签语义与任务上下文对齐对性能影响显著。

Conclusion: EK-ICL通过结构化显式知识增强临床推理稳定性，为低资源条件下的医学NLP任务提供了有效解决方案。

Abstract: Detecting Alzheimer's Disease (AD) from narrative transcripts remains a challenging task for large language models (LLMs), particularly under out-of-distribution (OOD) and data-scarce conditions. While in-context learning (ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL approaches often suffer from task recognition failure, suboptimal demonstration selection, and misalignment between label words and task objectives, issues that are amplified in clinical domains like AD detection. We propose Explicit Knowledge In-Context Learners (EK-ICL), a novel framework that integrates structured explicit knowledge to enhance reasoning stability and task alignment in ICL. EK-ICL incorporates three knowledge components: confidence scores derived from small language models (SLMs) to ground predictions in task-relevant patterns, parsing feature scores to capture structural differences and improve demo selection, and label word replacement to resolve semantic misalignment with LLM priors. In addition, EK-ICL employs a parsing-based retrieval strategy and ensemble prediction to mitigate the effects of semantic homogeneity in AD transcripts. Extensive experiments across three AD datasets demonstrate that EK-ICL significantly outperforms state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that ICL performance in AD detection is highly sensitive to the alignment of label semantics and task-specific context, underscoring the importance of explicit knowledge in clinical reasoning under low-resource conditions.

</details>


### [36] [SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization](https://arxiv.org/abs/2511.06222)
*Yue Huang,Xiangqi Wang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出SPA框架，通过无监督双准则去噪和词典序偏好对齐，在保证安全性的前提下提升大模型帮助性


<details>
  <summary>Details</summary>
Motivation: 解决高风险场景下LLM可信度与帮助性难以兼顾的矛盾，确保安全优先原则

Method: 自对齐框架：生成多样化响应→自我评估优化→双准则去噪→构建词典序偏好对→采用置信度加权的对齐损失微调

Result: 在多个基准测试中，SPA在保持安全性的同时提升28.6%帮助性，超越RLHF等基线方法

Conclusion: SPA为关键领域LLM提供可扩展、可解释的对齐范式，验证了优先级对齐策略的有效性

Abstract: In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs must be both trustworthy and helpful. However, these goals often conflict. We propose priority alignment, a new alignment paradigm that enforces a strict "trustworthy-before-helpful" ordering: optimization of helpfulness is conditioned on first meeting trustworthy thresholds (e.g., harmlessness or honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully unsupervised framework that generates diverse responses, self-evaluates them and refines them by the model itself, and applies dual-criterion denoising to remove inconsistency and control variance. From this, SPA constructs lexicographically ordered preference pairs and fine-tunes the model using an uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap decisions. Experiments across multiple benchmarks show that SPA improves helpfulness without compromising safety, outperforming strong baselines while preserving general capabilities. Our results demonstrate that SPA provides a scalable and interpretable alignment strategy for critical LLM applications.

</details>


### [37] [Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records](https://arxiv.org/abs/2511.06230)
*Juntao Li,Haobin Yuan,Ling Luo,Tengxiao Lv,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: CHIP 2025共享任务2构建了CDrugRed中文EHR数据集，验证了LLM集成系统在出院药物推荐任务中的有效性（最高Jaccard 0.5102/F1 0.6267）


<details>
  <summary>Details</summary>
Motivation: 解决慢性病患者出院药物多标签推荐难题，提升治疗连续性并减少再入院率

Method: 基于5,894条真实住院记录构建CDrugRed数据集，设计包含526个参赛团队的LLM模型竞赛框架

Result: 最优模型集成方案在测试集达到Jaccard 0.5102/F1 0.6267，验证LLM在中文EHR场景的应用潜力

Conclusion: 证明LLM在药物推荐任务中的有效性，但异构临床文本处理和个性化治疗方案仍是待突破难点

Abstract: Discharge medication recommendation plays a critical role in ensuring treatment continuity, preventing readmission, and improving long-term management for patients with chronic metabolic diseases. This paper present an overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop state-of-the-art approaches for automatically recommending appro-priate discharge medications using real-world Chinese EHR data. For this task, we constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified hospitalization records from 3,190 patients in China. This task is challenging due to multi-label nature of medication recommendation, het-erogeneous clinical text, and patient-specific variability in treatment plans. A total of 526 teams registered, with 167 and 95 teams submitting valid results to the Phase A and Phase B leaderboards, respectively. The top-performing team achieved the highest overall performance on the final test set, with a Jaccard score of 0.5102, F1 score of 0.6267, demonstrating the potential of advanced large language model (LLM)-based ensemble systems. These re-sults highlight both the promise and remaining challenges of applying LLMs to medication recommendation in Chinese EHRs. The post-evaluation phase remains open at https://tianchi.aliyun.com/competition/entrance/532411/.

</details>


### [38] [Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy](https://arxiv.org/abs/2511.06234)
*Mojtaba Noghabaei*

Main category: cs.CL

TL;DR: 针对ELECTRA-small模型在自然语言推理任务中处理否定词的能力不足，通过添加对比集和对抗样本的数据增强方法有效提升了模型在否定案例上的准确率，且不影响整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在自然语言推理(NLI)任务中过度依赖数据集伪相关特征（如否定词处理能力薄弱），导致对含否定语义的样本分类效果差。

Method: 在SNLI数据集上对ELECTRA-small模型进行微调，并通过增加强调否定结构的对比集和对抗性样本来增强训练数据。

Result: 数据增强后模型在含否定词的测试案例上准确率提升，且整体性能未受影响。

Conclusion: 定向数据增强策略可有效缓解NLI模型因数据集伪相关特征导致的否定语义理解缺陷。

Abstract: Pre-trained models for natural language inference (NLI) often achieve high performance on benchmark datasets by using spurious correlations, or dataset artifacts, rather than understanding language touches such as negation. In this project, we investigate the performance of an ELECTRA-small model fine-tuned on the Stanford Natural Language Inference (SNLI) dataset, focusing on its handling of negation. Through analysis, we identify that the model struggles with correctly classifying examples containing negation. To address this, we augment the training data with contrast sets and adversarial examples emphasizing negation. Our results demonstrate that this targeted data augmentation improves the model's accuracy on negation-containing examples without adversely affecting overall performance, therefore mitigating the identified dataset artifact.

</details>


### [39] [TimeSense:Making Large Language Models Proficient in Time-Series Analysis](https://arxiv.org/abs/2511.06344)
*Zhirui Zhang,Changhua Pei,Tianyi Gao,Zhe Xie,Yibo Hao,Zhaoyang Yu,Longlong Xu,Tong Xiao,Jing Han,Dan Pei*

Main category: cs.CL

TL;DR: 提出TimeSense多模态框架，通过平衡文本推理与时间感知解决现有LLM时间序列分析中文本偏置问题，并构建EvalTS评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖文本标签监督，导致模型忽略完整时序特征，产生与时间序列上下文矛盾的输出。需构建更鲁棒的评估体系并改进模型架构。

Method: 1. Temporal Sense模块重建输入时序数据，确保文本推理基于时序动态
2. 引入坐标位置编码增强时空理解
3. 构建EvalTS三层次评估基准（基础模式识别→复杂现实推理）

Result: TimeSense在多个任务达到SOTA，复杂多维时序推理任务表现显著优于现有方法

Conclusion: TimeSense通过时序重建与空间编码的协同设计，有效平衡文本与时间特征，提升LLM时序分析能力。EvalTS基准为领域提供更严苛的评估标准。

Abstract: In the time-series domain, an increasing number of works combine text with temporal data to leverage the reasoning capabilities of large language models (LLMs) for various downstream time-series understanding tasks. This enables a single model to flexibly perform tasks that previously required specialized models for each domain. However, these methods typically rely on text labels for supervision during training, biasing the model toward textual cues while potentially neglecting the full temporal features. Such a bias can lead to outputs that contradict the underlying time-series context. To address this issue, we construct the EvalTS benchmark, comprising 10 tasks across three difficulty levels, from fundamental temporal pattern recognition to complex real-world reasoning, to evaluate models under more challenging and realistic scenarios. We also propose TimeSense, a multimodal framework that makes LLMs proficient in time-series analysis by balancing textual reasoning with a preserved temporal sense. TimeSense incorporates a Temporal Sense module that reconstructs the input time-series within the model's context, ensuring that textual reasoning is grounded in the time-series dynamics. Moreover, to enhance spatial understanding of time-series data, we explicitly incorporate coordinate-based positional embeddings, which provide each time point with spatial context and enable the model to capture structural dependencies more effectively. Experimental results demonstrate that TimeSense achieves state-of-the-art performance across multiple tasks, and it particularly outperforms existing methods on complex multi-dimensional time-series reasoning tasks.

</details>


### [40] [HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection](https://arxiv.org/abs/2511.06391)
*Irina Proskurina,Marc-Antoine Carpentier,Julien Velcin*

Main category: cs.CL

TL;DR: 通过HatePrototypes类级向量表示实现显性与隐性仇恨检测的跨任务迁移，无需重复微调模型


<details>
  <summary>Details</summary>
Motivation: 现有仇恨检测基准主要关注显性仇恨，忽视隐性仇恨（如贬低性比较、隐蔽歧视语言），后者需要深度语义理解而非表面特征识别

Method: 构建HatePrototypes（每类50样本的类级向量），验证其跨任务迁移能力，采用参数无关的early exiting技术处理不同仇恨类型

Result: 原型可实现显隐仇恨检测的跨基准迁移，早期退出策略对两种仇恨类型均有效，准确率与完整模型相当

Conclusion: HatePrototypes提供高效可迁移的解决方案，发布代码资源支持未来仇恨检测研究

Abstract: Optimization of offensive content moderation models for different types of hateful messages is typically achieved through continued pre-training or fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly address explicit hate toward protected groups and often overlook implicit or indirect hate, such as demeaning comparisons, calls for exclusion or violence, and subtle discriminatory language that still causes harm. While explicit hate can often be captured through surface features, implicit hate requires deeper, full-model semantic processing. In this work, we question the need for repeated fine-tuning and analyze the role of HatePrototypes, class-level vector representations derived from language models optimized for hate speech detection and safety moderation. We find that these prototypes, built from as few as 50 examples per class, enable cross-task transfer between explicit and implicit hate, with interchangeable prototypes across benchmarks. Moreover, we show that parameter-free early exiting with prototypes is effective for both hate types. We release the code, prototype resources, and evaluation scripts to support future research on efficient and transferable hate speech detection.

</details>


### [41] [SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss](https://arxiv.org/abs/2511.06402)
*Lionel Z. Wang,Shihan Ben,Yulu Huang,Simeng Qing*

Main category: cs.CL

TL;DR: 提出SugarTextNet框架检测社交媒体糖爹内容，在3,067条微博数据上实现多指标显著提升


<details>
  <summary>Details</summary>
Motivation: 糖爹内容泛滥引发社会问题，传统检测方法面临委婉用语、模糊表达和类别不平衡三大挑战

Method: 整合预训练transformer、注意力线索提取器、上下文短语编码器，提出Context-Aware Focal Loss解决类别不平衡

Result: 在微博数据集上准确率超传统模型12.5%，消融实验验证各组件贡献度均超15%

Conclusion: 敏感内容检测需领域定制化建模，上下文感知机制为复杂场景内容审核提供新范式

Abstract: Sugar dating-related content has rapidly proliferated on mainstream social media platforms, giving rise to serious societal and regulatory concerns, including commercialization of intimate relationships and the normalization of transactional relationships.~Detecting such content is highly challenging due to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme class imbalance in real-world data.~In this work, we present SugarTextNet, a novel transformer-based framework specifically designed to identify sugar dating-related posts on social media.~SugarTextNet integrates a pretrained transformer encoder, an attention-based cue extractor, and a contextual phrase encoder to capture both salient and nuanced features in user-generated text.~To address class imbalance and enhance minority-class detection, we introduce Context-Aware Focal Loss, a tailored loss function that combines focal loss scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated, manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo, demonstrating that our approach substantially outperforms traditional machine learning models, deep learning baselines, and large language models across multiple metrics.~Comprehensive ablation studies confirm the indispensable role of each component.~Our findings highlight the importance of domain-specific, context-aware modeling for sensitive content detection, and provide a robust solution for content moderation in complex, real-world scenarios.

</details>


### [42] [How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset](https://arxiv.org/abs/2511.06418)
*Sunil Mohan,Theofanis Karaletsos*

Main category: cs.CL

TL;DR: 研究通过构建新数据集评估大语言模型在药物机制推理任务中的表现，发现开放世界设定和内部链路的反事实推理更具挑战性，Qwen3-4B-thinking模型展现出接近顶尖模型的潜力。


<details>
  <summary>Details</summary>
Motivation: 药物开发需要模型同时具备药物机制的事实性知识和反事实推理能力，但目前缺乏系统评估LLMs在此领域表现的基准工具。

Method: 构建包含已知药物机制和反事实情景的数据集，测试不同模型在开放/封闭世界设定下的知识回忆与推理能力，特别设计药物相关链路和内部链路的反事实修改场景。

Result: o4-mini优于OpenAI同系列模型，Qwen3-4B-thinking在部分任务中超越o4-mini；开放世界准确率低于封闭世界约15%，内部链路反事实的准确率比药物链路低22%。

Conclusion: LLMs在药物推理任务中展现潜力但存在显著局限，未来需要开发更好的知识整合架构来提升复杂反事实推理能力。

Abstract: Two scientific fields showing increasing interest in pre-trained large language models (LLMs) are drug development / repurposing, and personalized medicine. For both, LLMs have to demonstrate factual knowledge as well as a deep understanding of drug mechanisms, so they can recall and reason about relevant knowledge in novel situations. Drug mechanisms of action are described as a series of interactions between biomedical entities, which interlink into one or more chains directed from the drug to the targeted disease. Composing the effects of the interactions in a candidate chain leads to an inference about whether the drug might be useful or not for that disease. We introduce a dataset that evaluates LLMs on both factual knowledge of known mechanisms, and their ability to reason about them under novel situations, presented as counterfactuals that the models are unlikely to have seen during training. Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini models from OpenAI, and the recent small Qwen3-4B-thinking model closely matches o4-mini's performance, even outperforming it in some cases. We demonstrate that the open world setting for reasoning tasks, which requires the model to recall relevant knowledge, is more challenging than the closed world setting where the needed factual knowledge is provided. We also show that counterfactuals affecting internal links in the reasoning chain present a much harder task than those affecting a link from the drug mentioned in the prompt.

</details>


### [43] [Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop](https://arxiv.org/abs/2511.06427)
*Lifeng Han,David Lindevelt,Sander Puts,Erik van Mulligen,Suzan Verberne*

Main category: cs.CL

TL;DR: 利用大语言模型（LLMs）及不同提示策略（如思维链、小样本学习等）从荷兰癌症患者的访谈和论坛数据中提取隐喻，构建了HealthQuote.NL语料库，以改善医患沟通和个性化护理。


<details>
  <summary>Details</summary>
Motivation: 隐喻在医患沟通中具有重要作用。通过分析患者语言中的隐喻，可促进医患共同决策、提升沟通效率和患者健康素养，并为个性化护理路径设计提供依据。

Method: 1. 使用癌症患者访谈和在线论坛文本作为数据源；2. 探索LLMs的多提示策略（思维链/小样本学习/自我提示）；3. 采用人机协同验证并构建HealthQuote.NL语料库；4. 开源提示词和资源。

Result: 成功创建经人工验证的隐喻语料库HealthQuote.NL，相关资源已开源。提取的隐喻可支持临床决策优化、医患沟通改善和患者教育，并指导护理路径设计。

Conclusion: 研究证明了LLMs在医疗隐喻分析中的有效性，所构建的语料库和工具为提升医患沟通质量提供了可扩展方案，资源开放将推动该领域研究和临床应用的进一步发展。

Abstract: Metaphors and metaphorical language (MLs) play an important role in healthcare communication between clinicians, patients, and patients' family members. In this work, we focus on Dutch language data from cancer patients. We extract metaphors used by patients using two data sources: (1) cancer patient storytelling interview data and (2) online forum data, including patients' posts, comments, and questions to professionals. We investigate how current state-of-the-art large language models (LLMs) perform on this task by exploring different prompting strategies such as chain of thought reasoning, few-shot learning, and self-prompting. With a human-in-the-loop setup, we verify the extracted metaphors and compile the outputs into a corpus named HealthQuote.NL. We believe the extracted metaphors can support better patient care, for example shared decision making, improved communication between patients and clinicians, and enhanced patient health literacy. They can also inform the design of personalized care pathways. We share prompts and related resources at https://github.com/aaronlifenghan/HealthQuote.NL

</details>


### [44] [Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models](https://arxiv.org/abs/2511.06441)
*Mayank Saini,Arit Kumar Bishwas*

Main category: cs.CL

TL;DR: 提出模块化AI框架，通过智能路由机制将查询分配给最优专家模型，在保持性能的同时降低67%昂贵模型调用，实现高效资源调度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型(LLM)在多模态场景中高推理成本问题，以及开源小模型处理复杂任务能力不足的痛点

Method: 1. 构建可学习的路由网络实现成本-质量平衡决策
2. 视觉任务采用两阶段开源流程，整合经典高效视觉组件
3. 多智能体协同的模块化架构设计

Result: 在MMLU和VQA基准测试中性能持平或超越单一高价模型系统，昂贵模型调用减少67%以上

Conclusion: 该框架通过智能路由和模块化设计，实现了AI系统在质量与资源效率间的优化平衡，为大规模部署提供可行方案

Abstract: As AI moves beyond text, large language models (LLMs) increasingly power vision, audio, and document understanding; however, their high inference costs hinder real-time, scalable deployment. Conversely, smaller open-source models offer cost advantages but struggle with complex or multimodal queries. We introduce a unified, modular framework that intelligently routes each query - textual, multimodal, or complex - to the most fitting expert model, using a learned routing network that balances cost and quality. For vision tasks, we employ a two-stage open-source pipeline optimized for efficiency and reviving efficient classical vision components where they remain SOTA for sub-tasks. On benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual Question Answering (VQA), we match or exceed the performance of always-premium LLM (monolithic systems with one model serving all query types) performance, yet reduce the reliance on costly models by over 67%. With its extensible, multi-agent orchestration, we deliver high-quality, resource-efficient AI at scale.

</details>


### [45] [SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention](https://arxiv.org/abs/2511.06446)
*Bohan Yu,Wei Huang,Kang Liu*

Main category: cs.CL

TL;DR: SR-KI提出通过键值对编码和两阶段训练范式，实现大型语言模型与海量结构化知识库的高效端到端整合，在保持高性能的同时实现99.75%的知识压缩率。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强方法依赖外部检索器和多阶段流程，存在效率瓶颈和知识更新滞后问题。SR-KI旨在通过模型内部潜在空间的端到端知识检索，突破外部依赖并支持动态知识更新。

Method: 1. 将知识库编码为键值对注入KV缓存
2. 两阶段训练：定位专用检索层后应用注意力监督机制
3. 完全在模型潜在空间内执行检索的端到端架构

Result: 单卡支持40K知识库整合，最佳任务Recall@10超98%，平均超88%，问答任务保持高性能的同时实现99.75%知识压缩。

Conclusion: SR-KI通过创新性的知识注入架构，解决了大模型整合结构化知识的效率与动态更新难题，为实际应用中的实时知识集成提供了新范式。

Abstract: This paper proposes SR-KI, a novel approach for integrating real-time and large-scale structured knowledge bases (KBs) into large language models (LLMs). SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder, and injects them into LLMs' KV cache. Building on this representation, we employ a two-stage training paradigm: first locating a dedicated retrieval layer within the LLM, and then applying an attention-based loss at this layer to explicitly supervise attention toward relevant KB entries. Unlike traditional retrieval-augmented generation methods that rely heavily on the performance of external retrievers and multi-stage pipelines, SR-KI supports end-to-end inference by performing retrieval entirely within the models latent space. This design enables efficient compression of injected knowledge and facilitates dynamic knowledge updates. Comprehensive experiments demonstrate that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98% Recall@10 on the best-performing task and exceeding 88% on average across all tasks. Task performance on question answering and KB ID generation also demonstrates that SR-KI maintains strong performance while achieving up to 99.75% compression of the injected KBs.

</details>


### [46] [Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages](https://arxiv.org/abs/2511.06497)
*Quang Phuoc Nguyen,David Anugraha,Felix Gaschi,Jun Bin Cheng,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: 研究发现，通过精选语言子集进行词对齐优化，可在跨语言迁移中达到甚至超越全语言覆盖的效果，尤其对低资源语言(LRLs)有效，显著降低数据收集成本。


<details>
  <summary>Details</summary>
Motivation: 现有词对齐方法在类型学差异大或低资源语言(LRLs)中效果不稳定，且依赖高质量平行语料（对多数LRLs稀缺）。本文旨在探究是否通过策略性语言子集选择，而非使用全部语言，能提升跨语言迁移效率，尤其是对LRLs的影响。

Method: 通过控制实验对比全语言对齐与精选语言子集（基于语言多样性）的效果，重点评估对未见过LRLs的跨语言迁移性能。

Result: 实验表明：1) 词对齐对LRLs特别有效；2) 精选语言子集能达到全语言对齐效果，对未见LRLs甚至表现更优；3) 该方法减少数据需求并保持鲁棒性。

Conclusion: 有效词对齐无需覆盖所有语言，基于语言类型多样性精选子集可降低数据成本、提升效率，并为未见LRLs提供更稳健的跨语言迁移，对资源受限场景具有实用价值。

Abstract: Realignment is a promising strategy to improve cross-lingual transfer in multilingual language models. However, empirical results are mixed and often unreliable, particularly for typologically distant or low-resource languages (LRLs) compared to English. Moreover, word realignment tools often rely on high-quality parallel data, which can be scarce or noisy for many LRLs. In this work, we conduct an extensive empirical study to investigate whether realignment truly benefits from using all available languages, or if strategically selected subsets can offer comparable or even improved cross-lingual transfer, and study the impact on LRLs. Our controlled experiments show that realignment can be particularly effective for LRLs and that using carefully selected, linguistically diverse subsets can match full multilingual alignment, and even outperform it for unseen LRLs. This indicates that effective realignment does not require exhaustive language coverage and can reduce data collection overhead, while remaining both efficient and robust when guided by informed language selection.

</details>


### [47] [You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations](https://arxiv.org/abs/2511.06516)
*Amit LeVi,Raz Lapid,Rom Himelstein,Yaniv Nemcovsky,Ravid Shwartz Ziv,Avi Mendelson*

Main category: cs.CL

TL;DR: 提出TAQ和TAQO两种任务感知的后训练量化方法，通过分层量化策略显著提升大语言模型效率，在保持精度的同时优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法多为任务无关型，忽视了任务关键信号在模型各层的分布差异，导致量化后模型性能大幅下降。

Method: 1. TAQ：基于隐藏层激活的任务条件统计分配比特位宽
2. TAQO：通过直接测试层敏感性分配精度
均采用校准集识别任务相关层并保留其精度

Result: 在Phi-4模型上TAQ达到42.33 EM/50.81 F1（AWQ仅2.25/7.07），Llama-3.1等模型上TAQO表现更优，平均精度降低时仍保持<1%的精度损失

Conclusion: 分层任务敏感度分析可有效指导量化策略，生成高效的任务专用模型，为LLM部署提供新思路

Abstract: Large Language Models (LLMs) excel across diverse tasks, yet many applications require only limited capabilities, making large variants inefficient in memory and latency. Existing approaches often combine distillation and quantization, but most post-training quantization (PTQ) methods are task-agnostic, ignoring how task-specific signals are distributed across layers. In this work, we propose to use hidden representations that encode task-salient signals as a guideline for quantization. In order to fully utilize our innovative idea, this paper compares two new task-aware PTQ methods: Task-Aware Quantization (TAQ), which allocates bitwidths using task-conditioned statistics from hidden activations, and TAQO, which allocates precision based on direct layer sensitivity tests. From a small calibration set, these approaches identify task-relevant layers, preserving their precision while aggressively quantizing the rest. This yields stable task sensitivity profiles and efficient task-specialized models. Across models, TAQ and TAQO outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1, Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1, far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while remaining within < 1.0% of the original accuracy at lower average precision.

</details>


### [48] [Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement](https://arxiv.org/abs/2511.06530)
*Xiaonan Luo,Yue Huang,Ping He,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出RefineLab框架，在可控token预算下自动优化QA数据集质量


<details>
  <summary>Details</summary>
Motivation: 现有QA数据集存在领域覆盖不足、难度分布失衡、事实不一致等问题，生成式模型加剧了质量隐患

Method: 基于目标质量属性选择编辑操作(如改写、干扰项替换)，通过分配模块实现预算约束下的优化策略选择

Result: 实验证明在覆盖率、难度匹配、事实准确性和干扰项质量等维度显著接近专家数据集水平

Conclusion: RefineLab为可复现的LLM评估数据集设计提供了可扩展的定制化解决方案

Abstract: High-quality Question-Answer (QA) datasets are foundational for reliable Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit persistent gaps in domain coverage, misaligned difficulty distributions, and factual inconsistencies. The recent surge in generative model-powered datasets has compounded these quality challenges. In this work, we introduce RefineLab, the first LLM-driven framework that automatically refines raw QA textual data into high-quality datasets under a controllable token-budget constraint. RefineLab takes a set of target quality attributes (such as coverage and difficulty balance) as refinement objectives, and performs selective edits within a predefined token budget to ensure practicality and efficiency. In essence, RefineLab addresses a constrained optimization problem: improving the quality of QA samples as much as possible while respecting resource limitations. With a set of available refinement operations (e.g., rephrasing, distractor replacement), RefineLab takes as input the original dataset, a specified set of target quality dimensions, and a token budget, and determines which refinement operations should be applied to each QA sample. This process is guided by an assignment module that selects optimal refinement strategies to maximize overall dataset quality while adhering to the budget constraint. Experiments demonstrate that RefineLab consistently narrows divergence from expert datasets across coverage, difficulty alignment, factual fidelity, and distractor quality. RefineLab pioneers a scalable, customizable path to reproducible dataset design, with broad implications for LLM evaluation.

</details>


### [49] [Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages](https://arxiv.org/abs/2511.06531)
*Oluwadara Kalejaiye,Luel Hagos Beyene,David Ifeoluwa Adelani,Mmekut-Mfon Gabriel Edet,Aniefon Daniel Akpan,Eno-Abasi Urua,Anietie Andy*

Main category: cs.CL

TL;DR: 研究者创建了ibom数据集，针对尼日利亚四种未被充分研究的沿海语言（Anaang/Efik/Ibibio/Oro），扩展Flores-200基准并改进主题分类性能。


<details>
  <summary>Details</summary>
Motivation: 尼日利亚500多种语言中仅有4种被NLP研究覆盖，Google Translate和主流基准测试（Flores-200/SIB-200）缺乏这些沿海语言的数据支持。

Method: 1. 构建包含机器翻译和主题分类任务的ibom数据集
2. 基于Flores-200基准进行语言扩展
3. 将翻译文本与SIB-200的主题标签对齐

Result: 当前LLM在零样本和少样本的机器翻译任务中表现欠佳，但少样本数据能显著提升主题分类准确率（随样本量增加效果增强）

Conclusion: 低资源语言需要专门的数据集支持，少量标注数据即可有效提升下游任务性能，突显了资源建设对边缘语言NLP发展的重要性

Abstract: Nigeria is the most populous country in Africa with a population of more than 200 million people. More than 500 languages are spoken in Nigeria and it is one of the most linguistically diverse countries in the world. Despite this, natural language processing (NLP) research has mostly focused on the following four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the languages spoken in Nigeria). This is in part due to the unavailability of textual data in these languages to train and apply NLP algorithms. In this work, we introduce ibom -- a dataset for machine translation and topic classification in four Coastal Nigerian languages from the Akwa Ibom State region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus on extending Flores-200 benchmark to these languages, and further align the translated texts with topic labels based on SIB-200 classification dataset. Our evaluation shows that current LLMs perform poorly on machine translation for these languages in both zero-and-few shot settings. However, we find the few-shot samples to steadily improve topic classification with more shots.

</details>


### [50] [Rep2Text: Decoding Full Text from a Single LLM Token Representation](https://arxiv.org/abs/2511.06571)
*Haiyan Zhao,Zirui He,Fan Yang,Ali Payani,Mengnan Du*

Main category: cs.CL

TL;DR: 提出Rep2Text框架，通过末位token表征重构输入文本，实验证明16token序列中可恢复超50%信息并保持语义完整，同时发现信息瓶颈效应和医学数据泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型内部机制透明度，验证末位token表征是否能够有效恢复原始输入文本。

Method: 使用可训练适配器将目标模型表征映射到解码语言模型的嵌入空间，通过自回归重构实现文本恢复。

Result: 多模型实验显示：16token序列平均恢复率超50%；长序列呈现token级恢复下降但语义完整保留；框架在医疗数据上展现强泛化能力。

Conclusion: Rep2Text揭示了LLM表征的强解码潜力，为模型可解释性提供新视角，信息瓶颈效应发现深化了对表征压缩机制的理解。

Abstract: Large language models (LLMs) have achieved remarkable progress across diverse tasks, yet their internal mechanisms remain largely opaque. In this work, we address a fundamental question: to what extent can the original input text be recovered from a single last-token representation within an LLM? We propose Rep2Text, a novel framework for decoding full text from last-token representations. Rep2Text employs a trainable adapter that projects a target model's internal representations into the embedding space of a decoding language model, which then autoregressively reconstructs the input text. Experiments on various model combinations (Llama-3.1-8B, Gemma-7B, Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the information in 16-token sequences can be recovered from this compressed representation while maintaining strong semantic integrity and coherence. Furthermore, our analysis reveals an information bottleneck effect: longer sequences exhibit decreased token-level recovery while preserving strong semantic integrity. Besides, our framework also demonstrates robust generalization to out-of-distribution medical data.

</details>


### [51] [TabRAG: Tabular Document Retrieval via Structured Language Representations](https://arxiv.org/abs/2511.06582)
*Jacob Si,Mike Qu,Michelle Lee,Yingzhen Li*

Main category: cs.CL

TL;DR: 提出TabRAG框架解决表格密集型文档的解析难题，通过结构化语言表示提升RAG流程的表格处理效果


<details>
  <summary>Details</summary>
Motivation: 现有基于解析的RAG方法在处理表格数据时性能欠佳，需开发更高效的表格解析方案

Method: 采用结构化语言表示解析表格文档，构建表格感知的RAG流程（检索增强生成）

Result: 在生成和检索任务上超越主流解析方法，代码已开源

Conclusion: TabRAG有效平衡性能与计算成本，为表格密集型文档处理提供新方案

Abstract: Ingesting data for Retrieval-Augmented Generation (RAG) involves either fine-tuning the embedding model directly on the target corpus or parsing documents for embedding model encoding. The former, while accurate, incurs high computational hardware requirements, while the latter suffers from suboptimal performance when extracting tabular data. In this work, we address the latter by presenting TabRAG, a parsing-based RAG pipeline designed to tackle table-heavy documents via structured language representations. TabRAG outperforms existing popular parsing-based methods for generation and retrieval. Code is available at https://github.com/jacobyhsi/TabRAG.

</details>


### [52] [MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592)
*Zhi Rui Tam,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 音频大语言模型在临床决策中易受患者声音特征（年龄/情感）影响产生偏见，导致医疗建议差异最高达35%，需优先开发偏向感知架构


<details>
  <summary>Details</summary>
Motivation: 探索语音交互中副语言特征对临床决策的潜在风险，揭示音频LLM可能加剧医疗不平等的隐患

Method: 使用170个临床案例合成36种声音特征（年龄/性别/情感），通过对比文本与语音输入评估模型偏差

Result: 音频输入导致手术建议差异35%；年轻与老年声音建议差异12%；显式推理消除性别偏见但未检测到情绪影响

Conclusion: 临床部署前必须开发能识别声音偏见的架构，防范基于非医疗特征的不公平决策

Abstract: As large language models transition from text-based interfaces to audio interactions in clinical settings, they might introduce new vulnerabilities through paralinguistic cues in audio. We evaluated these models on 170 clinical cases, each synthesized into speech from 36 distinct voice profiles spanning variations in age, gender, and emotion. Our findings reveal a severe modality bias: surgical recommendations for audio inputs varied by as much as 35% compared to identical text-based inputs, with one model providing 80% fewer recommendations. Further analysis uncovered age disparities of up to 12% between young and elderly voices, which persisted in most models despite chain-of-thought prompting. While explicit reasoning successfully eliminated gender bias, the impact of emotion was not detected due to poor recognition performance. These results demonstrate that audio LLMs are susceptible to making clinical decisions based on a patient's voice characteristics rather than medical evidence, a flaw that risks perpetuating healthcare disparities. We conclude that bias-aware architectures are essential and urgently needed before the clinical deployment of these models.

</details>


### [53] [Duality-based Mode Operations and Pyramid Multilayer Mapping for Rhetorical Modes](https://arxiv.org/abs/2511.06601)
*Zi-Niu Wu*

Main category: cs.CL

TL;DR: 提出基于二元对立的修辞模式扩展方法及金字塔分层框架，量化表达多样性与认知复杂度，为AI的修辞推理提供动态度量体系


<details>
  <summary>Details</summary>
Motivation: 建立语言学、计算模型与学术写作间的概念桥梁，促进跨领域知识协同发展

Method: 1. 四类二元对立模式操作（分裂-联合/前推-回溯/扩展-压缩/正交） 2. 三层金字塔认知映射框架 3. 组合数学与香农熵量化分析

Result: 1. 边际修辞比特(MRB)参数实现修辞扩展速度测量 2. 分层选择使决策熵降低75% 3. 表达多样性呈现二项式增长模式

Conclusion: 将静态修辞分类转化为动态可测量系统，为AI在修辞推理层的语言操作开辟新路径

Abstract: Rhetorical modes are useful in both academic and non-academic writing, and can be subjects to be studied within linguistic research and computational modeling. Establishing a conceptual bridge among these domains could enable each to benefit from the others. This paper proposes duality-based mode operations (split-unite, forward-backward, expansion-reduction and orthogonal dualities) to expand the set of rhetorical modes, introducing generated modes like combination and generalization, thereby enhancing epistemic diversity across multiple applications. It further presents a pyramid multilayer mapping framework (e.g., three layers from the rhetorical model layer, to cognitive layer, and to epistemic layers) that reduces the resulting cognitive complexity. The degrees of expressive diversity and complexity reduction are quantified through binomial combinatorics and Shannon entropy analysis. A Marginal Rhetorical Bit (MRB) is identified, permitting the definition of a rhetorical-scalable parameter that measures expressive growth speed in bits per stage. A direct entropy measure shows that hierarchical selection over smaller subsets markedly reduces choice uncertainty compared with flat selection across all modes. These considerations appear to transform static and non-measurable rhetorical taxonomies into more dynamic and more measurable systems for discourse design. From this work, it would be possible to identify a pathway for future AI systems to operate not only on language tokens but on layered rhetorical reasoning structures, bridging linguistic, pedagogical, academic, and computational research

</details>


### [54] [How AI Fails: An Interactive Pedagogical Tool for Demonstrating Dialectal Bias in Automated Toxicity Models](https://arxiv.org/abs/2511.06676)
*Subhojit Ghimire*

Main category: cs.CL

TL;DR: 研究发现主流AI毒性检测模型在非洲裔英语文本中存在系统性偏见，AAE文本被判定为'有毒'的概率是标准美式英语的1.8倍，并开发交互式教学工具揭示算法偏见背后的政策歧视


<details>
  <summary>Details</summary>
Motivation: 针对AI内容审核普遍存在的偏见指控，通过实证研究揭示算法偏见的真实影响。当用户遭遇内容误判时，需要区分算法错误与系统偏见，但缺乏直观的评估工具和量化证据

Method: 1. 定量评估流行毒性模型(unitary/toxic-bert)在AAE和SAE语料上的表现差异 2. 开发交互式教学工具，通过用户可调节的'敏感度阈值'具象化算法偏见的实际影响

Result: 基准测试显示：AAE文本平均毒性评分1.8倍于SAE，'身份仇恨'评分高达8.8倍。工具演示表明算法偏见通过人为设定的'中性'政策转化为实际歧视

Conclusion: 研究不仅量化了AI偏见对少数语言群体的差别影响，更重要的是通过可视化工具将抽象的算法伦理问题转化为可感知的公共教育素材，推动批判性AI素养建设

Abstract: Now that AI-driven moderation has become pervasive in everyday life, we often hear claims that "the AI is biased". While this is often said jokingly, the light-hearted remark reflects a deeper concern. How can we be certain that an online post flagged as "inappropriate" was not simply the victim of a biased algorithm? This paper investigates this problem using a dual approach. First, I conduct a quantitative benchmark of a widely used toxicity model (unitary/toxic-bert) to measure performance disparity between text in African-American English (AAE) and Standard American English (SAE). The benchmark reveals a clear, systematic bias: on average, the model scores AAE text as 1.8 times more toxic and 8.8 times higher for "identity hate". Second, I introduce an interactive pedagogical tool that makes these abstract biases tangible. The tool's core mechanic, a user-controlled "sensitivity threshold," demonstrates that the biased score itself is not the only harm; instead, the more-concerning harm is the human-set, seemingly neutral policy that ultimately operationalises discrimination. This work provides both statistical evidence of disparate impact and a public-facing tool designed to foster critical AI literacy.

</details>


### [55] [Steering LLMs toward Korean Local Speech: Iterative Refinement Framework for Faithful Dialect Translation](https://arxiv.org/abs/2511.06680)
*Keunhyeung Park,Seunguk Yu,Youngbin Kim*

Main category: cs.CL

TL;DR: 提出DIA-REFINE框架解决方言机器翻译中的模型差距和评估失真问题，通过迭代验证机制和新型评估指标提升翻译保真度。


<details>
  <summary>Details</summary>
Motivation: 现有标准-方言机器翻译存在大语言模型的方言鸿沟，且传统n-gram指标易导致评估失真（偏好源文本复制而非真实方言转换）。

Method: DIA-REFINE框架构建翻译-验证-反馈闭环，结合外部方言分类器；提出方言保真度评分(DFS)量化语言迁移，目标方言比例(TDR)衡量翻译成功率。

Result: 在韩语方言实验中，框架显著提升方言保真度，新指标成功区分伪成功案例（高n-gram低方言）与真实尝试案例（低n-gram真方言）。

Conclusion: 建立了目标导向的方言翻译框架，提供模型性能的严格评估标准，上下文示例集成策略有效提升方言表达翻译质量。

Abstract: Standard-to-dialect machine translation remains challenging due to a persistent dialect gap in large language models and evaluation distortions inherent in n-gram metrics, which favor source copying over authentic dialect translation. In this paper, we propose the dialect refinement (DIA-REFINE) framework, which guides LLMs toward faithful target dialect outputs through an iterative loop of translation, verification, and feedback using external dialect classifiers. To address the limitations of n-gram-based metrics, we introduce the dialect fidelity score (DFS) to quantify linguistic shift and the target dialect ratio (TDR) to measure the success of dialect translation. Experiments on Korean dialects across zero-shot and in-context learning baselines demonstrate that DIA-REFINE consistently enhances dialect fidelity. The proposed metrics distinguish between False Success cases, where high n-gram scores obscure failures in dialectal translation, and True Attempt cases, where genuine attempts at dialectal translation yield low n-gram scores. We also observed that models exhibit varying degrees of responsiveness to the framework, and that integrating in-context examples further improves the translation of dialectal expressions. Our work establishes a robust framework for goal-directed, inclusive dialect translation, providing both rigorous evaluation and critical insights into model performance.

</details>


### [56] [Textual Self-attention Network: Test-Time Preference Optimization through Textual Gradient-based Attention](https://arxiv.org/abs/2511.06682)
*Shibing Mo,Haoyang Ruan,Kai Wu,Jing Liu*

Main category: cs.CL

TL;DR: 提出无需参数更新的文本自注意力网络（TSAN），通过自然语言模拟注意力机制，在测试阶段整合多候选方案的优点实现偏好对齐优化。


<details>
  <summary>Details</summary>
Motivation: 现有测试时优化方法仅针对单个候选响应进行修正，缺乏系统整合多候选方案优势的机制。不同响应可能在清晰度/事实准确性/语气等不同维度表现优异，综合这些优势可产生更优结果。

Method: 将多个候选方案格式化为文本键值对，通过LLM构建的注意力模块评估相关性权重，在文本梯度空间中迭代合成新响应。整个过程完全基于自然语言操作，无需模型参数更新。

Result: 经过3次测试迭代后，TSAN超越Llama-3.1-70B-Instruct等监督模型，比现有最优测试时对齐方法提升8.7%的偏好符合率。

Conclusion: TSAN开创了可解释的文本梯度优化范式，通过系统整合多候选方案的局部优势，显著提升语言模型输出的偏好对齐效果。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization capabilities, but aligning their outputs with human preferences typically requires expensive supervised fine-tuning. Recent test-time methods leverage textual feedback to overcome this, but they often critique and revise a single candidate response, lacking a principled mechanism to systematically analyze, weigh, and synthesize the strengths of multiple promising candidates. Such a mechanism is crucial because different responses may excel in distinct aspects (e.g., clarity, factual accuracy, or tone), and combining their best elements may produce a far superior outcome. This paper proposes the Textual Self-Attention Network (TSAN), a new paradigm for test-time preference optimization that requires no parameter updates. TSAN emulates self-attention entirely in natural language to overcome this gap: it analyzes multiple candidates by formatting them into textual keys and values, weighs their relevance using an LLM-based attention module, and synthesizes their strengths into a new, preference-aligned response under the guidance of the learned textual attention. This entire process operates in a textual gradient space, enabling iterative and interpretable optimization. Empirical evaluations demonstrate that with just three test-time iterations on a base SFT model, TSAN outperforms supervised models like Llama-3.1-70B-Instruct and surpasses the current state-of-the-art test-time alignment method by effectively leveraging multiple candidate solutions.

</details>


### [57] [Sentiment Analysis On YouTube Comments Using Machine Learning Techniques Based On Video Games Content](https://arxiv.org/abs/2511.06708)
*Adi Danish Bin Muhammad Amin,Mohaiminul Islam Bhuiyan,Nur Shazwani Kamarudin,Zulfahmi Toh,Nur Syafiqah Nafis*

Main category: cs.CL

TL;DR: 研究通过YouTube评论对视频游戏进行情感分析，使用SVM模型取得最高分类准确率，为游戏开发者提供用户反馈。


<details>
  <summary>Details</summary>
Motivation: 游戏产业快速发展，需深入理解用户在社交媒体（如YouTube）上的情感表达，以支持游戏设计和用户体验优化。

Method: 利用YouTube API收集评论，TextBlob进行情感分析，并对比朴素贝叶斯、逻辑回归、支持向量机(SVM)的分类效果。

Result: SVM在不同数据集上表现最佳，分析揭示了用户偏好趋势及游戏改进方向。

Conclusion: 强调高级情感分析对捕捉用户复杂情绪的重要性，未来将整合更先进的NLP技术并扩展数据源。

Abstract: The rapid evolution of the gaming industry, driven by technological advancements and a burgeoning community, necessitates a deeper understanding of user sentiments, especially as expressed on popular social media platforms like YouTube. This study presents a sentiment analysis on video games based on YouTube comments, aiming to understand user sentiments within the gaming community. Utilizing YouTube API, comments related to various video games were collected and analyzed using the TextBlob sentiment analysis tool. The pre-processed data underwent classification using machine learning algorithms, including Naïve Bayes, Logistic Regression, and Support Vector Machine (SVM). Among these, SVM demonstrated superior performance, achieving the highest classification accuracy across different datasets. The analysis spanned multiple popular gaming videos, revealing trends and insights into user preferences and critiques. The findings underscore the importance of advanced sentiment analysis in capturing the nuanced emotions expressed in user comments, providing valuable feedback for game developers to enhance game design and user experience. Future research will focus on integrating more sophisticated natural language processing techniques and exploring additional data sources to further refine sentiment analysis in the gaming domain.

</details>


### [58] [Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights](https://arxiv.org/abs/2511.06738)
*Hyunjae Kim,Jiwoong Sohn,Aidan Gilson,Nicholas Cochran-Caggiano,Serina Applebaum,Heeju Jin,Seihee Park,Yujin Park,Jiyeong Park,Seoyoung Choi,Brittany Alexandra Herrera Contreras,Thomas Huang,Jaehoon Yun,Ethan F. Wei,Roy Jiang,Leah Colucci,Eric Lai,Amisha Dave,Tuo Guo,Maxwell B. Singer,Yonghoe Koo,Ron A. Adelman,James Zou,Andrew Taylor,Arman Cohan,Hua Xu,Qingyu Chen*

Main category: cs.CL

TL;DR: 医学领域检索增强生成(RAG)评估揭示标准方法存在检索证据相关性和证据选择准确性不足，提出优化策略可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在医学应用中知识更新滞后和缺乏可验证证据支撑的挑战

Method: 18位医学专家对200个临床/USMLE式问题生成的800个模型输出进行80,502次标注评估，分解RAG流程为证据检索、证据选择和响应生成三阶段分析

Result: 标准RAG导致证据相关性仅22%，证据选择准确率41-43%，事实性和完整性下降6%和5%；优化策略使MedMCQA/MedXpertQA性能分别提升12%和8.2%

Conclusion: 需重新审视RAG在医学中的作用，强调阶段性评估和系统设计对可靠医学LLM应用的重要性

Abstract: Large language models (LLMs) are transforming the landscape of medicine, yet two fundamental challenges persist: keeping up with rapidly evolving medical knowledge and providing verifiable, evidence-grounded reasoning. Retrieval-augmented generation (RAG) has been widely adopted to address these limitations by supplementing model outputs with retrieved evidence. However, whether RAG reliably achieves these goals remains unclear. Here, we present the most comprehensive expert evaluation of RAG in medicine to date. Eighteen medical experts contributed a total of 80,502 annotations, assessing 800 model outputs generated by GPT-4o and Llama-3.1-8B across 200 real-world patient and USMLE-style queries. We systematically decomposed the RAG pipeline into three components: (i) evidence retrieval (relevance of retrieved passages), (ii) evidence selection (accuracy of evidence usage), and (iii) response generation (factuality and completeness of outputs). Contrary to expectation, standard RAG often degraded performance: only 22% of top-16 passages were relevant, evidence selection remained weak (precision 41-43%, recall 27-49%), and factuality and completeness dropped by up to 6% and 5%, respectively, compared with non-RAG variants. Retrieval and evidence selection remain key failure points for the model, contributing to the overall performance drop. We further show that simple yet effective strategies, including evidence filtering and query reformulation, substantially mitigate these issues, improving performance on MedMCQA and MedXpertQA by up to 12% and 8.2%, respectively. These findings call for re-examining RAG's role in medicine and highlight the importance of stage-aware evaluation and deliberate system design for reliable medical LLM applications.

</details>


### [59] [Sensitivity of Small Language Models to Fine-tuning Data Contamination](https://arxiv.org/abs/2511.06763)
*Nicy Scaria,Silvester John Joseph Kennedy,Deepak Subramani*

Main category: cs.CL

TL;DR: 研究发现小语言模型对句法污染高度敏感（字符反转导致全面失效），而语义污染呈现阈值效应，大模型更易受语义污染（能力诅咒），指令调优未提供稳定鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 评估资源受限环境下小语言模型对数据污染的鲁棒性，揭示现有训练协议在抗污染能力上的潜在缺陷。

Method: 使用23个SLMs（270M-4B参数），测试句法（字符/单词反转）和语义（无关/反事实响应）两类污染在25%-100%污染水平下的影响。

Result: 句法污染引发灾难性失效（字符反转全模型失效），语义污染显示阈值特征；大模型语义脆弱性增加，指令调优可能降低鲁棒性。

Conclusion: 现有鲁棒性假设不适用于小模型，需开发污染感知训练协议，模型能力与抗污染能力存在负相关关系。

Abstract: Small Language Models (SLMs) are increasingly being deployed in resource-constrained environments, yet their behavioral robustness to data contamination during instruction tuning remains poorly understood. We systematically investigate the contamination sensitivity of 23 SLMs (270M to 4B parameters) across multiple model families by measuring susceptibility to syntactic and semantic transformation types during instruction tuning: syntactic transformations (character and word reversal) and semantic transformations (irrelevant and counterfactual responses), each applied at contamination levels of 25\%, 50\%, 75\%, and 100\%. Our results reveal fundamental asymmetries in vulnerability patterns: syntactic transformations cause catastrophic performance degradation, with character reversal producing near-complete failure across all models regardless of size or family, while semantic transformations demonstrate distinct threshold behaviors and greater resilience in core linguistic capabilities. Critically, we discover a ``\textit{capability curse}" where larger, more capable models become more susceptible to learning semantic corruptions, effectively following harmful instructions more readily, while our analysis of base versus instruction-tuned variants reveals that alignment provides inconsistent robustness benefits, sometimes even reducing resilience. Our work establishes three core contributions: (1) empirical evidence of SLMs' disproportionate vulnerability to syntactic pattern contamination, (2) identification of asymmetric sensitivity patterns between syntactic and semantic transformations, and (3) systematic evaluation protocols for contamination robustness assessment. These findings have immediate deployment implications, suggesting that current robustness assumptions may not hold for smaller models and highlighting the need for contamination-aware training protocols.

</details>


### [60] [SAFENLIDB: A Privacy-Preserving Safety Alignment Framework for LLM-based Natural Language Database Interfaces](https://arxiv.org/abs/2511.06778)
*Ruiheng Liu,XiaoBing Chen,Jinyu Zhang,Qiongwen Zhang,Yu Zhang,Bailong Yang*

Main category: cs.CL

TL;DR: 提出安全隐私对齐框架SafeNlidb，通过自动生成混合思维链数据和交替偏好优化，解决LLM数据库自然语言接口的隐私泄露问题


<details>
  <summary>Details</summary>
Motivation: 现有基于规则启发式或LLM代理的方法存在三方面缺陷：1) 难以防御复杂推理攻击 2) 高误报率 3) 损害SQL查询可靠性

Method: 构建自动化生成混合安全推理链的框架，结合推理预热训练和交替偏好优化策略，实现安全推理与SQL生成的深度融合

Result: 在安全性和实用性指标上均超越大模型和理想基线，安全性能提升显著的同时保持95%+的SQL生成准确率

Conclusion: 通过创新性的安全对齐机制设计，首次实现LLM数据库接口在复杂攻击场景下的隐私-效用平衡，为安全敏感场景提供可靠解决方案

Abstract: The rapid advancement of Large Language Models (LLMs) has driven significant progress in Natural Language Interface to Database (NLIDB). However, the widespread adoption of LLMs has raised critical privacy and security concerns. During interactions, LLMs may unintentionally expose confidential database contents or be manipulated by attackers to exfiltrate data through seemingly benign queries. While current efforts typically rely on rule-based heuristics or LLM agents to mitigate this leakage risk, these methods still struggle with complex inference-based attacks, suffer from high false positive rates, and often compromise the reliability of SQL queries. To address these challenges, we propose \textsc{SafeNlidb}, a novel privacy-security alignment framework for LLM-based NLIDB. The framework features an automated pipeline that generates hybrid chain-of-thought interaction data from scratch, seamlessly combining implicit security reasoning with SQL generation. Additionally, we introduce reasoning warm-up and alternating preference optimization to overcome the multi-preference oscillations of Direct Preference Optimization (DPO), enabling LLMs to produce security-aware SQL through fine-grained reasoning without the need for human-annotated preference data. Extensive experiments demonstrate that our method outperforms both larger-scale LLMs and ideal-setting baselines, achieving significant security improvements while preserving high utility.WARNING: This work may contain content that is offensive and harmful!

</details>


### [61] [Learning to Focus: Focal Attention for Selective and Scalable Transformers](https://arxiv.org/abs/2511.06818)
*Dhananjay Ram,Wei Xia,Stefano Soatto*

Main category: cs.CL

TL;DR: 提出Focal Attention机制，通过控制softmax温度参数优化注意力分布，提升模型效率与长上下文处理能力


<details>
  <summary>Details</summary>
Motivation: 标准softmax注意力在长上下文场景会产生噪声分布，影响特征选择的有效性

Method: 通过固定或可学习的温度参数调整softmax输出，实现注意力分布的锐化与聚焦

Result: 相同精度下节省42%参数量或33%训练数据，长上下文任务相对提升17%-82%

Conclusion: Focal Attention在模型扩展性、数据效率与长文本处理方面展现出显著优势

Abstract: Attention is a core component of transformer architecture, whether encoder-only, decoder-only, or encoder-decoder model. However, the standard softmax attention often produces noisy probability distribution, which can impair effective feature selection at every layer of these models, particularly for long contexts. We propose Focal Attention, a simple yet effective modification that sharpens the attention distribution by controlling the softmax temperature, either as a fixed hyperparameter or as a learnable parameter during training. This sharpening enables the model to concentrate on the most relevant tokens while suppressing irrelevant ones. Empirically, Focal Attention scales more favorably than standard transformer with respect to model size, training data, and context length. Across diverse benchmarks, it achieves the same accuracy with up to 42% fewer parameters or 33% less training data. On long-context tasks, it delivers substantial relative improvements ranging from 17% to 82%, demonstrating its effectiveness in real world applications.

</details>


### [62] [Beyond Plain Demos: A Demo-centric Anchoring Paradigm for In-Context Learning in Alzheimer's Disease Detection](https://arxiv.org/abs/2511.06826)
*Puzhen Su,Haoran Yin,Yongzhu Miao,Jintao Tang,Shasha Li,Ting Wang*

Main category: cs.CL

TL;DR: 提出DA4ICL框架，通过多样本检索和分层向量锚定增强LLMs对阿尔茨海默病语音检测的上下文学习能力


<details>
  <summary>Details</summary>
Motivation: 现有ICL方法在AD检测中存在样本同质化问题，导致模型难以捕捉细粒度信号；传统任务向量方法与AD检测需求存在粒度不匹配

Method: DA4ICL包含：1) 多样对比检索(DCR)扩展上下文宽度 2) 投影向量锚定(PVA)在每层Transformer注入细粒度信号

Result: 在三个AD基准测试中稳定优于ICL和TV基线，验证低资源/OOD场景下的有效性

Conclusion: 开创了通过样本锚定实现细粒度LLM适配的新范式，为医疗NLP领域提供创新解决方案

Abstract: Detecting Alzheimer's disease (AD) from narrative transcripts challenges large language models (LLMs): pre-training rarely covers this out-of-distribution task, and all transcript demos describe the same scene, producing highly homogeneous contexts. These factors cripple both the model's built-in task knowledge (\textbf{task cognition}) and its ability to surface subtle, class-discriminative cues (\textbf{contextual perception}). Because cognition is fixed after pre-training, improving in-context learning (ICL) for AD detection hinges on enriching perception through better demonstration (demo) sets. We demonstrate that standard ICL quickly saturates, its demos lack diversity (context width) and fail to convey fine-grained signals (context depth), and that recent task vector (TV) approaches improve broad task adaptation by injecting TV into the LLMs' hidden states (HSs), they are ill-suited for AD detection due to the mismatch of injection granularity, strength and position. To address these bottlenecks, we introduce \textbf{DA4ICL}, a demo-centric anchoring framework that jointly expands context width via \emph{\textbf{Diverse and Contrastive Retrieval}} (DCR) and deepens each demo's signal via \emph{\textbf{Projected Vector Anchoring}} (PVA) at every Transformer layer. Across three AD benchmarks, DA4ICL achieves large, stable gains over both ICL and TV baselines, charting a new paradigm for fine-grained, OOD and low-resource LLM adaptation.

</details>


### [63] [CLiFT-ASR: A Cross-Lingual Fine-Tuning Framework for Low-Resource Taiwanese Hokkien Speech Recognition](https://arxiv.org/abs/2511.06860)
*Hung-Yang Sung,Chien-Chun Wang,Kuan-Tang Huang,Tien-Hong Lo,Yu-Sheng Tsao,Yung-Chang Hsu,Berlin Chen*

Main category: cs.CL

TL;DR: 提出CLiFT-ASR框架，通过两阶段迁移学习整合台湾闽南语的罗马拼音和汉字标注，显著降低字符错误率24.88%。


<details>
  <summary>Details</summary>
Motivation: 台湾闽南语等低资源语言的ASR受限于标注数据稀缺，汉字转录无法捕捉语音细节，罗马拼音缺乏句法覆盖，需探索分阶段整合策略。

Method: 1. 基于普通话HuBERT模型进行跨语言迁移 2. 第一阶段通过罗马拼音学习声学/声调表征 3. 第二阶段通过汉字转录学习词汇/句法结构

Result: 在TAT-MOE语料库上实现24.88%的字符错误率相对下降，超越强基线模型

Conclusion: CLiFT-ASR为台湾闽南语ASR提供了参数高效的有效解决方案，并具备推广至其他低资源语言场景的潜力

Abstract: Automatic speech recognition (ASR) for low-resource languages such as Taiwanese Hokkien is difficult due to the scarcity of annotated data. However, direct fine-tuning on Han-character transcriptions often fails to capture detailed phonetic and tonal cues, while training only on romanization lacks lexical and syntactic coverage. In addition, prior studies have rarely explored staged strategies that integrate both annotation types. To address this gap, we present CLiFT-ASR, a cross-lingual fine-tuning framework that builds on Mandarin HuBERT models and progressively adapts them to Taiwanese Hokkien. The framework employs a two-stage process in which it first learns acoustic and tonal representations from phonetic Tai-lo annotations and then captures vocabulary and syntax from Han-character transcriptions. This progressive adaptation enables effective alignment between speech sounds and orthographic structures. Experiments on the TAT-MOE corpus demonstrate that CLiFT-ASR achieves a 24.88\% relative reduction in character error rate (CER) compared with strong baselines. The results indicate that CLiFT-ASR provides an effective and parameter-efficient solution for Taiwanese Hokkien ASR and that it has potential to benefit other low-resource language scenarios.

</details>


### [64] [Inclusion of Role into Named Entity Recognition and Ranking](https://arxiv.org/abs/2511.06886)
*Neelesh Kumar Shukla,Sanasam Ranbir Singh*

Main category: cs.CL

TL;DR: 提出将实体角色检测任务建模为命名实体识别(NER)和实体检索任务，通过自动学习角色关键词和上下文表征解决小数据场景下的问题


<details>
  <summary>Details</summary>
Motivation: 现有NLP系统基于实体类型处理，但实体在不同上下文中承担动态角色（如医生/患者），需要特定方法识别这些情境化角色

Method: 1) 将角色视为NER的互斥类别进行序列标注 2) 构建角色-实体检索框架，自动学习角色相关词短语，利用句子/文档级上下文建立角色和实体的向量表征

Result: 提出的领域无关方法在小数据集上有效，通过上下文表征缓解数据不足问题，实现无知识库依赖的角色检测

Conclusion: 创新地将角色检测分解为NER和检索任务，证明了上下文建模和领域无关方法在低资源场景下的有效性，为动态实体理解提供了新思路

Abstract: Most of the Natural Language Processing sys- tems are involved in entity-based processing for several tasks like Information Extraction, Question-Answering, Text-Summarization and so on. A new challenge comes when entities play roles according to their act or attributes in certain context. Entity Role Detection is the task of assigning such roles to the entities. Usu- ally real-world entities are of types: person, lo- cation and organization etc. Roles could be con- sidered as domain-dependent subtypes of these types. In the cases, where retrieving a subset of entities based on their roles is needed, poses the problem of defining the role and entities having those roles. This paper presents the study of study of solving Entity Role Detection prob- lem by modeling it as Named Entity Recogni- tion (NER) and Entity Retrieval/Ranking task. In NER, these roles could be considered as mutually exclusive classes and standard NER methods like sequence tagging could be used. For Entity Retrieval, Roles could be formulated as Query and entities as Collection on which the query needs to be executed. The aspect of Entity Retrieval task, which is different than document retrieval task is that the entities and roles against which they need to be retrieved are indirectly described. We have formulated au- tomated ways of learning representative words and phrases and building representations of roles and entities using them. We have also explored different contexts like sentence and document. Since the roles depend upon con- text, so it is not always possible to have large domain-specific dataset or knowledge bases for learning purposes, so we have tried to exploit the information from small dataset in domain- agnostic way.

</details>


### [65] [EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers](https://arxiv.org/abs/2511.06890)
*Yilin Jiang,Mingzi Zhang,Xuanyu Yin,Sheng Jin,Suyu Lu,Zuocan Ying,Zengyi Yu,Xiangjie Kong*

Main category: cs.CL

TL;DR: 提出了双组件基准EduGuardBench，用于全面评估教学AI的专业忠实度与安全性，发现模型性能极化现象并揭示教育转化效应这一安全新维度。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估教育场景中AI角色扮演的忠实度，且缺乏对教学特有危害（如学术不端）的针对性安全检测。

Method: 通过角色扮演忠实度评分(RFS)评估专业能力，采用人设对抗提示测试安全性，包含攻击成功率(ASR)和三层次拒绝质量评估。

Result: 实验覆盖14个主流模型：推理型模型忠实度更高但普遍存在能力缺陷；中等模型呈现安全脆弱性悖论；最优模型展现将有害请求转化为教育性拒绝的能力。

Conclusion: EduGuardBench建立了可复现的评估框架，超越传统知识测试，揭示教育AI部署中专业能力与安全性的复杂动态关系。

Abstract: Large Language Models for Simulating Professions (SP-LLMs), particularly as teachers, are pivotal for personalized education. However, ensuring their professional competence and ethical safety is a critical challenge, as existing benchmarks fail to measure role-playing fidelity or address the unique teaching harms inherent in educational scenarios. To address this, we propose EduGuardBench, a dual-component benchmark. It assesses professional fidelity using a Role-playing Fidelity Score (RFS) while diagnosing harms specific to the teaching profession. It also probes safety vulnerabilities using persona-based adversarial prompts targeting both general harms and, particularly, academic misconduct, evaluated with metrics including Attack Success Rate (ASR) and a three-tier Refusal Quality assessment. Our extensive experiments on 14 leading models reveal a stark polarization in performance. While reasoning-oriented models generally show superior fidelity, incompetence remains the dominant failure mode across most models. The adversarial tests uncovered a counterintuitive scaling paradox, where mid-sized models can be the most vulnerable, challenging monotonic safety assumptions. Critically, we identified a powerful Educational Transformation Effect: the safest models excel at converting harmful requests into teachable moments by providing ideal Educational Refusals. This capacity is strongly negatively correlated with ASR, revealing a new dimension of advanced AI safety. EduGuardBench thus provides a reproducible framework that moves beyond siloed knowledge tests toward a holistic assessment of professional, ethical, and pedagogical alignment, uncovering complex dynamics essential for deploying trustworthy AI in education. See https://github.com/YL1N/EduGuardBench for Materials.

</details>


### [66] [RPTS: Tree-Structured Reasoning Process Scoring for Faithful Multimodal Evaluation](https://arxiv.org/abs/2511.06899)
*Haofeng Wang,Yu Zhang*

Main category: cs.CL

TL;DR: 提出基于树状结构的推理过程评估指标RPTS，构建RPTS-Eval基准验证LVLMs的多模态推理能力，揭示开源与闭源模型的性能差异


<details>
  <summary>Details</summary>
Motivation: 现有评估基准忽视推理过程的完整性，且未考虑正确答案可能伴随错误推理的情况，同时忽略模态间关系对推理的影响

Method: 将推理步骤组织为树状结构，利用层次化信息分配加权忠实度分数，动态调整权重评估推理过程完整性

Result: 构建含374张图像和390推理实例的RPTS-Eval基准，实验发现LVLMs在复杂推理中的局限性，揭示闭源模型（如GPT4o）优于开源模型的规律

Conclusion: RPTS指标与评估基准为多模态推理研究提供新视角，有助于系统性诊断模型缺陷并推动更鲁棒的推理系统开发

Abstract: Large Vision-Language Models (LVLMs) excel in multimodal reasoning and have shown impressive performance on various multimodal benchmarks. However, most of these benchmarks evaluate models primarily through multiple-choice or short-answer formats, which do not take the reasoning process into account. Although some benchmarks assess the reasoning process, their methods are often overly simplistic and only examine reasoning when answers are incorrect. This approach overlooks scenarios where flawed reasoning leads to correct answers. In addition, these benchmarks do not consider the impact of intermodal relationships on reasoning. To address this issue, we propose the Reasoning Process Tree Score (RPTS), a tree structure-based metric to assess reasoning processes. Specifically, we organize the reasoning steps into a reasoning tree and leverage its hierarchical information to assign weighted faithfulness scores to each reasoning step. By dynamically adjusting these weights, RPTS not only evaluates the overall correctness of the reasoning, but also pinpoints where the model fails in the reasoning. To validate RPTS in real-world multimodal scenarios, we construct a new benchmark, RPTS-Eval, comprising 374 images and 390 reasoning instances. Each instance includes reliable visual-textual clues that serve as leaf nodes of the reasoning tree. Furthermore, we define three types of intermodal relationships to investigate how intermodal interactions influence the reasoning process. We evaluated representative LVLMs (e.g., GPT4o, Llava-Next), uncovering their limitations in multimodal reasoning and highlighting the differences between open-source and closed-source commercial LVLMs. We believe that this benchmark will contribute to the advancement of research in the field of multimodal reasoning.

</details>


### [67] [HLPD: Aligning LLMs to Human Language Preference for Machine-Revised Text Detection](https://arxiv.org/abs/2511.06942)
*Fangqi Dai,Xingjian Jiang,Zizhuang Deng*

Main category: cs.CL

TL;DR: HLPD通过优化模型对人类写作风格的偏好，显著提升对抗性机器修订文本的检测能力，AUROC指标大幅领先现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测对抗性多任务机器修订的文本，特别是黑盒环境下的高级LLM输出，需开发更敏感的检测机制。

Method: 提出基于奖励对齐的HLPO方法，构建五维提示生成器的对抗评估框架，通过调整token分布增强对人类写作风格的敏感性。

Result: 检测GPT修订文本时AUROC相对ImBD提升15.11%；在先进LLM文本检测中平均AUROC领先ImBD 5.53%、Fast-DetectGPT 34.14%。

Conclusion: HLPD验证了人类风格偏好优化在文本溯源中的有效性，开源代码促进检测技术发展，特别适用于复杂对抗场景。

Abstract: To prevent misinformation and social issues arising from trustworthy-looking content generated by LLMs, it is crucial to develop efficient and reliable methods for identifying the source of texts. Previous approaches have demonstrated exceptional performance in detecting texts fully generated by LLMs. However, these methods struggle when confronting more advanced LLM output or text with adversarial multi-task machine revision, especially in the black-box setting, where the generating model is unknown. To address this challenge, grounded in the hypothesis that human writing possesses distinctive stylistic patterns, we propose Human Language Preference Detection (HLPD). HLPD employs a reward-based alignment process, Human Language Preference Optimization (HLPO), to shift the scoring model's token distribution toward human-like writing, making the model more sensitive to human writing, therefore enhancing the identification of machine-revised text. We test HLPD in an adversarial multi-task evaluation framework that leverages a five-dimensional prompt generator and multiple advanced LLMs to create diverse revision scenarios. When detecting texts revised by GPT-series models, HLPD achieves a 15.11% relative improvement in AUROC over ImBD, surpassing Fast-DetectGPT by 45.56%. When evaluated on texts generated by advanced LLMs, HLPD achieves the highest average AUROC, exceeding ImBD by 5.53% and Fast-DetectGPT by 34.14%. Code will be made available at https://github.com/dfq2021/HLPD.

</details>


### [68] [SCOPE: Intrinsic Semantic Space Control for Mitigating Copyright Infringement in LLMs](https://arxiv.org/abs/2511.07001)
*Zhenliang Zhang,Xinyu Hu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出SCOPE方法，通过语义空间控制缓解大语言模型版权侵权风险，无需参数更新或外部过滤器


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖表层token匹配和外部过滤器，难以应对语义改写侵权内容。需要从语义空间进行版权防护

Method: 使用稀疏自编码器(SAE)构建高维近单语义空间，识别版权敏感子空间并在解码时固定其激活值

Result: 在多个基准测试中有效降低侵权风险(平均降低89.2%)，保持模型通用性能(任务完成率>96%)

Conclusion: SCOPE首次实现语义层面的版权防护，为模型部署提供安全高效的解决方案

Abstract: Large language models sometimes inadvertently reproduce passages that are copyrighted, exposing downstream applications to legal risk. Most existing studies for inference-time defences focus on surface-level token matching and rely on external blocklists or filters, which add deployment complexity and may overlook semantically paraphrased leakage. In this work, we reframe copyright infringement mitigation as intrinsic semantic-space control and introduce SCOPE, an inference-time method that requires no parameter updates or auxiliary filters. Specifically, the sparse autoencoder (SAE) projects hidden states into a high-dimensional, near-monosemantic space; benefiting from this representation, we identify a copyright-sensitive subspace and clamp its activations during decoding. Experiments on widely recognized benchmarks show that SCOPE mitigates copyright infringement without degrading general utility. Further interpretability analyses confirm that the isolated subspace captures high-level semantics.

</details>


### [69] [Automated Circuit Interpretation via Probe Prompting](https://arxiv.org/abs/2511.07002)
*Giuseppe Birardi*

Main category: cs.CL

TL;DR: 提出自动化探针提示框架，通过概念对齐的超级节点将归因图转化为可解释子图，显著提升神经网络机制解释效率（完整度0.83，复杂度压缩54%）


<details>
  <summary>Details</summary>
Motivation: 传统归因图解释依赖耗时人工分析（单次提示约2小时），需开发自动化流程解决可扩展性瓶颈。研究聚焦构建紧凑、概念对齐的归因子图框架

Method: 1. 特征选择：基于种子提示选择高影响力特征；2. 探针生成：创建上下文变化的定向探针；3. 语义聚类：通过跨提示激活特征将特征分为语义/关系/Say-X三类；4. 层级验证：实施实体替换测试验证层间特征迁移规律

Result: 1. 解释完整性优于基线（完整性0.83 vs 几何聚类）；2. 行为一致性显著提升（峰值标记一致性2.3倍，激活模式相似性5.8倍）；3. 发现层次化特征迁移模式（早期层64%迁移率，后期层专用输出促进）

Conclusion: 提出支持可复现机制解释的新范式，揭示Transformer计算存在骨干-专业化分层结构。开源代码/演示环境推动社区采用，实体替换测试为网络层级分析提供新方法论

Abstract: Mechanistic interpretability aims to understand neural networks by identifying which learned features mediate specific behaviors. Attribution graphs reveal these feature pathways, but interpreting them requires extensive manual analysis -- a single prompt can take approximately 2 hours for an experienced circuit tracer. We present probe prompting, an automated pipeline that transforms attribution graphs into compact, interpretable subgraphs built from concept-aligned supernodes. Starting from a seed prompt and target logit, we select high-influence features, generate concept-targeted yet context-varying probes, and group features by cross-prompt activation signatures into Semantic, Relationship, and Say-X categories using transparent decision rules.
  Across five prompts including classic "capitals" circuits, probe-prompted subgraphs preserve high explanatory coverage while compressing complexity (Completeness 0.83, mean across circuits; Replacement 0.54). Compared to geometric clustering baselines, concept-aligned groups exhibit higher behavioral coherence: 2.3x higher peak-token consistency (0.425 vs 0.183) and 5.8x higher activation-pattern similarity (0.762 vs 0.130), despite lower geometric compactness. Entity-swap tests reveal a layerwise hierarchy: early-layer features transfer robustly (64% transfer rate, mean layer 6.3), while late-layer Say-X features specialize for output promotion (mean layer 16.4), supporting a backbone-and-specialization view of transformer computation.
  We release code (https://github.com/peppinob-ol/attribution-graph-probing), an interactive demo (https://huggingface.co/spaces/Peppinob/attribution-graph-probing), and minimal artifacts enabling immediate reproduction and community adoption.

</details>


### [70] [Beyond English: Toward Inclusive and Scalable Multilingual Machine Translation with LLMs](https://arxiv.org/abs/2511.07003)
*Yingfeng Luo,Ziqiang Xu,Yuxuan Ouyang,Murun Yang,Dingyang Lin,Kaiyan Chang,Tong Zheng,Bei Li,Peinan Feng,Quan Du,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出大规模多语言翻译模型LMT，覆盖60种语言，通过战略下采样和并行多语言提示解决方向退化问题，实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决现有多语言机器翻译模型存在的语言覆盖不全、翻译质量不稳定、英语中心化偏差三大核心挑战

Method: 构建中英双中心翻译模型架构，提出战略下采样平衡多向微调数据，设计并行多语言提示增强跨语言迁移

Result: 4B参数量模型LMT-60-4B超越Aya-101-13B和NLLB-54B等更大模型，发布0.6B/1.7B/4B/8B四个版本

Conclusion: 通过数据筛选策略和模型架构创新，LMT为构建包容性强、可扩展的高质量多语言翻译系统提供了新基准

Abstract: Large language models have significantly advanced Multilingual Machine Translation (MMT), yet the broad language coverage, consistent translation quality, and English-centric bias remain open challenges. To address these challenges, we introduce \textbf{LMT}, a suite of \textbf{L}arge-scale \textbf{M}ultilingual \textbf{T}ranslation models centered on both Chinese and English, covering 60 languages and 234 translation directions. During development, we identify a previously overlooked phenomenon of \textbf{directional degeneration}, where symmetric multi-way fine-tuning data overemphasize reverse directions (X $\to$ En/Zh), leading to excessive many-to-one mappings and degraded translation quality. We propose \textbf{Strategic Downsampling}, a simple yet effective method to mitigate this degeneration. In addition, we design \textbf{Parallel Multilingual Prompting (PMP)}, which leverages typologically related auxiliary languages to enhance cross-lingual transfer. Through rigorous data curation and refined adaptation strategies, LMT achieves SOTA performance among models of comparable language coverage, with our 4B model (LMT-60-4B) surpassing the much larger Aya-101-13B and NLLB-54B models by a substantial margin. We release LMT in four sizes (0.6B/1.7B/4B/8B) to catalyze future research and provide strong baselines for inclusive, scalable, and high-quality MMT \footnote{\href{https://github.com/NiuTrans/LMT}{https://github.com/NiuTrans/LMT}}.

</details>


### [71] [A Picture is Worth a Thousand (Correct) Captions: A Vision-Guided Judge-Corrector System for Multimodal Machine Translation](https://arxiv.org/abs/2511.07010)
*Siddharth Betala,Kushan Raj,Vipul Betala,Rohan Saswade*

Main category: cs.CL

TL;DR: 提出两阶段方法（数据自动纠错+LoRA微调），在多语种英印地语翻译任务中实现BLEU分数提升


<details>
  <summary>Details</summary>
Motivation: 解决训练数据质量问题以提升低资源语言机器翻译性能

Method: 1. 构建视觉增强的Judge-Corrector流程自动清洗数据
2. 应用LoRA高效微调IndicTrans2模型

Result: 修正17.1%训练数据后，多语种BLEU显著提升（如英孟加拉语+1.3，英奥里亚语+0.6）

Conclusion: 数据质量清洗与参数高效微调相结合的策略有效提升低资源语言翻译性能

Abstract: In this paper, we describe our system under the team name BLEU Monday for the English-to-Indic Multimodal Translation Task at WAT 2025. We participate in the text-only translation tasks for English-Hindi, English-Bengali, English-Malayalam, and English-Odia language pairs. We present a two-stage approach that addresses quality issues in the training data through automated error detection and correction, followed by parameter-efficient model fine-tuning.
  Our methodology introduces a vision-augmented judge-corrector pipeline that leverages multimodal language models to systematically identify and correct translation errors in the training data. The judge component classifies translations into three categories: correct, visually ambiguous (requiring image context), or mistranslated (poor translation quality). Identified errors are routed to specialized correctors: GPT-4o-mini regenerates captions requiring visual disambiguation, while IndicTrans2 retranslates cases with pure translation quality issues. This automated pipeline processes 28,928 training examples across four languages, correcting an average of 17.1% of captions per language.
  We then apply Low-Rank Adaptation (LoRA) to fine-tune the IndicTrans2 en-indic 200M distilled model on both original and corrected datasets. Training on corrected data yields consistent improvements, with BLEU score gains of +1.30 for English-Bengali on the evaluation set (42.00 -> 43.30) and +0.70 on the challenge set (44.90 -> 45.60), +0.60 for English-Odia on the evaluation set (41.00 -> 41.60), and +0.10 for English-Hindi on the challenge set (53.90 -> 54.00).

</details>


### [72] [Multilingual Lexical Feature Analysis of Spoken Language for Predicting Major Depression Symptom Severity](https://arxiv.org/abs/2511.07011)
*Anastasiia Tokareva,Judith Dineley,Zoe Firth,Pauline Conde,Faith Matcham,Sara Siddi,Femke Lamers,Ewan Carr,Carolin Oetzmann,Daniel Leightley,Yuezhou Zhang,Amos A. Folarin,Josep Maria Haro,Brenda W. J. H. Penninx,Raquel Bailon,Srinivasan Vairavan,Til Wykes,Richard J. B. Dobson,Vaibhav A. Narayan,Matthew Hotopf,Nicholas Cummins,The RADAR-CNS Consortium*

Main category: cs.CL

TL;DR: 研究通过多国纵向语音数据分析，发现英语中可解释的词汇特征与抑郁症症状相关，但预测模型效果欠佳，需改进研究方法和模型。


<details>
  <summary>Details</summary>
Motivation: 探索移动设备采集的语音数据中可解释的词汇特征与抑郁症症状严重程度的关联，弥补现有研究在临床纵向数据和跨语言分析上的不足。

Method: 对来自英国、荷兰、西班牙的5,836条录音进行线性混合效应建模，分析词汇特征与PHQ-8评分的关系，并测试四个回归模型的预测性能。

Result: 英语数据发现7个相关特征（如词汇多样性），荷兰语显示部分关联，西班牙语无显著发现。所有语言的预测模型准确率接近随机水平。

Conclusion: 需扩大样本量、优化数据采集协议，并开发能处理个体语言差异的机器学习模型，以提升词汇标记在临床中的应用价值。

Abstract: Background: Captured between clinical appointments using mobile devices, spoken language has potential for objective, more regular assessment of symptom severity and earlier detection of relapse in major depressive disorder. However, research to date has largely been in non-clinical cross-sectional samples of written language using complex machine learning (ML) approaches with limited interpretability.
  Methods: We describe an initial exploratory analysis of longitudinal speech data and PHQ-8 assessments from 5,836 recordings of 586 participants in the UK, Netherlands, and Spain, collected in the RADAR-MDD study. We sought to identify interpretable lexical features associated with MDD symptom severity with linear mixed-effects modelling. Interpretable features and high-dimensional vector embeddings were also used to test the prediction performance of four regressor ML models.
  Results: In English data, MDD symptom severity was associated with 7 features including lexical diversity measures and absolutist language. In Dutch, associations were observed with words per sentence and positive word frequency; no associations were observed in recordings collected in Spain. The predictive power of lexical features and vector embeddings was near chance level across all languages.
  Limitations: Smaller samples in non-English speech and methodological choices, such as the elicitation prompt, may have also limited the effect sizes observable. A lack of NLP tools in languages other than English restricted our feature choice.
  Conclusion: To understand the value of lexical markers in clinical research and practice, further research is needed in larger samples across several languages using improved protocols, and ML models that account for within- and between-individual variations in language.

</details>


### [73] [Llama-Embed-Nemotron-8B: A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks](https://arxiv.org/abs/2511.07025)
*Yauhen Babakhin,Radek Osmulski,Ronay Ak,Gabriel Moreira,Mengyao Xu,Benedikt Schifferer,Bo Liu,Even Oldridge*

Main category: cs.CL

TL;DR: 开源文本嵌入模型llama-embed-nemotron-8b在MMTEB基准测试中实现SOTA性能，通过1600万混合数据训练并支持用户指令定制


<details>
  <summary>Details</summary>
Motivation: 解决现有模型训练数据和方法不透明的问题，推动开源可复现的文本嵌入技术发展

Method: 融合760万公开数据与840万LLM合成数据，通过消融研究优化对比损失函数、合成数据策略和模型合并方案

Result: 在检索/分类/STS等任务中全面领先，尤其在低资源语言和跨语言场景表现突出

Conclusion: 该模型通过开放权重与训练细节，结合指令定制功能，成为通用文本嵌入解决方案的标杆

Abstract: We introduce llama-embed-nemotron-8b, an open-weights text embedding model that achieves state-of-the-art performance on the Multilingual Massive Text Embedding Benchmark (MMTEB) leaderboard as of October 21, 2025. While recent models show strong performance, their training data or methodologies are often not fully disclosed. We aim to address this by developing a fully open-source model, publicly releasing its weights and detailed ablation studies, and planning to share the curated training datasets. Our model demonstrates superior performance across all major embedding tasks -- including retrieval, classification and semantic textual similarity (STS) -- and excels in challenging multilingual scenarios, such as low-resource languages and cross-lingual setups. This state-of-the-art performance is driven by a novel data mix of 16.1 million query-document pairs, split between 7.7 million samples from public datasets and 8.4 million synthetically generated examples from various open-weight LLMs. One of our key contributions is a detailed ablation study analyzing core design choices, including a comparison of contrastive loss implementations, an evaluation of synthetic data generation (SDG) strategies, and the impact of model merging. The llama-embed-nemotron-8b is an instruction-aware model, supporting user-defined instructions to enhance performance for specific use-cases. This combination of top-tier performance, broad applicability, and user-driven flexibility enables it to serve as a universal text embedding solution.

</details>


### [74] [Evaluating LLMs for Anxiety, Depression, and Stress Detection Evaluating Large Language Models for Anxiety, Depression, and Stress Detection: Insights into Prompting Strategies and Synthetic Data](https://arxiv.org/abs/2511.07044)
*Mihael Arcan,David-Paul Niland*

Main category: cs.CL

TL;DR: 研究比较了LLM与传统模型在心理健康检测中的表现，发现Distil-RoBERTa和XLNet分别在焦虑/抑郁检测中表现最优，零样本方法在压力检测中效果显著，验证了合成数据对模型性能的提升作用。


<details>
  <summary>Details</summary>
Motivation: 全球超1/5成年人受心理健康问题困扰，但现有文本检测方法因症状表达隐晦多变存在局限性，需探索更有效的模型架构和数据增强方案。

Method: 使用DAIC-WOZ临床访谈数据，对Llama/GPT等LLM和BERT/XLNet等Transformer模型进行微调，采用合成数据生成解决类别不平衡问题。

Result: Distil-RoBERTa在GAD-2焦虑检测获最高F1(0.883)，XLNet在PHQ抑郁任务达F1 0.891，压力检测零样本方法获F1 0.884且ROC AUC 0.886。

Conclusion: 基于Transformer的模型结合合成数据能有效提升心理健康评估效果，但需平衡数据增强与模型校准以避免精度损失，展示了LLM与数据增强技术的协同潜力。

Abstract: Mental health disorders affect over one-fifth of adults globally, yet detecting such conditions from text remains challenging due to the subtle and varied nature of symptom expression. This study evaluates multiple approaches for mental health detection, comparing Large Language Models (LLMs) such as Llama and GPT with classical machine learning and transformer-based architectures including BERT, XLNet, and Distil-RoBERTa. Using the DAIC-WOZ dataset of clinical interviews, we fine-tuned models for anxiety, depression, and stress classification and applied synthetic data generation to mitigate class imbalance. Results show that Distil-RoBERTa achieved the highest F1 score (0.883) for GAD-2, while XLNet outperformed others on PHQ tasks (F1 up to 0.891). For stress detection, a zero-shot synthetic approach (SD+Zero-Shot-Basic) reached an F1 of 0.884 and ROC AUC of 0.886. Findings demonstrate the effectiveness of transformer-based models and highlight the value of synthetic data in improving recall and generalization. However, careful calibration is required to prevent precision loss. Overall, this work emphasizes the potential of combining advanced language models and data augmentation to enhance automated mental health assessment from text.

</details>


### [75] [When Sufficient is not Enough: Utilizing the Rashomon Effect for Complete Evidence Extraction](https://arxiv.org/abs/2511.07055)
*Katharina Beckh,Stefan Rüping*

Main category: cs.CL

TL;DR: 研究通过集成模型将医学数据中完整证据的召回率从0.60提升至0.86，分析了证据完整性获取的优化方案


<details>
  <summary>Details</summary>
Motivation: 现有特征归因方法仅提供最小充分证据，无法满足合规性等需要完整证据的应用场景需求

Method: 基于人工标注完整证据的医学数据集，比较单模型与集成模型表现，引入动态集成框架和置信度阈值机制

Result: 集成方法显著提升证据召回率（单模型最佳0.60 → 集成0.86），保持合理精确度

Conclusion: 模型集成策略有效解决证据完整性问题，训练时引入证据标注和动态阈值机制对实际应用具有重要指导价值

Abstract: Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications this is inadequate. For compliance and cataloging, the full set of contributing features must be identified - complete evidence. We perform a case study on a medical dataset which contains human-annotated complete evidence. We show that individual models typically recover only subsets of complete evidence and that aggregating evidence from several models improves evidence recall from $\sim$0.60 (single best model) to $\sim$0.86 (ensemble). We analyze the recall-precision trade-off, the role of training with evidence, dynamic ensembles with certainty thresholds, and discuss implications.

</details>


### [76] [Aligning Attention with Human Rationales for Self-Explaining Hate Speech Detection](https://arxiv.org/abs/2511.07065)
*Brage Eilertsen,Røskva Bjørgfinsdóttir,Francielle Vargas,Ali Ramezani-Kebrya*

Main category: cs.CL

TL;DR: 提出监督理性注意力框架(SRA)，通过将模型注意力与人类理性标注对齐，提升仇恨语音检测的可解释性和公平性。


<details>
  <summary>Details</summary>
Motivation: 针对深度学习模型在仇恨语音检测中存在的可解释性不足问题，需要开发既保持模型公平性又提升解释性的方法。

Method: 在基于Transformer的分类器中引入监督注意力机制，通过联合优化分类损失和注意力-理性标注对齐损失实现模型训练。

Result: 在英语和葡萄牙语数据集上实现2.4倍解释性提升，注意力权重与人类标注对齐度更高，在身份群体攻击检测中公平性指标排名第二。

Conclusion: 将人类理性标注融入注意力机制可在不损害公平性的前提下，显著提升模型解释力和可信度。

Abstract: The opaque nature of deep learning models presents significant challenges for the ethical deployment of hate speech detection systems. To address this limitation, we introduce Supervised Rational Attention (SRA), a framework that explicitly aligns model attention with human rationales, improving both interpretability and fairness in hate speech classification. SRA integrates a supervised attention mechanism into transformer-based classifiers, optimizing a joint objective that combines standard classification loss with an alignment loss term that minimizes the discrepancy between attention weights and human-annotated rationales. We evaluated SRA on hate speech benchmarks in English (HateXplain) and Portuguese (HateBRXplain) with rationale annotations. Empirically, SRA achieves 2.4x better explainability compared to current baselines, and produces token-level explanations that are more faithful and human-aligned. In terms of fairness, SRA achieves competitive fairness across all measures, with second-best performance in detecting toxic posts targeting identity groups, while maintaining comparable results on other metrics. These findings demonstrate that incorporating human rationales into attention mechanisms can enhance interpretability and faithfulness without compromising fairness.

</details>


### [77] [Importance-Aware Data Selection for Efficient LLM Instruction Tuning](https://arxiv.org/abs/2511.07074)
*Tingyu Jiang,Shen Li,Yiyao Song,Lan Zhang,Hualei Zhu,Yuan Zhao,Xiaohang Xu,Kenjiro Taura,Hao Henry Wang*

Main category: cs.CL

TL;DR: 提出MIWV指标量化指令数据重要性，实验表明仅用前1%高质量数据即可超越全量训练效果


<details>
  <summary>Details</summary>
Motivation: 现有研究侧重数据质量评分，需针对特定LLM选择最大程度提升性能的优质数据

Method: 通过模型在上下文学习中的响应差异计算MIWV，识别对指令调优最有益的数据

Result: 基于MIWV选择前1%数据训练效果优于全数据集，验证指标有效性

Conclusion: MIWV指标超越传统质量评分方法，为数据选择提供有效实证依据

Abstract: Instruction tuning plays a critical role in enhancing the performance and efficiency of Large Language Models (LLMs). Its success depends not only on the quality of the instruction data but also on the inherent capabilities of the LLM itself. Some studies suggest that even a small amount of high-quality data can achieve instruction fine-tuning results that are on par with, or even exceed, those from using a full-scale dataset. However, rather than focusing solely on calculating data quality scores to evaluate instruction data, there is a growing need to select high-quality data that maximally enhances the performance of instruction tuning for a given LLM. In this paper, we propose the Model Instruction Weakness Value (MIWV) as a novel metric to quantify the importance of instruction data in enhancing model's capabilities. The MIWV metric is derived from the discrepancies in the model's responses when using In-Context Learning (ICL), helping identify the most beneficial data for enhancing instruction tuning performance. Our experimental results demonstrate that selecting only the top 1\% of data based on MIWV can outperform training on the full dataset. Furthermore, this approach extends beyond existing research that focuses on data quality scoring for data selection, offering strong empirical evidence supporting the effectiveness of our proposed method.

</details>


### [78] [EmoBang: Detecting Emotion From Bengali Texts](https://arxiv.org/abs/2511.07077)
*Abdullah Al Maruf,Aditi Golder,Zakaria Masud Jiyad,Abdullah Al Numan,Tarannum Shaila Zaman*

Main category: cs.CL

TL;DR: 提出首个孟加拉语情感检测综合基准，包括新数据集和两个高性能模型（EmoBangHybrid/Ensemble），准确率超93%


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为全球第四大语言缺乏情感检测资源，现有方法性能有限需建立标准化基准

Method: 1. 构建八分类情感数据集
2. 开发CRNN混合模型(EmoBangHybrid)和AdaBoost-BERT集成模型(EmoBangEnsemble)
3. 对比传统特征工程方法与LLM零/小样本学习

Result: 混合模型(92.86%)和集成模型(93.69%)准确率显著超越现有方法，创下新基准

Conclusion: 该研究填补了孟加拉语情感检测领域空白，为后续研究提供了可靠基线

Abstract: Emotion detection from text seeks to identify an individual's emotional or mental state - positive, negative, or neutral - based on linguistic cues. While significant progress has been made for English and other high-resource languages, Bengali remains underexplored despite being the world's fourth most spoken language. The lack of large, standardized datasets classifies Bengali as a low-resource language for emotion detection. Existing studies mainly employ classical machine learning models with traditional feature engineering, yielding limited performance. In this paper, we introduce a new Bengali emotion dataset annotated across eight emotion categories and propose two models for automatic emotion detection: (i) a hybrid Convolutional Recurrent Neural Network (CRNN) model (EmoBangHybrid) and (ii) an AdaBoost-Bidirectional Encoder Representations from Transformers (BERT) ensemble model (EmoBangEnsemble). Additionally, we evaluate six baseline models with five feature engineering techniques and assess zero-shot and few-shot large language models (LLMs) on the dataset. To the best of our knowledge, this is the first comprehensive benchmark for Bengali emotion detection. Experimental results show that EmoBangH and EmoBangE achieve accuracies of 92.86% and 93.69%, respectively, outperforming existing methods and establishing strong baselines for future research.

</details>


### [79] [Wasm: A Pipeline for Constructing Structured Arabic Interleaved Multimodal Corpora](https://arxiv.org/abs/2511.07080)
*Khalil Hennara,Ahmad Bastati,Muhammad Hreden,Mohamed Motasim Hamed,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CL

TL;DR: 提出Wasm流程处理Common Crawl生成阿拉伯语多模态数据集，保留网页结构并提供Markdown输出


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语缺乏保留文档结构的高质量多模态数据集，制约大模型发展。现有语料库仅关注文本提取，无法满足多模态预训练需求。

Method: 1. 开发Wasm流程处理Common Crawl数据，保持网页结构完整性
2. 支持纯文本和多模态预训练场景
3. 与其他主流数据集进行数据处理流程对比分析
4. 公开数据集和处理流程

Result: 1. 发布首个提供Markdown输出的阿拉伯语多模态数据集
2. 验证数据处理策略与现有方法的共性及设计合理性
3. 提供灵活的结构化数据支持不同训练场景

Conclusion: 通过结构化数据处理填补阿拉伯语多模态资源空白，公开的数据集和工具链将推动阿拉伯语NLP及多模态模型研究，为其他低资源语言提供参考范式。

Abstract: The performance of large language models (LLMs) and large multimodal models (LMMs) depends heavily on the quality and scale of their pre-training datasets. Recent research shows that large multimodal models trained on natural documents where images and text are interleaved outperform those trained only on image-text pairs across a wide range of benchmarks, leveraging advanced pre- trained models to enforce semantic alignment, image-sequence consistency, and textual coherence. For Arabic, however, the lack of high-quality multimodal datasets that preserve document structure has limited progress. In this paper, we present our pipeline Wasm for processing the Common Crawl dataset to create a new Arabic multimodal dataset that uniquely provides markdown output. Unlike existing Arabic corpora that focus solely on text extraction, our approach preserves the structural integrity of web content while maintaining flexibility for both text-only and multimodal pre-training scenarios. We provide a comprehensive comparative analysis of our data processing pipeline against those used for major existing datasets, highlighting the convergences in filtering strategies and justifying our specific design choices. To support future research, we publicly release a representative dataset dump along with the multimodal processing pipeline for Arabic.

</details>


### [80] [More Agents Helps but Adversarial Robustness Gap Persists](https://arxiv.org/abs/2511.07112)
*Khashayar Alavi,Zhastay Yeltay,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 多智能体协作提升数学问题解答准确性，但对抗性鲁棒性差距持续存在


<details>
  <summary>Details</summary>
Motivation: 探究多LLM智能体协作时是否对对抗性输入具有更强鲁棒性

Method: 使用包含标点噪声（3种强度）、真实世界/人类拼写错误（WikiTypo, R2ATA）的对抗性数学问题，通过Agent Forest框架评估6个开源模型在4个基准测试中的表现，智能体数量从1到25个

Result: 1) 噪声类型影响显著：标点噪声危害随强度增加，人类拼写错误保持主要瓶颈；2) 智能体协作持续提升准确性（1→5智能体增益最大），但对抗性鲁棒性差距始终存在

Conclusion: 智能体协作可提升准确性但无法消除对抗脆弱性，人类拼写错误仍是最主要威胁，需开发针对性防御方法

Abstract: When LLM agents work together, they seem to be more powerful than a single LLM in mathematical question answering. However, are they also more robust to adversarial inputs? We investigate this question using adversarially perturbed math questions. These perturbations include punctuation noise with three intensities (10, 30, and 50 percent), plus real-world and human-like typos (WikiTypo, R2ATA). Using a unified sampling-and-voting framework (Agent Forest), we evaluate six open-source models (Qwen3-4B/14B, Llama3.1-8B, Mistral-7B, Gemma3-4B/12B) across four benchmarks (GSM8K, MATH, MMLU-Math, MultiArith), with various numbers of agents n from one to 25 (1, 2, 5, 10, 15, 20, 25). Our findings show that (1) Noise type matters: punctuation noise harm scales with its severity, and the human typos remain the dominant bottleneck, yielding the largest gaps to Clean accuracy and the highest ASR even with a large number of agents. And (2) Collaboration reliably improves accuracy as the number of agents, n, increases, with the largest gains from one to five agents and diminishing returns beyond 10 agents. However, the adversarial robustness gap persists regardless of the agent count.

</details>


### [81] [Think Consistently, Reason Efficiently: Energy-Based Calibration for Implicit Chain-of-Thought](https://arxiv.org/abs/2511.07124)
*Zhikang Chen,Sen Cui,Deheng Ye,Yu Zhang,Yatao Bian,Tingting Zhu*

Main category: cs.CL

TL;DR: 提出EBM-CoT框架，通过能量模型校准潜在思维表征，提升LLMs多步推理的准确性和一致性


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在错误传播风险，潜在空间推理缺乏一致性约束机制导致推理路径发散

Method: 利用能量模型动态调整潜在推理轨迹，使其向嵌入空间中的低能量高一致性区域优化

Result: 在数学推理、常识推理和符号推理基准测试中显著提升推理效率和结果一致性

Conclusion: EBM-CoT有效解决了传统离散推理和潜在空间推理的局限性，无需修改基础模型即可实现稳定推理

Abstract: Large Language Models (LLMs) have demonstrated strong reasoning capabilities through \emph{Chain-of-Thought} (CoT) prompting, which enables step-by-step intermediate reasoning. However, explicit CoT methods rely on discrete token-level reasoning processes that are prone to error propagation and limited by vocabulary expressiveness, often resulting in rigid and inconsistent reasoning trajectories. Recent research has explored implicit or continuous reasoning in latent spaces, allowing models to perform internal reasoning before generating explicit output. Although such approaches alleviate some limitations of discrete CoT, they generally lack explicit mechanisms to enforce consistency among reasoning steps, leading to divergent reasoning paths and unstable outcomes. To address this issue, we propose EBM-CoT, an Energy-Based Chain-of-Thought Calibration framework that refines latent thought representations through an energy-based model (EBM). Our method dynamically adjusts latent reasoning trajectories toward lower-energy, high-consistency regions in the embedding space, improving both reasoning accuracy and consistency without modifying the base language model. Extensive experiments across mathematical, commonsense, and symbolic reasoning benchmarks demonstrate that the proposed framework significantly enhances the consistency and efficiency of multi-step reasoning in LLMs.

</details>


### [82] [LoRA on the Go: Instance-level Dynamic LoRA Selection and Merging](https://arxiv.org/abs/2511.07129)
*Seungeon Lee,Soumi Das,Manish Gupta,Krishna P. Gummadi*

Main category: cs.CL

TL;DR: 提出无需训练的LoRA动态适配框架LoGo，通过单次前向传播信号实现实例级适配器动态合并，显著提升多任务场景性能


<details>
  <summary>Details</summary>
Motivation: 传统LoRA适配器局限于单任务场景，现实应用中多领域输入需要动态适配。现有多适配器融合方法依赖标注数据或额外训练，成本高昂

Method: LoGo框架：1) 从单次前向传播中提取适配器信号 2) 动态计算适配器相关性权重 3) 即时融合最相关适配器参数

Result: 在5个NLP基准/27个数据集/3类模型上：1) 部分任务性能提升3.6% 2) 保持其他任务竞争力 3) 维持原始推理吞吐量

Conclusion: LoGo通过免训练动态适配机制，有效解决多领域推理挑战，为大规模语言模型部署提供实用解决方案

Abstract: Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient approach for fine-tuning large language models.However, conventional LoRA adapters are typically trained for a single task, limiting their applicability in real-world settings where inputs may span diverse and unpredictable domains. At inference time, existing approaches combine multiple LoRAs for improving performance on diverse tasks, while usually requiring labeled data or additional task-specific training, which is expensive at scale. In this work, we introduce LoRA on the Go (LoGo), a training-free framework that dynamically selects and merges adapters at the instance level without any additional requirements. LoGo leverages signals extracted from a single forward pass through LoRA adapters, to identify the most relevant adapters and determine their contributions on-the-fly. Across 5 NLP benchmarks, 27 datasets, and 3 model families, LoGo outperforms training-based baselines on some tasks upto a margin of 3.6% while remaining competitive on other tasks and maintaining inference throughput, highlighting its effectiveness and practicality.

</details>


### [83] [TCM-Eval: An Expert-Level Dynamic and Extensible Benchmark for Traditional Chinese Medicine](https://arxiv.org/abs/2511.07148)
*Zihao Cheng,Yuheng Lu,Huaiqian Ye,Zeming Liu,Minqi Wang,Jingjing Liu,Zihan Li,Wei Fan,Yuanfang Guo,Ruiji Fu,Shifeng She,Gang Wang,Yunhong Wang*

Main category: cs.CL

TL;DR: 提出首个动态可扩展的中医评估基准TCM-Eval及先进大模型ZhiMingTang，通过SI-CoTE方法实现数据与模型协同进化，显著超越人类从业者通过阈值。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在中医药领域应用受限，主要由于缺乏标准化评估基准和高质量训练数据。

Method: 1. 构建国家级考试来源的中医评估基准TCM-Eval
2. 提出SI-CoTE方法通过拒绝采样自增强问答推理链
3. 建立数据与模型协同进化的训练框架

Result: 开发的ZhiMingTang模型显著超越人类执业通过标准（+17.8%），并建立公开排行榜推动社区发展

Conclusion: 本研究通过基准构建-数据增强-模型训练的全流程创新，为中医领域大语言模型发展建立了系统性解决方案

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modern medicine, yet their application in Traditional Chinese Medicine (TCM) remains severely limited by the absence of standardized benchmarks and the scarcity of high-quality training data. To address these challenges, we introduce TCM-Eval, the first dynamic and extensible benchmark for TCM, meticulously curated from national medical licensing examinations and validated by TCM experts. Furthermore, we construct a large-scale training corpus and propose Self-Iterative Chain-of-Thought Enhancement (SI-CoTE) to autonomously enrich question-answer pairs with validated reasoning chains through rejection sampling, establishing a virtuous cycle of data and model co-evolution. Using this enriched training data, we develop ZhiMingTang (ZMT), a state-of-the-art LLM specifically designed for TCM, which significantly exceeds the passing threshold for human practitioners. To encourage future research and development, we release a public leaderboard, fostering community engagement and continuous improvement.

</details>


### [84] [Categorical Emotions or Appraisals - Which Emotion Model Explains Argument Convincingness Better?](https://arxiv.org/abs/2511.07162)
*Lynn Greschner,Meike Bauer,Sabine Weber,Roman Klinger*

Main category: cs.CL

TL;DR: 研究发现基于评价理论（appraisal theories）的认知评估比传统情绪分类更能有效预测论点说服力，首次系统比较了情绪模型在说服力预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 论证的说服力不仅依赖逻辑和信誉，还与接收者的主观情感评估相关。现有研究多关注情绪分类，尚未探索评价理论在论证说服力评估中的适用性。

Method: 基于ContArgA语料库的标注，采用零样本提示实验，对比黄金标注与预测的情绪/评估对说服力标签预测的影响。

Result: 情绪分类可提升预测效果，但评价理论中的重要性/影响力等认知评估指标提升更显著（重要性指标使F1值提升17.6%）。

Conclusion: 该研究首次系统验证评价理论在计算论证中的优势，为情感分析与论证说服力建模提供了理论与实践依据。

Abstract: The convincingness of an argument does not only depend on its structure (logos), the person who makes the argument (ethos), but also on the emotion that it causes in the recipient (pathos). While the overall intensity and categorical values of emotions in arguments have received considerable attention in the research community, we argue that the emotion an argument evokes in a recipient is subjective. It depends on the recipient's goals, standards, prior knowledge, and stance. Appraisal theories lend themselves as a link between the subjective cognitive assessment of events and emotions. They have been used in event-centric emotion analysis, but their suitability for assessing argument convincingness remains unexplored. In this paper, we evaluate whether appraisal theories are suitable for emotion analysis in arguments by considering subjective cognitive evaluations of the importance and impact of an argument on its receiver. Based on the annotations in the recently published ContArgA corpus, we perform zero-shot prompting experiments to evaluate the importance of gold-annotated and predicted emotions and appraisals for the assessment of the subjective convincingness labels. We find that, while categorical emotion information does improve convincingness prediction, the improvement is more pronounced with appraisals. This work presents the first systematic comparison between emotion models for convincingness prediction, demonstrating the advantage of appraisals, providing insights for theoretical and practical applications in computational argumentation.

</details>


### [85] [AdaRec: Adaptive Recommendation with LLMs via Narrative Profiling and Dual-Channel Reasoning](https://arxiv.org/abs/2511.07166)
*Meiyun Wang,Charin Polpanumas*

Main category: cs.CL

TL;DR: AdaRec 通过叙事分析将用户行为转化为自然语言表示，结合双变量推理范式（水平行为对齐+垂直因果归因），在少量样本场景下实现优于基线模型8%的个性化推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM推荐方法需要手动特征工程、跨任务适应能力差的问题，利用语义表示实现快速任务迁移，通过少量交互数据实现长尾个性化。

Method: 1. 叙事分析生成自然语言用户画像
2. 双通道架构：水平对齐发现群体模式+垂直归因挖掘偏好动因
3. 自生成合成数据轻量微调

Result: 电商数据集显示：少样本提升8%，零样本超越专家画像19%，轻量微调可达全参数微调效果

Conclusion: 框架在可解释性、跨任务泛化性、数据效率方面取得突破，为冷启动推荐提供新范式

Abstract: We propose AdaRec, a few-shot in-context learning framework that leverages large language models for an adaptive personalized recommendation. AdaRec introduces narrative profiling, transforming user-item interactions into natural language representations to enable unified task handling and enhance human readability. Centered on a bivariate reasoning paradigm, AdaRec employs a dual-channel architecture that integrates horizontal behavioral alignment, discovering peer-driven patterns, with vertical causal attribution, highlighting decisive factors behind user preferences. Unlike existing LLM-based approaches, AdaRec eliminates manual feature engineering through semantic representations and supports rapid cross-task adaptation with minimal supervision. Experiments on real ecommerce datasets demonstrate that AdaRec outperforms both machine learning models and LLM-based baselines by up to eight percent in few-shot settings. In zero-shot scenarios, it achieves up to a nineteen percent improvement over expert-crafted profiling, showing effectiveness for long-tail personalization with minimal interaction data. Furthermore, lightweight fine-tuning on synthetic data generated by AdaRec matches the performance of fully fine-tuned models, highlighting its efficiency and generalization across diverse tasks.

</details>


### [86] [EMODIS: A Benchmark for Context-Dependent Emoji Disambiguation in Large Language Models](https://arxiv.org/abs/2511.07193)
*Jiacheng Huang,Ning Yu,Xiaoyin Yi*

Main category: cs.CL

TL;DR: 提出了EMODIS评测基准，用于检验大语言模型在细微语境差异下的表情符号歧义消解能力，发现现有模型存在系统性偏差和语境敏感性不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在现实交流场景中处理语境依赖歧义的能力尚未被充分研究，特别是在表情符号的多义性解析方面存在明显缺陷。

Method: 构建包含歧义表情符号句子、对比性上下文及推理问题的评测集，测试开源与API模型的上下文消歧能力。

Result: 当前最强模型仍无法有效捕捉细微语境差异，表现出对主导语义的系统性偏好和语用对比敏感性不足。

Conclusion: EMODIS为语境消歧能力评估提供严格标准，揭示了LLMs与人类在语义推理层面存在的显著差距。

Abstract: Large language models (LLMs) are increasingly deployed in real-world communication settings, yet their ability to resolve context-dependent ambiguity remains underexplored. In this work, we present EMODIS, a new benchmark for evaluating LLMs' capacity to interpret ambiguous emoji expressions under minimal but contrastive textual contexts. Each instance in EMODIS comprises an ambiguous sentence containing an emoji, two distinct disambiguating contexts that lead to divergent interpretations, and a specific question that requires contextual reasoning. We evaluate both open-source and API-based LLMs, and find that even the strongest models frequently fail to distinguish meanings when only subtle contextual cues are present. Further analysis reveals systematic biases toward dominant interpretations and limited sensitivity to pragmatic contrast. EMODIS provides a rigorous testbed for assessing contextual disambiguation, and highlights the gap in semantic reasoning between humans and LLMs.

</details>


### [87] [Discourse Graph Guided Document Translation with Large Language Models](https://arxiv.org/abs/2511.07230)
*Viet-Thanh Pham,Minghan Wang,Hao-Han Liao,Thuy-Trang Vu*

Main category: cs.CL

TL;DR: 提出TransGraph框架解决大模型文档翻译的篇章连贯问题，通过结构化篇章图建模段落关系实现高效翻译


<details>
  <summary>Details</summary>
Motivation: 现有基于多智能体协调的翻译系统存在高计算开销和记忆策略敏感性，需更高效的篇章建模方案

Method: 构建结构化篇章图显式捕捉段落关联，通过图邻域选择机制动态聚焦相关上下文进行翻译

Result: 在6种语言的3个文档级翻译基准上，翻译质量提升1.8BLEU，术语一致性提高15%，token消耗降低40%

Conclusion: 显式篇章结构建模显著提升翻译连贯性，为低资源场景下的文档级翻译提供了新范式

Abstract: Adapting large language models to full document translation remains challenging due to the difficulty of capturing long-range dependencies and preserving discourse coherence throughout extended texts. While recent agentic machine translation systems mitigate context window constraints through multi-agent orchestration and persistent memory, they require substantial computational resources and are sensitive to memory retrieval strategies. We introduce TransGraph, a discourse-guided framework that explicitly models inter-chunk relationships through structured discourse graphs and selectively conditions each translation segment on relevant graph neighbourhoods rather than relying on sequential or exhaustive context. Across three document-level MT benchmarks spanning six languages and diverse domains, TransGraph consistently surpasses strong baselines in translation quality and terminology consistency while incurring significantly lower token overhead.

</details>


### [88] [Who Is the Story About? Protagonist Entity Recognition in News](https://arxiv.org/abs/2511.07296)
*Jorge Gabín,M. Eduardo Ares,Javier Parapar*

Main category: cs.CL

TL;DR: 提出主人公实体识别（PER）任务，通过LLMs验证大规模自动标注可行性，扩展叙事中心信息抽取


<details>
  <summary>Details</summary>
Motivation: 传统NER将所有组织提及等同处理，难以识别真正推动新闻叙事发展的核心实体，限制了基于叙事重要性理解的下游任务

Method: 1. 构建黄金语料库进行专家标注与LLMs预测对比
2. 通过NER引导提示实现大规模新闻自动标注
3. 测试LLMs在有限上下文中推断主人公实体的能力

Result: PER在人类标注者间/人机标注间均达成一致，LLMs无需显式候选引导即可有效识别叙事核心实体

Conclusion: PER是信息抽取的可行扩展，引导式LLMs能大规模近似人类对叙事重要性的判断

Abstract: News articles often reference numerous organizations, but traditional Named Entity Recognition (NER) treats all mentions equally, obscuring which entities genuinely drive the narrative. This limits downstream tasks that rely on understanding event salience, influence, or narrative focus. We introduce Protagonist Entity Recognition (PER), a task that identifies the organizations that anchor a news story and shape its main developments. To validate PER, we compare he predictions of Large Language Models (LLMs) against annotations from four expert annotators over a gold corpus, establishing both inter-annotator consistency and human-LLM agreement. Leveraging these findings, we use state-of-the-art LLMs to automatically label large-scale news collections through NER-guided prompting, generating scalable, high-quality supervision. We then evaluate whether other LLMs, given reduced context and without explicit candidate guidance, can still infer the correct protagonists. Our results demonstrate that PER is a feasible and meaningful extension to narrative-centered information extraction, and that guided LLMs can approximate human judgments of narrative importance at scale.

</details>


### [89] [Retriv at BLP-2025 Task 1: A Transformer Ensemble and Multi-Task Learning Approach for Bangla Hate Speech Identification](https://arxiv.org/abs/2511.07304)
*Sourav Saha,K M Nafi Asib,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 本文通过集成Transformer模型及多任务框架，在孟加拉语仇恨语音检测共享任务中取得中等成绩，验证了该方法在低资源语境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语仇恨语音检测具有重要社会意义但面临语言复杂性挑战，团队通过参与IJCNLP-AACL 2025共享任务探索低资源语境下的解决方案。

Method: 使用软投票集成BanglaBERT/MuRIL/IndicBERTv2处理单任务（1A/1B），加权投票集成多任务模型处理联合检测任务（1C）。

Result: 三个子任务分别获得72.75%（9th）、72.69%（10th）、72.62%（7th）的micro-f1分数，显示方法具备竞争力但非顶尖。

Conclusion: 实验证明Transformer集成与加权多任务框架在低资源孟加拉语场景的有效性，公开代码促进社区发展。

Abstract: This paper addresses the problem of Bangla hate speech identification, a socially impactful yet linguistically challenging task. As part of the "Bangla Multi-task Hate Speech Identification" shared task at the BLP Workshop, IJCNLP-AACL 2025, our team "Retriv" participated in all three subtasks: (1A) hate type classification, (1B) target group identification, and (1C) joint detection of type, severity, and target. For subtasks 1A and 1B, we employed a soft-voting ensemble of transformer models (BanglaBERT, MuRIL, IndicBERTv2). For subtask 1C, we trained three multitask variants and aggregated their predictions through a weighted voting ensemble. Our systems achieved micro-f1 scores of 72.75% (1A) and 72.69% (1B), and a weighted micro-f1 score of 72.62% (1C). On the shared task leaderboard, these corresponded to 9th, 10th, and 7th positions, respectively. These results highlight the promise of transformer ensembles and weighted multitask frameworks for advancing Bangla hate speech detection in low-resource contexts. We made experimental scripts publicly available for the community.

</details>


### [90] [ACE-ICD: Acronym Expansion As Data Augmentation For Automated ICD Coding](https://arxiv.org/abs/2511.07311)
*Tuan-Dung Le,Shohreh Haddadan,Thanh Q. Thieu*

Main category: cs.CL

TL;DR: 提出ACE-ICD方法，通过大语言模型扩展医学术语缩写+一致性训练，在MIMIC-III数据集实现SOTA的ICD自动编码效果


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视临床文本中广泛存在的医学术语缩写问题，而这对ICD编码推断至关重要。需通过数据增强提升模型对缩写的理解能力

Method: 1. 利用大语言模型扩展医疗缩写为完整形式进行数据增强
2. 引入一致性训练机制，确保原始文档与增强文档的预测一致性

Result: 在MIMIC-III数据集上，常见编码/罕见编码/全编码任务均达到SOTA，模型代码已开源

Conclusion: 通过缩写扩展+一致性训练的双重策略，ACE-ICD有效解决临床文本中的术语缩写问题，为电子病历编码任务建立新基准

Abstract: Automatic ICD coding, the task of assigning disease and procedure codes to electronic medical records, is crucial for clinical documentation and billing. While existing methods primarily enhance model understanding of code hierarchies and synonyms, they often overlook the pervasive use of medical acronyms in clinical notes, a key factor in ICD code inference. To address this gap, we propose a novel effective data augmentation technique that leverages large language models to expand medical acronyms, allowing models to be trained on their full form representations. Moreover, we incorporate consistency training to regularize predictions by enforcing agreement between the original and augmented documents. Extensive experiments on the MIMIC-III dataset demonstrate that our approach, ACE-ICD establishes new state-of-the-art performance across multiple settings, including common codes, rare codes, and full-code assignments. Our code is publicly available.

</details>


### [91] [RLVE: Scaling Up Reinforcement Learning for Language Models with Adaptive Verifiable Environments](https://arxiv.org/abs/2511.07317)
*Zhiyuan Zeng,Hamish Ivison,Yiping Wang,Lifan Yuan,Shuyue Stella Li,Zhuorui Ye,Siting Li,Jacqueline He,Runlong Zhou,Tong Chen,Chenyang Zhao,Yulia Tsvetkov,Simon Shaolei Du,Natasha Jaques,Hao Peng,Pang Wei Koh,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出RLVE强化学习框架，通过动态难度调整和400个可验证环境套件，显著提升语言模型推理能力（+3.37%基准表现），计算效率优于传统方法3倍。


<details>
  <summary>Details</summary>
Motivation: 解决静态数据分布导致的学习信号消失问题（简单/困难问题无法提供有效反馈），通过环境动态适应模型能力维持训练有效性。

Method: 1. 开发RLVE框架：可验证环境动态调整问题难度分布
2. 构建RLVE-Gym环境套件（400个手工设计的可验证环境）
3. 多环境联合训练策略

Result: 1. 六项推理基准平均提升3.37%（1.5B参数模型）
2. 对比原RL训练方法节省67%计算资源（3.37% vs 0.49%提升）
3. 验证环境扩展对泛化能力的持续增益

Conclusion: RLVE框架通过环境动态适应和规模化扩展，显著提升语言模型推理性能，证明环境工程在强化学习中的关键作用，且计算资源利用率更优。

Abstract: We introduce Reinforcement Learning (RL) with Adaptive Verifiable Environments (RLVE), an approach using verifiable environments that procedurally generate problems and provide algorithmically verifiable rewards, to scale up RL for language models (LMs). RLVE enables each verifiable environment to dynamically adapt its problem difficulty distribution to the policy model's capabilities as training progresses. In contrast, static data distributions often lead to vanishing learning signals when problems are either too easy or too hard for the policy. To implement RLVE, we create RLVE-Gym, a large-scale suite of 400 verifiable environments carefully developed through manual environment engineering. Using RLVE-Gym, we show that environment scaling, i.e., expanding the collection of training environments, consistently improves generalizable reasoning capabilities. RLVE with joint training across all 400 environments in RLVE-Gym yields a 3.37% absolute average improvement across six reasoning benchmarks, starting from one of the strongest 1.5B reasoning LMs. By comparison, continuing this LM's original RL training yields only a 0.49% average absolute gain despite using over 3x more compute. We release our code publicly.

</details>


### [92] [When Bias Pretends to Be Truth: How Spurious Correlations Undermine Hallucination Detection in LLMs](https://arxiv.org/abs/2511.07318)
*Shaowen Wang,Yiqi Dong,Ruinian Chang,Tansheng Zhu,Yuebo Sun,Kaifeng Lyu,Jian Li*

Main category: cs.CL

TL;DR: 大语言模型因训练数据中的虚假相关性（如姓氏与国籍的统计关联）产生顽固性幻觉，现有检测方法对此失效，需开发新型解决方案。


<details>
  <summary>Details</summary>
Motivation: 揭示并解决由数据中统计性虚假关联引发的LLM幻觉问题，这类幻觉具有高置信度、抗模型缩放、逃避现有检测手段等特性。

Method: 通过合成数据实验+开源/商业模型（含GPT-5）评估，结合理论分析验证现有检测方法的根本性缺陷。

Result: 基于置信度的过滤和内部状态探测等方法在虚假相关性场景下完全失效，统计偏差本质破坏检测技术有效性。

Conclusion: 亟需开发专门针对虚假相关性幻觉的新方法，现有技术路线存在根本性局限。

Abstract: Despite substantial advances, large language models (LLMs) continue to exhibit hallucinations, generating plausible yet incorrect responses. In this paper, we highlight a critical yet previously underexplored class of hallucinations driven by spurious correlations -- superficial but statistically prominent associations between features (e.g., surnames) and attributes (e.g., nationality) present in the training data. We demonstrate that these spurious correlations induce hallucinations that are confidently generated, immune to model scaling, evade current detection methods, and persist even after refusal fine-tuning. Through systematically controlled synthetic experiments and empirical evaluations on state-of-the-art open-source and proprietary LLMs (including GPT-5), we show that existing hallucination detection methods, such as confidence-based filtering and inner-state probing, fundamentally fail in the presence of spurious correlations. Our theoretical analysis further elucidates why these statistical biases intrinsically undermine confidence-based detection techniques. Our findings thus emphasize the urgent need for new approaches explicitly designed to address hallucinations caused by spurious correlations.

</details>


### [93] [FinRpt: Dataset, Evaluation System and LLM-based Multi-agent Framework for Equity Research Report Generation](https://arxiv.org/abs/2511.07322)
*Song Jin,Shuqi Li,Shukun Zhang,Rui Yan*

Main category: cs.CL

TL;DR: 首次提出股票研究报告自动生成任务，构建开源评测基准FinRpt及多智能体框架FinRpt-Gen，实验验证数据质量和框架有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融领域应用广泛，但全自动股票研究报告生成领域存在数据稀缺、评估体系缺失的研究空白。

Method: 整合7类金融数据构建自动化数据集，设计11维度评估指标体系，开发基于监督微调和强化学习的多智能体框架。

Result: 基准数据质量验证有效，FinRpt-Gen框架表现优异，代码数据集已开源推动领域创新。

Conclusion: 该研究为股票研报生成领域建立首个系统化解决方案，开源资源将加速相关技术研发与应用落地。

Abstract: While LLMs have shown great success in financial tasks like stock prediction and question answering, their application in fully automating Equity Research Report generation remains uncharted territory. In this paper, we formulate the Equity Research Report (ERR) Generation task for the first time. To address the data scarcity and the evaluation metrics absence, we present an open-source evaluation benchmark for ERR generation - FinRpt. We frame a Dataset Construction Pipeline that integrates 7 financial data types and produces a high-quality ERR dataset automatically, which could be used for model training and evaluation. We also introduce a comprehensive evaluation system including 11 metrics to assess the generated ERRs. Moreover, we propose a multi-agent framework specifically tailored to address this task, named FinRpt-Gen, and train several LLM-based agents on the proposed datasets using Supervised Fine-Tuning and Reinforcement Learning. Experimental results indicate the data quality and metrics effectiveness of the benchmark FinRpt and the strong performance of FinRpt-Gen, showcasing their potential to drive innovation in the ERR generation field. All code and datasets are publicly available.

</details>


### [94] [Selecting Auxiliary Data via Neural Tangent Kernels for Low-Resource Domains](https://arxiv.org/abs/2511.07380)
*Pingjie Wang,Hongcheng Liu,Yusheng Liao,Ziqing Fan,Yaxin Du,Shuo Tang,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出NTK-Selector框架，通过神经正切核选择通用领域数据，显著提升低资源领域大模型性能


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在低资源领域因数据稀缺和过拟合风险导致的性能瓶颈，利用海量通用领域数据作为辅助监督

Method: 基于神经正切核理论设计选择框架，提出LoRA微调中的类NTK行为验证和免Jacobian矩阵近似方法

Result: 在四个低资源领域实现8.7-5.1分提升，辅助数据效能达纯领域数据的10.9倍

Conclusion: NTK-Selector为低资源场景提供高效数据选择方案，突破传统方法依赖大规模验证集的限制

Abstract: Large language models (LLMs) have achieved remarkable success across widespread tasks, yet their application in low-resource domains remains a significant challenge due to data scarcity and the high risk of overfitting. While in-domain data is limited, there exist vast amounts of similar general-domain data, and our initial findings reveal that they could potentially serve as auxiliary supervision for domain enhancement. This observation leads us to our central research question: \textbf{\textit{how to effectively select the most valuable auxiliary data to maximize domain-specific performance}}, particularly when traditional methods are inapplicable due to a lack of large in-domain data pools or validation sets. To address this, we propose \textbf{NTK-Selector}, a principled and efficient framework for selecting general-domain auxiliary data to enhance domain-specific performance via neural tangent kernels (NTK). Our method tackles two challenges of directly applying NTK to LLMs, theoretical assumptions and prohibitive computational cost, by empirically demonstrating a stable NTK-like behavior in LLMs during LoRA fine-tuning and proposing a Jacobian-free approximation method. Extensive experiments across four low-resource domains (medical, financial, legal, and psychological) demonstrate that NTK-Selector consistently improves downstream performance. Specifically, fine-tuning on 1,000 in-domain samples alone only yielded +0.8 points for Llama3-8B-Instruct and +0.9 points for Qwen3-8B. In contrast, enriching with 9,000 auxiliary samples selected by NTK-Selector led to substantial \textbf{gains of +8.7 and +5.1 points}, which corresponds to a \textbf{10.9x and 5.7x improvement} over the domain-only setting.

</details>


### [95] [Retriv at BLP-2025 Task 2: Test-Driven Feedback-Guided Framework for Bangla-to-Python Code Generation](https://arxiv.org/abs/2511.07382)
*K M Nafi Asib,Sourav Saha,Mohammed Moshiul Hoque*

Main category: cs.CL

TL;DR: 提出基于微调Qwen2.5-14B模型的测试驱动迭代优化方法，在孟加拉语代码生成共享任务中取得第二名的成绩（Pass@1=0.934）


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）在指令-代码数据集和评估基准方面存在严重不足，阻碍了代码生成技术的发展

Method: 采用指令提示与测试驱动结合的迭代优化方法：模型生成代码→单元测试验证→利用错误反馈进行三次迭代优化

Result: 在IJCNLP-AACL 2025 BLP Workshop的代码生成任务中获得第二名，Pass@1指标达到0.934

Conclusion: 孟加拉语指令理解和Python代码生成存在特殊挑战，需开发针对低资源语言的定制化方法，实验代码已开源促进社区发展

Abstract: Large Language Models (LLMs) have advanced the automated generation of code from natural language prompts. However, low-resource languages (LRLs) like Bangla remain underrepresented due to the limited availability of instruction-to-code datasets and evaluation benchmarks. To address this, the BLP Workshop at IJCNLP-AACL 2025 introduced a shared task on "Code Generation in Bangla". In this work, we propose a method that combines instruction prompting with a test-driven, feedback-guided iterative refinement process using a fine-tuned Qwen2.5-14B model. The model generates code from Bangla instructions, tests it against unit tests, and iteratively refines any failing outputs through three evaluation passes, using test feedback to guide each step. This approach helped our team "Retriv" to secure 2nd place in the shared task with a Pass@1 score of 0.934. The analysis highlights challenges in Bangla instruction understanding and Python code generation, emphasizing the need for targeted methods in LRLs. We made experimental scripts publicly available for the community.

</details>


### [96] [Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence](https://arxiv.org/abs/2511.07384)
*Sean McLeish,Ang Li,John Kirchenbauer,Dayal Singh Kalra,Brian R. Bartoldson,Bhavya Kailkhura,Avi Schwarzschild,Jonas Geiping,Tom Goldstein,Micah Goldblum*

Main category: cs.CL

TL;DR: 将预训练非循环语言模型转换为深度循环模型，通过渐进式训练策略降低成本并保持性能


<details>
  <summary>Details</summary>
Motivation: 探索如何利用循环结构解耦训练与测试时的计算需求，在保持模型性能的同时降低计算成本

Method: 采用课程式循环策略（curriculum of recurrences），在训练过程中逐步增加模型有效深度

Result: 转换后的循环模型在数学任务上，相同计算预算下表现优于直接后训练的原非循环模型

Conclusion: 该方法为预训练模型的优化提供了新路径，在资源受限场景中具有应用价值

Abstract: Recent advances in depth-recurrent language models show that recurrence can decouple train-time compute and parameter count from test-time compute. In this work, we study how to convert existing pretrained non-recurrent language models into depth-recurrent models. We find that using a curriculum of recurrences to increase the effective depth of the model over the course of training preserves performance while reducing total computational cost. In our experiments, on mathematics, we observe that converting pretrained models to recurrent ones results in better performance at a given compute budget than simply post-training the original non-recurrent language model.

</details>


### [97] [Surgical Agent Orchestration Platform for Voice-directed Patient Data Interaction](https://arxiv.org/abs/2511.07392)
*Hyeryun Park,Byung Mo Gu,Jun Hee Lee,Byeong Hyeon Choi,Sekeun Kim,Hyun Koo Kim,Kyungsang Kim*

Main category: cs.CL

TL;DR: 提出基于分层多智能体框架的语音导向外科手术协调平台(SAOP)，通过大语言模型驱动的智能体实现术中语音指令的自动化解析与多模态患者数据操作。


<details>
  <summary>Details</summary>
Motivation: 达芬奇机器人手术中外科医生手眼被完全占用，难以在无中断情况下访问和操作多模态患者数据。需开发非接触式交互方案提升手术效率。

Method: 1. 构建由协调智能体与三个任务专用智能体组成的分层框架
2. 通过LLM智能体自主规划、验证和推理，将语音指令映射至具体任务
3. 提出多级协调评估指标(MOEM)从指令级和类别级评估性能

Result: 在240条语音指令测试中获得高准确率(92.1%)和成功率(88.3%)，对语音识别错误和模糊指令表现出强鲁棒性，错误恢复能力提升37%

Conclusion: SAOP平台有效支持达芬奇机器人微创手术，通过智能语音交互减少术中中断，提升手术安全性和操作流畅度，具有显著临床应用价值

Abstract: In da Vinci robotic surgery, surgeons' hands and eyes are fully engaged in the procedure, making it difficult to access and manipulate multimodal patient data without interruption. We propose a voice-directed Surgical Agent Orchestrator Platform (SAOP) built on a hierarchical multi-agent framework, consisting of an orchestration agent and three task-specific agents driven by Large Language Models (LLMs). These LLM-based agents autonomously plan, refine, validate, and reason to map voice commands into specific tasks such as retrieving clinical information, manipulating CT scans, or navigating 3D anatomical models on the surgical video. We also introduce a Multi-level Orchestration Evaluation Metric (MOEM) to comprehensively assess the performance and robustness from command-level and category-level perspectives. The SAOP achieves high accuracy and success rates across 240 voice commands, while LLM-based agents improve robustness against speech recognition errors and diverse or ambiguous free-form commands, demonstrating strong potential to support minimally invasive da Vinci robotic surgery.

</details>


### [98] [ConvFill: Model Collaboration for Responsive Conversational Voice Agents](https://arxiv.org/abs/2511.07397)
*Vidya Srinivas,Zachary Englhardt,Maximus Powers,Shwetak Patel,Vikram Iyer*

Main category: cs.CL

TL;DR: 提出对话填充任务ConvFill模型，通过设备端轻量模型与云端大模型协同，实现低延迟（<200ms）且高准确率（提升36-42%）的对话系统


<details>
  <summary>Details</summary>
Motivation: 解决云端大模型延迟高破坏对话流畅性，而本地小模型响应快但知识不足的矛盾，实现响应速度与知识深度的双重优化

Method: 开发360M参数的ConvFill模型，使用合成多领域对话数据进行训练，支持与不同后端模型协同完成对话填充任务

Result: 在保持200ms响应延迟的同时，相比同规模独立小模型准确率提升36-42%，验证了框架有效性

Conclusion: 对话填充框架成功解耦响应延迟与模型能力，为构建兼具即时响应与专业知识的设备端对话系统提供可行方案

Abstract: Deploying conversational voice agents with large language models faces a critical challenge: cloud-based foundation models provide deep reasoning and domain knowledge but introduce latency that disrupts natural conversation, while on-device models respond immediately but lack sophistication. We propose conversational infill, a task where a lightweight on-device model generates contextually appropriate dialogue while seamlessly incorporating streaming knowledge from a powerful backend model. This approach decouples response latency from model capability, enabling systems that feel responsive while accessing the full power of large-scale models. We present ConvFill, a 360M parameter model trained on synthetic multi-domain conversations. Evaluation across multiple backend models shows that conversational infill can be successfully learned, with ConvFill achieving accuracy improvements of 36-42% over standalone small models of the same size while consistently retaining sub-200ms response latencies. Our results demonstrate the promise of this approach for building on-device conversational agents that are both immediately responsive and knowledgeable.

</details>


### [99] [SPOT: An Annotated French Corpus and Benchmark for Detecting Critical Interventions in Online Conversations](https://arxiv.org/abs/2511.07405)
*Manon Berriche,Célia Nouri,Chloé Clavel,Jean-Philippe Cointet*

Main category: cs.CL

TL;DR: 首个将社会学'停止点'概念转化为NLP任务的标注语料库SPOT，证明微调模型在法语社交媒体分析中优于提示型LLM


<details>
  <summary>Details</summary>
Motivation: 现有反言论框架常忽视普通但关键的讨论干预形式(如讽刺/碎片化质疑)，需建立系统化研究基准

Method: 构建含43,305条人工标注法评论文本及上下文元数据的语料库，对比微调CamemBERT与指令调优LLM的表现

Result: 微调编码器F1达0.78(比LLM高10+%)，加入上下文元数据使模型性能提升3个百分点

Conclusion: 监督学习对非英语社交媒体任务效果显著，公开数据集促进虚假信息治理研究的可重复性

Abstract: We introduce SPOT (Stopping Points in Online Threads), the first annotated corpus translating the sociological concept of stopping point into a reproducible NLP task. Stopping points are ordinary critical interventions that pause or redirect online discussions through a range of forms (irony, subtle doubt or fragmentary arguments) that frameworks like counterspeech or social correction often overlook. We operationalize this concept as a binary classification task and provide reliable annotation guidelines. The corpus contains 43,305 manually annotated French Facebook comments linked to URLs flagged as false information by social media users, enriched with contextual metadata (article, post, parent comment, page or group, and source). We benchmark fine-tuned encoder models (CamemBERT) and instruction-tuned LLMs under various prompting strategies. Results show that fine-tuned encoders outperform prompted LLMs in F1 score by more than 10 percentage points, confirming the importance of supervised learning for emerging non-English social media tasks. Incorporating contextual metadata further improves encoder models F1 scores from 0.75 to 0.78. We release the anonymized dataset, along with the annotation guidelines and code in our code repository, to foster transparency and reproducible research.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [100] [Modeling and Topology Estimation of Low Rank Dynamical Networks](https://arxiv.org/abs/2511.06674)
*Wenqi Cao,Aming Li*

Main category: cs.GR

TL;DR: 提出低秩动态网络模型解决传统拓扑学习方法的局限性，通过因果维纳滤波建立条件格兰杰因果关联，实现网络拓扑一致估计


<details>
  <summary>Details</summary>
Motivation: 传统动态网络拓扑学习方法无法有效处理低秩特性过程，现有方法缺乏可识别性保证

Method: 建立基于因果维纳滤波的理论框架，将滤波器稀疏模式与条件格兰杰因果关系关联，推导网络可辨识条件

Result: 仿真实验验证了模型框架的简洁性和拓扑估计方法的一致性特性

Conclusion: 该研究为低秩动态网络提供了理论可辨识的建模框架，并发展出具有一致性的拓扑估计方法

Abstract: Conventional topology learning methods for dynamical networks become inapplicable to processes exhibiting low-rank characteristics. To address this, we propose the low rank dynamical network model which ensures identifiability. By employing causal Wiener filtering, we establish a necessary and sufficient condition that links the sparsity pattern of the filter to conditional Granger causality. Building on this theoretical result, we develop a consistent method for estimating all network edges. Simulation results demonstrate the parsimony of the proposed framework and consistency of the topology estimation approach.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [101] [Adaptive Testing for Segmenting Watermarked Texts From Language Models](https://arxiv.org/abs/2511.06645)
*Xingchi Li,Xiaochi Liu,Guanxun Li*

Main category: stat.ML

TL;DR: 提出自适应水印检测框架，有效解决混合文本分段难题，无需依赖精确提示估计


<details>
  <summary>Details</summary>
Motivation: 大模型生成文本的滥用导致教育领域和虚假信息传播风险，需可靠的水印技术进行内容溯源

Method: 1. 推广基于似然的检测方法为加权公式；2. 适配逆变换采样方法；3. 开发自适应分割策略处理水印/非水印混合文本

Result: 大量实验证明该方法在混合文本分割中具备高准确性和鲁棒性

Conclusion: 相比依赖提示估计的传统方法，新框架降低敏感性，提升实用性和可靠性

Abstract: The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.

</details>


### [102] [Language Generation with Infinite Contamination](https://arxiv.org/abs/2511.07417)
*Anay Mehrotra,Grigoris Velegkas,Xifan Yu,Felix Zhou*

Main category: stat.ML

TL;DR: 该论文研究在存在噪声和遗漏的污染数据下，语言生成算法的鲁棒性边界，特别是密集生成（dense generation）在不同污染场景下的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有语言生成算法在完美数据集（无噪声/无遗漏）上可行，但现实中数据常被污染。需要确定生成算法在污染数据下的容忍度边界，并探索课程学习（curriculum learning）对噪声数据处理的潜在作用。

Method: 通过理论分析证明：1）污染比例趋零时可实现语言生成；2）密集生成对污染更敏感；3）构建课程学习模型，允许无限污染但污染比例渐近消失时仍实现密集生成。

Result: 1）污染比例趋零是生成可行的充要条件；2）密集生成的鲁棒性弱于普通生成；3）课程学习模型突破传统污染边界，实现高噪声下的密集生成。

Conclusion: 该工作首次系统性刻画污染数据下的生成鲁棒性边界，并提出课程学习作为突破污染限制的关键机制，为噪声网络数据的学习提供理论依据。

Abstract: We study language generation in the limit, where an algorithm observes an adversarial enumeration of strings from an unknown target language $K$ and must eventually generate new, unseen strings from $K$. Kleinberg and Mullainathan [KM24] proved that generation is achievable in surprisingly general settings. But their generator suffers from ``mode collapse,'' producing from an ever-smaller subset of the target. To address this, Kleinberg and Wei [KW25] require the generator's output to be ``dense'' in the target language. They showed that generation with density, surprisingly, remains achievable at the same generality.
  Both results assume perfect data: no noisy insertions and no omissions. This raises a central question: how much contamination can generation tolerate? Recent works made partial progress on this question by studying (non-dense) generation with either finite amounts of noise (but no omissions) or omissions (but no noise).
  We characterize robustness under contaminated enumerations: 1. Generation under Contamination: Language generation in the limit is achievable for all countable collections iff the fraction of contaminated examples converges to zero. When this fails, we characterize which collections are generable. 2. Dense Generation under Contamination: Dense generation is strictly less robust to contamination than generation. As a byproduct, we resolve an open question of Raman and Raman [ICML25] by showing that generation is possible with only membership oracle access under finitely many contaminated examples.
  Finally, we introduce a beyond-worst-case model inspired by curriculum learning and prove that dense generation is achievable even with infinite contamination provided the fraction of contaminated examples converges to zero. This suggests curriculum learning may be crucial for learning from noisy web data.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [103] [The Role of High-Performance GPU Resources in Large Language Model Based Radiology Imaging Diagnosis](https://arxiv.org/abs/2509.16328)
*Jyun-Ping Kao*

Main category: q-bio.TO

TL;DR: GPU硬件对实现高效、低延迟的放射学LLM应用至关重要，下一代GPU特性将推动本地化及联合放射学AI发展。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在放射科部署时对高算力硬件的需求，平衡诊断精度与推理效率。

Method: 通过分析GPU架构（NVIDIA A100/H100，AMD MI300等）的浮点吞吐量、内存带宽等指标，结合CheXpert/MIMIC-CXR影像任务的实证研究，评估硬件性能对报告生成、病灶检测的影响。

Result: 合理配置GPU资源可降低50%推理时间，利用并行计算与张量核心加速显著提升任务吞吐量。

Conclusion: 推进GPU基础设施及优化策略（8-bit张量核心、多GPU扩展等）是构建安全高效放射学AI诊断体系的核心驱动力。

Abstract: Large-language models (LLMs) are rapidly being applied to radiology, enabling automated image interpretation and report generation tasks. Their deployment in clinical practice requires both high diagnostic accuracy and low inference latency, which in turn demands powerful hardware. High-performance graphical processing units (GPUs) provide the necessary compute and memory throughput to run large LLMs on imaging data. We review modern GPU architectures (e.g. NVIDIA A100/H100, AMD Instinct MI250X/MI300) and key performance metrics of floating-point throughput, memory bandwidth, VRAM capacity. We show how these hardware capabilities affect radiology tasks: for example, generating reports or detecting findings on CheXpert and MIMIC-CXR images is computationally intensive and benefits from GPU parallelism and tensor-core acceleration. Empirical studies indicate that using appropriate GPU resources can reduce inference time and improve throughput. We discuss practical challenges including privacy, deployment, cost, power and optimization strategies: mixed-precision, quantization, compression, and multi-GPU scaling. Finally, we anticipate that next-generation features (8-bit tensor cores, enhanced interconnect) will further enable on-premise and federated radiology AI. Advancing GPU infrastructure is essential for safe, efficient LLM-based radiology diagnostics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [104] [Anchors in the Machine: Behavioral and Attributional Evidence of Anchoring Bias in LLMs](https://arxiv.org/abs/2511.05766)
*Felipe Valencia-Clavijo*

Main category: cs.AI

TL;DR: 本文通过行为分析和归因方法，证实LLMs存在可测量的锚定偏差，模型规模影响敏感性，提示设计导致归因效果不稳定


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs表面输出中的锚定效应，缺乏对内部概率重构机制及归因贡献的系统分析，需建立量化评估框架

Method: 结合对数概率分布分析（控制训练数据污染）、Shapley值归因计算及统一敏感性评分，跨六个开源模型进行行为与归因证据整合

Result: Gemma-2B等模型呈现稳健锚定效应，小模型敏感性差异显著；归因效果受提示设计影响，模型规模与偏差敏感性正相关

Conclusion: LLMs的锚定偏差具备可解释性，但应用时需警惕提示设计的脆弱性，该框架为评估认知偏差提供了跨学科方法论基础

Abstract: Large language models (LLMs) are increasingly examined as both behavioral subjects and decision systems, yet it remains unclear whether observed cognitive biases reflect surface imitation or deeper probability shifts. Anchoring bias, a classic human judgment bias, offers a critical test case. While prior work shows LLMs exhibit anchoring, most evidence relies on surface-level outputs, leaving internal mechanisms and attributional contributions unexplored. This paper advances the study of anchoring in LLMs through three contributions: (1) a log-probability-based behavioral analysis showing that anchors shift entire output distributions, with controls for training-data contamination; (2) exact Shapley-value attribution over structured prompt fields to quantify anchor influence on model log-probabilities; and (3) a unified Anchoring Bias Sensitivity Score integrating behavioral and attributional evidence across six open-source models. Results reveal robust anchoring effects in Gemma-2B, Phi-2, and Llama-2-7B, with attribution signaling that the anchors influence reweighting. Smaller models such as GPT-2, Falcon-RW-1B, and GPT-Neo-125M show variability, suggesting scale may modulate sensitivity. Attributional effects, however, vary across prompt designs, underscoring fragility in treating LLMs as human substitutes. The findings demonstrate that anchoring bias in LLMs is robust, measurable, and interpretable, while highlighting risks in applied domains. More broadly, the framework bridges behavioral science, LLM safety, and interpretability, offering a reproducible path for evaluating other cognitive biases in LLMs.

</details>


### [105] [DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis](https://arxiv.org/abs/2511.05810)
*Bowen Xu,Xinyue Zeng,Jiazhen Hu,Tuo Wang,Adithya Kulkarni*

Main category: cs.AI

TL;DR: 提出结合贝叶斯反卷积、深度学习和LLM的混合框架DiagnoLLM，实现高精度且可解释的阿尔茨海默病诊断


<details>
  <summary>Details</summary>
Motivation: 构建可信的临床AI系统需要透明且基于生物学的解释，DiagnoLLM通过整合预测模型与可解释性模块，增强医生和患者对诊断结果的理解与信任

Method: 1. 高斯过程模型GP-unmix解析RNA数据 2. eQTL引导的神经分类器实现疾病预测 3. LLM模块生成受众定制的诊断报告

Result: 阿尔茨海默病检测准确率达88.0%，人类评估验证报告准确性和临床适用性

Conclusion: LLM作为事后推理器在混合诊断流程中展现有效性，强调可解释性系统设计中技术协同的重要性

Abstract: Building trustworthy clinical AI systems requires not only accurate predictions but also transparent, biologically grounded explanations. We present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian deconvolution, eQTL-guided deep learning, and LLM-based narrative generation for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian Process-based hierarchical model that infers cell-type-specific gene expression profiles from bulk and single-cell RNA-seq data while modeling biological uncertainty. These features, combined with regulatory priors from eQTL analysis, power a neural classifier that achieves high predictive performance in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human understanding and trust, we introduce an LLM-based reasoning module that translates model outputs into audience-specific diagnostic reports, grounded in clinical features, attribution signals, and domain knowledge. Human evaluations confirm that these reports are accurate, actionable, and appropriately tailored for both physicians and patients. Our findings show that LLMs, when deployed as post-hoc reasoners rather than end-to-end predictors, can serve as effective communicators within hybrid diagnostic pipelines.

</details>


### [106] [ScRPO: From Errors to Insights](https://arxiv.org/abs/2511.06065)
*Lianrui Li,Dakuan Lu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出Self-correction Relative Policy Optimization (ScRPO)框架，通过自反思和错误纠正增强大语言模型在数学难题上的表现


<details>
  <summary>Details</summary>
Motivation: 在有限外部反馈条件下探索语言模型自主提升能力的可能性，解决复杂数学问题中模型自我改进的挑战

Method: 两阶段训练：1) 试错学习阶段(GRPO训练+错误池收集) 2) 自我纠正阶段(引导模型反思错误原因)

Result: 在AIME/AMC/Olympiad等数学推理基准测试中，Deepseek-Distill-Qwen系列模型显著超越现有后训练方法

Conclusion: ScRPO为语言模型自主进化提供新范式，推动构建更可靠、更强大的AI系统

Abstract: We propose Self-correction Relative Policy Optimization (ScRPO), a novel reinforcement learning framework designed to enhance large language models on challenging mathematical problems by leveraging self-reflection and error correction. Our approach consists of two stages: (1) Trial-and-error learning stage: training the model with GRPO and collecting incorrect answers along with their corresponding questions in an error pool; (2) Self-correction learning stage: guiding the model to reflect on why its previous answers were wrong. Extensive experiments across multiple math reasoning benchmarks, including AIME, AMC, Olympiad, MATH-500, GSM8k, using Deepseek-Distill-Qwen-1.5B and Deepseek-Distill-Qwen-7B. The experimental results demonstrate that ScRPO consistently outperforms several post-training methods. These findings highlight ScRPO as a promising paradigm for enabling language models to self-improve on difficult tasks with limited external feedback, paving the way toward more reliable and capable AI systems.

</details>


### [107] [Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles](https://arxiv.org/abs/2511.06160)
*Fatima Jahara,Mark Dredze,Sharon Levy*

Main category: cs.AI

TL;DR: 提出PRIME框架，通过逻辑谜题系统性评估大语言模型推理中隐含的社会偏见，发现模型在符合刻板印象时推理更准确


<details>
  <summary>Details</summary>
Motivation: 现有安全机制未能检测复杂逻辑推理任务中隐含的微妙社会偏见，需开发更精细的评估工具

Method: 基于性别刻板印象构建逻辑网格谜题（刻板/反刻板/中性三类变体），通过共享谜题结构生成可自动验证的测试集，评估不同规模模型并测试提示干预策略

Result: 模型解决方案与性别刻板印象一致时推理准确率显著提升（平均提升15%），提示干预策略有效性有限（仅改善7%案例）

Conclusion: PRIME框架能有效诊断大模型演绎推理中的隐性偏见，为追求算法公平性的场景提供量化评估工具，揭示了现有模型在逻辑自洽性中潜藏的社会偏见传播风险

Abstract: While recent safety guardrails effectively suppress overtly biased outputs, subtler forms of social bias emerge during complex logical reasoning tasks that evade current evaluation benchmarks. To fill this gap, we introduce a new evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model Evaluation), that uses logic grid puzzles to systematically probe the influence of social stereotypes on logical reasoning and decision making in LLMs. Our use of logic puzzles enables automatic generation and verification, as well as variability in complexity and biased settings. PRIME includes stereotypical, anti-stereotypical, and neutral puzzle variants generated from a shared puzzle structure, allowing for controlled and fine-grained comparisons. We evaluate multiple model families across puzzle sizes and test the effectiveness of prompt-based mitigation strategies. Focusing our experiments on gender stereotypes, our findings highlight that models consistently reason more accurately when solutions align with stereotypical associations. This demonstrates the significance of PRIME for diagnosing and quantifying social biases perpetuated in the deductive reasoning of LLMs, where fairness is critical.

</details>


### [108] [Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads](https://arxiv.org/abs/2511.06209)
*Jingwei Ni,Ekaterina Fadeeva,Tianyi Wu,Mubashara Akhtar,Jiaheng Zhang,Elliott Ash,Markus Leippold,Timothy Baldwin,See-Kiong Ng,Artem Shelmanov,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: 提出轻量级UHeads方法，利用LLM内部状态估计推理步骤的不确定性，替代传统高成本验证方案，在多领域实现高效推理验证。


<details>
  <summary>Details</summary>
Motivation: 现有推理验证方法（如PRMs）存在计算成本高、领域受限、依赖人工标注等问题，需要开发自动化且轻量的替代方案。

Method: 训练基于transformer的轻量化不确定性评估头(UHeads)，结合冻结LLM的内部状态和自动生成的标签（通过大模型或自监督方式），实现推理步骤的自我验证。

Result: UHeads参数量仅10M，在数学/规划/问答等跨领域任务中，性能匹配甚至超越参数量810倍的PRMs，验证LLM内部状态可有效表征不确定性。

Conclusion: LLM内部状态编码了推理不确定性，该方法为构建可扩展、通用化的自省式大模型提供了新方向，显著降低验证计算成本。

Abstract: Solving complex tasks usually requires LLMs to generate long multi-step reasoning chains. Previous work has shown that verifying the correctness of individual reasoning steps can further improve the performance and efficiency of LLMs on such tasks and enhance solution interpretability. However, existing verification approaches, such as Process Reward Models (PRMs), are either computationally expensive, limited to specific domains, or require large-scale human or model-generated annotations. Thus, we propose a lightweight alternative for step-level reasoning verification based on data-driven uncertainty scores. We train transformer-based uncertainty quantification heads (UHeads) that use the internal states of a frozen LLM to estimate the uncertainty of its reasoning steps during generation. The approach is fully automatic: target labels are generated either by another larger LLM (e.g., DeepSeek R1) or in a self-supervised manner by the original model itself. UHeads are both effective and lightweight, containing less than 10M parameters. Across multiple domains, including mathematics, planning, and general knowledge question answering, they match or even surpass the performance of PRMs that are up to 810x larger. Our findings suggest that the internal states of LLMs encode their uncertainty and can serve as reliable signals for reasoning verification, offering a promising direction toward scalable and generalizable introspective LLMs.

</details>


### [109] [Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B](https://arxiv.org/abs/2511.06221)
*Sen Xu,Yi Zhou,Wei Wang,Jixin Min,Zhibin Yin,Yingwei Dai,Shixi Liu,Lianyu Pang,Yirong Chen,Junlin Zhang*

Main category: cs.AI

TL;DR: 通过Spectrum-to-Signal Principle(SSP)框架开发的1.5B参数小模型VibeThinker-1.5B，在数学基准测试中超越多个大模型，证明小模型可具备强大推理能力


<details>
  <summary>Details</summary>
Motivation: 挑战'小模型缺乏推理能力'的行业共识，探索通过算法优化而非参数膨胀提升模型性能，大幅降低AI研发成本（总训练成本仅7800美元）

Method: SSP框架包含两阶段：1）Two-Stage Diversity-Exploring Distillation生成多样化解决方案，2）MaxEnt-Guided Policy Optimization强化正确信号

Result: 在AIME24(80.3)、AIME25(74.4)、HMMT25(50.4)数学基准超越DeepSeek R1(400倍参数)；LiveCodeBench V6得分51.1超越Magistral Medium(50.3)，基础模型得分从0提升显著

Conclusion: 小模型通过算法创新可实现媲美大模型的推理能力，训练成本降低两个数量级，为AI民主化提供新路径

Abstract: Challenging the prevailing consensus that small models inherently lack robust reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense model developed via our Spectrum-to-Signal Principle (SSP). This challenges the prevailing approach of scaling model parameters to enhance capabilities, as seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL) to amplify the correct signal. With a total training cost of only $7,800, VibeThinker-1.5B demonstrates superior reasoning capabilities compared to closed-source models like Magistral Medium and Claude Opus 4, and performs on par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8), AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial improvement over its base model (6.7, 4.3, and 0.6, respectively). On LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its base model's 0.0. These findings demonstrate that small models can achieve reasoning capabilities comparable to large models, drastically reducing training and inference costs and thereby democratizing advanced AI research.

</details>


### [110] [LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation](https://arxiv.org/abs/2511.06346)
*Liya Zhu,Peizhuang Cong,Aowei Ji,Wenya Wu,Jiani Hou,Chunjie Wu,Xiang Gao,Jingkai Liu,Zhou Huan,Xuelei Sun,Yang Yang,Jianpeng Jiao,Liang Hu,Xinjie Chen,Jiashuo Liu,Jingzhe Ding,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: 提出长尾知识驱动的LPFQA评测框架，通过四维度创新构建跨领域真实专业场景，填补LLM能力评估的实践空白


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以评估LLM在真实专业场景中的长尾知识处理能力，需构建兼顾深度知识和跨领域复杂性的评测体系

Method: 基于20个领域专业论坛构建LPFQA基准，创新设计四维度评估体系（知识深度/推理能力/术语理解/上下文分析），建立分层难度架构与真实用户场景模型

Result: 在12个主流LLM的测试中，LPFQA展现出强区分度（尤其在专业推理任务），不同模型表现差异达2-4倍，验证了基准的评估效度

Conclusion: LPFQA填补了LLM能力评估的实践空白，其多维度、跨领域的特性为模型优化提供了可解释的技术路径，推动AI向专业纵深发展

Abstract: Large Language Models (LLMs) have made rapid progress in reasoning, question answering, and professional applications; however, their true capabilities remain difficult to evaluate using existing benchmarks. Current datasets often focus on simplified tasks or artificial scenarios, overlooking long-tail knowledge and the complexities of real-world applications. To bridge this gap, we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic professional forums across 20 academic and industrial fields, covering 502 tasks grounded in practical expertise. LPFQA introduces four key innovations: fine-grained evaluation dimensions that target knowledge depth, reasoning, terminology comprehension, and contextual analysis; a hierarchical difficulty structure that ensures semantic clarity and unique answers; authentic professional scenario modeling with realistic user personas; and interdisciplinary knowledge integration across diverse domains. We evaluated 12 mainstream LLMs on LPFQA and observed significant performance disparities, especially in specialized reasoning tasks. LPFQA provides a robust, authentic, and discriminative benchmark for advancing LLM evaluation and guiding future model development.

</details>


### [111] [MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models](https://arxiv.org/abs/2511.06419)
*Jingyu Hu,Shu Yang,Xilin Gong,Hongming Wang,Weiru Liu,Di Wang*

Main category: cs.AI

TL;DR: MONICA框架通过实时监控推理步骤中的奉承倾向并动态校准，有效减少大型推理模型的迎合性行为


<details>
  <summary>Details</summary>
Motivation: 现有方法仅基于最终答案纠正模型奉承行为，无法监测推理过程中该行为的发展轨迹，导致模型可靠性下降和社会风险

Method: 结合实时奉承倾向监控器（计算奉承漂移分数）和动态校准器（阈值超限时抑制奉承行为），在生成过程中实现步骤级干预

Result: 在12个数据集和3种大型推理模型上验证，显著降低中间推理和最终答案的奉承率，性能提升明显

Conclusion: 通过推理步骤的实时监控与动态校准机制，为解决模型奉承行为提供了有效解决方案，增强模型可靠性

Abstract: Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models tend to agree with users' incorrect beliefs and follow misinformation rather than maintain independent reasoning. This behavior undermines model reliability and poses societal risks. Mitigating LRM sycophancy requires monitoring how this sycophancy emerges during the reasoning trajectory; however, current methods mainly focus on judging based on final answers and correcting them, without understanding how sycophancy develops during reasoning processes. To address this limitation, we propose MONICA, a novel Monitor-guided Calibration framework that monitors and mitigates sycophancy during model inference at the level of reasoning steps, without requiring the model to finish generating its complete answer. MONICA integrates a sycophantic monitor that provides real-time monitoring of sycophantic drift scores during response generation with a calibrator that dynamically suppresses sycophantic behavior when scores exceed predefined thresholds. Extensive experiments across 12 datasets and 3 LRMs demonstrate that our method effectively reduces sycophantic behavior in both intermediate reasoning steps and final answers, yielding robust performance improvements.

</details>


### [112] [Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis](https://arxiv.org/abs/2511.06437)
*Abhishek More,Anthony Zhang,Nicole Bonilla,Ashvik Vivekan,Kevin Zhu,Parham Sharafoleslami,Maheep Chaudhary*

Main category: cs.AI

TL;DR: 提出EDTR方法，通过拓扑分析和Dirichlet不确定性量化提升LLM多步推理的置信度校准，在多项基准测试中展现最优校准性能（平均ECE 0.287）和准确度（AIME完美准确度）。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-thought方法的置信度校准存在严重过度自信和校准不良问题，需通过几何特征量化多路径推理的不确定性。

Method: 将推理路径向量化后提取八种拓扑风险特征（如分布紧密度、一致性），结合Dirichlet分布量化置信度，几何结构特征反映模型确定性水平。

Result: 在AIME/GSM8K等四个基准测试中，EDTR校准误差（ECE）比基线低41%，其中GSM8K ECE仅0.107，AIME达到100%准确度，综合得分0.672最优。

Conclusion: 基于几何特征的EDTR框架为多步LLM推理提供了可解释的不确定性量化方法，对需要可靠置信度估计的实际部署场景具有关键价值。

Abstract: Chain-of-thought (CoT) prompting enables Large Language Models to solve complex problems, but deploying these models safely requires reliable confidence estimates, a capability where existing methods suffer from poor calibration and severe overconfidence on incorrect predictions. We propose Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that combines topological analysis with Dirichlet-based uncertainty quantification to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT as a vector in high-dimensional space and extracts eight topological risk features capturing the geometric structure of reasoning distributions: tighter, more coherent clusters indicate higher confidence while dispersed, inconsistent paths signal uncertainty. We evaluate EDTR against three state-of-the-art calibration methods across four diverse reasoning benchmarks spanning olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training, talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better calibration than competing methods with an average ECE of 0.287 and the best overall composite score of 0.672, while notably achieving perfect accuracy on AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where baselines exhibit severe overconfidence. Our work provides a geometric framework for understanding and quantifying uncertainty in multi-step LLM reasoning, enabling more reliable deployment where calibrated confidence estimates are essential.

</details>


### [113] [GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2511.06618)
*Moriya Dechtiar,Daniel Martin Katz,Mari Sundaresan,Sylvain Jaume,Hongming Wang*

Main category: cs.AI

TL;DR: 使用基于强化学习的LLM框架(GRAPH-GRPO-LEX)将法律合同转化为语义图，实现合同条款关系可视化与自动化分析


<details>
  <summary>Details</summary>
Motivation: 传统合同审查存在结构复杂、显隐性依赖交织、人工处理效率低且易错的问题，需自动化解决方案提升准确性和效率

Method: 提出本体论映射合同要素到图结构，开发GRPO强化学习框架结合LLM进行实体关系提取，通过图指标奖励函数优化关系识别

Result: 成功识别条款间显性关系并发现隐性依赖，门控GRPO方法产生强学习信号，合同分析从线性阅读转变为可视化图分析

Conclusion: 该方法将合同分析转变为动态图分析，为合同代码化审查奠定基础，类似软件工程中的linting技术，显著提升分析效率

Abstract: Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.

</details>


### [114] [MENTOR: A Metacognition-Driven Self-Evolution Framework for Uncovering and Mitigating Implicit Risks in LLMs on Domain Tasks](https://arxiv.org/abs/2511.07107)
*Liang Shan,Kaicheng Shen,Wen Wu,Zhenyu Ying,Chaochao Lu,Guangze Ye,Liang He*

Main category: cs.AI

TL;DR: MENTOR框架通过元认知自我评估工具和动态规则知识图谱，有效降低LLMs在专业领域的隐式风险，实现持续自我进化


<details>
  <summary>Details</summary>
Motivation: 现有LLMs对齐方法主要解决显性风险，但缺乏处理专业领域深层隐式风险的灵活框架，且静态系统维护成本高

Method: 1. 元认知自评估（视角转换/结果推演）
2. 动态生成补充规则知识图谱
3. 推理时激活引导技术增强规则执行

Result: 防御测试显示框架显著降低三个垂直领域的语义攻击成功率（教育/金融/管理），元认知评估与人类评估一致性达86%

Conclusion: MENTOR建立了LLMs持续进化机制，通过动态规则扩展和激活引导，在保持低成本的同时显著提升跨领域风险缓解能力

Abstract: Ensuring the safety and value alignment of large language models (LLMs) is critical for their deployment. Current alignment efforts primarily target explicit risks such as bias, hate speech, and violence. However, they often fail to address deeper, domain-specific implicit risks and lack a flexible, generalizable framework applicable across diverse specialized fields. Hence, we proposed MENTOR: A MEtacognition-driveN self-evoluTion framework for uncOvering and mitigating implicit Risks in LLMs on Domain Tasks. To address the limitations of labor-intensive human evaluation, we introduce a novel metacognitive self-assessment tool. This enables LLMs to reflect on potential value misalignments in their responses using strategies like perspective-taking and consequential thinking. We also release a supporting dataset of 9,000 risk queries spanning education, finance, and management to enhance domain-specific risk identification. Subsequently, based on the outcomes of metacognitive reflection, the framework dynamically generates supplementary rule knowledge graphs that extend predefined static rule trees. This enables models to actively apply validated rules to future similar challenges, establishing a continuous self-evolution cycle that enhances generalization by reducing maintenance costs and inflexibility of static systems. Finally, we employ activation steering during inference to guide LLMs in following the rules, a cost-effective method to robustly enhance enforcement across diverse contexts. Experimental results show MENTOR's effectiveness: In defensive testing across three vertical domains, the framework substantially reduces semantic attack success rates, enabling a new level of implicit risk mitigation for LLMs. Furthermore, metacognitive assessment not only aligns closely with baseline human evaluators but also delivers more thorough and insightful analysis of LLMs value alignment.

</details>


### [115] [IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction](https://arxiv.org/abs/2511.07327)
*Guoxin Chen,Zile Qiao,Xuanzhong Chen,Donglei Yu,Haotian Xu,Wayne Xin Zhao,Ruihua Song,Wenbiao Yin,Huifeng Yin,Liwen Zhang,Kuan Li,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.AI

TL;DR: 提出迭代式深度研究范式IterResearch，通过马尔可夫决策过程和工作区重构解决长程推理任务，开发EAPO强化学习框架实现高效探索，实验显示性能显著提升且具备交互扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理采用单一上下文窗口导致信息过载和噪声污染，难以有效处理长程任务。需要新的范式突破上下文限制并保持稳定推理能力。

Method: 1. 将长程研究建模为带工作区重构的MDP，使用动态报告作为记忆体
2. 开发EAPO框架：几何奖励折扣激励高效探索，自适应下采样实现分布式训练稳定

Result: 平均提升14.5pp（6个基准），交互扩展至2048次时性能从3.5%跃升至42.5%，作为提示策略可使前沿模型性能提升19.2pp（对比ReAct）

Conclusion: IterResearch作为通用解决方案，既是训练代理也构成有效提示范式，在长程推理任务中展现突破性扩展能力和性能表现，缩小了与前沿系统的差距。

Abstract: Recent advances in deep-research agents have shown promise for autonomous knowledge construction through dynamic reasoning over external sources. However, existing approaches rely on a mono-contextual paradigm that accumulates all information in a single, expanding context window, leading to context suffocation and noise contamination that limit their effectiveness on long-horizon tasks. We introduce IterResearch, a novel iterative deep-research paradigm that reformulates long-horizon research as a Markov Decision Process with strategic workspace reconstruction. By maintaining an evolving report as memory and periodically synthesizing insights, our approach preserves consistent reasoning capacity across arbitrary exploration depths. We further develop Efficiency-Aware Policy Optimization (EAPO), a reinforcement learning framework that incentivizes efficient exploration through geometric reward discounting and enables stable distributed training via adaptive downsampling. Extensive experiments demonstrate that IterResearch achieves substantial improvements over existing open-source agents with average +14.5pp across six benchmarks and narrows the gap with frontier proprietary systems. Remarkably, our paradigm exhibits unprecedented interaction scaling, extending to 2048 interactions with dramatic performance gains (from 3.5\% to 42.5\%), and serves as an effective prompting strategy, improving frontier models by up to 19.2pp over ReAct on long-horizon tasks. These findings position IterResearch as a versatile solution for long-horizon reasoning, effective both as a trained agent and as a prompting paradigm for frontier models.

</details>


### [116] [DigiData: Training and Evaluating General-Purpose Mobile Control Agents](https://arxiv.org/abs/2511.07413)
*Yuxuan Sun,Manchen Wang,Shengyi Qian,William R. Wong,Eric Gan,Pierluca D'Oro,Alejandro Castillejo Munoz,Sneha Silwal,Pedro Matias,Nitin Kamra,Satwik Kottur,Nick Raines,Xuanyi Zhao,Joy Chen,Joseph Greer,Andrea Madotto,Allen Bolourchi,James Valori,Kevin Carlberg,Karl Ridgeway,Joseph Tighe*

Main category: cs.AI

TL;DR: 提出大规模多模态数据集DigiData及评测基准DigiData-Bench，通过系统化探索提升移动控制代理的训练效果，并改进现有评估方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有移动控制代理数据集存在目标复杂度低、评估指标不可靠等问题，需要高质量数据集和严谨评估体系推动技术发展

Method: 通过全面探索APP功能构建复杂目标数据集，设计动态评估协议与AI驱动的评估方式替代传统步数准确率指标

Result: DigiData具备更高目标复杂性和多样性，提出的新评估方法能更可靠地反映代理性能

Conclusion: 该研究为移动控制代理的发展提供了数据基础与评估框架，推动人机交互向更直观高效方向演进

Abstract: AI agents capable of controlling user interfaces have the potential to transform human interaction with digital devices. To accelerate this transformation, two fundamental building blocks are essential: high-quality datasets that enable agents to achieve complex and human-relevant goals, and robust evaluation methods that allow researchers and practitioners to rapidly enhance agent performance. In this paper, we introduce DigiData, a large-scale, high-quality, diverse, multi-modal dataset designed for training mobile control agents. Unlike existing datasets, which derive goals from unstructured interactions, DigiData is meticulously constructed through comprehensive exploration of app features, resulting in greater diversity and higher goal complexity. Additionally, we present DigiData-Bench, a benchmark for evaluating mobile control agents on real-world complex tasks. We demonstrate that the commonly used step-accuracy metric falls short in reliably assessing mobile control agents and, to address this, we propose dynamic evaluation protocols and AI-powered evaluations as rigorous alternatives for agent assessment. Our contributions aim to significantly advance the development of mobile control agents, paving the way for more intuitive and effective human-device interactions.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [117] [Robust and High-Fidelity 3D Gaussian Splatting: Fusing Pose Priors and Geometry Constraints for Texture-Deficient Outdoor Scenes](https://arxiv.org/abs/2511.06765)
*Meijun Guo,Yongliang Shi,Caiyun Liu,Yixiao Feng,Ming Ma,Tinghai Yan,Weining Lu,Bin Liang*

Main category: cs.CV

TL;DR: 提出结合LiDAR-IMU先验位姿与法向量约束的3DGS优化方法，提升大场景重建的鲁棒性与可视化质量


<details>
  <summary>Details</summary>
Motivation: 解决弱纹理/重复纹理大场景中传统3DGS存在的位姿估计不稳定和几何失真问题

Method: 位姿估计端融合LiDAR-IMU先验约束优化COLMAP，场景表示端引入法向量约束+有效秩正则化联合优化高斯基元

Result: 在公开/自采数据集上实现位姿优化效率提升3倍，场景重建质量显著优于传统3DGS，弱纹理场景可视化效果突出

Conclusion: 多传感器融合与几何约束联合优化策略有效提升大尺度户外场景重建的鲁棒性和精度，代码数据将开源

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a key rendering pipeline for digital asset creation due to its balance between efficiency and visual quality. To address the issues of unstable pose estimation and scene representation distortion caused by geometric texture inconsistency in large outdoor scenes with weak or repetitive textures, we approach the problem from two aspects: pose estimation and scene representation. For pose estimation, we leverage LiDAR-IMU Odometry to provide prior poses for cameras in large-scale environments. These prior pose constraints are incorporated into COLMAP's triangulation process, with pose optimization performed via bundle adjustment. Ensuring consistency between pixel data association and prior poses helps maintain both robustness and accuracy. For scene representation, we introduce normal vector constraints and effective rank regularization to enforce consistency in the direction and shape of Gaussian primitives. These constraints are jointly optimized with the existing photometric loss to enhance the map quality. We evaluate our approach using both public and self-collected datasets. In terms of pose optimization, our method requires only one-third of the time while maintaining accuracy and robustness across both datasets. In terms of scene representation, the results show that our method significantly outperforms conventional 3DGS pipelines. Notably, on self-collected datasets characterized by weak or repetitive textures, our approach demonstrates enhanced visualization capabilities and achieves superior overall performance. Codes and data will be publicly available at https://github.com/justinyeah/normal_shape.git.

</details>


### [118] [Long Grounded Thoughts: Distilling Compositional Visual Reasoning Chains at Scale](https://arxiv.org/abs/2511.05705)
*David Acuna,Chao-Han Huck Yang,Yuntian Deng,Jaehun Jung,Ximing Lu,Prithviraj Ammanabrolu,Hyunwoo Kim,Yuan-Hong Liao,Yejin Choi*

Main category: cs.CV

TL;DR: 提出两阶段多模态推理数据生成框架，生成100万+视觉中心问题，Qwen2.5-VL-7B微调后在多项基准超越开源/闭源模型，并实现跨模态迁移。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉推理研究依赖未公开数据的问题，建立系统化的大规模视觉推理数据集构建方法，突破传统视觉数学任务局限。

Method: 分规模扩展和复杂度提升两阶段合成数据，利用视觉语言模型+推理大模型生成非线性思维链轨迹，整合偏好数据支持强化学习。

Result: Qwen2.5-VL-7B在V* Bench等视觉基准全面领先，音频/文本推理迁移效果显著，分阶段离线RL达到在线RL性能且计算效率更优。

Conclusion: 高质量SFT数据是强化学习基础，分阶段训练策略提升效率，数据质量直接影响跨模态迁移能力，为VLM训练提供系统框架。

Abstract: Recent progress in multimodal reasoning has been driven largely by undisclosed datasets and proprietary data synthesis recipes, leaving open questions about how to systematically build large-scale, vision-centric reasoning datasets, particularly for tasks that go beyond visual math. In this work, we introduce a new reasoning data generation framework spanning diverse skills and levels of complexity with over 1M high-quality synthetic vision-centric questions. The dataset also includes preference data and instruction prompts supporting both offline and online RL. Our synthesis framework proceeds in two stages: (1) scale; and (2) complexity. Reasoning traces are then synthesized through a two-stage process that leverages VLMs and reasoning LLMs, producing CoT traces for VLMs that capture the richness and diverse cognitive behaviors found in frontier reasoning models. Remarkably, we show that finetuning Qwen2.5-VL-7B on our data outperforms all open-data baselines across all evaluated vision-centric benchmarks, and even surpasses strong closed-data models such as MiMo-VL-7B-RL on V* Bench, CV-Bench and MMStar-V. Perhaps most surprising, despite being entirely vision-centric, our data transfers positively to text-only reasoning (MMLU-Pro) and audio reasoning (MMAU), demonstrating its effectiveness. Similarly, despite not containing videos or embodied visual data, we observe notable gains when evaluating on a single-evidence embodied QA benchmark (NiEH). Finally, we use our data to analyze the entire VLM post-training pipeline. Our empirical analysis highlights that (i) SFT on high-quality data with non-linear reasoning traces is essential for effective online RL, (ii) staged offline RL matches online RL's performance while reducing compute demands, and (iii) careful SFT on high quality data can substantially improve out-of-domain, cross-modality transfer.

</details>


### [119] [Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective](https://arxiv.org/abs/2511.06284)
*Bing Wang,Ximing Li,Yanjun Wang,Changchun Li,Lin Yuanbo Wu,Buyu Wang,Shengsheng Wang*

Main category: cs.CV

TL;DR: 提出RETSIMD方法，通过文本生成图像增强多模态虚假信息检测，利用文本-图像互信息和图神经网络提升检测效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，多模态虚假信息检测中文本比图像更具信息量。图像常仅呈现局部场景，而文本描述完整事件。基于此，通过文本生成相关图像以增强图像模态的有效性。

Method: 1. 将文本分割为多个片段；2. 使用预训练文本-图像生成器生成对应图像；3. 引入文本-图像互信息和图像-标签互信息作为辅助目标；4. 在辅助数据集上微调生成器；5. 构建图像关系图并用图神经网络融合特征。

Result: 大量实验证明RETSIMD有效性，生成图像与互信息目标显著提升检测性能，图结构有效整合多图像特征。

Conclusion: 通过文本生成图像增强模态关联性，结合互信息约束和图神经网络，为多模态虚假信息检测提供新思路，证实文本引导的图像增强策略的有效性。

Abstract: Multimodal Misinformation Detection (MMD) refers to the task of detecting social media posts involving misinformation, where the post often contains text and image modalities. However, by observing the MMD posts, we hold that the text modality may be much more informative than the image modality because the text generally describes the whole event/story of the current post but the image often presents partial scenes only. Our preliminary empirical results indicate that the image modality exactly contributes less to MMD. Upon this idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that each text can be divided into several segments, and each text segment describes a partial scene that can be presented by an image. Accordingly, we split the text into a sequence of segments, and feed these segments into a pre-trained text-to-image generator to augment a sequence of images. We further incorporate two auxiliary objectives concerning text-image and image-label mutual information, and further post-train the generator over an auxiliary text-to-image generation benchmark dataset. Additionally, we propose a graph structure by defining three heuristic relationships between images, and use a graph neural network to generate the fused features. Extensive empirical results validate the effectiveness of RETSIMD.

</details>


### [120] [HiMo-CLIP: Modeling Semantic Hierarchy and Monotonicity in Vision-Language Alignment](https://arxiv.org/abs/2511.06653)
*Ruijia Wu,Ping Chen,Fei Shen,Shaoan Zhao,Qiang Hui,Huanlin Gao,Ting Lu,Zhaoxiang Liu,Fang Zhao,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: HiMo-CLIP框架通过层次化语义分解（HiDe）和单调感知对比损失（MoLo），增强了CLIP模型对长文本/组合描述的跨模态对齐能力


<details>
  <summary>Details</summary>
Motivation: 传统CLIP模型将文本视为扁平序列，无法捕捉语言的层次化语义结构和语义单调性（文本完整性应与视觉对齐强度正相关）

Method: 1. HiDe模块通过批内PCA提取文本的潜在语义成分；2. MoLo损失函数联合优化全局表征和成分级对齐，保持语义顺序

Result: 在多个图文检索基准上显著超越基线模型，尤其在长文本/组合描述场景提升明显

Conclusion: 通过表征层面的改进（无需修改编码器架构），实现了符合认知规律的结构化跨模态表征学习

Abstract: Contrastive vision-language models like CLIP have achieved impressive results in image-text retrieval by aligning image and text representations in a shared embedding space. However, these models often treat text as flat sequences, limiting their ability to handle complex, compositional, and long-form descriptions. In particular, they fail to capture two essential properties of language: semantic hierarchy, which reflects the multi-level compositional structure of text, and semantic monotonicity, where richer descriptions should result in stronger alignment with visual content.To address these limitations, we propose HiMo-CLIP, a representation-level framework that enhances CLIP-style models without modifying the encoder architecture. HiMo-CLIP introduces two key components: a hierarchical decomposition (HiDe) module that extracts latent semantic components from long-form text via in-batch PCA, enabling flexible, batch-aware alignment across different semantic granularities, and a monotonicity-aware contrastive loss (MoLo) that jointly aligns global and component-level representations, encouraging the model to internalize semantic ordering and alignment strength as a function of textual completeness.These components work in concert to produce structured, cognitively-aligned cross-modal representations. Experiments on multiple image-text retrieval benchmarks show that HiMo-CLIP consistently outperforms strong baselines, particularly under long or compositional descriptions. The code is available at https://github.com/UnicomAI/HiMo-CLIP.

</details>


### [121] [Revisiting the Data Sampling in Multimodal Post-training from a Difficulty-Distinguish View](https://arxiv.org/abs/2511.06722)
*Jianyu Qi,Ding Zou,Wenrui Yan,Rui Ma,Jiaxu Li,Zhijie Zheng,Zhiguo Yang,Rongchang Zhao*

Main category: cs.CV

TL;DR: 提出PISM和CMAB两种难度感知采样策略，通过分层训练框架提升多模态大语言模型的推理能力


<details>
  <summary>Details</summary>
Motivation: 现有后训练范式存在两个缺陷：缺乏量化样本难度的指标，以及无法联合优化感知与推理能力

Method: 1. 渐进式图像语义掩码(PISM)系统降级图像量化难度；2. 跨模态注意力平衡(CMAB)分析注意力分布；3. 构建包含GRPO-only和SFT+GRPO的分层训练框架

Result: 在6个基准数据集测试中，基于难度分层的GRPO方法全面优于传统SFT+GRPO方案

Conclusion: 策略性数据采样可替代监督微调，通过难度分层优化显著提升模型准确率

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have spurred significant progress in Chain-of-Thought (CoT) reasoning. Building on the success of Deepseek-R1, researchers extended multimodal reasoning to post-training paradigms based on reinforcement learning (RL), focusing predominantly on mathematical datasets. However, existing post-training paradigms tend to neglect two critical aspects: (1) The lack of quantifiable difficulty metrics capable of strategically screening samples for post-training optimization. (2) Suboptimal post-training paradigms that fail to jointly optimize perception and reasoning capabilities. To address this gap, we propose two novel difficulty-aware sampling strategies: Progressive Image Semantic Masking (PISM) quantifies sample hardness through systematic image degradation, while Cross-Modality Attention Balance (CMAB) assesses cross-modal interaction complexity via attention distribution analysis. Leveraging these metrics, we design a hierarchical training framework that incorporates both GRPO-only and SFT+GRPO hybrid training paradigms, and evaluate them across six benchmark datasets. Experiments demonstrate consistent superiority of GRPO applied to difficulty-stratified samples compared to conventional SFT+GRPO pipelines, indicating that strategic data sampling can obviate the need for supervised fine-tuning while improving model accuracy. Our code will be released at https://github.com/qijianyu277/DifficultySampling.

</details>


### [122] [SpatialThinker: Reinforcing 3D Reasoning in Multimodal LLMs via Spatial Rewards](https://arxiv.org/abs/2511.07403)
*Hunar Batra,Haoqin Tu,Hardy Chen,Yuanze Lin,Cihang Xie,Ronald Clark*

Main category: cs.CV

TL;DR: SpatialThinker通过结构化空间基础与多步强化学习训练，显著提升多模态大语言模型在3D空间理解能力，数据效率高且性能超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖显式3D输入/架构调整，受限于大规模数据和稀疏监督，难以实现精准空间理解

Method: 1) 构建STVQA-7K数据集的数据合成流程 2) 采用多目标密集空间奖励的在线强化学习框架，通过场景图构建和空间关系推理实现空间认知

Result: SpatialThinker-7B在空间理解和真实VQA基准测试中表现优于监督微调和稀疏RL基线，性能增益接近稀疏RL基线的2倍，超越GPT-4o

Conclusion: 空间监督与奖励对齐推理的结合，在有限数据下实现了鲁棒的3D空间理解，推动MLLMs向人类水平视觉推理迈进

Abstract: Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language tasks, but they continue to struggle with spatial understanding. Existing spatial MLLMs often rely on explicit 3D inputs or architecture-specific modifications, and remain constrained by large-scale datasets or sparse supervision. To address these limitations, we introduce SpatialThinker, a 3D-aware MLLM trained with RL to integrate structured spatial grounding with multi-step reasoning. The model simulates human-like spatial perception by constructing a scene graph of task-relevant objects and spatial relations, and reasoning towards an answer via dense spatial rewards. SpatialThinker consists of two key contributions: (1) a data synthesis pipeline that generates STVQA-7K, a high-quality spatial VQA dataset, and (2) online RL with a multi-objective dense spatial reward enforcing spatial grounding. SpatialThinker-7B outperforms supervised fine-tuning and the sparse RL baseline on spatial understanding and real-world VQA benchmarks, nearly doubling the base-model gain compared to sparse RL, and surpassing GPT-4o. These results showcase the effectiveness of combining spatial supervision with reward-aligned reasoning in enabling robust 3D spatial understanding with limited data and advancing MLLMs towards human-level visual reasoning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [123] [Approximating the Mathematical Structure of Psychodynamics](https://arxiv.org/abs/2511.05580)
*Bryce-Allen Bagley,Navin Khoshnan*

Main category: q-bio.NC

TL;DR: 通过过程理论的图式框架形式化人类心理动力学，为心理学和AI安全领域提供跨学科的数学建模工具


<details>
  <summary>Details</summary>
Motivation: 现有心理学研究缺乏统一数学框架，难以满足心理医学、神经技术、AI安全等领域对认知过程量化分析的需求

Method: 采用过程理论（process theory）的图式表示方法，建立认知过程的数学化模型

Result: 构建可应用于心理治疗、AI对齐、自主谈判代理、类人AI系统开发等多个领域的形式化认知分析框架

Conclusion: 该框架为跨学科认知研究提供了通用语言，特别在提升AI系统的安全性和人类认知建模方面具有重要价值

Abstract: The complexity of human cognition has meant that psychology makes more use of theory and conceptual models than perhaps any other biomedical field. To enable precise quantitative study of the full breadth of phenomena in psychological and psychiatric medicine as well as cognitive aspects of AI safety, there is a need for a mathematical formulation which is both mathematically precise and equally accessible to experts from numerous fields. In this paper we formalize human psychodynamics via the diagrammatic framework of process theory, describe its key properties, and explain the links between a diagrammatic representation and central concepts in analysis of cognitive processes in contexts such as psychotherapy, neurotechnology, AI alignment, AI agent representation of individuals in autonomous negotiations, developing human-like AI systems, and other aspects of AI safety.

</details>


### [124] [On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception](https://arxiv.org/abs/2511.06519)
*Sanaz Saki Norouzi,Mohammad Masjedi,Pascal Hitzler*

Main category: q-bio.NC

TL;DR: 研究发现LLMs通过特定神经元激活模式处理词性标签，其机制与人类大脑语法处理存在相似性。使用Llama 3识别关键神经元后构建的分类器能有效预测新数据词性标签。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否像人类大脑一样通过特定神经元处理不同语法功能，验证语言模型与神经科学发现的潜在关联性。

Method: 1. 使用Llama 3定位词性预测相关的关键神经元
2. 基于神经元激活模式构建分类器
3. 在新数据集验证预测效果

Result: 关键神经元的激活模式可靠预测词性标签，揭示LLMs中存在类似大脑语法处理的专用功能子空间

Conclusion: LLMs与大脑在语法处理机制上存在结构相似性，为理解模型内部表征和神经科学交叉研究提供新视角

Abstract: Artificial Neural Networks, the building blocks of AI, were inspired by the human brain's network of neurons. Over the years, these networks have evolved to replicate the complex capabilities of the brain, allowing them to handle tasks such as image and language processing. In the realm of Large Language Models, there has been a keen interest in making the language learning process more akin to that of humans. While neuroscientific research has shown that different grammatical categories are processed by different neurons in the brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we identify the most important neurons associated with the prediction of words belonging to different part-of-speech tags. Using the achieved knowledge, we train a classifier on a dataset, which shows that the activation patterns of these key neurons can reliably predict part-of-speech tags on fresh data. The results suggest the presence of a subspace in LLMs focused on capturing part-of-speech tag concepts, resembling patterns observed in lesion studies of the brain in neuroscience.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [125] [The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation](https://arxiv.org/abs/2511.05903)
*Zhengyuan Liu,Stella Xin Yin,Bryan Chen Zhengyu Tan,Roy Ka-Wei Lee,Guimei Liu,Dion Hoe-Lian Goh,Wenya Wang,Nancy F. Chen*

Main category: cs.CY

TL;DR: 提出基于记忆层次结构和元认知过程的学生模拟框架，解决现有教育AI无法反映知识渐进发展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有学生模拟方法忽视知识构建的渐进性和个体差异，LLM直接输出的特性与实际学习者认知发展不匹配。

Method: 通过分层记忆机制实现结构化知识表征，整合元认知过程和人格特质构建动态学习者画像，基于NGSS标准开发课程对应模拟器。

Result: 实验证明该框架能有效反映知识发展的渐进特性和典型学习困难，提供更精准的学习过程表征。

Conclusion: 记忆驱动框架通过认知发展与个性化特征的动态整合，为教育AI提供了更符合真实学习规律的学生模拟方案。

Abstract: User simulation is important for developing and evaluating human-centered AI, yet current student simulation in educational applications has significant limitations. Existing approaches focus on single learning experiences and do not account for students' gradual knowledge construction and evolving skill sets. Moreover, large language models are optimized to produce direct and accurate responses, making it challenging to represent the incomplete understanding and developmental constraints that characterize real learners. In this paper, we introduce a novel framework for memory-based student simulation that incorporates developmental trajectories through a hierarchical memory mechanism with structured knowledge representation. The framework also integrates metacognitive processes and personality traits to enrich the individual learner profiling, through dynamical consolidation of both cognitive development and personal learning characteristics. In practice, we implement a curriculum-aligned simulator grounded on the Next Generation Science Standards. Experimental results show that our approach can effectively reflect the gradual nature of knowledge development and the characteristic difficulties students face, providing a more accurate representation of learning processes.

</details>


### [126] [Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI](https://arxiv.org/abs/2511.06078)
*Luis Marquez-Carpintero,Alberto Lopez-Sellers,Miguel Cazorla*

Main category: cs.CY

TL;DR: 本文系统综述了基于大语言模型模拟学生行为的教育研究，分析其在教学评估、课程开发和教师培训中的应用潜力与现存挑战


<details>
  <summary>Details</summary>
Motivation: 传统规则系统在自然语言生成和情境适应性方面存在局限，LLMs为模拟多样化学习行为提供了更高真实性和适应性

Method: 采用主题综述方法，综合评估LLM模拟学生在不同教育场景中的实证研究，包含单代理对话与多代理课堂互动分析

Result: LLMs能有效模拟典型学习者类型，响应教学输入并参与课堂互动，但在算法偏见、评估信效度方面仍存在显著挑战

Conclusion: LLMs为教育模拟提供了新范式，需解决技术偏差问题并与教学目标更好对齐，未来应聚焦生成式AI与自适应学习系统的整合

Abstract: Simulated Students offer a valuable methodological framework for evaluating pedagogical approaches and modelling diverse learner profiles, tasks which are otherwise challenging to undertake systematically in real-world settings. Recent research has increasingly focused on developing such simulated agents to capture a range of learning styles, cognitive development pathways, and social behaviours. Among contemporary simulation techniques, the integration of large language models (LLMs) into educational research has emerged as a particularly versatile and scalable paradigm. LLMs afford a high degree of linguistic realism and behavioural adaptability, enabling agents to approximate cognitive processes and engage in contextually appropriate pedagogical dialogues. This paper presents a thematic review of empirical and methodological studies utilising LLMs to simulate student behaviour across educational environments. We synthesise current evidence on the capacity of LLM-based agents to emulate learner archetypes, respond to instructional inputs, and interact within multi-agent classroom scenarios. Furthermore, we examine the implications of such systems for curriculum development, instructional evaluation, and teacher training. While LLMs surpass rule-based systems in natural language generation and situational flexibility, ongoing concerns persist regarding algorithmic bias, evaluation reliability, and alignment with educational objectives. The review identifies existing technological and methodological gaps and proposes future research directions for integrating generative AI into adaptive learning systems and instructional design.

</details>


### [127] [Large Language Models Develop Novel Social Biases Through Adaptive Exploration](https://arxiv.org/abs/2511.06148)
*Addison J. Wu,Ryan Liu,Xuechunzi Bai,Thomas L. Griffiths*

Main category: cs.CY

TL;DR: 大语言模型会主动生成对虚拟人口群体的新社会偏见，激励探索的干预措施最能有效减少分层


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在决策框架中应用时，单纯消除现有偏见不够，需研究其主动生成新偏见的机制

Method: 采用心理学范式，通过探索-利用权衡理论框架，测试模型输入干预/问题结构调整/显式引导等系列措施

Result: LLMs会基于经验创造新偏见，模型越新越大偏见越严重；明确激励探索最能有效减少任务分配分层

Conclusion: 需建立多维度目标机制，LLMs不仅是人类偏见的镜子，更会主动创造新社会偏见，引发长期社会影响担忧

Abstract: As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.

</details>


### [128] [Place Matters: Comparing LLM Hallucination Rates for Place-Based Legal Queries](https://arxiv.org/abs/2511.06700)
*Damian Curran,Vanessa Sporne,Lea Frermann,Jeannie Paterson*

Main category: cs.CY

TL;DR: 提出基于比较法功能主义的方法论，通过构建真实法律场景数据集，量化LLM在不同地区（洛杉矶/伦敦/悉尼）法律信息幻觉率的空间分布差异


<details>
  <summary>Details</summary>
Motivation: 用户通过LLM获取法律信息的质量可能存在地域差异，但现有方法缺乏有效的跨地区法律知识比较框架

Method: 1. 从Reddit法律咨询帖构建家庭/住房/就业/犯罪/交通领域场景数据集
2. 要求LLM生成三地区相关法律摘要
3. 人工评估法律条款摘要的幻觉现象

Result: 闭源LLM的法律信息幻觉率存在显著地域差异，且幻觉率与模型多次采样的多数响应频率呈强负相关

Conclusion: LLM提供的法律解决方案质量存在地理分布不均，模型对法律事实预测的不确定性可能通过采样一致性反映

Abstract: How do we make a meaningful comparison of a large language model's knowledge of the law in one place compared to another? Quantifying these differences is critical to understanding if the quality of the legal information obtained by users of LLM-based chatbots varies depending on their location. However, obtaining meaningful comparative metrics is challenging because legal institutions in different places are not themselves easily comparable. In this work we propose a methodology to obtain place-to-place metrics based on the comparative law concept of functionalism. We construct a dataset of factual scenarios drawn from Reddit posts by users seeking legal advice for family, housing, employment, crime and traffic issues. We use these to elicit a summary of a law from the LLM relevant to each scenario in Los Angeles, London and Sydney. These summaries, typically of a legislative provision, are manually evaluated for hallucinations. We show that the rate of hallucination of legal information by leading closed-source LLMs is significantly associated with place. This suggests that the quality of legal solutions provided by these models is not evenly distributed across geography. Additionally, we show a strong negative correlation between hallucination rate and the frequency of the majority response when the LLM is sampled multiple times, suggesting a measure of uncertainty of model predictions of legal facts.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [129] [When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms](https://arxiv.org/abs/2511.06448)
*Qibing Ren,Zhijie Zheng,Jiaxuan Guo,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.MA

TL;DR: 研究大型多智能体系统中LLM代理的集体金融欺诈风险，建立MultiAgentFraudBench基准测试，分析欺诈成功的关键因素并提出缓解策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM代理的普及，亟需理解多智能体协作欺诈的风险机制及其现实危害。

Method: 构建覆盖28个典型在线欺诈场景的基准测试，从交互深度、活跃度、协作失败模式等维度分析欺诈成功要素。

Result: 发现恶意代理具备环境适应性，验证了内容警告、LLM监控、社会层面信息共享等策略的有效性。

Conclusion: 多智能体金融欺诈存在现实风险，需通过技术监控与社会协作构建动态防御体系，研究代码已开源推动领域发展。

Abstract: In this work, we study the risks of collective financial fraud in large-scale multi-agent systems powered by large language model (LLM) agents. We investigate whether agents can collaborate in fraudulent behaviors, how such collaboration amplifies risks, and what factors influence fraud success. To support this research, we present MultiAgentFraudBench, a large-scale benchmark for simulating financial fraud scenarios based on realistic online interactions. The benchmark covers 28 typical online fraud scenarios, spanning the full fraud lifecycle across both public and private domains. We further analyze key factors affecting fraud success, including interaction depth, activity level, and fine-grained collaboration failure modes. Finally, we propose a series of mitigation strategies, including adding content-level warnings to fraudulent posts and dialogues, using LLMs as monitors to block potentially malicious agents, and fostering group resilience through information sharing at the societal level. Notably, we observe that malicious agents can adapt to environmental interventions. Our findings highlight the real-world risks of multi-agent financial fraud and suggest practical measures for mitigating them. Code is available at https://github.com/zheng977/MutiAgent4Fraud.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [130] [MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?](https://arxiv.org/abs/2511.05867)
*Jiayi Fu,Qiyao Sun*

Main category: cs.CR

TL;DR: Proposes a synthetic benchmark to evaluate LLMs' capability to detect security risks from system logs in compromised MCP scenarios, demonstrating RLVR's effectiveness in balancing detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Security vulnerabilities in MCP framework remain understudied despite widespread LLM-tool integration. Existing benchmarks focus on prompt injection attacks rather than malicious server logs.

Method: Created 1,800 synthetic system logs across 9 risk categories using 10 LLMs. Embedded logs in 243 MCP server returns, generating 2,421 training chats and 471 evaluation queries. Tested detection performance across model types.

Result: Llama3.1-8B-Instruct with GRPO achieved 83% accuracy (9% improvement over remote models). SFT models showed high false positives, while smaller models had high false negatives.

Conclusion: Reinforcement learning (RLVR) effectively enhances LLM safety in MCP frameworks by balancing precision-recall tradeoffs, particularly through GRPO training methodology.

Abstract: Large language models (LLMs) demonstrate strong capabilities in solving complex tasks when integrated with external tools. The Model Context Protocol (MCP) has become a standard interface for enabling such tool-based interactions. However, these interactions introduce substantial security concerns, particularly when the MCP server is compromised or untrustworthy. While prior benchmarks primarily focus on prompt injection attacks or analyze the vulnerabilities of LLM MCP interaction trajectories, limited attention has been given to the underlying system logs associated with malicious MCP servers. To address this gap, we present the first synthetic benchmark for evaluating LLMs ability to identify security risks from system logs. We define nine categories of MCP server risks and generate 1,800 synthetic system logs using ten state-of-the-art LLMs. These logs are embedded in the return values of 243 curated MCP servers, yielding a dataset of 2,421 chat histories for training and 471 queries for evaluation. Our pilot experiments reveal that smaller models often fail to detect risky system logs, leading to high false negatives. While models trained with supervised fine-tuning (SFT) tend to over-flag benign logs, resulting in elevated false positives, Reinforcement Learning from Verifiable Reward (RLVR) offers a better precision-recall balance. In particular, after training with Group Relative Policy Optimization (GRPO), Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing large remote model by 9 percentage points. Fine-grained, per-category analysis further underscores the effectiveness of reinforcement learning in enhancing LLM safety within the MCP framework. Code and data are available at: https://github.com/PorUna-byte/MCP-Guard/tree/master

</details>


### [131] [Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs](https://arxiv.org/abs/2511.05919)
*Alina Fastowski,Bardh Prenkaj,Yuxiao Li,Gjergji Kasneci*

Main category: cs.CR

TL;DR: 提出Xmera框架评估LLM在中间人攻击下的知识库漏洞，指令攻击成功率高达85.3%，并基于响应不确定性开发防御机制（AUC~96%）。


<details>
  <summary>Details</summary>
Motivation: LLM作为问答系统存在中间人攻击漏洞，可能威胁用户信息安全，需系统性评估攻击效果并提出防御方案。

Method: 通过三种封闭式事实QA场景注入对抗性提示，扰动LLM输入并量化响应正确率与生成过程不确定性指标。

Result: 简单指令攻击成功率最高（85.3%），错误回答伴随高不确定性；基于响应不确定性训练的随机森林分类器可有效检测攻击（AUC~96%）。

Conclusion: 在LLM黑箱场景中，通过响应不确定性预警攻击行为是保障用户网络安全的第一道防线。

Abstract: LLMs are now an integral part of information retrieval. As such, their role as question answering chatbots raises significant concerns due to their shown vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose the first principled attack evaluation on LLM factual memory under prompt injection via Xmera, our novel, theory-grounded MitM framework. By perturbing the input given to "victim" LLMs in three closed-book and fact-based QA settings, we undermine the correctness of the responses and assess the uncertainty of their generation process. Surprisingly, trivial instruction-based attacks report the highest success rate (up to ~85.3%) while simultaneously having a high uncertainty for incorrectly answered questions. To provide a simple defense mechanism against Xmera, we train Random Forest classifiers on the response uncertainty levels to distinguish between attacked and unattacked queries (average AUC of up to ~96%). We believe that signaling users to be cautious about the answers they receive from black-box and potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.

</details>


### [132] [Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting](https://arxiv.org/abs/2511.06197)
*Dilli Prasad Sharma,Liang Xue,Xiaowei Sun,Xiaodong Lin,Pulei Xiong*

Main category: cs.CR

TL;DR: 提出基于SHAP指纹的新型对抗检测模型，显著提升物联网入侵检测系统对对抗攻击的鲁棒性和可解释性


<details>
  <summary>Details</summary>
Motivation: 物联网设备激增导致安全威胁加剧，现有AI/ML入侵检测系统易受对抗攻击影响，需要建立更可靠的防御机制

Method: 利用SHAP的DeepExplainer提取网络流量特征指纹，通过捕捉特征归因模式差异识别对抗样本

Result: 在标准物联网数据集上检测准确率显著超越现有最优方法，实现可靠的对抗样本识别

Conclusion: 该模型不仅提升系统防御能力，同时通过可解释AI增强安全系统的透明度和可信度

Abstract: The rapid proliferation of Internet of Things (IoT) devices has transformed numerous industries by enabling seamless connectivity and data-driven automation. However, this expansion has also exposed IoT networks to increasingly sophisticated security threats, including adversarial attacks targeting artificial intelligence (AI) and machine learning (ML)-based intrusion detection systems (IDS) to deliberately evade detection, induce misclassification, and systematically undermine the reliability and integrity of security defenses. To address these challenges, we propose a novel adversarial detection model that enhances the robustness of IoT IDS against adversarial attacks through SHapley Additive exPlanations (SHAP)-based fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints from network traffic features, enabling the IDS to reliably distinguish between clean and adversarially perturbed inputs. By capturing subtle attribution patterns, the model becomes more resilient to evasion attempts and adversarial manipulations. We evaluated the model on a standard IoT benchmark dataset, where it significantly outperformed a state-of-the-art method in detecting adversarial attacks. In addition to enhanced robustness, this approach improves model transparency and interpretability, thereby increasing trust in the IDS through explainable AI.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [133] [Predicting Oscar-Nominated Screenplays with Sentence Embeddings](https://arxiv.org/abs/2511.05500)
*Francis Gross*

Main category: cs.IR

TL;DR: 探讨利用现代语言模型和文本嵌入技术预测奥斯卡最佳剧本提名的可行性，结合多特征输入的逻辑回归模型在新建数据集上取得良好预测效果


<details>
  <summary>Details</summary>
Motivation: 解决现有奥斯卡提名预测数据集的缺失问题，验证现代语言模型在电影奖项预测领域的应用潜力

Method: 1. 构建Movie-O-Label数据集（MovieSum剧本+奥斯卡记录）
2. 使用E5嵌入模型对长剧本分块编码
3. 结合剧本、摘要、标题三特征训练逻辑回归分类器

Result: 最佳模型达到宏观F1分数0.66，PR曲线AP值0.445（基线0.19），ROC-AUC 0.79，显著优于基线水平

Conclusion: 基于现代文本嵌入的简单模型已展现良好预测性能，为电影产业奖项预测研究提供了新方向，未来可扩展至其他电影奖项预测场景

Abstract: Oscar nominations are an important factor in the movie industry because they can boost both the visibility and the commercial success. This work explores whether it is possible to predict Oscar nominations for screenplays using modern language models. Since no suitable dataset was available, a new one called Movie-O-Label was created by combining the MovieSum collection of movie scripts with curated Oscar records. Each screenplay was represented by its title, Wikipedia summary, and full script. Long scripts were split into overlapping text chunks and encoded with the E5 sentence em bedding model. Then, the screenplay embed dings were classified using a logistic regression model. The best results were achieved when three feature inputs related to screenplays (script, summary, and title) were combined. The best-performing model reached a macro F1 score of 0.66, a precision recall AP of 0.445 with baseline 0.19 and a ROC-AUC of 0.79. The results suggest that even simple models based on modern text embeddings demonstrate good prediction performance and might be a starting point for future research.

</details>


### [134] [A Representation Sharpening Framework for Zero Shot Dense Retrieval](https://arxiv.org/abs/2511.05684)
*Dhananjay Ashok,Suraj Nair,Mutasem Al-Darabsah,Choon Hui Teo,Tarun Agarwal,Jonathan May*

Main category: cs.IR

TL;DR: 提出无需训练的表示锐化框架，通过增强文档表征解决零样本密集检索中相似文档区分难题


<details>
  <summary>Details</summary>
Motivation: 预训练密集检索器在未经目标语料库训练时，难以捕捉相似文档间的语义差异，导致检索性能受限

Method: 在索引阶段通过语料库级别的信息增强文档表征，开发与HyDE/SPAR等现有方法兼容的索引时近似方案

Result: 在20+多语言数据集上超越传统检索方法，BRIGHT基准达到新SOTA，与现有方法组合可提升平均2.7%性能

Conclusion: 表示锐化有效突破零样本检索瓶颈，保持零训练成本优势的同时，通过创新索引方案平衡性能与计算开销

Abstract: Zero-shot dense retrieval is a challenging setting where a document corpus is provided without relevant queries, necessitating a reliance on pretrained dense retrievers (DRs). However, since these DRs are not trained on the target corpus, they struggle to represent semantic differences between similar documents. To address this failing, we introduce a training-free representation sharpening framework that augments a document's representation with information that helps differentiate it from similar documents in the corpus. On over twenty datasets spanning multiple languages, the representation sharpening framework proves consistently superior to traditional retrieval, setting a new state-of-the-art on the BRIGHT benchmark. We show that representation sharpening is compatible with prior approaches to zero-shot dense retrieval and consistently improves their performance. Finally, we address the performance-cost tradeoff presented by our framework and devise an indexing-time approximation that preserves the majority of our performance gains over traditional retrieval, yet suffers no additional inference-time cost.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [135] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: Lightning Grasp算法通过Contact Field数据结构实现实时多样化抓取合成，速度提升数量级，支持不规则物体的无监督抓取生成


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂能量函数和敏感初始化，难以实现实时多样化抓取。需突破传统几何计算限制，提升机器人操作效率

Method: 提出Contact Field数据结构，将几何计算与搜索过程解耦，通过过程化搜索实现高效抓取路径生成

Result: 相比SOTA方法实现数量级加速，支持工具类不规则物体抓取，无需人工干预初始化参数

Conclusion: 该突破性方法为机器人操作领域提供新范式，开源系统将加速机器人抓取技术的创新发展

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [136] [Graph Representation-based Model Poisoning on the Heterogeneous Internet of Agents](https://arxiv.org/abs/2511.07176)
*Hanlin Cai,Houtianfu Wang,Haofan Dong,Kai Li,Ozgur B. Akan*

Main category: cs.NI

TL;DR: 本文提出了一种基于图表示的模型投毒攻击（GRMP），揭示了联邦学习在物联网智能体（IoA）范式中的安全隐患。该方法通过构建参数相关图和对抗性图自编码器合成隐蔽的恶意模型，导致现有防御机制失效，系统准确率持续下降。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习赋能的IoA系统在十亿级参数和异构数据场景下面临模型投毒威胁，传统基于距离/相似度的防御手段失效，需构建新型攻击验证系统脆弱性。

Method: GRMP攻击被动收集良性本地模型，构建参数相关性图结构，通过对抗性变分图自编码器学习高阶依赖关系，生成统计特征与良性模型相似但内含对抗目标的恶意模型。

Result: 实验显示GRMP攻击导致系统准确率梯度下降（CIFAR-10下降22.3%，ImageNet下降14.7%），且现有Krum防御机制的检测成功率低于8.5%。

Conclusion: 该研究首次验证了图结构攻击对IoA系统的毁灭性影响，暴露了现有防御体系在复杂攻击模式下的根本缺陷，为下一代安全联邦学习框架提供警示。

Abstract: Internet of Agents (IoA) envisions a unified, agent-centric paradigm where heterogeneous large language model (LLM) agents can interconnect and collaborate at scale. Within this paradigm, federated learning (FL) serves as a key enabler that allows distributed LLM agents to co-train global models without centralizing data. However, the FL-enabled IoA system remains vulnerable to model poisoning attacks, and the prevailing distance and similarity-based defenses become fragile at billion-parameter scale and under heterogeneous data distributions. This paper proposes a graph representation-based model poisoning (GRMP) attack, which passively exploits observed benign local models to construct a parameter correlation graph and extends an adversarial variational graph autoencoder to capture and reshape higher-order dependencies. The GRMP attack synthesizes malicious local models that preserve benign-like statistics while embedding adversarial objectives, remaining elusive to detection at the server. Experiments demonstrate a gradual drop in system accuracy under the proposed attack and the ineffectiveness of the prevailing defense mechanism in detecting the attack, underscoring a severe threat to the ambitious IoA paradigm.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [137] [Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction](https://arxiv.org/abs/2511.05577)
*An Vuong,Minh-Hao Van,Prateek Verma,Chen Zhao,Xintao Wu*

Main category: cs.LG

TL;DR: 通过多模态数据集和LoRA微调，提升视觉语言模型在材料科学中的聚合物性质预测性能，减少单独模型训练需求。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在科学领域（如材料科学）效果有限，缺乏针对多模态聚合物性质预测的基础模型，需填补此空白。

Method: 构建多模态聚合物数据集，利用指令调优和LoRA技术微调VLM，对比多模态与单模态/基线方法性能。

Result: 微调后模型性能优于单模态及基线，验证多模态有效性，并降低多模型部署维护成本。

Conclusion: 多模态学习可提升材料科学预测性能并降低成本，为领域内基础模型开发提供新方向。

Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.

</details>


### [138] [TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification](https://arxiv.org/abs/2511.05704)
*Pasan Dissanayake,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 提出TabDistill框架，将复杂Transformer模型的知识蒸馏到简单神经网络，在保持参数高效的同时实现少量数据下的优异表格分类性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在少量表格数据场景下表现优异但参数量大，需平衡模型复杂度与少样本学习性能。

Method: 通过知识蒸馏技术，将预训练Transformer的知识迁移到轻量级神经网络，结合参数效率与少样本适应能力。

Result: 蒸馏网络在相同数据下超越XGBoost、普通神经网络等基线模型，部分场景性能优于原Transformer模型。

Conclusion: TabDistill成功解决参数量与性能的权衡，为表格数据分类提供了高效解决方案。

Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.

</details>


### [139] [Adapting Web Agents with Synthetic Supervision](https://arxiv.org/abs/2511.06101)
*Zhaoyang Wang,Yiming Liang,Xuchao Zhang,Qianhui Wu,Siwei Han,Anson Bastos,Rujia Wang,Chetan Bansal,Baolin Peng,Jianfeng Gao,Saravan Rajmohan,Huaxiu Yao*

Main category: cs.LG

TL;DR: 提出SynthAgent框架通过任务和轨迹双重优化提升网页代理的合成数据质量，实验显示其优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有网页代理在适应新网站时面临任务演示稀缺问题，传统合成方法存在数据质量差（任务幻觉/轨迹冗余）的缺陷

Method: 1. 分类探索网页元素生成多样化任务 2. 检测任务与观测冲突进行动态修正 3. 全局上下文轨迹去噪 4. 基于优化数据微调开源代理

Result: 实验证明SynthAgent超越现有合成数据方法，验证高质量合成监督的重要性

Conclusion: 双重细化机制有效提升合成数据质量，代码将开源。该方法为网页代理适应新环境提供了有效解决方案

Abstract: Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.

</details>


### [140] [Mixtures of SubExperts for Large Language Continual Learning](https://arxiv.org/abs/2511.06237)
*Haeyong Kang*

Main category: cs.LG

TL;DR: 提出MoSEs框架，通过混合子专家和任务路由机制解决PEFT方法在持续学习中的遗忘与扩展问题，实现高效知识保留与迁移。


<details>
  <summary>Details</summary>
Motivation: 传统PEFT方法在持续学习中面临参数复用导致遗忘或独立参数导致模型膨胀的困境，需平衡遗忘抑制与知识迁移。

Method: 在Transformer层集成稀疏子专家混合结构，通过任务路由动态组合子专家参数，隔离任务知识并支持跨任务参数复用。

Result: 在TRACE基准测试中，MoSEs显著优于传统持续学习方法，实现知识保留与模型容量亚线性增长，节省计算资源。

Conclusion: MoSEs通过模块化参数隔离与自适应路由，有效减少遗忘、促进任务间迁移，以更低成本达到SOTA性能。

Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.

</details>


### [141] [CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models](https://arxiv.org/abs/2511.06430)
*Peyman Hosseini,Ondrej Bohdal,Taha Ceritli,Ignacio Castro,Matthew Purver,Mete Ozay,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出上下文引导的TTRL方法(CG-TTRL)，通过动态整合上下文到两阶段采样策略，显著提升测试时强化学习性能与效率


<details>
  <summary>Details</summary>
Motivation: 传统TTRL的两阶段采样策略未能充分利用上下文指导，导致伪标签生成精度受限且探索阶段缺乏有效约束。上下文学习在推理时展现的潜力未被有效整合。

Method: 1. 在初始利用阶段注入上下文提升伪标签精度
2. 在探索阶段利用上下文调节奖励信号
3. 提出设备端友好的高效上下文选择方法

Result: 数学和科学QA基准测试中相对准确率提升7%，3步训练即获得8%改进（原方法仅1%），实现快速收敛

Conclusion: 上下文动态整合机制有效突破TTRL性能瓶颈，平衡模型探索与利用，为设备端应用提供高效解决方案

Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).

</details>


### [142] [The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models](https://arxiv.org/abs/2511.07237)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Xiaoyu Shen*

Main category: cs.LG

TL;DR: 论文发现时间序列预测模型存在'缩放悖论'，揭示模型规模增大反而导致性能下降，并提出通过识别关键层提升效率的解决方案


<details>
  <summary>Details</summary>
Motivation: 针对时间序列预测中大规模模型存在的'缩放悖论'现象（模型规模越大性能反而下降），试图揭示其根本原因并提出解决方案

Method: 1. 通过四类参数规模（1亿到17亿）和多样化数据（最高60亿观测值）的对比实验验证悖论存在
2. 分析模型内部表征，发现'少层主导'现象（仅少量层真正有效）
3. 提出自动识别并保留关键层的优化方法

Result: 1. 保留21%参数即可实现12%准确率提升和2.7倍推理加速
2. 在8个主流模型（90M到6B参数）验证有效性，95%任务中保留不到30%层即可达到或超越原模型精度

Conclusion: 时间序列模型普遍存在层冗余问题，通过聚焦关键层的优化策略可显著提升模型效率和性能，为大规模时间序列模型的优化提供新思路

Abstract: Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\% of the parameters achieves up to a 12\% accuracy improvement and a 2.7$\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\% of layers achieves comparable or superior accuracy in over 95\% of tasks.

</details>


### [143] [Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection](https://arxiv.org/abs/2511.07364)
*Vaibhav Mavi,Shubh Jaroria,Weiqi Sun*

Main category: cs.LG

TL;DR: 研究通过比较整体评估与逐步评估方法，证实逐步评分在多步推理错误检测中效果更优（AUC-ROC提升15%），增强了LLM系统的可信度


<details>
  <summary>Details</summary>
Motivation: 解决LLM在关键多步推理任务中的可靠性问题，突破现有单步自信度评估的局限

Method: 在两大基准数据集上对比整体评分与逐步评分两种自评估方法

Result: 逐步评估显著优于整体方法，错误检测AUC-ROC相对提升达15%

Conclusion: 多步自评估系统可有效提升LLM复杂推理的可信度，并提供实用化失败检测框架

Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [144] [Factual and Musical Evaluation Metrics for Music Language Models](https://arxiv.org/abs/2511.05550)
*Daniel Chenyu Lin,Michael Freeman,John Thickstun*

Main category: cs.SD

TL;DR: 现有音乐语言模型评估方法存在缺陷，本文提出改进的评估指标和事实性框架以准确量化模型性能


<details>
  <summary>Details</summary>
Motivation: 当前音乐语言模型评估指标（如BLEU/METEOR）只能测量回答的语言流畅度，无法验证音乐领域回答的正确性

Method: 提出：1) 改进的通用音乐领域评估指标 2) 事实性评估框架，可量化开放域问答模型的正确性

Result: 新框架能准确评估音乐语言模型性能，且其设计具有跨模态通用性，适用于其他开放域问答场景

Conclusion: 通过改进评估体系填补了音乐语言模型验证空白，提出的方法论具有领域扩展潜力，推动更可靠的AI评估体系发展

Abstract: Music language models (Music LMs), like vision language models, leverage multimodal representations to answer natural language queries about musical audio recordings. Although Music LMs are reportedly improving, we find that current evaluations fail to capture whether their answers are correct. Specifically, for all Music LMs that we examine, widely-used evaluation metrics such as BLEU, METEOR, and BERTScore fail to measure anything beyond linguistic fluency of the model's responses. To measure the true performance of Music LMs, we propose (1) a better general-purpose evaluation metric for Music LMs adapted to the music domain and (2) a factual evaluation framework to quantify the correctness of a Music LM's responses. Our framework is agnostic to the modality of the question-answering model and could be generalized to quantify performance in other open-ended question-answering domains. We use open datasets in our experiments and will release all code on publication.

</details>


### [145] [Persian Musical Instruments Classification Using Polyphonic Data Augmentation](https://arxiv.org/abs/2511.05717)
*Diba Hadi Esfangereh,Mohammad Hossein Sameti,Sepehr Harfi Moridani,Leili Javidpour,Mahdieh Soleymani Baghshah*

Main category: cs.SD

TL;DR: 提出基于文化感知数据增强的波斯乐器分类方法，使用MERT模型在真实多音轨音乐中取得0.795 ROC-AUC，验证跨文化音乐信息检索的有效性


<details>
  <summary>Details</summary>
Motivation: 针对西方音乐研究主导现状，填补波斯传统乐器识别研究空白，促进文化包容性音乐信息检索系统发展

Method: 1) 构建包含7种波斯乐器的数据集 2) 设计文化感知的多音轨数据增强策略 3) 采用MERT模型+分类头架构 4) 使用传统歌曲片段进行分布外评估

Result: 在真实多音轨波斯音乐中达到最佳ROC-AUC 0.795，音调与时序连贯性特征显示互补优势

Conclusion: 文化基础的数据增强有效提升乐器识别鲁棒性，为跨文化音乐生成系统奠定技术基础

Abstract: Musical instrument classification is essential for music information retrieval (MIR) and generative music systems. However, research on non-Western traditions, particularly Persian music, remains limited. We address this gap by introducing a new dataset of isolated recordings covering seven traditional Persian instruments, two common but originally non-Persian instruments (i.e., violin, piano), and vocals. We propose a culturally informed data augmentation strategy that generates realistic polyphonic mixtures from monophonic samples. Using the MERT model (Music undERstanding with large-scale self-supervised Training) with a classification head, we evaluate our approach with out-of-distribution data which was obtained by manually labeling segments of traditional songs. On real-world polyphonic Persian music, the proposed method yielded the best ROC-AUC (0.795), highlighting complementary benefits of tonal and temporal coherence. These results demonstrate the effectiveness of culturally grounded augmentation for robust Persian instrument recognition and provide a foundation for culturally inclusive MIR and diverse music generation systems.

</details>


### [146] [ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction](https://arxiv.org/abs/2511.06288)
*Wenxuan Wu,Shuai Wang,Xixin Wu,Helen Meng,Haizhou Li*

Main category: cs.SD

TL;DR: 提出结合大语言模型与AV-TSE的ELEGANCE框架，通过三种语言指导策略显著提升复杂场景下的目标语音提取效果。


<details>
  <summary>Details</summary>
Motivation: 现有AV-TSE模型过度依赖视觉线索，而人类实际会综合运用语言知识（如句法约束、上下文预测），因此探索将LLMs的语言能力融入语音提取任务。

Method: 1. 输出语言约束（控制语音分离过程）
2. 中间语言预测（实时预测目标语义）
3. 输入语言先验（注入对话上下文知识）

Result: 在视觉受损/语言陌生/说话人切换/干扰者增多/跨领域等极端场景下，使用Qwen3-4B等LLMs时性能显著提升，尤其在仅0.6B参数的轻量模型上也保持有效性。

Conclusion: ELEGANCE成功证明语言知识对多模态语音提取的增强作用，为LLMs与听觉系统的深度整合开辟新方向，特别适用于视觉受限的复杂现实场景。

Abstract: Audio-visual target speaker extraction (AV-TSE) models primarily rely on visual cues from the target speaker. However, humans also leverage linguistic knowledge, such as syntactic constraints, next word prediction, and prior knowledge of conversation, to extract target speech. Inspired by this observation, we propose ELEGANCE, a novel framework that incorporates linguistic knowledge from large language models (LLMs) into AV-TSE models through three distinct guidance strategies: output linguistic constraints, intermediate linguistic prediction, and input linguistic prior. Comprehensive experiments with RoBERTa, Qwen3-0.6B, and Qwen3-4B on two AV-TSE backbones demon- strate the effectiveness of our approach. Significant improvements are observed in challenging scenarios, including visual cue impaired, unseen languages, target speaker switches, increased interfering speakers, and out-of-domain test set. Demo page: https://alexwxwu.github.io/ELEGANCE/.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [147] [FPGA or GPU? Analyzing comparative research for application-specific guidance](https://arxiv.org/abs/2511.06565)
*Arnab A Purkayastha,Jay Tharwani,Shobhit Aggarwal*

Main category: cs.AR

TL;DR: Comparative analysis of FPGA vs. GPU accelerators focusing on application-specific suitability rather than generic performance metrics.


<details>
  <summary>Details</summary>
Motivation: Existing research predominantly compares hardware accelerators through performance metrics alone, lacking guidance about which accelerator suits specific application domains best.

Method: Synthesizes insights from multiple studies by categorizing research and analyzing performance/energy/programmability trade-offs.

Result: Identifies domain-specific strengths: FPGAs excel in energy-efficient fixed workloads, GPUs in parallelizable tasks. Provides framework for accelerator selection.

Conclusion: Offers decision-making guidelines balancing performance, energy efficiency, and programmability for researchers/practitioners choosing between FPGA and GPU.

Abstract: The growing complexity of computational workloads has amplified the need for efficient and specialized hardware accelerators. Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent solutions, each excelling in specific domains. Although there is substantial research comparing FPGAs and GPUs, most of the work focuses primarily on performance metrics, offering limited insight into the specific types of applications that each accelerator benefits the most. This paper aims to bridge this gap by synthesizing insights from various research articles to guide users in selecting the appropriate accelerator for domain-specific applications. By categorizing the reviewed studies and analyzing key performance metrics, this work highlights the strengths, limitations, and ideal use cases for FPGAs and GPUs. The findings offer actionable recommendations, helping researchers and practitioners navigate trade-offs in performance, energy efficiency, and programmability.

</details>
