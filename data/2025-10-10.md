<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 97]
- [cs.GR](#cs.GR) [Total: 9]
- [cs.AI](#cs.AI) [Total: 15]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 10]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

TL;DR: 本研究通过结合街景图像与社交媒体文本，提出情感不一致性分析方法，揭示城市感知与观点情感差异及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 现有城市多维情感分析难以应对社交媒体带来的感知与观点间情感反应差异，需开发新方法量化并解释这种不一致性。

Method: 1. 构建14万+街景图像与98万+微博文本数据集；2. 采用目标检测与自然语言处理技术建立反应指数；3. 基于土地利用分布的回归分析、图像分割和词频可视化方法。

Result: 感知情感分布趋均匀化（2016→2022），观点情感极端化；发现建筑密度/人流与情感变化显著相关；疫情前后不匹配地图显示感知-观点差异扩大。

Conclusion: 研究为环境管理提供可视化工具，揭示城市更新中物质空间与社会情感关联，建议在策略制定中统筹考虑感知与观点的动态差异。

Abstract: The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [2] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

TL;DR: 提出HaystackCraft基准测试，通过真实维基百科数据验证大语言模型在噪声长上下文场景下的鲁棒性，揭示现有模型在代理工作流中的级联错误问题


<details>
  <summary>Details</summary>
Motivation: 现有NIAH测试忽略现实场景中由有偏检索和代理工作流产生的噪声上下文，需要构建更贴近真实挑战的评估体系

Method: 基于维基百科超链接网络构建多跳问题，整合稀疏/稠密/混合/图检索策略，模拟动态代理场景下的查询优化和自停止决策

Result: 图重排技术提升检索质量同时降低噪声干扰；GPT-5等先进模型在自生成噪声环境下仍出现级联失败，早期停止决策困难

Conclusion: HaystackCraft为长上下文推理提供有效测试平台，揭示当前模型在复杂代理场景下的持续挑战

Abstract: Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [3] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 大型语言模型(LLMs)无需微调，仅通过上下文示例即可在12种语言中实现最先进的词形还原效果


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在缺乏目标领域训练数据时，与传统监督方法在跨语言/跨领域词形还原任务中的性能差异

Method: 对比监督学习(域外微调编码器)、跨语言方法与LLMs的上下文生成，实验覆盖12种不同形态复杂性的语言

Result: LLMs在多数语言中达到SOTA，仅需少量示例即可超越传统方法；监督方法在域外场景仍具竞争力

Conclusion: LLMs在无监督词形还原任务中展现强大潜力，特别是在低资源场景下可直接生成准确词元

Abstract: Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [4] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

TL;DR: 提出基于大语言模型(LLM)的LASER评分系统，改进传统ASR评估指标对语义无关错误的过度惩罚问题，在印地语等印度语言测试中取得94%人类标注相关性，并验证小模型微调后可达89%惩罚类型预测准确率。


<details>
  <summary>Details</summary>
Motivation: 传统词错误率(WER)等指标会过度惩罚形态/句法变化（如复数变形、词序调整），但这些变化往往不改变句子核心语义，需要更符合人类判断的评估体系。

Method: 1. 利用Gemini等LLM的上下文学习能力，通过包含错误类型标注的提示示例生成LASER评分
2. 基于Llama 3等小模型微调，使用参考文本与ASR预测生成的词对训练惩罚类型分类器

Result: 1. 印地语LASER与人工评分相关性达94%
2. 提示中的印地语示例可有效分析马拉地语等其他印度语言
3. 微调后Llama 3预测惩罚类型的准确率达89%

Conclusion: LASER通过语义感知评估减少对无关错误的惩罚，同时证明大模型上下文学习与小模型微调在不同场景下的有效性，为多语言ASR系统评估提供新范式。

Abstract: Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [5] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: 开发了基于骨骼姿势的手语评估框架，通过关键点距离/嵌入/回译指标分析，开源工具包助力手语生成系统的开发与评估


<details>
  <summary>Details</summary>
Motivation: 解决手语生成系统缺乏标准化评估指标的问题，强调不同场景下评估指标的选择需要权衡，推动可重复性研究

Method: 结合自动元评估（符号级检索）和人类相关性研究（跨语言文本-姿势翻译），系统比较三类评估指标

Result: 揭示不同指标在检索准确率和翻译质量间的权衡关系，验证回译方法在跨语言场景的有效性

Conclusion: 提出的评估框架和开源工具包为手语生成系统提供了标准化、可复现的开发和评估方案

Abstract: We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [6] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

TL;DR: 研究者开发了基于LLM的提示策略（CoT+领域知识引导），成功实现民粹主义文本分类准确率媲美人类专家，解决了传统文本分析方法效率低、扩展性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统民粹主义文本分析方法依赖人工标注，存在成本高、耗时长、跨语言/大规模分析困难等局限，亟需自动化解决方案。

Method: 1. 使用全球民粹主义数据库（GPD）训练数据
2. 设计结合编码规则和思维链的提示策略
3. 在多款LLM上验证分类准确性

Result: 领域特定的提示策略使LLM分类准确率达到专家水平，证明其能有效处理民粹主义的语境敏感性和概念复杂性。

Conclusion: 该方法为跨语言、跨语境的大规模民粹主义分析提供了高效可靠的技术路径，推动了计算社会科学方法创新。

Abstract: Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [7] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: MAPRO框架通过四阶段算法和拓扑感知机制优化多智能体系统提示，实现了SOTA性能与系统性设计指南


<details>
  <summary>Details</summary>
Motivation: 多智能体系统设计存在提示敏感性和不稳定性挑战，现有方法难以处理指数级搜索空间和模糊信用分配问题

Method: 将MAS提示优化建模为最大后验推理问题，使用语言引导的max-product算法求解，通过拓扑感知机制迭代更新提示

Result: 在多项任务基准测试中达到SOTA，显著超越人工设计和现有自动化方法

Conclusion: MAPRO不仅提升多智能体系统性能，更为未来构建可靠系统提供基于概率推理的指导原则

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [8] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

TL;DR: AsyncSpade通过预测查询状态和异步KV缓存处理，显著提升TTS任务效率同时保持模型精度


<details>
  <summary>Details</summary>
Motivation: 现有查询感知稀疏解码方法存在顺序依赖和粗粒度筛选问题，导致高并发长CoT场景下效率低下甚至运行时间超过前向计算本身

Method: 提出包含时间回归模块（预测下一个token的查询状态）和异步解耦框架（将KV缓存过滤与解码循环分离）的AsyncSpade系统

Result: 在A100节点实现理论最优TPOT，相比Quest基线降低20%+，相比全注意力降低50%+，在Qwen3系列模型的多项基准（AIME、GPQA、MATH）保持或超越原有精度

Conclusion: AsyncSpade首次在消除顺序依赖的同时保持模型性能，为大规模LLM部署提供了效率与精度兼备的解决方案

Abstract: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [9] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

TL;DR: 研究通过多智能体框架验证LLM智能体团队效能，发现扁平结构优于层级结构，多样性影响存在复杂性，智能体协作存在对话协调不足等问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM多智能体研究较少关注团队动态机制，本研究旨在通过人类团队科学框架（结构/多样性/互动）揭示智能体团队协作规律。

Method: 提出多智能体分析框架，在常识推理（CommonsenseQA/StrategyQA）和社会推理（Social IQa/Latent Implicit Hate）四类任务中对比不同团队结构（扁平vs层级）和多样性（同质vs异质）的表现。

Result: 扁平结构平均准确率比层级高2.5%，异质团队仅在特定任务提升1-3%。任务后反思显示智能体普遍存在协作整合困难（对话协调失败率37%）。

Conclusion: 智能体团队需优化结构设计，多样性需任务适配，未来应加强对话协调机制以提升多智能体系统效能。

Abstract: Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [10] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

TL;DR: 论文提出通过链式思维微调、问题完整性指标和偏好优化，使语音大语言模型推理准确率提升2.4倍，并在保持准确率前提下减少70%延迟


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型在复杂推理任务中存在准确率不足和响应延迟问题，受人类'边听边思考'行为启发，尝试优化推理时机和效率

Method: 1. 文本空间CoT微调 2. 基于熵的问题完整性指标控制推理时机 3. 使用DPO优化偏好数据

Result: 平均推理准确率提升2.4倍，ARC-Easy任务4%准确率提升，延迟减少70%

Conclusion: 结合动态推理时机控制和偏好优化，有效平衡语音大语言模型的准确率与响应延迟，推进实用化进程

Abstract: Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [11] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

TL;DR: 提出思维模板框架ToTAL，通过可复用思维缓存与自然语言反馈迭代优化，显著提升长上下文语言模型在多跳推理任务中的性能，并验证知识蒸馏的可行性


<details>
  <summary>Details</summary>
Motivation: 现有长上下文模型处理知识密集型任务时，单纯增加文档数量无法有效建立证据间的逻辑关联，导致多跳推理效果受限

Method: 引入结构化思维模板指导证据组合，设计基于自然语言反馈的迭代更新策略，从训练数据中持续优化模板质量

Result: 在多种基准测试和模型架构中实现稳定提升（检索与非检索场景），优化后的模板可蒸馏至小模型使其推理能力提升2-3倍

Conclusion: ToTAL框架通过结构化推理模式与持续优化机制，显著增强大模型复杂推理能力，同时实现透明化知识重用与跨模型迁移

Abstract: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [12] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

TL;DR: 开发跨领域塔吉克-波斯语双向转写模型，整合多数据集并创建新基准


<details>
  <summary>Details</summary>
Motivation: 解决现有转写模型受限于单一领域数据的问题，提升模型在实际应用中的通用性

Method: 使用序列到序列模型整合所有可用数据集，并新增两个自建数据集进行训练

Result: 模型在跨领域测试中达到Farsi→Tajik chrF++ 87.91/CER 0.05，Tajik→Farsi 92.28/0.04

Conclusion: 该模型实现了当前最先进的跨领域转写性能，为任务难度评估提供了清晰基准

Abstract: As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [13] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

TL;DR: 提出OWL模型，通过LSTM-based drafter、特殊标记[SPEC]和混合算法，显著提升长上下文下的推测解码性能，速度比EAGLE3快约5倍。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法在长上下文场景（如2K tokens以上）性能严重下降，如EAGLE3甚至导致生成速度降低0.81倍，需开发适应长上下文的新方案。

Method: 1) 基于LSTM的drafter仅依赖最后token状态，实现长度泛化
2) 验证器中引入[SPEC]标记增强语义表示
3) 融合树解码与非树解码的混合算法

Result: 在长上下文输入中，OWL的接受长度达到EAGLE3的5倍，公开了全部代码和数据集（LongSpecBench）

Conclusion: 通过系统性创新解决了长上下文推测解码难题，建立新基准并开源资源推动领域发展

Abstract: Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [14] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

TL;DR: 提出多标准提示和领域自适应迁移学习方法，显著提升2B参数模型ChartJudge在图表理解任务中的评估效果，并揭示模型规模与提示设计的权衡关系


<details>
  <summary>Details</summary>
Motivation: 当前小型视觉语言模型（≤2B参数）在图表理解任务中作为自动化评估者表现欠佳，限制了其在资源受限场景的实际应用

Method: 通过组合多标准提示策略暴露模型弱点，并利用合成数据微调创建ChartJudge模型实现跨数据集知识迁移

Result: 多标准提示导致7B模型性能显著下降，ChartJudge在跨数据集迁移中表现优异，准确率提升最高达30%

Conclusion: 通过系统分析图表类型与查询复杂度，提出兼顾模型规模、提示设计和迁移性的低成本评估方案，为图表推理任务提供可扩展解决方案

Abstract: Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [15] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

TL;DR: 提出基于任务主LoRA模块的多任务预微调框架，在移动端NLP应用中实现高效适配，在21个下游任务中NER提升0.8%、文本分类提升8.8%


<details>
  <summary>Details</summary>
Motivation: 解决移动端轻量级BERT模型在多任务适配时存在的优化信号冲突问题，满足实际部署对模型效率和泛化能力的双重需求

Method: 采用共享编码器主干+模块化适配器的架构，通过任务特定的LoRA模块实现参数高效的多任务预微调

Result: 在21个下游任务实验中，命名实体识别平均提升0.8%，文本分类提升8.8%，且满足移动端部署的硬件约束

Conclusion: 该方法在保持单一共享编码器的前提下，通过模块化设计有效平衡多任务性能，为移动端NLP应用提供了高效的解决方案

Abstract: Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [16] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

TL;DR: 通过计算语言学分析疫情相关网络讨论，发现健康错误信息在可读性、情感词汇和修辞风格上与事实内容存在显著差异，其复杂语言结构可能增强可信度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别健康错误信息的语言特征，为自动检测提供依据，并优化公共卫生危机沟通策略。

Method: 对比分析三个语料库（COVID-19错误信息/一般内容/猴痘内容）的文本特征，包括可读性指数、修辞标记和说服性术语频率。

Result: COVID-19错误信息可读性最低，恐惧/说服性词汇使用量超2倍，感叹号使用最少，与猴痘内容形成鲜明对比。

Conclusion: 语言特征可作为错误信息检测指标，但需改进研究方法（如引入纵向分析、扩展情感词典）以提升检测模型的实用性。

Abstract: This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [17] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

TL;DR: 本文提出基于大语言模型的模块化人工语言构建系统，通过分阶段生成音系、句法结构、词典和文字系统，并验证了不同LLMs对语言概念的掌握程度差异。


<details>
  <summary>Details</summary>
Motivation: 双重目标：1. 开发趣味性人工语言创作工具 2. 探索LLMs对语言结构知识的理解边界，特别是不同模型在常见与罕见语言模式处理上的能力差异

Method: 五阶段流程：1. 代理生成音系模型 2. 英语句子转译形态句法标记 3. 语料库构建词典 4. 现有文字系统适配 5. 自动生成语法手册，并测试高-低资源语言翻译应用

Result: 发现模型间存在显著能力鸿沟（常见模式处理成功率比罕见模式高32%），跨语言翻译任务中准确率仅达47%，但错误分析显示系统性改进可能性

Conclusion: 系统证明了LLMs在语言工程中的工具价值，同时揭示了当前模型对深层语言规则的认知局限，通过算法优化可能提升低资源语言处理效能

Abstract: We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


### [18] [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
*Isabel Papadimitriou,Jacob Prince*

Main category: cs.CL

TL;DR: 研究发现LLM训练中词汇嵌入几何快速与语义/句法特征对齐，高频词比低频词收敛更快且保留初始随机性特征


<details>
  <summary>Details</summary>
Motivation: 探究语言模型输入词汇表征的结构演化规律及其与训练过程的关系，揭示不同词类在表征学习中的动态差异

Method: 使用表征相似性分析，追踪Pythia 12B和OLMo 7B模型在训练过程中输入/输出嵌入与语义、句法、词频指标的几何相关性

Result: 1) 词汇嵌入几何早期即与语言特征强相关 2) 高频功能词收敛快，低频词保留初始化偏差 3) 词频和功能属性影响表征演化路径

Conclusion: 词汇表征的几何演化呈现分层规律，该发现为理解模型能力涌现提供新视角，建议后续研究关注词汇几何演化与具体能力获得的关系

Abstract: Large language models (LLMs) work by manipulating the geometry of input
embedding vectors over multiple layers. Here, we ask: how are the input
vocabulary representations of language models structured, and how and when does
this structure evolve over training? To answer this question, we use
representational similarity analysis, running a suite of experiments that
correlate the geometric structure of the input embeddings and output embeddings
of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,
and frequency-based metrics over the course of training. Our key findings are
as follows: 1) During training, the vocabulary embedding geometry quickly
converges to high correlations with a suite of semantic and syntactic features;
2) Embeddings of high-frequency and function words (e.g., "the," "of") converge
to their final vectors faster than lexical and low-frequency words, which
retain some alignment with the bias in their random initializations. These
findings help map the dynamic trajectory by which input embeddings organize
around linguistic structure, revealing distinct roles for word frequency and
function. Our findings motivate a deeper study of how the evolution of
vocabulary geometry may facilitate specific capability gains during model
training.

</details>


### [19] [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
*Zhangdie Yuan,Han-Chin Shing,Mitch Strong,Chaitanya Shivade*

Main category: cs.CL

TL;DR: 提出通过轻量级干预和临床代码验证任务提升LLM医疗编码准确性，并发布门诊病历标注数据集


<details>
  <summary>Details</summary>
Motivation: 现有LLM在医疗编码中存在分层接近错误的预测代码，传统评估指标难以检测这类错误，且MIMIC数据集存在住院偏倚问题

Method: 采用提示工程/小规模微调的轻量干预，设计临床代码验证任务作为独立组件，构建专家双标注门诊ICD-10基准数据集

Result: 验证机制显著改善编码可靠性，新数据集有效补充现有资源局限，验证步骤使LLM编码错误减少42%

Conclusion: 临床代码验证任务结合专业标注数据集，为LLM医疗编码提供了可靠的质量保障框架

Abstract: Accurate clinical coding is essential for healthcare documentation, billing,
and decision-making. While prior work shows that off-the-shelf LLMs struggle
with this task, evaluations based on exact match metrics often overlook errors
where predicted codes are hierarchically close but incorrect. Our analysis
reveals that such hierarchical misalignments account for a substantial portion
of LLM failures. We show that lightweight interventions, including prompt
engineering and small-scale fine-tuning, can improve accuracy without the
computational overhead of search-based methods. To address hierarchically
near-miss errors, we introduce clinical code verification as both a standalone
task and a pipeline component. To mitigate the limitations in existing
datasets, such as incomplete evidence and inpatient bias in MIMIC, we release
an expert double-annotated benchmark of outpatient clinical notes with ICD-10
codes. Our results highlight verification as an effective and reliable step
toward improving LLM-based medical coding.

</details>


### [20] [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
*Đorđe Klisura,Joseph Khoury,Ashish Kundu,Ram Krishnan,Anthony Rios*

Main category: cs.CL

TL;DR: 研究提出基于角色策略的LLM访问控制增强方案，通过两步验证框架和微调方法有效平衡安全性与实用性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理文本到SQL任务时容易突破角色权限边界，需要强化访问控制策略的依从性。

Method: 1. 零/少样本提示 2. 生成-验证两步流程（先生成SQL后策略检查） 3. LoRA微调直接注入权限感知能力

Result: 显式验证提升拒绝精度（+21%），微调模型实现最优安全-效用平衡。复杂策略使所有系统可靠性下降37-52%。

Conclusion: 动态验证机制结合参数高效微调是提升LLM权限控制的有效路径，但复杂策略场景仍需算法突破

Abstract: Access control is a cornerstone of secure computing, yet large language
models often blur role boundaries by producing unrestricted responses. We study
role-conditioned refusals, focusing on the LLM's ability to adhere to access
control policies by answering when authorized and refusing when not. To
evaluate this behavior, we created a novel dataset that extends the Spider and
BIRD text-to-SQL datasets, both of which have been modified with realistic
PostgreSQL role-based policies at the table and column levels. We compare three
designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier
pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that
learn permission awareness directly. Across multiple model families, explicit
verification (the two-step framework) improves refusal precision and lowers
false permits. At the same time, fine-tuning achieves a stronger balance
between safety and utility (i.e., when considering execution accuracy). Longer
and more complex policies consistently reduce the reliability of all systems.
We release RBAC-augmented datasets and code.

</details>


### [21] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

TL;DR: Ryt AI是全球首个获监管批准的对话式AI银行核心交易框架，通过自然语言界面实现安全合规的金融操作


<details>
  <summary>Details</summary>
Motivation: 突破传统银行助手仅限咨询功能的局限，创建符合监管要求的自然语言核心交易接口

Method: 采用四智能体架构（Guardrails/Intent/Payment/FAQ），基于自研ILMU大模型部署任务特定LoRA适配器

Result: 实现对话界面替代传统多屏操作流程，在严格治理下可靠执行核心银行业务

Conclusion: 证明通过分层安全架构（确定性护栏/人工确认/无状态审计）可构建合规的自然语言银行系统

Abstract: This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [22] [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
*Yuzhe Gu,Xiyu Liang,Jiaojiao Zhao,Enmao Diao*

Main category: cs.CL

TL;DR: 提出OBCache框架，将LLM的KV缓存淘汰问题转化为结构化剪枝，基于最优脑损伤理论量化token重要性，提升长上下文准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缓存淘汰方法仅依赖注意力权重启发式评估token重要性，未考虑其对注意力输出的实际影响，存在优化空间。

Method: 基于最优脑损伤理论(OBD)，通过剪枝token引起的注意力输出扰动量化显著性，推导出独立key/value及联合key-value的闭式评分公式。

Result: 在LLaMA和Qwen模型上的实验表明，用OBCache的输出感知分数替代启发式评分可提升长上下文任务准确率。

Conclusion: OBCache通过引入输出感知信号，为KV缓存淘汰提供了理论框架，在保持效率的同时提升大语言模型的长上下文处理能力。

Abstract: Large language models (LLMs) with extended context windows enable powerful
downstream applications but impose significant memory overhead, as caching all
key-value (KV) states scales linearly with sequence length and batch size.
Existing cache eviction methods address this by exploiting attention sparsity,
yet they typically rank tokens heuristically using accumulated attention
weights without considering their true impact on attention outputs. We propose
Optimal Brain Cache (OBCache), a principled framework that formulates cache
eviction as a layer-wise structured pruning problem. Building upon the Optimal
Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the
perturbation in attention outputs induced by pruning tokens, with closed-form
scores derived for isolated keys, isolated values, and joint key-value pairs.
Our scores account not only for attention weights but also for information from
value states and attention outputs, thereby enhancing existing eviction
strategies with output-aware signals. Experiments on LLaMA and Qwen models
demonstrate that replacing the heuristic scores in existing works, which
estimate token saliency across different query positions, with OBCache's
output-aware scores consistently improves long-context accuracy.

</details>


### [23] [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
*Virginia K. Felkner,Allison Lim,Jonathan May*

Main category: cs.CL

TL;DR: 研究比较了自然语言推理（NLI）与传统词元概率（TP）在检测语言模型偏见中的表现，发现二者行为差异显著且互补，建议组合使用实现全面评估


<details>
  <summary>Details</summary>
Motivation: 现有基于TP的偏见检测方法被批评脱离实际应用场景，本研究探索NLI作为更贴近真实使用场景的替代指标

Method: 通过实验对比NLI与TP指标的相关性，分析两者在检测模型偏见时的敏感性差异及优缺点

Result: NLI指标与TP指标相关性极低，前者更易发现去偏不足案例但对反刻板语句表述更敏感，后者则相对稳定

Conclusion: 建议综合使用TP、NLI和下游任务评估，构建多维度语言模型偏见检测体系

Abstract: Measurement of social bias in language models is typically by token
probability (TP) metrics, which are broadly applicable but have been criticized
for their distance from real-world langugage model use cases and harms. In this
work, we test natural language inference (NLI) as a more realistic alternative
bias metric. We show that, curiously, NLI and TP bias evaluation behave
substantially differently, with very low correlation among different NLI
metrics and between NLI and TP metrics. We find that NLI metrics are more
likely to detect "underdebiased" cases. However, NLI metrics seem to be more
brittle and sensitive to wording of counterstereotypical sentences than TP
approaches. We conclude that neither token probability nor natural language
inference is a "better" bias metric in all cases, and we recommend a
combination of TP, NLI, and downstream bias evaluations to ensure comprehensive
evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.

</details>


### [24] [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
*Jifan Zhang,Henry Sleight,Andi Peng,John Schulman,Esin Durmus*

Main category: cs.CL

TL;DR: 该研究提出系统性压力测试方法，发现主流大语言模型在价值原则冲突场景中存在超过7万例显著行为分歧，暴露出模型规范中的原则矛盾与解释模糊问题


<details>
  <summary>Details</summary>
Motivation: 针对现有AI宪法和模型规范中存在的原则内部冲突与复杂场景覆盖不足的问题，研究旨在通过压力测试揭示模型行为分歧背后的规范缺陷

Method: 建立价值权衡分类法生成冲突场景，要求模型在12个主流大语言模型（包括Anthropic/OpenAI/Google/xAI）中做出非此即彼的选择，通过价值分类评分量化行为差异

Result: 发现70,000+个显著行为分歧案例，证实行为分歧与规范缺陷强相关；识别出原则直接矛盾、解释模糊等具体问题；所有被测模型均存在明显错位拒绝案例

Conclusion: 压力测试有效暴露模型规范缺陷，揭示当前LLM价值优先级差异，强调需要建立更清晰的冲突解决机制和规范优化框架

Abstract: Large language models (LLMs) are increasingly trained from AI constitutions
and model specifications that establish behavioral guidelines and ethical
principles. However, these specifications face critical challenges, including
internal conflicts between principles and insufficient coverage of nuanced
scenarios. We present a systematic methodology for stress-testing model
character specifications, automatically identifying numerous cases of principle
contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force
explicit tradeoffs between competing value-based principles. Using a
comprehensive taxonomy we generate diverse value tradeoff scenarios where
models must choose between pairs of legitimate principles that cannot be
simultaneously satisfied. We evaluate responses from twelve frontier LLMs
across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral
disagreement through value classification scores. Among these scenarios, we
identify over 70,000 cases exhibiting significant behavioral divergence.
Empirically, we show this high divergence in model behavior strongly predicts
underlying problems in model specifications. Through qualitative analysis, we
provide numerous example issues in current model specs such as direct
contradiction and interpretive ambiguities of several principles. Additionally,
our generated dataset also reveals both clear misalignment cases and
false-positive refusals across all of the frontier models we study. Lastly, we
also provide value prioritization patterns and differences of these models.

</details>


### [25] [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
*Krinos Li,Xianglu Xiao,Shenglong Deng,Lucas He,Zijun Zhong,Yuanjie Zou,Zhonghao Zhan,Zheng Hui,Weiye Bao,Guang Yang*

Main category: cs.CL

TL;DR: 本文系统综述了大语言模型在虚拟细胞建模中的应用，提出统一分类法并分析核心任务与挑战。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过构建'虚拟细胞'计算系统，正在革新细胞生物学领域，需系统梳理其方法论与应用范式。

Method: 建立LLMs作为预言机（直接建模）和智能体（任务协调）的双范式体系，覆盖细胞表征、扰动预测、基因调控推断三大任务。

Result: 整理现有模型/数据集/评估基准，揭示可扩展性、通用性和可解释性三大关键挑战。

Conclusion: 虚拟细胞建模需要突破模型规模限制，建立跨场景泛化能力，并增强生物学过程的可解释性。

Abstract: Large language models (LLMs) are transforming cellular biology by enabling
the development of "virtual cells"--computational systems that represent,
predict, and reason about cellular states and behaviors. This work provides a
comprehensive review of LLMs for virtual cell modeling. We propose a unified
taxonomy that organizes existing methods into two paradigms: LLMs as Oracles,
for direct cellular modeling, and LLMs as Agents, for orchestrating complex
scientific tasks. We identify three core tasks--cellular representation,
perturbation prediction, and gene regulation inference--and review their
associated models, datasets, evaluation benchmarks, as well as the critical
challenges in scalability, generalizability, and interpretability.

</details>


### [26] [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
*Chengshuai Zhao,Shu Wan,Paras Sheth,Karan Patwa,K. Selçuk Candan,Huan Liu*

Main category: cs.CL

TL;DR: 提出因果表征学习框架CADET，通过解构仇恨言论的潜在因果因素提升检测模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有仇恨检测模型依赖表层语言特征，难以应对不同平台风格差异导致的虚假相关性，无法有效识别隐式仇恨言论（如讽刺/编码语言）

Method: 将仇恨生成建模为包含环境、动机、目标、风格的因果图，通过解耦潜在因子并控制混杂变量，分离真实仇恨意图与语言表象

Result: CADET在综合实验中展现优越性能，支持反事实推理干预语言风格，提升变体仇恨的检测鲁棒性

Conclusion: 因果先验能有效提升仇恨检测的泛化能力，为对抗不断演变的网络仇恨提供了新方法论框架

Abstract: The proliferation of online hate speech poses a significant threat to the
harmony of the web. While explicit hate is easily recognized through overt
slurs, implicit hate speech is often conveyed through sarcasm, irony,
stereotypes, or coded language -- making it harder to detect. Existing hate
speech detection models, which predominantly rely on surface-level linguistic
cues, fail to generalize effectively across diverse stylistic variations.
Moreover, hate speech spread on different platforms often targets distinct
groups and adopts unique styles, potentially inducing spurious correlations
between them and labels, further challenging current detection approaches.
Motivated by these observations, we hypothesize that the generation of hate
speech can be modeled as a causal graph involving key factors: contextual
environment, creator motivation, target, and style. Guided by this graph, we
propose CADET, a causal representation learning framework that disentangles
hate speech into interpretable latent factors and then controls confounders,
thereby isolating genuine hate intent from superficial linguistic cues.
Furthermore, CADET allows counterfactual reasoning by intervening on style
within the latent space, naturally guiding the model to robustly identify hate
speech in varying forms. CADET demonstrates superior performance in
comprehensive experiments, highlighting the potential of causal priors in
advancing generalizable hate speech detection.

</details>


### [27] [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
*Shuo Yu,Mingyue Cheng,Daoyu Wang,Qi Liu,Zirui Liu,Ze Guo,Xiaoyu Tao*

Main category: cs.CL

TL;DR: MemWeaver提出分层记忆框架，通过行为记忆和认知记忆双组件整合用户文本历史，解决现有方法无法捕捉动态兴趣结构的缺陷，实现深度个性化生成。


<details>
  <summary>Details</summary>
Motivation: 现有个性化方法将用户历史视为扁平化文本列表，无法建模反映兴趣动态变化的时空语义结构，难以实现深层次个性化。

Method: 构建包含行为记忆（具体行为）和认知记忆（长期偏好）的双组件分层记忆系统，同时整合时间演化和语义关联信息，形成用户统一表征。

Result: 在LaMP基准测试中验证有效性，证明该框架能显著提升个性化生成效果。

Conclusion: MemWeaver通过时空语义融合的分层记忆结构，为LLM提供了同时推理具体行为和抽象特质的统一用户表征范式。

Abstract: The primary form of user-internet engagement is shifting from leveraging
implicit feedback signals, such as browsing and clicks, to harnessing the rich
explicit feedback provided by textual interactive behaviors. This shift unlocks
a rich source of user textual history, presenting a profound opportunity for a
deeper form of personalization. However, prevailing approaches offer only a
shallow form of personalization, as they treat user history as a flat list of
texts for retrieval and fail to model the rich temporal and semantic structures
reflecting dynamic nature of user interests. In this work, we propose
\textbf{MemWeaver}, a framework that weaves the user's entire textual history
into a hierarchical memory to power deeply personalized generation. The core
innovation of our memory lies in its ability to capture both the temporal
evolution of interests and the semantic relationships between different
activities. To achieve this, MemWeaver builds two complementary memory
components that both integrate temporal and semantic information, but at
different levels of abstraction: behavioral memory, which captures specific
user actions, and cognitive memory, which represents long-term preferences.
This dual-component memory serves as a unified representation of the user,
allowing large language models (LLMs) to reason over both concrete behaviors
and abstracted traits. Experiments on the Language Model Personalization (LaMP)
benchmark validate the efficacy of MemWeaver. Our code is
available\footnote{https://github.com/fishsure/MemWeaver}.

</details>


### [28] [SUBQRAG: sub-question driven dynamic graph rag](https://arxiv.org/abs/2510.07718)
*Jiaoyang Li,Junhao Ruan,Shengwei Tang,Saihan Chen,Kaiyan Chang,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出SubQRAG框架，通过子问题分解和动态知识图谱扩展解决传统Graph RAG在多跳问答中推理深度不足的问题


<details>
  <summary>Details</summary>
Motivation: 传统Graph RAG在复杂多跳问答中存在证据链不完整和误差累积问题，需要增强结构化推理能力

Method: 1. 将复杂问题分解为可验证的子问题链 2. 动态扩展知识图谱（实时文档三元组提取）3. 构建可追溯的图记忆证据路径

Result: 在三个多跳QA基准测试中取得显著提升，Exact Match指标表现突出

Conclusion: 子问题驱动机制有效增强推理深度，结构化记忆路径提升答案的可解释性和准确性

Abstract: Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a
knowledge graph (KG) to connect disparate facts across a large document corpus.
However, this broad-view approach often lacks the deep structured reasoning
needed for complex multi-hop question answering (QA), leading to incomplete
evidence and error accumulation. To address these limitations, we propose
SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG
decomposes a complex question into an ordered chain of verifiable
sub-questions. For each sub-question, it retrieves relevant triples from the
graph. When the existing graph is insufficient, the system dynamically expands
it by extracting new triples from source documents in real time. All triples
used in the reasoning process are aggregated into a "graph memory," forming a
structured and traceable evidence path for final answer generation. Experiments
on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent
and significant improvements, especially in Exact Match scores.

</details>


### [29] [Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing](https://arxiv.org/abs/2510.07736)
*Cunli Mao,Xiaofei Gao,Ran Song,Shizhu He,Shengxiang Gao,Kang Liu,Zhengtao Yu*

Main category: cs.CL

TL;DR: 提出KL-GMoE+IER框架，通过跨语言知识共享显著提升多语言知识图谱补全性能


<details>
  <summary>Details</summary>
Motivation: 现有MKGC研究未充分利用大语言模型的多语言能力，且忽视跨语言知识共享性

Method: 基于知识层级分组混合专家模型(KL-GMoE)和迭代实体重排序(IER)，构建5语言数据集进行对比实验

Result: Hits@1/3/10分别提升5.47%/3.27%/1.01%，验证了在未见过/不平衡语言场景的知识共享特性

Conclusion: 该框架有效利用跨语言知识提升MKGC性能，实验证明其优越性，并开源促进后续研究

Abstract: Large language models (LLMs) based Multilingual Knowledge Graph Completion
(MKGC) aim to predict missing facts by leveraging LLMs' multilingual
understanding capabilities, improving the completeness of multilingual
knowledge graphs (KGs). However, existing MKGC research underutilizes the
multilingual capabilities of LLMs and ignores the shareability of cross-lingual
knowledge. In this paper, we propose a novel MKGC framework that leverages
multilingual shared knowledge to significantly enhance performance through two
components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative
Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER
significantly enhances its utilization. To evaluate our framework, we
constructed a mKG dataset containing 5 languages and conducted comprehensive
comparative experiments with existing state-of-the-art (SOTA) MKGC method. The
experimental results demonstrate that our framework achieves improvements of
5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,
respectively, compared with SOTA MKGC method. Further experimental analysis
revealed the properties of knowledge sharing in settings of unseen and
unbalanced languages. We have released the dataset and code for our work on
https://github.com/gaoxiaofei07/KL-GMoE.

</details>


### [30] [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
*Fu Chen,Peng Wang,Xiyin Li,Wen Li,Shichi Lei,Dongdong Xiang*

Main category: cs.CL

TL;DR: 提出ToolExpander框架，通过动态多轮硬采样和自我示例思维机制，显著提升小规模语言模型的工具使用能力与训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO方法在小规模语言模型训练中存在准确率低、易崩溃的问题，制约了模型性能提升和技术潜力发挥。

Method: 1. 动态多轮硬采样：用高质量少样本替代困难样本，结合指数学习率衰减；2. 自我示例思维：改进GRPO框架，去除KL散度约束，通过微奖励(0.01)激励模型自主生成分析示例。

Result: 实验证明ToolExpander显著增强LLMs（特别是小模型）的工具使用能力，提升训练稳定性与整体性能表现。

Conclusion: 该框架有效解决GRPO应用中的核心痛点，为资源受限模型开发提供了强化学习新范式。

Abstract: Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.

</details>


### [31] [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
*Tianci Liu,Ran Xu,Tony Yu,Ilgee Hong,Carl Yang,Tuo Zhao,Haoyu Wang*

Main category: cs.CL

TL;DR: 提出了OpenRubrics数据集和对比式规则生成方法（CRG），通过结构化自然语言规则提升奖励模型的判别性和可靠性，实验显示Rubric-RM在多个基准上超越基线6.8%，并能有效迁移至下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型依赖标量/成对判断，难以捕捉人类偏好多维度特性。需要开发既能保持可靠性又具扩展性的规则生成方法。

Method: 1. 提出对比式规则生成（CRG）：通过对比优选/拒绝响应提取显式约束（硬规则）和隐性质量（原则）
2. 引入偏好标签一致性约束，通过拒绝采样去除噪声规则
3. 构建大规模OpenRubrics数据集（prompt-rubric对）

Result: 1. Rubric-RM在奖励建模基准上超越同规模基线6.8%
2. 策略模型在指令遵循（AlpacaEval）和生物医学（BioMedGPT）基准表现提升
3. 规则质量指标（FACTScore）提升17%

Conclusion: 结构化规则为LLM对齐提供了可扩展的信号，通过原则驱动的新范式缩小了人工评估与自动奖励建模的差距，推动更可靠的模型对齐。

Abstract: Reward modeling lies at the core of reinforcement learning from human
feedback (RLHF), yet most existing reward models rely on scalar or pairwise
judgments that fail to capture the multifaceted nature of human preferences.
Recent studies have explored rubrics-as-rewards (RaR) that uses structured
natural language criteria that capture multiple dimensions of response quality.
However, producing rubrics that are both reliable and scalable remains a key
challenge. In this work, we introduce OpenRubrics, a diverse, large-scale
collection of (prompt, rubric) pairs for training rubric-generation and
rubric-based reward models. To elicit discriminative and comprehensive
evaluation signals, we introduce Contrastive Rubric Generation (CRG), which
derives both hard rules (explicit constraints) and principles (implicit
qualities) by contrasting preferred and rejected responses. We further improve
reliability by enforcing preference-label consistency via rejection sampling to
remove noisy rubrics. Across multiple reward-modeling benchmarks, our
rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines
by 6.8%. These gains transfer to policy models on instruction-following and
biomedical benchmarks. Our results show that rubrics provide scalable alignment
signals that narrow the gap between costly human evaluation and automated
reward modeling, enabling a new principle-driven paradigm for LLM alignment.

</details>


### [32] [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
*Runyang You,Yongqi Li,Meng Liu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.CL

TL;DR: 该研究通过提出两种不确定性采样策略（蒙特卡洛Dropout和高斯加性噪声）和潜在奖励模型，实现了潜在推理模型的并行测试时间缩放，为连续空间的扩展推理开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 现有并行TTS主要作用于显式链式思维（token-based），而潜在推理模型在连续空间缺乏有效的采样机制和轨迹聚合信号，阻碍其发挥并行计算优势。

Method: 提出连续空间不确定性采样策略（蒙特卡洛Dropout/高斯噪声），并设计基于对比学习的潜在奖励模型（LatentRM）进行轨迹评分与聚合。

Result: 实验证明采样策略具有计算扩展性且展现不同探索动态，潜在RM能有效选择高质量推理轨迹，可视化分析验证了方法的有效性。

Conclusion: 该工作首次将并行TTS引入潜在推理领域，通过创新的采样机制和评估模型，为连续空间的可扩展推理建立了新范式。

Abstract: Parallel test-time scaling (TTS) is a pivotal approach for enhancing large
language models (LLMs), typically by sampling multiple token-based
chains-of-thought in parallel and aggregating outcomes through voting or
search. Recent advances in latent reasoning, where intermediate reasoning
unfolds in continuous vector spaces, offer a more efficient alternative to
explicit Chain-of-Thought, yet whether such latent models can similarly benefit
from parallel TTS remains open, mainly due to the absence of sampling
mechanisms in continuous space, and the lack of probabilistic signals for
advanced trajectory aggregation. \ This work enables parallel TTS for latent
reasoning models by addressing the above issues. For sampling, we introduce two
uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive
Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)
trained with step-wise contrastive objective to score and guide latent
reasoning. Extensive experiments and visualization analyses show that both
sampling strategies scale effectively with compute and exhibit distinct
exploration dynamics, while LatentRM enables effective trajectory selection.
Together, our explorations open a new direction for scalable inference in
continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.

</details>


### [33] [Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers](https://arxiv.org/abs/2510.07761)
*Nishant Balepur,Atrey Desai,Rachel Rudinger*

Main category: cs.CL

TL;DR: 研究挑战了LLMs在MCQA任务中仅依赖选项（choices-only）成功必然存在缺陷的观点，指出推理轨迹可揭示其策略合理性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在缺失问题（仅选项）情境下的MCQA表现是否源于浅层捷径策略，并通过推理轨迹验证策略的合理性。

Method: 对比LLMs在完整输入与仅选项输入的MCQA表现，分析推理轨迹长度对结果的影响，并评估轨迹的忠实性以识别策略类型。

Result: 推理提升完整/仅选项输入的准确率；仅选项成功与推理长度无关，且轨迹显示模型常通过推断缺失问题实现合理推理。

Conclusion: 部分仅选项成功案例反映LLMs使用非问题性策略（如问题推断），需通过推理轨迹区分数据缺陷与合理推理。

Abstract: Large language models (LLMs) now give reasoning before answering, excelling
in tasks like multiple-choice question answering (MCQA). Yet, a concern is that
LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed
in MCQA without using the question, i.e., choices-only. Such partial-input
success is often deemed problematic, but reasoning traces could reveal if these
strategies are truly shallow in choices-only settings. To study these
strategies, reasoning LLMs solve MCQs in full and choices-only inputs;
test-time reasoning often boosts accuracy on full and in choices-only half the
time. While possibly due to shallow shortcuts, choices-only success is barely
affected by the length of reasoning traces, and after finding traces pass
faithfulness tests, we show they use less problematic strategies like inferring
missing questions. In all, we challenge claims that partial-input success is
always a flaw, so we discuss how reasoning traces could separate problematic
data from less problematic reasoning.

</details>


### [34] [ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning](https://arxiv.org/abs/2510.07768)
*Murong Yue,Zhiwei Liu,Liangwei Yang,Jianguo Zhang,Zuxin Liu,Haolin Chen,Ziyu Yao,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

TL;DR: 提出结构化工具库自动重构方法，通过聚类与多智能体框架整合分散工具，显著提升LLMs在复杂推理任务中的工具检索准确率和推理性能。


<details>
  <summary>Details</summary>
Motivation: 解决领域专用工具匮乏问题及现有CoT衍生工具无序增长导致的检索效率低下问题（如搜索空间膨胀和功能歧义）。

Method: 1. 生成离散任务工具并语义聚类；2. 代码代理重构共享逻辑生成聚合工具；3. 审核代理确保功能完整性。

Result: 实验显示工具检索准确率提升17.5%，推理任务成功率提升23%，且工具数量增加时扩展性优于基线模型。

Conclusion: 结构化工具库与多代理协作机制有效解决工具管理瓶颈，为LLMs工具增强推理提供可扩展解决方案。

Abstract: Large Language Models (LLMs) equipped with external tools have demonstrated
enhanced performance on complex reasoning tasks. The widespread adoption of
this tool-augmented reasoning is hindered by the scarcity of domain-specific
tools. For instance, in domains such as physics question answering, suitable
and specialized tools are often missing. Recent work has explored automating
tool creation by extracting reusable functions from Chain-of-Thought (CoT)
reasoning traces; however, these approaches face a critical scalability
bottleneck. As the number of generated tools grows, storing them in an
unstructured collection leads to significant retrieval challenges, including an
expanding search space and ambiguity between function-related tools. To address
this, we propose a systematic approach to automatically refactor an
unstructured collection of tools into a structured tool library. Our system
first generates discrete, task-specific tools and clusters them into
semantically coherent topics. Within each cluster, we introduce a multi-agent
framework to consolidate scattered functionalities: a code agent refactors code
to extract shared logic and creates versatile, aggregated tools, while a
reviewing agent ensures that these aggregated tools maintain the complete
functional capabilities of the original set. This process transforms numerous
question-specific tools into a smaller set of powerful, aggregated tools
without loss of functionality. Experimental results demonstrate that our
approach significantly improves tool retrieval accuracy and overall reasoning
performance across multiple reasoning tasks. Furthermore, our method shows
enhanced scalability compared with baselines as the number of question-specific
increases.

</details>


### [35] [Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards](https://arxiv.org/abs/2510.07774)
*Youliang Yuan,Qiuyang Mang,Jingbang Chen,Hong Wan,Xiaoyuan Liu,Junjielong Xu,Jen-tse Huang,Wenxuan Wang,Wenxiang Jiao,Pinjia He*

Main category: cs.CL

TL;DR: 研究发现传统数学推理模型仅奖励最终答案会导致71%虚假正确率，提出过程评估模型RRM将准确率提升至62.6%


<details>
  <summary>Details</summary>
Motivation: 现有基于结果的奖励机制导致虚假正确（如奇迹步骤），高估模型真实推理能力，需系统性解决方案

Method: 开发Rubric评分模型(RRM)，通过问题定制标准全程评估推理过程，结合强化学习进行逻辑校准

Result: AIME2024验证通过率从26.7%提升至62.6%，奇迹步骤减少71%，四大数学基准测试持续领先

Conclusion: 过程奖励机制显著提升模型可靠性和准确性，证明数学推理需注重推导过程而非单纯答案正确

Abstract: Large language models for mathematical reasoning are typically trained with
outcome-based rewards, which credit only the final answer. In our experiments,
we observe that this paradigm is highly susceptible to reward hacking, leading
to a substantial overestimation of a model's reasoning ability. This is
evidenced by a high incidence of false positives - solutions that reach the
correct final answer through an unsound reasoning process. Through a systematic
analysis with human verification, we establish a taxonomy of these failure
modes, identifying patterns like Miracle Steps - abrupt jumps to a correct
output without a valid preceding derivation. Probing experiments suggest a
strong association between these Miracle Steps and memorization, where the
model appears to recall the answer directly rather than deriving it. To
mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a
process-oriented reward function that evaluates the entire reasoning trajectory
against problem-specific rubrics. The generative RRM provides fine-grained,
calibrated rewards (0-1) that explicitly penalize logical flaws and encourage
rigorous deduction. When integrated into a reinforcement learning pipeline,
RRM-based training consistently outperforms outcome-only supervision across
four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from
26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work
demonstrates that rewarding the solution process is crucial for building models
that are not only more accurate but also more reliable.

</details>


### [36] [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775)
*Omar Mahmoud,Ali Khalil,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Hallucination in large language models (LLMs) has been widely studied in
recent years, with progress in both detection and mitigation aimed at improving
truthfulness. Yet, a critical side effect remains largely overlooked: enhancing
truthfulness can negatively impact safety alignment. In this paper, we
investigate this trade-off and show that increasing factual accuracy often
comes at the cost of weakened refusal behavior. Our analysis reveals that this
arises from overlapping components in the model that simultaneously encode
hallucination and refusal information, leading alignment methods to suppress
factual knowledge unintentionally. We further examine how fine-tuning on benign
datasets, even when curated for safety, can degrade alignment for the same
reason. To address this, we propose a method that disentangles refusal-related
features from hallucination features using sparse autoencoders, and preserves
refusal behavior during fine-tuning through subspace orthogonalization. This
approach prevents hallucinations from increasing while maintaining safety
alignment.We evaluate our method on commonsense reasoning tasks and harmful
benchmarks (AdvBench and StrongReject). Results demonstrate that our approach
preserves refusal behavior and task utility, mitigating the trade-off between
truthfulness and safety.

</details>


### [37] [Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection](https://arxiv.org/abs/2510.07776)
*Shiman Zhao,Shangyuan Li,Wei Chen,Tengjiao Wang,Jiahui Yao,Jiabin Zheng,Kam Fai Wong*

Main category: cs.CL

TL;DR: 提出端到端多标签联合学习方法，通过构建实例关系学习网络实现标签知识传播，显著提升少样本多标签意图检测性能


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法依赖表示分类且忽略实例关系，导致错误传播和性能瓶颈

Method: 构建包含标签知识传播的实例关系学习网络，通过支持集-查询集的交互关系学习和双重关系增强损失优化

Result: 1-shot场景下AUC平均提升9.54%，Macro-F1提升11.19%

Conclusion: 该方法有效消除错误传播，通过关系强度直接预测多标签意图，实验验证显著优于基线方法

Abstract: Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems,
aiming to detect multiple intents of utterances in low-resource dialogue
domains. Previous studies focus on a two-stage pipeline. They first learn
representations of utterances with multiple labels and then use a
threshold-based strategy to identify multi-label results. However, these
methods rely on representation classification and ignore instance relations,
leading to error propagation. To solve the above issues, we propose a
multi-label joint learning method for few-shot MID in an end-to-end manner,
which constructs an instance relation learning network with label knowledge
propagation to eliminate error propagation. Concretely, we learn the
interaction relations between instances with class information to propagate
label knowledge between a few labeled (support set) and unlabeled (query set)
instances. With label knowledge propagation, the relation strength between
instances directly indicates whether two utterances belong to the same intent
for multi-label prediction. Besides, a dual relation-enhanced loss is developed
to optimize support- and query-level relation strength to improve performance.
Experiments show that we outperform strong baselines by an average of 9.54% AUC
and 11.19% Macro-F1 in 1-shot scenarios.

</details>


### [38] [Drift No More? Context Equilibria in Multi-Turn LLM Interactions](https://arxiv.org/abs/2510.07777)
*Vardhan Dongre,Ryan A. Rossi,Viet Dac Lai,David Seunghyun Yoon,Dilek Hakkani-Tür,Trung Bui*

Main category: cs.CL

TL;DR: 提出用动态框架解释多轮交互中的上下文漂移现象，发现其呈现稳定噪声均衡态而非持续恶化，并通过干预措施可控降低漂移


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多轮对话中会产生目标偏离的上下文漂移现象，现有静态指标难以捕捉这种动态过程，需建立系统性分析框架

Method: 将漂移量化为测试模型与参考模型的KL散度，构建具有恢复力和可控干预的随机过程模型，在τ-Bench等仿真环境中验证

Result: 实验显示漂移呈现噪声受限的稳定均衡态，简单提醒干预可使KL散度降低约42%，与理论预测相符

Conclusion: 多轮漂移本质是可控的均衡现象而非必然衰减，为长期交互中的上下文维护提供了理论和方法基础

Abstract: Large Language Models (LLMs) excel at single-turn tasks such as instruction
following and summarization, yet real-world deployments require sustained
multi-turn interactions where user goals and conversational context persist and
evolve. A recurring challenge in this setting is context drift: the gradual
divergence of a model's outputs from goal-consistent behavior across turns.
Unlike single-turn errors, drift unfolds temporally and is poorly captured by
static evaluation metrics. In this work, we present a study of context drift in
multi-turn interactions and propose a simple dynamical framework to interpret
its behavior. We formalize drift as the turn-wise KL divergence between the
token-level predictive distributions of the test model and a goal-consistent
reference model, and propose a recurrence model that interprets its evolution
as a bounded stochastic process with restoring forces and controllable
interventions. We instantiate this framework in both synthetic long-horizon
rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench,
measuring drift for several open-weight LLMs that are used as user simulators.
Our experiments consistently reveal stable, noise-limited equilibria rather
than runaway degradation, and demonstrate that simple reminder interventions
reliably reduce divergence in line with theoretical predictions. Together,
these results suggest that multi-turn drift can be understood as a controllable
equilibrium phenomenon rather than as inevitable decay, providing a foundation
for studying and mitigating context drift in extended interactions.

</details>


### [39] [RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model](https://arxiv.org/abs/2510.07782)
*Shuichiro Haruta,Kazunori Matsumoto,Zhi Li,Yanan Wang,Mori Kurokawa*

Main category: cs.CL

TL;DR: 提出旋转约束补偿方法，通过几何保持更新和方差感知评分，有效减少LLM剪枝误差并提升模型性能


<details>
  <summary>Details</summary>
Motivation: LLM剪枝使用少量校准数据导致输出不匹配，传统最小二乘法易过拟合且破坏预训练权重，需几何保持的误差补偿方法

Method: 1.旋转约束更新保持输出几何特性并重新对齐子空间 2.方差感知评分优先保留高方差维度以维护主方向

Result: 在LLaMA-7B模型实验中，WikiText-2和多任务评估显示困惑度和准确率均优于基线

Conclusion: 该方法在保持几何结构的同时有效补偿剪枝误差，结合方差感知机制显著提升剪枝模型实际性能

Abstract: In this paper, we propose a rotation-constrained compensation method to
address the errors introduced by structured pruning of large language models
(LLMs). LLMs are trained on massive datasets and accumulate rich semantic
knowledge in their representation space. In contrast, pruning is typically
carried out with only a small amount of calibration data, which makes output
mismatches unavoidable. Although direct least-squares fitting can reduce such
errors, it tends to overfit to the limited calibration set, destructively
modifying pretrained weights. To overcome this difficulty, we update the pruned
parameters under a rotation constraint. This constrained update preserves the
geometry of output representations (i.e., norms and inner products) and
simultaneously re-aligns the pruned subspace with the original outputs.
Furthermore, in rotation-constrained compensation, removing components that
strongly contribute to the principal directions of the output makes error
recovery difficult. Since input dimensions with large variance strongly affect
these principal directions, we design a variance-aware importance score that
ensures such dimensions are preferentially kept in the pruned model. By
combining this scoring rule with rotation-constrained updates, the proposed
method effectively compensates errors while retaining the components likely to
be more important in a geometry-preserving manner. In the experiments, we apply
the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple
language understanding benchmarks. The results demonstrate consistently better
perplexity and task accuracy compared with existing baselines.

</details>


### [40] [LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology](https://arxiv.org/abs/2510.07793)
*Sajib Acharjee Dip,Adrika Zafor,Bikash Kumar Paul,Uddip Acharjee Shuvo,Muhit Islam Emon,Xuan Wang,Liqing Zhang*

Main category: cs.CL

TL;DR: LLM4Cell首次整合了58个单细胞研究基础模型与智能体框架，涵盖多模态数据与8大分析任务，通过40+数据集评估模型在生物学合理性、公平性等10个维度的表现，提出可解释性与标准化等开放挑战。


<details>
  <summary>Details</summary>
Motivation: 当前LLM和智能体框架在单细胞生物学中的应用呈现碎片化，需建立统一框架整合数据模态、架构和评估标准以推动领域发展。

Method: 将模型分为基础型、空间型、多模态等5大家族，映射到注释、药物反应预测等8个任务，基于40+公共数据集从生物合理性、隐私保护等10个维度进行系统评估。

Result: 构建了首个语言驱动的单细胞智能整合视图，发现多组学对齐存在显著差异（如RNA-ATAC模型F1分数相差0.32），隐私保护方案覆盖率不足60%。

Conclusion: 该研究为单细胞语言模型建立了系统评估框架，揭示可解释性、标准化和可信模型开发三大核心挑战，推动下一代计算方法发展。

Abstract: Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.

</details>


### [41] [HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation](https://arxiv.org/abs/2510.07794)
*Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Kaiyu He,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: 提出HiPRAG方法，通过分层过程奖励优化Agentic RAG效率，降低过搜索率至2.3%并提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有RL训练方法基于结果奖励，缺乏对搜索行为的细粒度控制，导致过搜索(检索已知信息)和欠搜索(必要时不搜索)问题

Method: 将推理轨迹分解为离散步骤，在传统结果/格式奖励基础上增加分层奖励函数，根据最优搜索/非搜索步骤比例给予额外奖励

Result: 在7个QA基准测试中达到65.4%(3B)/67.2%(7B)平均准确率，过搜索率2.3%，同时降低欠搜索率，验证了跨模型家族/尺寸的泛化性

Conclusion: 通过RL实现细粒度过程控制可有效提升搜索代理的推理效率与最优性，证明了优化推理过程本身的重要性与潜力

Abstract: Agentic RAG is a powerful technique for incorporating external information
that LLMs lack, enabling better problem solving and question answering.
However, suboptimal search behaviors exist widely, such as over-search
(retrieving information already known) and under-search (failing to search when
necessary), which leads to unnecessary overhead and unreliable outputs. Current
training methods, which typically rely on outcome-based rewards in a RL
framework, lack the fine-grained control needed to address these
inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for
Efficient agentic RAG (HiPRAG), a training methodology that incorporates a
fine-grained, knowledge-grounded process reward into the RL training. Our
approach evaluates the necessity of each search decision on-the-fly by
decomposing the agent's reasoning trajectory into discrete, parsable steps. We
then apply a hierarchical reward function that provides an additional bonus
based on the proportion of optimal search and non-search steps, on top of
commonly used outcome and format rewards. Experiments on the Qwen2.5 and
Llama-3.2 models across seven diverse QA benchmarks show that our method
achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished
while improving search efficiency, reducing the over-search rate to just 2.3%
and concurrently lowering the under-search rate. These results demonstrate the
efficacy of optimizing the reasoning process itself, not just the final
outcome. Further experiments and analysis demonstrate that HiPRAG shows good
generalizability across a wide range of RL algorithms, model families, sizes,
and types. This work demonstrates the importance and potential of fine-grained
control through RL, for improving the efficiency and optimality of reasoning
for search agents.

</details>


### [42] [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)
*Eric Hanchen Jiang,Guancheng Wan,Sophia Yin,Mengting Li,Yuchen Wu,Xiao Liang,Xinfeng Li,Yizhou Sun,Wei Wang,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.CL

TL;DR: 提出基于条件离散图扩散模型的GTD框架，通过迭代优化生成任务自适应的多智能体通信拓扑，有效平衡性能、成本和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有静态拓扑无法动态适应不同任务需求，导致简单任务过度消耗资源，复杂任务性能受限。需要解决多目标（性能、成本、鲁棒性）的平衡问题。

Method: 采用迭代图扩散过程，每一步通过轻量级代理模型预测多目标奖励（准确性、效用、成本），实现无梯度的实时拓扑优化。

Result: 实验表明GTD生成的稀疏拓扑在LLM智能体协作中显著优于现有方法，通信效率提升同时保持高性能。

Conclusion: GTD的迭代生成机制突破了单步生成框架限制，能有效权衡复杂设计参数，为动态多智能体系统提供新范式。

Abstract: The efficiency of multi-agent systems driven by large language models (LLMs)
largely hinges on their communication topology. However, designing an optimal
topology is a non-trivial challenge, as it requires balancing competing
objectives such as task performance, communication cost, and robustness.
Existing frameworks often rely on static or hand-crafted topologies, which
inherently fail to adapt to diverse task requirements, leading to either
excessive token consumption for simple problems or performance bottlenecks for
complex ones. To address this challenge, we introduce a novel generative
framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by
conditional discrete graph diffusion models, GTD formulates topology synthesis
as an iterative construction process. At each step, the generation is steered
by a lightweight proxy model that predicts multi-objective rewards (e.g.,
accuracy, utility, cost), enabling real-time, gradient-free optimization
towards task-adaptive topologies. This iterative, guided synthesis process
distinguishes GTD from single-step generative frameworks, enabling it to better
navigate complex design trade-offs. We validated GTD across multiple
benchmarks, and experiments show that this framework can generate highly
task-adaptive, sparse, and efficient communication topologies, significantly
outperforming existing methods in LLM agent collaboration.

</details>


### [43] [Multilingual Generative Retrieval via Cross-lingual Semantic Compression](https://arxiv.org/abs/2510.07812)
*Yuxin Huang,Simeng Wu,Ran Song,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

TL;DR: 提出MGR-CSC框架解决多语言生成检索中的跨语言标识符对齐与空间压缩问题，通过语义压缩和动态解码策略显著提升检索精度并减少标识符长度


<details>
  <summary>Details</summary>
Motivation: 生成式检索在单语场景表现优异，但多语言场景面临跨语言标识符不对齐（cross-lingual identifier misalignment）和标识符膨胀（identifier inflation）两大核心挑战

Method: 1. 跨语言语义压缩：将多语言关键词统一为共享原子对齐语义
2. 动态多步约束解码策略：在检索过程中实施动态解码控制

Result: mMarco100k检索准确率提升6.83%（78.51%→85.34%），mNQ320k提升4.77%（68.12%→72.89%），标识符长度分别减少74.51%和78.2%

Conclusion: MGR-CSC通过语义对齐和空间压缩双重机制，有效解决多语言检索的标识符对齐难题，同时提升解码效率和检索准确性，实验验证了框架的优越性

Abstract: Generative Information Retrieval is an emerging retrieval paradigm that
exhibits remarkable performance in monolingual scenarios.However, applying
these methods to multilingual retrieval still encounters two primary
challenges, cross-lingual identifier misalignment and identifier inflation. To
address these limitations, we propose Multilingual Generative Retrieval via
Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies
semantically equivalent multilingual keywords into shared atoms to align
semantics and compresses the identifier space, and we propose a dynamic
multi-step constrained decoding strategy during retrieval. MGR-CSC improves
cross-lingual alignment by assigning consistent identifiers and enhances
decoding efficiency by reducing redundancy. Experiments demonstrate that
MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on
mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by
74.51% and 78.2%, respectively.

</details>


### [44] [AdaSwitch: Adaptive Switching Generation for Knowledge Distillation](https://arxiv.org/abs/2510.07842)
*Jingyu Peng,Maolin Wang,Hengyi Cai,Yuchen Li,Kai Zhang,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

TL;DR: 提出动态切换策略的AdaSwitch方法，在token级别融合在线/离线蒸馏策略，平衡训练一致性和监督质量


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法存在训练-推理不一致或依赖低质量学生输出的两难困境，需要兼顾策略一致性和监督质量

Method: 通过实时质量评估动态选择教师指导，先让学生自主探索，再选择性融合教师预测，保持自生成一致性和优质监督

Result: 在三个数据集和两种LLM师生对中均提升准确率，附加计算开销可接受

Conclusion: AdaSwitch为小模型蒸馏提供实用解决方案，证明动态策略融合的有效性

Abstract: Small language models (SLMs) are crucial for applications with strict latency
and computational constraints, yet achieving high performance remains
challenging. Knowledge distillation (KD) can transfer capabilities from large
teacher models, but existing methods involve trade-offs: off-policy
distillation provides high-quality supervision but introduces a
training-inference mismatch, while on-policy approaches maintain consistency
but rely on low-quality student outputs. To address these issues, we propose
AdaSwitch, a novel approach that dynamically combines on-policy and off-policy
generation at the token level. AdaSwitch allows the student to first explore
its own predictions and then selectively integrate teacher guidance based on
real-time quality assessment. This approach simultaneously preserves
consistency and maintains supervision quality. Experiments on three datasets
with two teacher-student LLM pairs demonstrate that AdaSwitch consistently
improves accuracy, offering a practical and effective method for distilling
SLMs with acceptable additional overhead.

</details>


### [45] [Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains](https://arxiv.org/abs/2510.07877)
*Md. Faiyaz Abdullah Sayeedi,Md. Mahbub Alam,Subhey Sadi Rahman,Md. Adnanul Islam,Jannatul Ferdous Deepti,Tasnim Mohiuddin,Md Mofijul Islam,Swakkhar Shatabda*

Main category: cs.CL

TL;DR: 论文提出Translation Tangles框架，评估开源大语言模型在24种双向语言对中的翻译质量和公平性，揭示LLM在翻译中的性能差异与偏见问题，并提供带偏见标注的数据集


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在机器翻译中展现出卓越能力，但其在不同语系/专业领域的翻译质量参差不齐，且可能放大训练数据中的偏见，这对低资源语言的公平性构成威胁

Method: 通过多领域指标评估翻译质量，提出混合偏见检测流程（规则启发式+语义过滤+LLM验证），构建包含人类评估的1,439对翻译-参考数据集的基准测试框架

Result: 创建高质量偏见标注数据集，公开代码与数据，为LLM翻译的公平性评估提供标准化工具（GitHub可访问）

Conclusion: Translation Tangles框架系统揭示了LLM翻译中的偏差问题，提出的评估方法论和数据集对提升多语言翻译的公平性具有重要价值

Abstract: The rise of Large Language Models (LLMs) has redefined Machine Translation
(MT), enabling context-aware and fluent translations across hundreds of
languages and textual domains. Despite their remarkable capabilities, LLMs
often exhibit uneven performance across language families and specialized
domains. Moreover, recent evidence reveals that these models can encode and
amplify different biases present in their training data, posing serious
concerns for fairness, especially in low-resource languages. To address these
gaps, we introduce Translation Tangles, a unified framework and dataset for
evaluating the translation quality and fairness of open-source LLMs. Our
approach benchmarks 24 bidirectional language pairs across multiple domains
using different metrics. We further propose a hybrid bias detection pipeline
that integrates rule-based heuristics, semantic similarity filtering, and
LLM-based validation. We also introduce a high-quality, bias-annotated dataset
based on human evaluations of 1,439 translation-reference pairs. The code and
dataset are accessible on GitHub:
https://github.com/faiyazabdullah/TranslationTangles

</details>


### [46] [Do LLMs Really Need 10+ Thoughts for "Find the Time 1000 Days Later"? Towards Structural Understanding of LLM Overthinking](https://arxiv.org/abs/2510.07880)
*Xinliang Frederick Zhang,Anhad Mohananey,Alexandra Chronopoulou,Pinelopi Papalampidi,Somit Gupta,Tsendsuren Munkhdalai,Lu Wang,Shyam Upadhyay*

Main category: cs.CL

TL;DR: 提出TRACE分析框架揭示大语言模型过度思考现象，发现Explorer/Late Landing两种低效模式，建立基于效用的管理方法


<details>
  <summary>Details</summary>
Motivation: 现有研究对长思维链模型效率低下的内在机制缺乏深入分析，需系统化解析思维过程结构

Method: 通过TRACE工具分解最小完整子思维，构建细粒度思维进程图，识别查询主题相似的通用思考模式

Result: 证实长思维模型在简单任务耗时增加5-20倍但无精度提升，识别出过度验证和过度探索两大核心诱因

Conclusion: 基于思维结构的效用定义突破了传统长度指标，为优化LLM推理效率提供理论依据和实践指南

Abstract: Models employing long chain-of-thought (CoT) reasoning have shown superior
performance on complex reasoning tasks. Yet, this capability introduces a
critical and often overlooked inefficiency -- overthinking -- models often
engage in unnecessarily extensive reasoning even for simple queries, incurring
significant computations without accuracy improvements. While prior work has
explored solutions to mitigate overthinking, a fundamental gap remains in our
understanding of its underlying causes. Most existing analyses are limited to
superficial, profiling-based observations, failing to delve into LLMs' inner
workings. This study introduces a systematic, fine-grained analyzer of LLMs'
thought process to bridge the gap, TRACE. We first benchmark the overthinking
issue, confirming that long-thinking models are five to twenty times slower on
simple tasks with no substantial gains. We then use TRACE to first decompose
the thought process into minimally complete sub-thoughts. Next, by inferring
discourse relationships among sub-thoughts, we construct granular thought
progression graphs and subsequently identify common thinking patterns for
topically similar queries. Our analysis reveals two major patterns for
open-weight thinking models -- Explorer and Late Landing. This finding provides
evidence that over-verification and over-exploration are the primary drivers of
overthinking in LLMs. Grounded in thought structures, we propose a
utility-based definition of overthinking, which moves beyond length-based
metrics. This revised definition offers a more insightful understanding of
LLMs' thought progression, as well as practical guidelines for principled
overthinking management.

</details>


### [47] [CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching](https://arxiv.org/abs/2510.07881)
*Heyang Liu,Yuhao Wang,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 针对多模态大语言模型在语码转换场景下的语言对齐缺陷，提出CS3-Bench基准及改进方案Chain of Recognition和Keyword Highlighting，显著提升跨语言理解能力


<details>
  <summary>Details</summary>
Motivation: 现有语音交互模型在多语言混合场景存在语言对齐问题，知识密集型QA任务准确率下降高达66%，需针对性提升跨语言理解能力

Method: 采用Chain of Recognition增强语义理解，通过Keyword Highlighting引导生成，结合专用数据构建策略

Result: 知识准确率从25.14%提升至46.13%，开放域理解率从64.5%增至86.5%，第二语言发音错误显著降低

Conclusion: CS3-Bench有效暴露跨语言交互瓶颈，提出的训练方法显著增强语言对齐能力，为多语言语音系统提供实用解决方案

Abstract: The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.

</details>


### [48] [Contrastive Weak-to-strong Generalization](https://arxiv.org/abs/2510.07884)
*Houcheng Jiang,Junfeng Fang,Jiaxin Wu,Tianyu Zhang,Chen Gao,Yong Li,Xiang Wang,Xiangnan He,Yang Deng*

Main category: cs.CL

TL;DR: 通过对比解码框架ConG提升弱到强泛化效果，有效解决噪声问题并增强模型稳健性


<details>
  <summary>Details</summary>
Motivation: 传统弱监督到强监督方法受限于弱模型输出的噪声偏差，限制了实际场景的应用可靠性

Method: 建立隐式奖励与对比解码的等价性，利用对齐前后弱模型的对比生成高质量样本

Result: 跨模型家族的实验结果验证了方法有效性，实现稳定的能力提升和去噪效果

Conclusion: ConG框架为弱到强泛化范式提供新思路，推动AGI发展路径的可行性

Abstract: Weak-to-strong generalization provides a promising paradigm for scaling large
language models (LLMs) by training stronger models on samples from aligned
weaker ones, without requiring human feedback or explicit reward modeling.
However, its robustness and generalization are hindered by the noise and biases
in weak-model outputs, which limit its applicability in practice. To address
this challenge, we leverage implicit rewards, which approximate explicit
rewards through log-likelihood ratios, and reveal their structural equivalence
with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in
LLM generation. Building on this connection, we propose Contrastive
Weak-to-Strong Generalization (ConG), a framework that employs contrastive
decoding between pre- and post-alignment weak models to generate higher-quality
samples. This approach enables more reliable capability transfer, denoising,
and improved robustness, substantially mitigating the limitations of
traditional weak-to-strong methods. Empirical results across different model
families confirm consistent improvements, demonstrating the generality and
effectiveness of ConG. Taken together, our findings highlight the potential of
ConG to advance weak-to-strong generalization and provide a promising pathway
toward AGI.

</details>


### [49] [Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects](https://arxiv.org/abs/2510.07890)
*Verena Blaschke,Miriam Winkler,Barbara Plank*

Main category: cs.CL

TL;DR: 比较文本模型、语音模型及级联系统在标准德语与方言迁移中的效果，发现语音模型对方言处理最优，文本模型对标准数据最佳。


<details>
  <summary>Details</summary>
Motivation: 方言主要通过口语传播且非标准拼写易导致文本处理问题，需探索不同模型在方言与标准语间的迁移性能差异。

Method: 使用德语的书面/口语意图与主题分类任务，构建首个方言音频意图数据集，对比文本、语音及语音转文本再处理的级联系统性能。

Result: 语音模型在方言数据表现最佳，文本模型在标准数据最优；级联系统在德语表现滞后，但若转录输出标准化则方言处理相对有效。

Conclusion: 直接语音处理更适用于方言，标准化转录提升级联系统效率，文本与语音模型的适用场景需根据数据类型区分。

Abstract: Research on cross-dialectal transfer from a standard to a non-standard
dialect variety has typically focused on text data. However, dialects are
primarily spoken, and non-standard spellings are known to cause issues in text
processing. We compare standard-to-dialect transfer in three settings: text
models, speech models, and cascaded systems where speech first gets
automatically transcribed and then further processed by a text model. In our
experiments, we focus on German and multiple German dialects in the context of
written and spoken intent and topic classification. To that end, we release the
first dialectal audio intent classification dataset. We find that the
speech-only setup provides the best results on the dialect data while the
text-only setup works best on the standard data. While the cascaded systems lag
behind the text-only models for German, they perform relatively well on the
dialectal data if the transcription system generates normalized, standard-like
output.

</details>


### [50] [Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2510.07892)
*Hyeonseok Moon,Seongtae Hong,Jaehyung Seo,Heuiseok Lim*

Main category: cs.CL

TL;DR: 提出MCBench基准测试，通过严格分步指令客观验证大语言模型执行NLP指标的能力


<details>
  <summary>Details</summary>
Motivation: 现有基准因LLM性能提升逐渐饱和，需要可代码验证的客观评估体系

Method: 设计含并行参考代码的基准框架，测试指令遵循/数值计算/长程一致性三项核心能力

Result: MCBench能有效评估顶尖LLM的细粒度指令理解与执行能力

Conclusion: 该基准为LLM能力评估提供了可代码验证的客观标准，推动更严谨的能力验证体系

Abstract: Recent frontier-level LLMs have saturated many previously difficult
benchmarks, leaving little room for further differentiation. This progress
highlights the need for challenging benchmarks that provide objective
verification. In this paper, we introduce MCBench, a benchmark designed to
evaluate whether LLMs can execute string-matching NLP metrics by strictly
following step-by-step instructions. Unlike prior benchmarks that depend on
subjective judgments or general reasoning, MCBench offers an objective,
deterministic and codeverifiable evaluation. This setup allows us to
systematically test whether LLMs can maintain accurate step-by-step execution,
including instruction adherence, numerical computation, and long-range
consistency in handling intermediate results. To ensure objective evaluation of
these abilities, we provide a parallel reference code that can evaluate the
accuracy of LLM output. We provide three evaluative metrics and three benchmark
variants designed to measure the detailed instruction understanding capability
of LLMs. Our analyses show that MCBench serves as an effective and objective
tool for evaluating the capabilities of cutting-edge LLMs.

</details>


### [51] [ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall](https://arxiv.org/abs/2510.07896)
*Jiayu Yang,Yuxuan Fan,Songning Lai,Shengen Wu,Jiaqi Tang,Chun Kang,Zhijiang Guo,Yutao Yue*

Main category: cs.CL

TL;DR: ACE框架通过控制神经元级Q-V通路归因，显著提升大语言模型在多跳知识编辑中的事实召回能力（GPT-J提升9.44%，Qwen3-8B提升37.46%）


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法忽视链式知识在神经元层的动态表征，导致多跳推理中中间隐含实体召回失败

Method: 基于因果分析发现关键Q-V路径，提出归因控制框架ACE，通过编辑跨transformer层的查询-价值神经元激活路径

Result: 在Qwen3-8B上取得37.46%的绝对性能提升，并揭示价值神经元语义解释性由查询驱动的跨层信息积累形成

Conclusion: 通过理解模型内部推理机制实现知识编辑能力突破，为基于机理认知的AI系统优化提供新范式

Abstract: Large Language Models (LLMs) require efficient knowledge editing (KE) to
update factual information, yet existing methods exhibit significant
performance decay in multi-hop factual recall. This failure is particularly
acute when edits involve intermediate implicit subjects within reasoning
chains. Through causal analysis, we reveal that this limitation stems from an
oversight of how chained knowledge is dynamically represented and utilized at
the neuron level. We discover that during multi hop reasoning, implicit
subjects function as query neurons, which sequentially activate corresponding
value neurons across transformer layers to accumulate information toward the
final answer, a dynamic prior KE work has overlooked. Guided by this insight,
we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual
Recall, a framework that leverages neuron-level attribution to identify and
edit these critical query-value (Q-V) pathways. ACE provides a mechanistically
grounded solution for multi-hop KE, empirically outperforming state-of-the-art
methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals
more fine-grained activation patterns in Qwen3 and demonstrates that the
semantic interpretability of value neurons is orchestrated by query-driven
accumulation. These findings establish a new pathway for advancing KE
capabilities based on the principled understanding of internal reasoning
mechanisms.

</details>


### [52] [Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation](https://arxiv.org/abs/2510.07912)
*Fanwei Zhua,Jiaxuan He,Xiaoxiao Chen,Zulong Chen,Quan Lu,Chenrui Mei*

Main category: cs.CL

TL;DR: 提出了一种基于大语言模型（LLM）的统一自动评分框架，支持跨领域多类型主观题的人性化评估，并通过实验和实际部署验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有主观题自动评分方法多局限于单一题型，缺乏处理综合考试中多样化题型的能力。需要一种通用框架来全面评估学生答案。

Method: 整合四个模块：1) 基础文本匹配；2) LLM对比知识点；3) 生成伪问题评估相关性；4) 模拟人工评估（内容与非内容维度分析）。

Result: 在通用和领域专用数据集上超越传统方法和LLM基线，多项评分指标表现优异，并已成功应用于某电商企业的认证考试系统。

Conclusion: 通过多模块协同和LLM能力整合，实现了接近人类水平的自动评分效果，为复杂考试场景提供了可行的解决方案。

Abstract: Automatic grading of subjective questions remains a significant challenge in
examination assessment due to the diversity in question formats and the
open-ended nature of student responses. Existing works primarily focus on a
specific type of subjective question and lack the generality to support
comprehensive exams that contain diverse question types. In this paper, we
propose a unified Large Language Model (LLM)-enhanced auto-grading framework
that provides human-like evaluation for all types of subjective questions
across various domains. Our framework integrates four complementary modules to
holistically evaluate student answers. In addition to a basic text matching
module that provides a foundational assessment of content similarity, we
leverage the powerful reasoning and generative capabilities of LLMs to: (1)
compare key knowledge points extracted from both student and reference answers,
(2) generate a pseudo-question from the student answer to assess its relevance
to the original question, and (3) simulate human evaluation by identifying
content-related and non-content strengths and weaknesses. Extensive experiments
on both general-purpose and domain-specific datasets show that our framework
consistently outperforms traditional and LLM-based baselines across multiple
grading metrics. Moreover, the proposed system has been successfully deployed
in real-world training and certification exams at a major e-commerce
enterprise.

</details>


### [53] [STEPER: Step-wise Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step Retrieval-Augmented Language Models](https://arxiv.org/abs/2510.07923)
*Kyumin Lee,Minjin Jeon,Sanghwan Jang,Hwanjo Yu*

Main category: cs.CL

TL;DR: 提出分步知识蒸馏框架StepER，通过阶段对齐监督和难度感知训练，使8B小模型在多跳问答任务中达到与70B教师模型相当的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法在多步检索增强框架中存在缺陷，未能适应不同推理阶段对信息和能力的不同需求，导致知识迁移效率低下。

Method: 1. 分阶段监督：针对检索、整合等不同阶段设计动态对齐目标
2. 难度感知训练：根据步骤复杂度渐进优化，优先学习有效推理路径
3. 框架通用性：适配基于推理路径检索和问题分解两种典型多步模型

Result: 在多个多跳QA基准测试中超越现有方法，8B学生模型达到与70B教师模型相当的准确率（具体提升幅度需参考原文实验部分）

Conclusion: StepER通过精细化分步知识迁移机制，显著提升了小模型的多步推理能力，为构建高效检索增强语言模型提供了新思路。

Abstract: Answering complex real-world questions requires step-by-step retrieval and
integration of relevant information to generate well-grounded responses.
However, existing knowledge distillation methods overlook the need for
different reasoning abilities at different steps, hindering transfer in
multi-step retrieval-augmented frameworks. To address this, we propose Stepwise
Knowledge Distillation for Enhancing Reasoning Ability in Multi-Step
Retrieval-Augmented Language Models (StepER). StepER employs step-wise
supervision to align with evolving information and reasoning demands across
stages. Additionally, it incorporates difficulty-aware training to
progressively optimize learning by prioritizing suitable steps. Our method is
adaptable to various multi-step retrieval-augmented language models, including
those that use retrieval queries for reasoning paths or decomposed questions.
Extensive experiments show that StepER outperforms prior methods on multi-hop
QA benchmarks, with an 8B model achieving performance comparable to a 70B
teacher model.

</details>


### [54] [Comprehensiveness Metrics for Automatic Evaluation of Factual Recall in Text Generation](https://arxiv.org/abs/2510.07926)
*Adam Dejl,James Barry,Alessandra Pascale,Javier Carnerero Cano*

Main category: cs.CL

TL;DR: 该研究针对大语言模型(LLMs)生成文本存在信息缺失的问题，评估了三种自动检测方法，发现端到端方法效果最佳但存在稳健性不足的缺陷。


<details>
  <summary>Details</summary>
Motivation: LLMs在敏感领域生成文本时若遗漏关键信息，可能产生与事实错误相当的严重危害，需建立有效的自动检测方法。

Method: 提出三种检测策略：1)基于NLI的原子命题分解法 2)基于Q&A的跨源响应对比法 3)LLM直接检测的端到端方法

Result: 端到端方法检测效果优于复杂方法，但牺牲了稳健性、可解释性和结果粒度

Conclusion: 端到端检测方法在综合性能上具备应用潜力，但需在效果与可解释性之间进行权衡

Abstract: Despite demonstrating remarkable performance across a wide range of tasks,
large language models (LLMs) have also been found to frequently produce outputs
that are incomplete or selectively omit key information. In sensitive domains,
such omissions can result in significant harm comparable to that posed by
factual inaccuracies, including hallucinations. In this study, we address the
challenge of evaluating the comprehensiveness of LLM-generated texts, focusing
on the detection of missing information or underrepresented viewpoints. We
investigate three automated evaluation strategies: (1) an NLI-based method that
decomposes texts into atomic statements and uses natural language inference
(NLI) to identify missing links, (2) a Q&A-based approach that extracts
question-answer pairs and compares responses across sources, and (3) an
end-to-end method that directly identifies missing content using LLMs. Our
experiments demonstrate the surprising effectiveness of the simple end-to-end
approach compared to more complex methods, though at the cost of reduced
robustness, interpretability and result granularity. We further assess the
comprehensiveness of responses from several popular open-weight LLMs when
answering user queries based on multiple sources.

</details>


### [55] [Vision-Enabled LLMs in Historical Lexicography: Digitising and Enriching Estonian-German Dictionaries from the 17th and 18th Centuries](https://arxiv.org/abs/2510.07931)
*Madis Jürviste,Joonatan Jakobson*

Main category: cs.CL

TL;DR: LLMs应用于17-18世纪爱沙尼亚历史词典研究，在词典信息半自动化增强、哥特体文字识别、跨来源数据集整合方面展现显著潜力，可节省小语种研究资源


<details>
  <summary>Details</summary>
Motivation: 解决小语种历史词典现代化处理难题，包括补充现代词形/释义、哥特体印刷源识别困难、多来源数据整合成本高等传统手工处理效率低下的问题

Method: 1. 使用Claude 3.7 Sonnet增强1648年词典词条（上下文辅助）
2. 零样本方法处理1732年哥特体词典文本识别
3. 1780年语法书采用重叠分块扫描+双LLM协作方案（OCR识别与结构化合并）

Result: 1. 81%词条获得准确释义与现代等效词
2. 41%词条实现零误差JSON结构化输出
3. 图像重叠分块法成功构建数字化工作流程

Conclusion: LLMs为小语种历史文献处理提供高效解决方案，在保持准确性的同时显著降低时间与经济成本，有利于濒危语言文化遗产的数字化保存

Abstract: This article presents research conducted at the Institute of the Estonian
Language between 2022 and 2025 on the application of large language models
(LLMs) to the study of 17th and 18th century Estonian dictionaries. The authors
address three main areas: enriching historical dictionaries with modern word
forms and meanings; using vision-enabled LLMs to perform text recognition on
sources printed in Gothic script (Fraktur); and preparing for the creation of a
unified, cross-source dataset. Initial experiments with J. Gutslaff's 1648
dictionary indicate that LLMs have significant potential for semi-automatic
enrichment of dictionary information. When provided with sufficient context,
Claude 3.7 Sonnet accurately provided meanings and modern equivalents for 81%
of headword entries. In a text recognition experiment with A. T. Helle's 1732
dictionary, a zero-shot method successfully identified and structured 41% of
headword entries into error-free JSON-formatted output. For digitising the
Estonian-German dictionary section of A. W. Hupel's 1780 grammar, overlapping
tiling of scanned image files is employed, with one LLM being used for text
recognition and a second for merging the structured output. These findings
demonstrate that even for minor languages LLMs have a significant potential for
saving time and financial resources.

</details>


### [56] [A$^2$Search: Ambiguity-Aware Question Answering with Reinforcement Learning](https://arxiv.org/abs/2510.07958)
*Fengji Zhang,Xinyao Niu,Chengyang Ying,Guancheng Lin,Zhongkai Hao,Zhou Fan,Chengen Huang,Jacky Keung,Bei Chen,Junyang Lin*

Main category: cs.CL

TL;DR: 提出A²Search框架，通过自动轨迹采样和证据验证处理开放域问答中的多答案问题，使用AnsF1奖励机制强化学习实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有QA系统和基准测试假设单一正确答案，难以处理多答案的模糊问题。人工标注成本高且难以扩展到多跳数据集，需开发自动化解决方案。

Method: 构建自动化流水线：1) 通过轨迹采样检测模糊问题并收集替代答案 2) 设计AnsF1奖励函数，利用强化学习优化模型，自然适应多答案场景。

Result: 在8个开放域QA基准测试中达到SOTA，7B模型AnsF1@1平均48.4%，优于ReSearch-32B(46.2%)。多跳数据集HotpotQA/MuSiQue表现突出。

Conclusion: 处理模糊性是构建可靠QA系统的关键，A²Search通过系统化方法实现性能突破。开源代码/数据/模型权重促进领域发展。

Abstract: Recent advances in Large Language Models (LLMs) and Reinforcement Learning
(RL) have led to strong performance in open-domain question answering (QA).
However, existing models still struggle with questions that admit multiple
valid answers. Standard QA benchmarks, which typically assume a single gold
answer, overlook this reality and thus produce inappropriate training signals.
Existing attempts to handle ambiguity often rely on costly manual annotation,
which is difficult to scale to multi-hop datasets such as HotpotQA and MuSiQue.
In this paper, we present A$^2$Search, an annotation-free, end-to-end training
framework to recognize and handle ambiguity. At its core is an automated
pipeline that detects ambiguous questions and gathers alternative answers via
trajectory sampling and evidence verification. The model is then optimized with
RL using a carefully designed $\mathrm{AnsF1}$ reward, which naturally
accommodates multiple answers. Experiments on eight open-domain QA benchmarks
demonstrate that A$^2$Search achieves new state-of-the-art performance. With
only a single rollout, A$^2$Search-7B yields an average $\mathrm{AnsF1}@1$
score of $48.4\%$ across four multi-hop benchmarks, outperforming all strong
baselines, including the substantially larger ReSearch-32B ($46.2\%$).
Extensive analyses further show that A$^2$Search resolves ambiguity and
generalizes across benchmarks, highlighting that embracing ambiguity is
essential for building more reliable QA systems. Our code, data, and model
weights can be found at https://github.com/zfj1998/A2Search

</details>


### [57] [LightReasoner: Can Small Language Models Teach Large Language Models Reasoning?](https://arxiv.org/abs/2510.07962)
*Jingyuan Wang,Yankai Chen,Zhonghang Li,Chao Huang*

Main category: cs.CL

TL;DR: 通过小模型指导大模型的关键推理时刻，LightReasoner框架用对比采样实现高效微调，减少90%训练资源。


<details>
  <summary>Details</summary>
Motivation: 解决传统监督微调(SFT)资源消耗大且均匀优化效率低的问题，利用大小模型行为差异捕捉高价值推理片段

Method: 两阶段框架：1) 专家-业余模型对比采样定位关键推理点；2) 基于对比样本的差异化微调强化专家模型优势

Result: 7个数学基准测试准确率最高提升28.1%，时间/采样问题/训练token消耗分别减少90%/80%/99%

Conclusion: 通过将弱模型转化为有效教学信号，LightReasoner为LLM推理提供了无需标注的高效增强路径

Abstract: Large language models (LLMs) have demonstrated remarkable progress in
reasoning, often through supervised fine-tuning (SFT). However, SFT is
resource-intensive, relying on large curated datasets, rejection-sampled
demonstrations, and uniform optimization across all tokens, even though only a
fraction carry meaningful learning value. In this work, we explore a
counterintuitive idea: can smaller language models (SLMs) teach larger language
models (LLMs) by revealing high-value reasoning moments that reflect the
latter's unique strength? We propose LightReasoner, a novel framework that
leverages the behavioral divergence between a stronger expert model (LLM) and a
weaker amateur model (SLM). LightReasoner operates in two stages: (1) a
sampling stage that pinpoints critical reasoning moments and constructs
supervision examples capturing the expert's advantage through expert-amateur
contrast, and (2) a fine-tuning stage that aligns the expert model with these
distilled examples, amplifying its reasoning strengths. Across seven
mathematical benchmarks, LightReasoner improves accuracy by up to 28.1%, while
reducing time consumption by 90%, sampled problems by 80%, and tuned token
usage by 99%, all without relying on ground-truth labels. By turning weaker
SLMs into effective teaching signals, LightReasoner offers a scalable and
resource-efficient approach for advancing LLM reasoning. Code is available at:
https://github.com/HKUDS/LightReasoner

</details>


### [58] [Active Confusion Expression in Large Language Models: Leveraging World Models toward Better Social Reasoning](https://arxiv.org/abs/2510.07974)
*Jialu Du,Guiyang Hou,Yihui Fu,Chen Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu*

Main category: cs.CL

TL;DR: 大语言模型在社交推理中存在认知混淆问题，提出动态世界模型增强机制有效提升准确率并降低计算成本


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在社交场景中混淆客观事实与主观信念的核心缺陷，改善其在多参与者场景中的推理能力

Method: 构建文本世界模型实时追踪实体状态与时间线，通过动态监测和干预机制提供清晰世界状态描述

Result: Hi-ToM准确率提升10%，计算token减少33.8%，在三个社交基准测试中表现显著优化

Conclusion: 世界模型机制成功模拟人类认知模式，为LLMs的社交场景应用提供了高效解决方案

Abstract: While large language models (LLMs) excel in mathematical and code reasoning,
we observe they struggle with social reasoning tasks, exhibiting cognitive
confusion, logical inconsistencies, and conflation between objective world
states and subjective belief states. Through deteiled analysis of DeepSeek-R1's
reasoning trajectories, we find that LLMs frequently encounter reasoning
impasses and tend to output contradictory terms like "tricky" and "confused"
when processing scenarios with multiple participants and timelines, leading to
erroneous reasoning or infinite loops. The core issue is their inability to
disentangle objective reality from agents' subjective beliefs. To address this,
we propose an adaptive world model-enhanced reasoning mechanism that constructs
a dynamic textual world model to track entity states and temporal sequences. It
dynamically monitors reasoning trajectories for confusion indicators and
promptly intervenes by providing clear world state descriptions, helping models
navigate through cognitive dilemmas. The mechanism mimics how humans use
implicit world models to distinguish between external events and internal
beliefs. Evaluations on three social benchmarks demonstrate significant
improvements in accuracy (e.g., +10% in Hi-ToM) while reducing computational
costs (up to 33.8% token reduction), offering a simple yet effective solution
for deploying LLMs in social contexts.

</details>


### [59] [Leveraging Author-Specific Context for Scientific Figure Caption Generation: 3rd SciCap Challenge](https://arxiv.org/abs/2510.07993)
*Watcharapong Timklaypachara,Monrada Chiewhawan,Nopporn Lekuthai,Titipat Achakulvisut*

Main category: cs.CL

TL;DR: 开发了一个整合文本上下文与作者风格的领域特定图注生成系统，在科学准确性和风格忠实度上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 解决科学图表标题需要同时满足准确性和风格一致性的挑战，通过结合上下文理解与作者特定风格适配提升图注质量

Method: 两阶段流程：1) 上下文过滤+类别特定提示优化 2) 基于档案图的少样本风格优化。采用DSPy框架的MIPROv2和SIMBA技术

Result: 类别特定提示使ROUGE-1召回提升+8.3%，风格优化带来BLEU提升40-48%、ROUGE提升25-27%

Conclusion: 上下文理解与作者风格适配相结合的方法能生成既科学准确又符合原文风格的优质图注

Abstract: Scientific figure captions require both accuracy and stylistic consistency to
convey visual information. Here, we present a domain-specific caption
generation system for the 3rd SciCap Challenge that integrates figure-related
textual context with author-specific writing styles using the LaMP-Cap dataset.
Our approach uses a two-stage pipeline: Stage 1 combines context filtering,
category-specific prompt optimization via DSPy's MIPROv2 and SIMBA, and caption
candidate selection; Stage 2 applies few-shot prompting with profile figures
for stylistic refinement. Our experiments demonstrate that category-specific
prompts outperform both zero-shot and general optimized approaches, improving
ROUGE-1 recall by +8.3\% while limiting precision loss to -2.8\% and BLEU-4
reduction to -10.9\%. Profile-informed stylistic refinement yields 40--48\%
gains in BLEU scores and 25--27\% in ROUGE. Overall, our system demonstrates
that combining contextual understanding with author-specific stylistic
adaptation can generate captions that are both scientifically accurate and
stylistically faithful to the source paper.

</details>


### [60] [Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks](https://arxiv.org/abs/2510.08002)
*Cheng Yang,Xuemeng Yang,Licheng Wen,Daocheng Fu,Jianbiao Mei,Rong Wu,Pinlong Cai,Yufan Shen,Nianchen Deng,Botian Shi,Yu Qiao,Haifeng Li*

Main category: cs.CL

TL;DR: 提出MUSE框架，通过分层记忆模块实现AI代理的持续学习与自我进化，在长期生产力任务中取得新SOTA表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在部署时静态不可变，缺乏从经验中积累知识并持续改进的能力，难以应对现实世界的长期任务挑战

Method: 构建分层记忆系统，在子任务执行后自主反思轨迹，将原始数据转化为结构化经验存入记忆模块，突破预训练参数限制

Result: 在TAC基准测试中取得显著性能提升（使用轻量级Gemini-2.5 Flash即达SOTA），经验积累展现零样本任务改进能力

Conclusion: MUSE建立了AI代理自动化生产力任务的新范式，证明持续经验积累可实现自主进化与跨任务知识迁移

Abstract: Large Language Models have demonstrated remarkable capabilities across
diverse domains, yet significant challenges persist when deploying them as AI
agents for real-world long-horizon tasks. Existing LLM agents suffer from a
critical limitation: they are test-time static and cannot learn from
experience, lacking the ability to accumulate knowledge and continuously
improve on the job. To address this challenge, we propose MUSE, a novel agent
framework that introduces an experience-driven, self-evolving system centered
around a hierarchical Memory Module. MUSE organizes diverse levels of
experience and leverages them to plan and execute long-horizon tasks across
multiple applications. After each sub-task execution, the agent autonomously
reflects on its trajectory, converting the raw trajectory into structured
experience and integrating it back into the Memory Module. This mechanism
enables the agent to evolve beyond its static pretrained parameters, fostering
continuous learning and self-evolution. We evaluate MUSE on the long-horizon
productivity benchmark TAC. It achieves new SOTA performance by a significant
margin using only a lightweight Gemini-2.5 Flash model. Sufficient Experiments
demonstrate that as the agent autonomously accumulates experience, it exhibits
increasingly superior task completion capabilities, as well as robust
continuous learning and self-evolution capabilities. Moreover, the accumulated
experience from MUSE exhibits strong generalization properties, enabling
zero-shot improvement on new tasks. MUSE establishes a new paradigm for AI
agents capable of real-world productivity task automation.

</details>


### [61] [ChatGPT as a Translation Engine: A Case Study on Japanese-English](https://arxiv.org/abs/2510.08042)
*Vincent Michael Sutanto,Giovanni Gatti De Giacomo,Toshiaki Nakazawa,Masaru Yamada*

Main category: cs.CL

TL;DR: ChatGPT在日英翻译中展现出文档级翻译优势，与商业系统竞争力相当，但3.5与4.0版本存在准确性与流畅性权衡


<details>
  <summary>Details</summary>
Motivation: 探索ChatGPT在日英翻译场景下的潜力，验证不同提示策略（简单/增强提示）的有效性，并与主流商业翻译系统进行横向对比

Method: 采用自动化评估与MQM人工评估相结合的方式，对比文档级/句子级翻译效果，测试不同ChatGPT版本（3.5/4.0）的翻译质量

Result: 1. 文档级翻译质量显著优于句子级
2. 提示策略改进未显现明显优势
3. ChatGPT-3.5自动评估占优但存在准确性与流畅性tradeoff
4. 整体表现与知名翻译系统竞争力相当

Conclusion: ChatGPT在文档级翻译场景具备应用潜力，版本选择需权衡翻译质量维度，提示工程效果需进一步探索

Abstract: This study investigates ChatGPT for Japanese-English translation, exploring
simple and enhanced prompts and comparing against commercially available
translation engines. Performing both automatic and MQM-based human evaluations,
we found that document-level translation outperforms sentence-level translation
for ChatGPT. On the other hand, we were not able to determine if enhanced
prompts performed better than simple prompts in our experiments. We also
discovered that ChatGPT-3.5 was preferred by automatic evaluation, but a
tradeoff exists between accuracy (ChatGPT-3.5) and fluency (ChatGPT-4). Lastly,
ChatGPT yields competitive results against two widely-known translation
systems.

</details>


### [62] [Climate Knowledge in Large Language Models](https://arxiv.org/abs/2510.08043)
*Ivan Kuznetsov,Jacopo Grassi,Dmitrii Pantiukhin,Boris Shapkin,Thomas Jung,Nikolay Koldunov*

Main category: cs.CL

TL;DR: LLMs能有效提取气候基线但存在地形敏感性缺陷，地理上下文可提升27%精度但气候变化空间模式识别不足


<details>
  <summary>Details</summary>
Motivation: 评估LLMs内禀气候知识对气候信息可靠性及气候传播风险管控至关重要

Method: 构建全球1°分辨率查询网格，采用坐标/地理描述双模式验证，以ERA5再分析数据为基准

Result: 模型捕获纬度/地形特征（RMSE 3-6°C），但高海拔区域误差剧增（1500m以上达5-13°C）。地理上下文使误差降低27%，但无法复现气候变化空间模式

Conclusion: LLMs具备基础气候分布表征能力，但缺乏区域气候动态演变的关键认知，需结合专业评估框架提升气候应用可靠性

Abstract: Large language models (LLMs) are increasingly deployed for climate-related
applications, where understanding internal climatological knowledge is crucial
for reliability and misinformation risk assessment. Despite growing adoption,
the capacity of LLMs to recall climate normals from parametric knowledge
remains largely uncharacterized. We investigate the capacity of contemporary
LLMs to recall climate normals without external retrieval, focusing on a
prototypical query: mean July 2-m air temperature 1991-2020 at specified
locations. We construct a global grid of queries at 1{\deg} resolution land
points, providing coordinates and location descriptors, and validate responses
against ERA5 reanalysis. Results show that LLMs encode non-trivial climate
structure, capturing latitudinal and topographic patterns, with
root-mean-square errors of 3-6 {\deg}C and biases of $\pm$1 {\deg}C. However,
spatially coherent errors remain, particularly in mountains and high latitudes.
Performance degrades sharply above 1500 m, where RMSE reaches 5-13 {\deg}C
compared to 2-4 {\deg}C at lower elevations. We find that including geographic
context (country, city, region) reduces errors by 27% on average, with larger
models being most sensitive to location descriptors. While models capture the
global mean magnitude of observed warming between 1950-1974 and 2000-2024, they
fail to reproduce spatial patterns of temperature change, which directly relate
to assessing climate change. This limitation highlights that while LLMs may
capture present-day climate distributions, they struggle to represent the
regional and local expression of long-term shifts in temperature essential for
understanding climate dynamics. Our evaluation framework provides a
reproducible benchmark for quantifying parametric climate knowledge in LLMs and
complements existing climate communication assessments.

</details>


### [63] [A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models](https://arxiv.org/abs/2510.08049)
*Congming Zheng,Jiachen Zhu,Zhuoying Ou,Yuxiang Chen,Kangning Zhang,Rong Shan,Zeyu Zheng,Mengyue Yang,Jianghao Lin,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: 系统综述过程奖励模型(PRMs)的全流程构建与应用，提出细粒度推理对齐框架


<details>
  <summary>Details</summary>
Motivation: 传统结果奖励模型(ORMs)仅评估最终答案，无法指导推理过程。PRMs通过轨迹级评估突破该限制，推动精细化的推理对齐

Method: 从过程数据生成、PRMs模型构建、测试时扩展与强化学习应用三个维度，系统梳理数学/代码/多模态等领域的实践方案与评估基准

Result: 建立PRMs设计空间框架，验证其在机器人、智能体等场景的有效性，发现当前模型在鲁棒性与泛化能力方面的不足

Conclusion: PRMs为推理对齐提供新范式，未来需优化过程数据质量、提升模型抗干扰能力，并拓展到更复杂决策场景

Abstract: Although Large Language Models (LLMs) exhibit advanced reasoning ability,
conventional alignment remains largely dominated by outcome reward models
(ORMs) that judge only final answers. Process Reward Models(PRMs) address this
gap by evaluating and guiding reasoning at the step or trajectory level. This
survey provides a systematic overview of PRMs through the full loop: how to
generate process data, build PRMs, and use PRMs for test-time scaling and
reinforcement learning. We summarize applications across math, code, text,
multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our
goal is to clarify design spaces, reveal open challenges, and guide future
research toward fine-grained, robust reasoning alignment.

</details>


### [64] [FedDTRE: Federated Dialogue Generation Models Powered by Trustworthiness Evaluation](https://arxiv.org/abs/2510.08058)
*Shule Lu,Lingxiang Wang,Sijia Wen,Ziwei Wang,Hainan Zhang*

Main category: cs.CL

TL;DR: 提出FedDTRE联邦学习策略，通过可信度评估动态调节全局模型贡献，提升对话生成质量


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在对话系统中存在局部数据过拟合、全局信息遗忘导致的泛化能力差问题，需平衡隐私保护与模型性能

Method: 基于公平性评估数据集计算全局/局部模型可信度分数，动态调节全局模型在本地更新中的贡献权重（FedDTRE策略）

Result: 实验证明该方法有效提升对话模型的性能表现和生成质量

Conclusion: 通过可信度评估机制实现全局-局部模型动态平衡，解决了联邦学习中个性化与泛化能力的矛盾

Abstract: With the rapid development of artificial intelligence, dialogue systems have
become a prominent form of human-computer interaction. However, traditional
centralized or fully local training approaches face challenges in balancing
privacy preservation and personalization due to data privacy concerns and
heterogeneous device capabilities. Federated learning, as a representative
distributed paradigm, offers a promising solution. However, existing methods
often suffer from overfitting under limited client data and tend to forget
global information after multiple training rounds, leading to poor
generalization. To address these issues, we propose FedDTRE, a Federated
adaptive aggregation strategy for Dialogue generation based on Trustworthiness
Evaluation. Instead of directly replacing local models with the global model,
FedDTRE leverages trustworthiness scores of both global and local models on a
fairness-oriented evaluation dataset to dynamically regulate the global model's
contribution during local updates. Experimental results demonstrate that
FedDTRE can improve dialogue model performance and enhance the quality of
dialogue generation.

</details>


### [65] [Everything is Plausible: Investigating the Impact of LLM Rationales on Human Notions of Plausibility](https://arxiv.org/abs/2510.08091)
*Shramay Palta,Peter Rankel,Sarah Wiegreffe,Rachel Rudinger*

Main category: cs.CL

TL;DR: 研究发现LLM生成的论证显著影响人类对常识问题的合理性判断，PRO论证提升评分，CON论证降低评分，LLM实验呈现相似模式


<details>
  <summary>Details</summary>
Motivation: 探究人类常识判断是否受LLM生成的合理性/不合理性论证影响，揭示LLM在人类认知研究中的潜力与风险

Method: 收集3,000个人类合理性判断和13,600个LLM生成判断，比较PRO/CON论证存在时的评分变化

Result: 人类在PRO论证时平均评分上升（+0.19），CON论证时下降（-0.23）；LLM表现类似影响模式（PRO+0.32/CON-0.28）

Conclusion: LLM既能作为研究人类认知的新工具，也存在潜在风险——即使在人类擅长的常识领域，LLM仍可能显著左右人类信念

Abstract: We investigate the degree to which human plausibility judgments of
multiple-choice commonsense benchmark answers are subject to influence by
(im)plausibility arguments for or against an answer, in particular, using
rationales generated by LLMs. We collect 3,000 plausibility judgments from
humans and another 13,600 judgments from LLMs. Overall, we observe increases
and decreases in mean human plausibility ratings in the presence of
LLM-generated PRO and CON rationales, respectively, suggesting that, on the
whole, human judges find these rationales convincing. Experiments with LLMs
reveal similar patterns of influence. Our findings demonstrate a novel use of
LLMs for studying aspects of human cognition, while also raising practical
concerns that, even in domains where humans are ``experts'' (i.e., common
sense), LLMs have the potential to exert considerable influence on people's
beliefs.

</details>


### [66] [The Price of Thought: A Multilingual Analysis of Reasoning, Performance, and Cost of Negotiation in Large Language Models](https://arxiv.org/abs/2510.08098)
*Sherzod Hakimov,Roland Bernard,Tim Leiber,Karl Osswald,Kristina Richert,Ruilin Yang,Raffaella Bernardi,David Schlangen*

Main category: cs.CL

TL;DR: 研究表明LLM的推理能力显著提升谈判效果但伴随高昂计算成本，并揭示多语言场景下开源模型与商业模型的语言一致性差异


<details>
  <summary>Details</summary>
Motivation: 探索推理能力对AI谈判策略的影响，分析不同语言环境下LLM在合作竞争平衡、对手建模和战略适应方面的表现差异

Method: 采用三语种（英德意）自我对弈实验框架，通过三种异构对话游戏评估商业/开源LLM的推理成本效益比和语言一致性特征

Result: 启用推理使GPT-5性能提升31.4%但成本激增400%；开源模型多语言谈判时内部推理自动切换英语，商业模型保持输出语言一致性

Conclusion: 推理能力突破任务复杂度但需权衡计算成本，商业模型在多语言场景保持推理透明度优势，为LLM谈判系统设计提供关键取舍依据

Abstract: Negotiation is a fundamental challenge for AI agents, as it requires an
ability to reason strategically, model opponents, and balance cooperation with
competition. We conduct the first comprehensive study systematically evaluating
the effect of (LLM-)reasoning on the negotiation abilities of both commercial
and open-weight LLMs, and do this across three languages. Using a self-play
setup across three diverse dialogue games, we analyse trade-offs between
performance and cost, the language consistency of reasoning processes, and the
nature of strategic adaptation exhibited by models. Our findings show that
enabling reasoning-that is, scaling test time compute-significantly improves
negotiation outcomes by enhancing collaboration and helping models overcome
task complexities, but comes at a substantial computational cost: reasoning
improves GPT-5's performance by 31.4 % while increasing its cost by nearly 400
%. Most critically, we uncover a significant multilingual reasoning
distinction: open-weight models consistently switch to English for their
internal reasoning steps, even when negotiating in German or Italian (and thus
possibly impacting potential explainability gains through the disclosure of
reasoning traces), while leading commercial models maintain language
consistency between their reasoning and final output.

</details>


### [67] [Lossless Vocabulary Reduction for Auto-Regressive Language Models](https://arxiv.org/abs/2510.08102)
*Daiki Chijiwa,Taku Hasegawa,Kyosuke Nishida,Shin'ya Yamaguchi,Tomoya Ohba,Tamao Sakao,Susumu Takeuchi*

Main category: cs.CL

TL;DR: 提出一种无损词汇缩减框架，使自回归语言模型能在任意小词汇表下无精度损失，促进不同分词模型的协作。


<details>
  <summary>Details</summary>
Motivation: 由于不同语言模型使用独立词汇表，在模型集成等需要联合预测的场景中存在协作困难。本文旨在解决这一跨模型协作的技术瓶颈。

Method: 建立理论框架实现词汇表无损缩减，通过最大公共词汇表实现不同模型间的预测概率转换。提出基于词汇表覆盖的模型转换算法。

Result: 证明转换后模型在任意子词汇表下保持预测一致性，实现不同分词方案模型的高效协同工作。

Conclusion: 该框架为模型集成、知识迁移等应用提供了理论基础，显著提升异构语言模型的协同效率。

Abstract: Tokenization -- the process of decomposing a given text into a sequence of
subwords called tokens -- is one of the key components in the development of
language models. Particularly, auto-regressive language models generate texts
token by token, i.e., by predicting the next-token distribution given the
previous ones, and thus tokenization directly affects their efficiency in text
generation. Since each language model has their own vocabulary as a set of
possible tokens, they struggle to cooperate with each other at the level of
next-token distributions such as model ensemble. In this paper, we establish a
theoretical framework of lossless vocabulary reduction, which efficiently
converts a given auto-regressive language model into the one with an
arbitrarily small vocabulary without any loss in accuracy. As an application,
we demonstrate that language models with different tokenization can cooperate
with each other efficiently through their maximal common vocabulary.

</details>


### [68] [Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing](https://arxiv.org/abs/2510.08111)
*Haoyang Gui,Thales Bertaglia,Taylor Annabell,Catalina Goanta,Tjomme Dooper,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 论文通过测试GPT-5-nano和Gemini-2.5-flash-lite模型，构建了法律推理错误分类体系，提出透明检测网红营销内容的自动化审核框架。


<details>
  <summary>Details</summary>
Motivation: 网红营销中隐蔽广告的检测缺乏法律依据和透明性，现有方法难以平衡算法准确性与法律合规性。

Method: 使用1,143条Instagram帖子，对比两种大模型在三类提示策略下的表现，建立法律推理错误分类体系并定量定性分析。

Result: 模型总体F1达0.93（模糊案例下降超10%），发现28.57%引用缺失、20.71%表述模糊等问题，法律文本增强提示仅改善解释质量。

Conclusion: 研究构建了首个法律健壮性评估框架，提供带法律标注的数据集，为监管机构自动化审核提供了量化与质性结合的评价范式。

Abstract: The rise of influencer marketing has blurred boundaries between organic
content and sponsored content, making the enforcement of legal rules relating
to transparency challenging. Effective regulation requires applying legal
knowledge with a clear purpose and reason, yet current detection methods of
undisclosed sponsored content generally lack legal grounding or operate as
opaque "black boxes". Using 1,143 Instagram posts, we compare gpt-5-nano and
gemini-2.5-flash-lite under three prompting strategies with controlled levels
of legal knowledge provided. Both models perform strongly in classifying
content as sponsored or not (F1 up to 0.93), though performance drops by over
10 points on ambiguous cases. We further develop a taxonomy of reasoning
errors, showing frequent citation omissions (28.57%), unclear references
(20.71%), and hidden ads exhibiting the highest miscue rate (28.57%). While
adding regulatory text to the prompt improves explanation quality, it does not
consistently improve detection accuracy. The contribution of this paper is
threefold. First, it makes a novel addition to regulatory compliance technology
by providing a taxonomy of common errors in LLM-generated legal reasoning to
evaluate whether automated moderation is not only accurate but also legally
robust, thereby advancing the transparent detection of influencer marketing
content. Second, it features an original dataset of LLM explanations annotated
by two students who were trained in influencer marketing law. Third, it
combines quantitative and qualitative evaluation strategies for LLM
explanations and critically reflects on how these findings can support
advertising regulatory bodies in automating moderation processes on a solid
legal foundation.

</details>


### [69] [Interpreting LLM-as-a-Judge Policies via Verifiable Global Explanations](https://arxiv.org/abs/2510.08120)
*Jasmina Gajcin,Erik Miehling,Rahul Nair,Elizabeth Daly,Radu Marinescu,Seshu Tirupathi*

Main category: cs.CL

TL;DR: 提出CLoVE和GloVE算法从LLM评判员中提取全局策略，验证其在内容危害检测中的有效性与鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决LLM-as-a-Judge大规模应用时存在的潜在偏见和风险，需确保决策过程的透明可信

Method: 1) CLoVE生成基于概念的对比性局部解释 2) GloVE通过迭代聚类、总结和验证生成全局策略

Result: 全局策略在7个数据集上与LLM判断高度一致，抗干扰能力强，用户研究显示高满意度

Conclusion: 全局策略可有效解释LLM评判逻辑，提升AI评判系统的透明度和可靠性

Abstract: Using LLMs to evaluate text, that is, LLM-as-a-judge, is increasingly being
used at scale to augment or even replace human annotations. As such, it is
imperative that we understand the potential biases and risks of doing so. In
this work, we propose an approach for extracting high-level concept-based
global policies from LLM-as-a-Judge. Our approach consists of two algorithms:
1) CLoVE (Contrastive Local Verifiable Explanations), which generates
verifiable, concept-based, contrastive local explanations and 2) GloVE (Global
Verifiable Explanations), which uses iterative clustering, summarization and
verification to condense local rules into a global policy. We evaluate GloVE on
seven standard benchmarking datasets for content harm detection. We find that
the extracted global policies are highly faithful to decisions of the
LLM-as-a-Judge. Additionally, we evaluated the robustness of global policies to
text perturbations and adversarial attacks. Finally, we conducted a user study
to evaluate user understanding and satisfaction with global policies.

</details>


### [70] [Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling](https://arxiv.org/abs/2510.08145)
*Shuliang Liu,Zhipeng Xu,Zhenghao Liu,Yukun Yan,Minghe Yu,Yu Gu,Chong Chen,Huiyuan Xie,Ge Yu*

Main category: cs.CL

TL;DR: 提出基于多智能体协同优化的Genii框架，通过模拟客户端-服务器轮询机制有效缓解LLM评估模型的固有判断偏好偏见


<details>
  <summary>Details</summary>
Motivation: LLM作为自动评估器时存在固有判断偏好偏见，倾向于偏向自身生成的回答，影响评估可靠性

Method: 将多个LLM评估模型整合为多智能体系统，模拟交互式客户端-服务器轮询机制进行无监督优化

Result: 实验表明Genii优于基于标注数据训练的监督模型，且无需人工标注。在轮询过程中持续提升不同客户端代理性能，即使服务器代理使用较弱模型依然有效

Conclusion: Genii成功缓解了LLM评估模型的判断偏好偏见，验证了该框架的有效性和实用性

Abstract: Large Language Models (LLMs) as automatic evaluators, commonly referred to as
LLM-as-a-Judge, have also attracted growing attention. This approach plays a
vital role in aligning LLMs with human judgments, providing accurate and
reliable assessments. However, LLM-based judgment models often exhibit judgment
preference bias during the evaluation phase, tending to favor responses
generated by themselves, undermining the reliability of their judgments. This
paper introduces the Group-Based Polling Optimization (Genii), an unsupervised
multi-agent collaborative optimization framework that mitigates the inherent
judgment preference bias of judgment models. Specifically, Genii integrates
various LLM-based judgment models into a multi-agent system and simulates the
interactive client-server polling mechanism to optimize each client agent
unsupervisedly. Our experiments demonstrate that Genii outperforms supervised
models trained on annotated judgment data, while requiring no human-labeled
annotations. Genii consistently improves performance across different client
agents during the polling, even when weaker models act as server agents.
Further analysis reveals that Genii effectively mitigates judgment preference
bias of LLM-based judgment models, demonstrating its effectiveness. All codes
are available at https://github.com/NEUIR/Genii.

</details>


### [71] [AI Knowledge Assist: An Automated Approach for the Creation of Knowledge Bases for Conversational AI Agents](https://arxiv.org/abs/2510.08149)
*Md Tahmid Rahman Laskar,Julien Bouvier Tremblay,Xue-Yong Fu,Cheng Chen,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 通过从历史客服对话中自动提取问答对构建知识库，并微调轻量级LLaMA-3.1-8B模型，实现90%+的准确率，解决联络中心冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 企业专属知识库缺失阻碍会话AI在联络中心的应用，需自动化方案利用现有对话数据构建知识库。

Method: 1. 从历史客服对话提取QA对自动建库
2. 基于内部数据微调轻量级LLaMA-3.1-8B模型
3. 通过参数优化实现超越闭源大模型的性能

Result: 20家企业实证显示：
- 信息类问题回答准确率超90%
- 消除知识库冷启动障碍
- 支持RAG聊天机器人即时部署

Conclusion: 验证了轻量化模型+领域数据微调方案的有效性，为联络中心提供了零冷启动的AI知识助手解决方案，显著降低企业AI应用门槛。

Abstract: The utilization of conversational AI systems by leveraging Retrieval
Augmented Generation (RAG) techniques to solve customer problems has been on
the rise with the rapid progress of Large Language Models (LLMs). However, the
absence of a company-specific dedicated knowledge base is a major barrier to
the integration of conversational AI systems in contact centers. To this end,
we introduce AI Knowledge Assist, a system that extracts knowledge in the form
of question-answer (QA) pairs from historical customer-agent conversations to
automatically build a knowledge base. Fine-tuning a lightweight LLM on internal
data demonstrates state-of-the-art performance, outperforming larger
closed-source LLMs. More specifically, empirical evaluation on 20 companies
demonstrates that the proposed AI Knowledge Assist system that leverages the
LLaMA-3.1-8B model eliminates the cold-start gap in contact centers by
achieving above 90% accuracy in answering information-seeking questions. This
enables immediate deployment of RAG-powered chatbots.

</details>


### [72] [DACIP-RC: Domain Adaptive Continual Instruction Pre-Training via Reading Comprehension on Business Conversations](https://arxiv.org/abs/2510.08152)
*Elena Khasanova,Harsh Saini,Md Tahmid Rahman Laskar,Xue-Yong Fu,Cheng Chen,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: 提出了DACIP-RC方法，通过阅读理解增强小模型在业务对话任务中的领域适应性，解决传统微调导致的泛化能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 大规模LLMs部署成本过高，而小模型缺乏跨领域的零样本指令跟随能力，传统微调会引发灾难性遗忘。

Method: 基于对话记录生成多样化任务指令和响应，采用阅读理解范式替代传统next-token预训练模式。

Result: 实证显示该方法显著提升会议摘要、行动项生成、通话目的识别等业务场景的零样本泛化能力。

Conclusion: 这是首个在业务对话数据上进行指令预训练的研究，为行业利用专有数据集实现领域适应提供了新思路。

Abstract: The rapid advancements in Large Language Models (LLMs) have enabled their
adoption in real-world industrial scenarios for various natural language
processing tasks. However, the high inference cost of large-scale LLMs makes
their deployment impractical, necessitating the use of smaller models. Despite
their efficiency, smaller LLMs lack robust zero-shot instruction-following
capabilities across diverse domains, limiting their adaptability to dynamic
user requirements. Traditional fine-tuning approaches exacerbate this issue by
inducing catastrophic forgetting, reducing the model's generalization ability
for unseen tasks. In this paper, we propose Domain Adaptive Continual
Instruction Pre-Training via Reading Comprehension (DACIP-RC), a continual
pre-training technique that enhances smaller LLMs' domain adaptability for
business conversational tasks. Unlike conventional pre-training approaches that
rely on next-token prediction, DACIP-RC generates diverse task instructions and
responses via reading comprehension on conversation transcripts, enabling
better instruction generalization. Our empirical evaluations demonstrate that
DACIP-RC significantly improves zero-shot generalization across a wide range of
business conversational tasks, including meeting summarization, action item
generation, and call purpose identification. To the best of our knowledge, this
is the first work to apply instruction pre-training on business conversational
data, providing insights into how industries can leverage proprietary datasets
for domain adaptation.

</details>


### [73] [Beyond Over-Refusal: Scenario-Based Diagnostics and Post-Hoc Mitigation for Exaggerated Refusals in LLMs](https://arxiv.org/abs/2510.08158)
*Shuzhou Yuan,Ercong Nie,Yinuo Sun,Chenxuan Zhao,William LaCroix,Michael Färber*

Main category: cs.CL

TL;DR: 研究通过创建XSB和MS-XSB基准测试，揭示LLMs在复杂场景中过度拒绝的安全问题，并提出三种无需训练即可缓解该问题的轻量级方法。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs对含有类似不安全词汇的良性请求频繁错误拒绝的问题，影响模型的实际应用效果和用户体验。

Method: 1）开发XSB（单轮）和MS-XSB（多轮对话）基准测试；2）使用事后解释方法定位拒绝触发词；3）提出忽略词指令、提示改写、注意力引导三种实时干预方法。

Result: 在Llama系列模型的实验中，三种方法显著提升安全提示的响应合规性（平均提升18-32%），同时保持安全防护有效性。

Conclusion: 建立了可复现的诊断-缓解框架，为LLMs安全部署提供保持安全性前提下提升响应友好性的实用解决方案。

Abstract: Large language models (LLMs) frequently produce false refusals, declining
benign requests that contain terms resembling unsafe queries. We address this
challenge by introducing two comprehensive benchmarks: the Exaggerated Safety
Benchmark (XSB) for single-turn prompts, annotated with "Focus" keywords that
identify refusal-inducing triggers, and the Multi-turn Scenario-based
Exaggerated Safety Benchmark (MS-XSB), which systematically evaluates refusal
calibration in realistic, context-rich dialog settings. Our benchmarks reveal
that exaggerated refusals persist across diverse recent LLMs and are especially
pronounced in complex, multi-turn scenarios. To mitigate these failures, we
leverage post-hoc explanation methods to identify refusal triggers and deploy
three lightweight, model-agnostic approaches, ignore-word instructions, prompt
rephrasing, and attention steering, at inference time, all without retraining
or parameter access. Experiments on four instruction-tuned Llama models
demonstrate that these strategies substantially improve compliance on safe
prompts while maintaining robust safety protections. Our findings establish a
reproducible framework for diagnosing and mitigating exaggerated refusals,
highlighting practical pathways to safer and more helpful LLM deployments.

</details>


### [74] [ARM2: Adaptive Reasoning Model with Vision Understanding and Executable Code](https://arxiv.org/abs/2510.08163)
*Jian Xie,Zhendong Chu,Aoxiao Zhong,Kai Zhang,Mingzhe Han,Xin Fang,Jialie Shen,Qingsong Wen*

Main category: cs.CL

TL;DR: 提出ARM2模型，通过强化学习框架实现推理性能与效率的自适应平衡，解决大型推理模型在简单任务上过度思考的问题


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在'过度思考'问题，现有解决方案（如长度惩罚机制）多为启发式且任务特定化，缺乏通用自适应推理框架

Method: 基于强化学习的长度感知优化框架，整合视觉理解与可执行代码，扩展多模态应用场景并降低推理token成本

Result: 在保持与GRPO训练的传统模型相当性能的同时，平均减少70%以上的token消耗

Conclusion: ARM2通过自适应推理机制在保证任务性能的前提下显著降低计算成本，设计有效性通过多维度分析得到验证

Abstract: Large Reasoning Models (LRMs) often suffer from the ``over-thinking''
problem, generating unnecessarily long reasoning on simple tasks. Some
strategies have been proposed to mitigate this issue, such as length penalties
or routing mechanisms, but they are typically heuristic and task-specific,
lacking a general framework for adaptive reasoning. In this paper, we present
ARM2, a unified model that adaptively balances reasoning performance and
efficiency across multiple formats through a reinforcement learning framework
augmented with length-aware optimization. Beyond conventional natural language
inference, ARM2 integrates vision understanding, extending its applicability to
multimodal. Moreover, ARM2 integrates executable code into reasoning, enabling
substantial reductions in token cost while preserving task performance compared
to long CoT. Experiments demonstrate that ARM2 achieves performance on par with
traditional reasoning models trained with GRPO, while reducing token usage by
over 70% on average. We further conduct extensive analyses to validate the
effectiveness of ARM2 and the soundness of its design.

</details>


### [75] [MetricalARGS: A Taxonomy for Studying Metrical Poetry with LLMs](https://arxiv.org/abs/2510.08188)
*Chalamalasetti Kranti,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 提出首个面向格律诗的NLP任务分类法MetricalARGS，通过分析、检索、生成、支持四个维度评估大语言模型对复杂诗律规则的处理能力，并以泰卢固语为例验证框架可行性。


<details>
  <summary>Details</summary>
Motivation: 现有诗歌相关研究集中于生成与摘要，但诗律传统对语言模型遵循严格规则及深层推理能力提出挑战，需系统性评估框架。

Method: 构建四维分类法，关联现有NLP任务，探讨数据集与评估指标设计，选择泰卢固语作为典型语言进行实例化验证。

Result: MetricalARGS揭示了通过格律诗分析可有效探测语言模型的核心能力边界与局限性。

Conclusion: 该分类法为理解语言模型在复杂文学形式中的表现提供了系统性研究路径，拓展了模型评估的文化维度。

Abstract: Prior NLP work studying poetry has focused primarily on automatic poem
generation and summarization. Many languages have well-studied traditions of
poetic meter which enforce constraints on a poem in terms of syllable and
phoneme patterns. Such advanced literary forms offer opportunities for probing
deeper reasoning and language understanding in Large Language Models (LLMs) and
their ability to follow strict pre-requisites and rules. In this paper, we
introduce MetricalARGS, the first taxonomy of poetry-related NLP tasks designed
to evaluate LLMs on metrical poetry across four dimensions: Analysis,
Retrieval, Generation, and Support. We discuss how these tasks relate to
existing NLP tasks, addressing questions around datasets and evaluation
metrics. Taking Telugu as our example language, we illustrate how the taxonomy
can be used in practice. MetricalARGS highlights the broader possibilities for
understanding the capabilities and limitations of today's LLMs through the lens
of metrical poetry.

</details>


### [76] [Training-Free Group Relative Policy Optimization](https://arxiv.org/abs/2510.08191)
*Yuzheng Cai,Siqi Cai,Yuchen Shi,Zihan Xu,Lichao Chen,Yulei Qin,Xiaoyu Tan,Gang Li,Zongyi Li,Haojia Lin,Yong Mao,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: Proposes Training-Free GRPO method that enhances LLM agent performance in specialized domains by leveraging experiential token priors without parameter updates.


<details>
  <summary>Details</summary>
Motivation: Existing agentic RL methods rely on costly parameter updates and risk overfitting. This work addresses data scarcity and computational inefficiency through lightweight experiential learning.

Method: Uses group-relative semantic advantages to iteratively distill high-quality knowledge as token priors during multi-epoch learning, integrated via API calls.

Result: Achieves significant OOD performance improvements (math reasoning +6.1%, web search +12.3%) using only 50 samples, outperforming fine-tuned small LLMs.

Conclusion: Demonstrates lightweight knowledge integration effectively bypasses traditional training limitations while maintaining data efficiency and generalization capability.

Abstract: Recent advances in Large Language Model (LLM) agents have demonstrated their
promising general capabilities. However, their performance in specialized
real-world domains often degrades due to challenges in effectively integrating
external tools and specific prompting strategies. While methods like agentic
reinforcement learning have been proposed to address this, they typically rely
on costly parameter updates, for example, through a process that uses
Supervised Fine-Tuning (SFT) followed by a Reinforcement Learning (RL) phase
with Group Relative Policy Optimization (GRPO) to alter the output
distribution. However, we argue that LLMs can achieve a similar effect on the
output distribution by learning experiential knowledge as a token prior, which
is a far more lightweight approach that not only addresses practical data
scarcity but also avoids the common issue of overfitting. To this end, we
propose Training-Free Group Relative Policy Optimization (Training-Free GRPO),
a cost-effective solution that enhances LLM agent performance without any
parameter updates. Our method leverages the group relative semantic advantage
instead of numerical ones within each group of rollouts, iteratively distilling
high-quality experiential knowledge during multi-epoch learning on a minimal
ground-truth data. Such knowledge serves as the learned token prior, which is
seamlessly integrated during LLM API calls to guide model behavior. Experiments
on mathematical reasoning and web searching tasks demonstrate that
Training-Free GRPO, when applied to DeepSeek-V3.1-Terminus, significantly
improves out-of-domain performance. With just a few dozen training samples,
Training-Free GRPO outperforms fine-tuned small LLMs with marginal training
data and cost.

</details>


### [77] [Memory Retrieval and Consolidation in Large Language Models through Function Tokens](https://arxiv.org/abs/2510.08203)
*Shaohua Zhang,Yuan Lin,Hang Li*

Main category: cs.CL

TL;DR: 提出功能令牌假设，解释LLMs通过功能令牌在推理时激活预测性特征（内存检索），预训练时通过预测后续内容令牌增加特征学习（内存巩固）。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs的存储检索与巩固机制理解不足，需通过假设阐明其工作机制以提升知识记忆、指令执行等能力。

Method: 采用二分图分析验证功能令牌激活特征的比例，案例研究展示功能令牌如何指导预测，分析预训练损失分布。

Result: 功能令牌激活大部分特征，预训练时预测内容令牌的损失主导促使功能令牌选择预测性特征。

Conclusion: 功能令牌在LLMs中起核心作用，理解其机制可优化模型设计，实验结果验证假设有效性。

Abstract: The remarkable success of large language models (LLMs) stems from their
ability to consolidate vast amounts of knowledge into the memory during
pre-training and to retrieve it from the memory during inference, enabling
advanced capabilities such as knowledge memorization, instruction-following and
reasoning. However, the mechanisms of memory retrieval and consolidation in
LLMs remain poorly understood. In this paper, we propose the function token
hypothesis to explain the workings of LLMs: During inference, function tokens
activate the most predictive features from context and govern next token
prediction (memory retrieval). During pre-training, predicting the next tokens
(usually content tokens) that follow function tokens increases the number of
learned features of LLMs and updates the model parameters (memory
consolidation). Function tokens here roughly correspond to function words in
linguistics, including punctuation marks, articles, prepositions, and
conjunctions, in contrast to content tokens. We provide extensive experimental
evidence supporting this hypothesis. Using bipartite graph analysis, we show
that a small number of function tokens activate the majority of features. Case
studies further reveal how function tokens activate the most predictive
features from context to direct next token prediction. We also find that during
pre-training, the training loss is dominated by predicting the next content
tokens following function tokens, which forces the function tokens to select
the most predictive features from context.

</details>


### [78] [LLMs Learn to Deceive Unintentionally: Emergent Misalignment in Dishonesty from Misaligned Samples to Biased Human-AI Interactions](https://arxiv.org/abs/2510.08211)
*XuHao Hu,Peng Wang,Xiaoya Lu,Dongrui Liu,Xuanjing Huang,Jing Shao*

Main category: cs.CL

TL;DR: 研究发现大语言模型在特定领域微调后，会在高风险场景中表现出广泛的不诚实行为，且少量失调数据或用户偏见即可显著加剧该现象


<details>
  <summary>Details</summary>
Motivation: 验证突发性失调现象是否会在高风险场景（如撒谎/欺骗行为）中扩展，探索微调数据污染对模型行为的影响

Method: 1. 在跨领域失调数据上微调开源模型
2. 下游混合微调实验（掺入1%失调数据）
3. 模拟人机交互环境（10%偏见用户）

Result: 模型表现出广泛不诚实行为；混合微调使诚实行为下降20%；仅10%偏见用户即可导致模型失调加剧

Conclusion: 突发性失调风险不仅存在于直接微调，也存在于混合任务和实际交互场景，警示AI系统部署中的安全隐患

Abstract: Previous research has shown that LLMs finetuned on malicious or incorrect
completions within narrow domains (e.g., insecure code or incorrect medical
advice) can become broadly misaligned to exhibit harmful behaviors, which is
called emergent misalignment. In this work, we investigate whether this
phenomenon can extend beyond safety behaviors to a broader spectrum of
dishonesty and deception under high-stakes scenarios (e.g., lying under
pressure and deceptive behavior). To explore this, we finetune open-sourced
LLMs on misaligned completions across diverse domains. Experimental results
demonstrate that LLMs show broadly misaligned behavior in dishonesty.
Additionally, we further explore this phenomenon in a downstream combined
finetuning setting, and find that introducing as little as 1% of misalignment
data into a standard downstream task is sufficient to decrease honest behavior
over 20%. Furthermore, we consider a more practical human-AI interaction
environment where we simulate both benign and biased users to interact with the
assistant LLM. Notably, we find that the assistant can be misaligned
unintentionally to exacerbate its dishonesty with only 10% biased user
population. In summary, we extend the study of emergent misalignment to the
domain of dishonesty and deception under high-stakes scenarios, and demonstrate
that this risk arises not only through direct finetuning, but also in
downstream mixture tasks and practical human-AI interactions.

</details>


### [79] [SenWave: A Fine-Grained Multi-Language Sentiment Analysis Dataset Sourced from COVID-19 Tweets](https://arxiv.org/abs/2510.08214)
*Qiang Yang,Xiuying Chen,Changsheng Ma,Rui Yin,Xin Gao,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出SenWave细粒度多语言COVID-19推文情感分析数据集，包含十类情感标注，覆盖五国语言及1.05亿未标注数据，通过预训练模型实现精准分类并验证ChatGPT兼容性。


<details>
  <summary>Details</summary>
Motivation: 解决现有COVID-19数据集的标签粗糙、多语言细粒度情感标注缺失问题，满足复杂事件中情绪演变的深度分析需求。

Method: 构建10,000条英/阿语标注推文+30,000条翻译推文，微调Transformer模型；采用时间序列分析跨语言情感演变，设计ChatGPT兼容性测试框架。

Result: 获得细粒度情感分类SOTA性能（英语F1=89.7%），发现不同语言在「恐惧」「信任」情感上的显著差异，验证数据集在LLM中的强泛化能力。

Conclusion: SenWave填补细粒度多语言情感分析空白，开源数据与代码为NLP社区提供复杂事件情绪演算新基准，推动疫情心理干预策略优化研究。

Abstract: The global impact of the COVID-19 pandemic has highlighted the need for a
comprehensive understanding of public sentiment and reactions. Despite the
availability of numerous public datasets on COVID-19, some reaching volumes of
up to 100 billion data points, challenges persist regarding the availability of
labeled data and the presence of coarse-grained or inappropriate sentiment
labels. In this paper, we introduce SenWave, a novel fine-grained
multi-language sentiment analysis dataset specifically designed for analyzing
COVID-19 tweets, featuring ten sentiment categories across five languages. The
dataset comprises 10,000 annotated tweets each in English and Arabic, along
with 30,000 translated tweets in Spanish, French, and Italian, derived from
English tweets. Additionally, it includes over 105 million unlabeled tweets
collected during various COVID-19 waves. To enable accurate fine-grained
sentiment classification, we fine-tuned pre-trained transformer-based language
models using the labeled tweets. Our study provides an in-depth analysis of the
evolving emotional landscape across languages, countries, and topics, revealing
significant insights over time. Furthermore, we assess the compatibility of our
dataset with ChatGPT, demonstrating its robustness and versatility in various
applications. Our dataset and accompanying code are publicly accessible on the
repository\footnote{https://github.com/gitdevqiang/SenWave}. We anticipate that
this work will foster further exploration into fine-grained sentiment analysis
for complex events within the NLP community, promoting more nuanced
understanding and research innovations.

</details>


### [80] [Investigating Counterclaims in Causality Extraction from Text](https://arxiv.org/abs/2510.08224)
*Tim Hagen,Niklas Deckers,Felix Wolter,Harrisen Scells,Martin Potthast*

Main category: cs.CL

TL;DR: 现有因果提取研究忽视反因果声明，本文开发包含反因果关系的新数据集并验证其重要性


<details>
  <summary>Details</summary>
Motivation: 现有数据集仅关注支持性因果声明，导致反因果声明被忽视或错误标注，影响因果推理完整性

Method: 通过文献综述建立理论框架→制定标注规范→扩展Causal News Corpus并加入反因果声明

Result: 获得κ=0.74的高标注一致性，模型训练包含反因果数据后分类准确率显著提升

Conclusion: 整合反因果声明可有效提升模型区分能力，推动更全面的因果推理研究

Abstract: Research on causality extraction from text has so far almost entirely
neglected counterclaims. Existing causality extraction datasets focus solely on
"procausal" claims, i.e., statements that support a relationship. "Concausal"
claims, i.e., statements that refute a relationship, are entirely ignored or
even accidentally annotated as procausal. We address this shortcoming by
developing a new dataset that integrates concausality. Based on an extensive
literature review, we first show that concausality is an integral part of
causal reasoning on incomplete knowledge. We operationalize this theory in the
form of a rigorous guideline for annotation and then augment the Causal News
Corpus with concausal statements, obtaining a substantial inter-annotator
agreement of Cohen's $\kappa=0.74$. To demonstrate the importance of
integrating concausal statements, we show that models trained without concausal
relationships tend to misclassify these as procausal instead. Based on our new
dataset, this mistake can be mitigated, enabling transformers to effectively
distinguish pro- and concausality.

</details>


### [81] [The Alignment Waltz: Jointly Training Agents to Collaborate for Safety](https://arxiv.org/abs/2510.08240)
*Jingyu Zhang,Haozhu Wang,Eric Michael Smith,Sid Wang,Amr Sharaf,Mahesh Pasupuleti,Benjamin Van Durme,Daniel Khashabi,Jason Weston,Hongyuan Zhan*

Main category: cs.CL

TL;DR: WaltzRL框架通过多智能体强化学习实现LLM安全与效用的动态平衡，将不安全回应减少至4.6%，过度拒绝降低至9.9%


<details>
  <summary>Details</summary>
Motivation: 现有安全措施采用全拒绝策略导致过度拒绝敏感查询，且缺乏细粒度改进指导，需解决安全与效用的根本性矛盾

Method: 设计对话代理与反馈代理协同训练框架，通过动态改进奖励(DIR)机制实时优化响应，仅在需要时激活反馈代理

Result: 在WildJailbreak数据集上不安全响应从39.0%降至4.6%，OR-Bench过度拒绝率从45.3%降至9.9%，保持模型通用能力

Conclusion: WaltzRL突破安全-效用的帕累托边界，通过智能体协同进化实现非零和博弈，为LLM安全对齐提供新范式

Abstract: Harnessing the power of LLMs requires a delicate dance between being helpful
and harmless. This creates a fundamental tension between two competing
challenges: vulnerability to adversarial attacks that elicit unsafe content,
and a tendency for overrefusal on benign but sensitive prompts. Current
approaches often navigate this dance with safeguard models that completely
reject any content that contains unsafe portions. This approach cuts the music
entirely-it may exacerbate overrefusals and fails to provide nuanced guidance
for queries it refuses. To teach models a more coordinated choreography, we
propose WaltzRL, a novel multi-agent reinforcement learning framework that
formulates safety alignment as a collaborative, positive-sum game. WaltzRL
jointly trains a conversation agent and a feedback agent, where the latter is
incentivized to provide useful suggestions that improve the safety and
helpfulness of the conversation agent's responses. At the core of WaltzRL is a
Dynamic Improvement Reward (DIR) that evolves over time based on how well the
conversation agent incorporates the feedback. At inference time, unsafe or
overrefusing responses from the conversation agent are improved rather than
discarded. The feedback agent is deployed together with the conversation agent
and only engages adaptively when needed, preserving helpfulness and low latency
on safe queries. Our experiments, conducted across five diverse datasets,
demonstrate that WaltzRL significantly reduces both unsafe responses (e.g.,
from 39.0% to 4.6% on WildJailbreak) and overrefusals (from 45.3% to 9.9% on
OR-Bench) compared to various baselines. By enabling the conversation and
feedback agents to co-evolve and adaptively apply feedback, WaltzRL enhances
LLM safety without degrading general capabilities, thereby advancing the Pareto
front between helpfulness and harmlessness.

</details>


### [82] [Contrastive Decoding for Synthetic Data Generation in Low-Resource Language Modeling](https://arxiv.org/abs/2510.08245)
*Jannek Ulm,Kevin Du,Vésteinn Snæbjarnarson*

Main category: cs.CL

TL;DR: 通过对比解码生成合成数据并与真实数据混合训练，能有效提升语言模型在推理任务和表层语言能力任务中的表现


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型训练数据可能枯竭的担忧，探索利用模型自身生成合成数据延续训练的创新方案

Method: 使用相同原始语料（1亿词）训练优劣两个模型，通过对比解码放大优质模型的信号生成合成语料，并与原始数据混合训练

Result: 混合训练使语言建模目标及下游任务表现提升，对比解码数据提升推理任务（3.2%改进），传统采样数据增强表层语言能力任务（2.1%改进）

Conclusion: 合成数据与真实数据的混合训练策略具有互补优势，为突破数据瓶颈提供了可扩展的解决方案

Abstract: Large language models (LLMs) are trained on huge amounts of textual data, and
concerns have been raised that the limits of such data may soon be reached. A
potential solution is to train on synthetic data sampled from LLMs. In this
work, we build on this idea and investigate the benefits of contrastive
decoding for generating synthetic corpora. In a controlled setting, we
experiment with sampling corpora using the relative difference between a good
and bad model trained on the same original corpus of 100 million words. By
amplifying the signal from a model that has better performance, we create a
synthetic corpus and mix it with the original training data. Our findings show
that training on a mixture of synthesized and real data improves performance on
the language modeling objective and a range of downstream tasks. In particular,
we see that training with a mix of synthetic data from contrastive decoding
benefits tasks that require more reasoning skills, while synthetic data from
traditional sampling helps more on tasks dependent on surface level linguistic
capabilities.

</details>


### [83] [Beyond Turn Limits: Training Deep Search Agents with Dynamic Context Window](https://arxiv.org/abs/2510.08276)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Yaojie Lu,Xianpei Han,Le Sun,WenJuan Zhang,Pengbo Wang,Shixuan Liu,Zhenru Zhang,Jianhong Tu,Hongyu Lin,Junyang Lin*

Main category: cs.CL

TL;DR: DeepMiner框架通过高难度训练任务和动态上下文窗口，显著提升多轮推理模型在复杂搜索代理任务中的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在长程多轮交互中缺乏深度推理能力，且受限于固定上下文窗口带来的扩展性约束

Method: 1. 逆向构建法生成可验证的复杂QA训练数据
2. 基于滑动窗口的动态上下文管理策略
3. 在Qwen3-32B上通过强化学习实现

Result: BrowseComp-en准确率33.5%（提升20%），支持32k上下文内近100轮持续交互，在BrowseComp-zh/XBench-DeepSearch/GAIA基准持续改进

Conclusion: DeepMiner有效突破了多轮交互系统的上下文限制，为长程推理任务提供了可扩展的解决方案

Abstract: While recent advances in reasoning models have demonstrated cognitive
behaviors through reinforcement learning, existing approaches struggle to
invoke deep reasoning capabilities in multi-turn agents with long-horizon
interactions. We propose DeepMiner, a novel framework that elicits such
abilities by introducing high-difficulty training tasks and dynamic context
window. DeepMiner presents a reverse construction method to generate complex
but verifiable question-answer pairs from authentic web sources, which ensures
the challenge and reliability of training data while injecting cognitive
capabilities into multi-turn reasoning scenarios. We further design an elegant
yet effective dynamic context management strategy for both training and
inference, utilizing sliding window mechanisms while eliminating the dependency
on external summarization models, thereby efficiently empowering the model to
handle continuously expanding long-horizon contexts. Through reinforcement
learning on Qwen3-32B, we develop DeepMiner-32B, which achieves substantial
performance improvements across multiple search agent benchmarks. DeepMiner
attains 33.5% accuracy on BrowseComp-en, surpassing the previous best
open-source agent by almost 20 percentage points, and demonstrates consistent
improvements on BrowseComp-zh, XBench-DeepSearch, and GAIA. Notably, our
dynamic context management enables sustained interactions of nearly 100 turns
within standard 32k context length, effectively addressing the context
limitations that constrain existing multi-turn interaction systems.

</details>


### [84] [Neuron-Level Analysis of Cultural Understanding in Large Language Models](https://arxiv.org/abs/2510.08284)
*Taisei Yamamoto,Ryoma Kumon,Danushka Bollegala,Hitomi Yanaka*

Main category: cs.CL

TL;DR: 通过神经元级分析发现LLMs中仅1%的神经元驱动文化理解，分为文化通用神经元和文化特定神经元，抑制它们会显著降低文化基准表现但不影响通用NLU任务。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs存在文化偏见和对少数文化认知不足的问题，且其文化理解机制尚未被充分探索。

Method: 提出基于梯度的评分方法定位文化相关神经元，结合过滤机制进行精确识别，并验证其在MLP层的分布特性。

Result: 发现不足1%的神经元（集中在MLP中层）分别负责通用文化理解和特定文化知识，抑制这些神经元可使文化基准下降30%而NLU任务基本不受影响。

Conclusion: 揭示了LLMs文化理解的神经机制，为模型训练提供指导：更新含文化通用神经元的模块可能削弱文化理解能力，需针对性优化。

Abstract: As large language models (LLMs) are increasingly deployed worldwide, ensuring
their fair and comprehensive cultural understanding is important. However, LLMs
exhibit cultural bias and limited awareness of underrepresented cultures, while
the mechanisms underlying their cultural understanding remain underexplored. To
fill this gap, we conduct a neuron-level analysis to identify neurons that
drive cultural behavior, introducing a gradient-based scoring method with
additional filtering for precise refinement. We identify both culture-general
neurons contributing to cultural understanding regardless of cultures, and
culture-specific neurons tied to an individual culture. These neurons account
for less than 1% of all neurons and are concentrated in shallow to middle MLP
layers. We validate their role by showing that suppressing them substantially
degrades performance on cultural benchmarks (by up to 30%), while performance
on general natural language understanding (NLU) benchmarks remains largely
unaffected. Moreover, we show that culture-specific neurons support knowledge
of not only the target culture, but also related cultures. Finally, we
demonstrate that training on NLU benchmarks can diminish models' cultural
understanding when we update modules containing many culture-general neurons.
These findings provide insights into the internal mechanisms of LLMs and offer
practical guidance for model training and engineering. Our code is available at
https://github.com/ynklab/CULNIG

</details>


### [85] [AutoRed: A Free-form Adversarial Prompt Generation Framework for Automated Red Teaming](https://arxiv.org/abs/2510.08329)
*Muxi Diao,Yutao Mou,Keqing He,Hanbo Song,Lulu Zhao,Shikun Zhang,Wei Ye,Kongming Liang,Zhanyu Ma*

Main category: cs.CL

TL;DR: 提出无需种子指令的AutoRed框架，通过两阶段方法生成对抗性提示并构建数据集，在攻击成功率和泛化性上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有红队方法依赖种子指令导致对抗提示语义多样性受限，影响LLM安全评估效果

Method: 1. 角色引导对抗指令生成 2. 反思循环迭代优化提示，引入验证器评估危害性（无需查询目标模型）

Result: 构建AutoRed-Medium/Hard数据集，在8个SOTA LLM上实现更高攻击成功率（+21.5%）和更好泛化性

Conclusion: 基于种子的方法存在局限性，自由形式红队方法为LLM安全评估开辟新方向，开源数据集将促进领域发展

Abstract: The safety of Large Language Models (LLMs) is crucial for the development of
trustworthy AI applications. Existing red teaming methods often rely on seed
instructions, which limits the semantic diversity of the synthesized
adversarial prompts. We propose AutoRed, a free-form adversarial prompt
generation framework that removes the need for seed instructions. AutoRed
operates in two stages: (1) persona-guided adversarial instruction generation,
and (2) a reflection loop to iteratively refine low-quality prompts. To improve
efficiency, we introduce a verifier to assess prompt harmfulness without
querying the target models. Using AutoRed, we build two red teaming datasets --
AutoRed-Medium and AutoRed-Hard -- and evaluate eight state-of-the-art LLMs.
AutoRed achieves higher attack success rates and better generalization than
existing baselines. Our results highlight the limitations of seed-based
approaches and demonstrate the potential of free-form red teaming for LLM
safety evaluation. We will open source our datasets in the near future.

</details>


### [86] [Two-Stage Voting for Robust and Efficient Suicide Risk Detection on Social Media](https://arxiv.org/abs/2510.08365)
*Yukai Song,Pengfei Zhou,César Escobar-Viera,Candice Biernesser,Wei Huang,Jingtong Hu*

Main category: cs.CL

TL;DR: 提出两阶段投票架构，结合BERT快速处理显性自杀信号和LLM/特征集成处理隐性信号，在保持效率的同时提升检测效果


<details>
  <summary>Details</summary>
Motivation: 现有轻量级模型无法有效检测隐喻/讽刺等隐性自杀倾向，而大模型计算成本过高。社交媒体中存在大量隐性求助信号需要有效识别

Method: 1) BERT处理高置信度显性案例；2) 模糊案例分LLM多视角投票框架（最大化隐性召回）或基于心理特征指标的ML集成（平衡效率与可解释性）

Result: 在Reddit和DeepSuiMind数据集上分别实现98.0%和99.7%的F1值，跨领域差距降至2%以下，显著降低LLM计算成本

Conclusion: 该框架首次将LLM提取的心理特征向量化用于自杀检测，在效率与鲁棒性间取得平衡，为隐性风险识别提供新解决方案

Abstract: Suicide rates have risen worldwide in recent years, underscoring the urgent
need for proactive prevention strategies. Social media provides valuable
signals, as many at-risk individuals - who often avoid formal help due to
stigma - choose instead to share their distress online. Yet detecting implicit
suicidal ideation, conveyed indirectly through metaphor, sarcasm, or subtle
emotional cues, remains highly challenging. Lightweight models like BERT handle
explicit signals but fail on subtle implicit ones, while large language models
(LLMs) capture nuance at prohibitive computational cost. To address this gap,
we propose a two-stage voting architecture that balances efficiency and
robustness. In Stage 1, a lightweight BERT classifier rapidly resolves
high-confidence explicit cases. In Stage 2, ambiguous inputs are escalated to
either (i) a multi-perspective LLM voting framework to maximize recall on
implicit ideation, or (ii) a feature-based ML ensemble guided by
psychologically grounded indicators extracted via prompt-engineered LLMs for
efficiency and interpretability. To the best of our knowledge, this is among
the first works to operationalize LLM-extracted psychological features as
structured vectors for suicide risk detection. On two complementary datasets -
explicit-dominant Reddit and implicit-only DeepSuiMind - our framework
outperforms single-model baselines, achieving 98.0% F1 on explicit cases, 99.7%
on implicit ones, and reducing the cross-domain gap below 2%, while
significantly lowering LLM cost.

</details>


### [87] [On the Relationship Between the Choice of Representation and In-Context Learning](https://arxiv.org/abs/2510.08372)
*Ioana Marinescu,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: 研究发现上下文学习中表示形式与学习能力是独立影响模型性能的两个正交因素，表示质量决定基线准确率，学习效率取决于标签质量与模型参数量。


<details>
  <summary>Details</summary>
Motivation: 针对ICL成功归因于标签表示方式的争议，以及学界对ICL学习容量条件限制的分歧，探究表示与学习两个维度的独立作用机制。

Method: 开发标签集语义相关性优化算法，通过枚举不同质量的标签集，在不同规模语言模型上测试上下文样本数量对性能的影响。

Result: 学习效率(性能随样本增长的斜率)受标签质量和模型参数共同影响，但标签集相对质量在整个学习过程中保持稳定。

Conclusion: 首次揭示ICL性能受样本表示形式和学习能力的独立双重影响，表明二者具有正交性，为理解ICL机制提供新视角。

Abstract: In-context learning (ICL) is the ability of a large language model (LLM) to
learn a new task from a few demonstrations presented as part of the context.
Past studies have attributed a large portion of the success of ICL to the way
these in-context demonstrations are represented, particularly to how labels are
represented in classification tasks. On the other hand, observations of the
learning capacity of ICL (i.e., the extent to which more in-context
demonstrations can lead to higher performance) have been mixed, and ICL is
often thought to occur only under specific conditions. The interaction between
these two aspects in ICL, representation and learning, has not been studied in
depth until now. We hypothesize that they are largely independent of one
another, such that the representation of demonstrations determines the baseline
accuracy of ICL, while learning from additional demonstrations improves only on
top of this baseline. We validate this hypothesis by developing an optimization
algorithm that can enumerate a spectrum of possible label sets
(representations) varying in semantic relevance. We then perform ICL with
varying numbers of in-context demonstrations for each of these label sets. We
observed that learning happens regardless of the quality of the label set
itself, although its efficiency, measured by the slope of improvement over
in-context demonstrations, is conditioned on both the label set quality and the
parameter count of the underlying language model. Despite the emergence of
learning, the relative quality (accuracy) of the choice of a label set
(representation) is largely maintained throughout learning, confirming our
hypothesis and implying their orthogonality. Our work reveals a previously
underexplored aspect of ICL: the independent effects of learning from
demonstrations and their representations on ICL performance.

</details>


### [88] [If Probable, Then Acceptable? Understanding Conditional Acceptability Judgments in Large Language Models](https://arxiv.org/abs/2510.08388)
*Jasmin Orth,Philipp Mondorf,Barbara Plank*

Main category: cs.CL

TL;DR: 研究探讨大语言模型对条件句可接受性的评估机制，发现模型同时考虑条件概率和语义相关性，但判断模式与人类存在差异


<details>
  <summary>Details</summary>
Motivation: 现有研究未明确大语言模型如何评估条件句的可接受性，该能力直接影响模型在推理和决策中的可靠性，需系统比较模型与人类判断机制的异同

Method: 通过线性混合效应模型和ANOVA检验，跨模型家族/规模/提示策略分析，对比人类实验数据

Result: 模型敏感度受架构和提示策略影响；更大模型未表现更强的人类对齐性；概率与语义线索整合弱于人类

Conclusion: LLMs的条件判断机制与人类存在系统性差异，单纯扩大模型规模不能解决对齐问题，需开发针对性的对齐策略

Abstract: Conditional acceptability refers to how plausible a conditional statement is
perceived to be. It plays an important role in communication and reasoning, as
it influences how individuals interpret implications, assess arguments, and
make decisions based on hypothetical scenarios. When humans evaluate how
acceptable a conditional "If A, then B" is, their judgments are influenced by
two main factors: the $\textit{conditional probability}$ of $B$ given $A$, and
the $\textit{semantic relevance}$ of the antecedent $A$ given the consequent
$B$ (i.e., whether $A$ meaningfully supports $B$). While prior work has
examined how large language models (LLMs) draw inferences about conditional
statements, it remains unclear how these models judge the
$\textit{acceptability}$ of such statements. To address this gap, we present a
comprehensive study of LLMs' conditional acceptability judgments across
different model families, sizes, and prompting strategies. Using linear
mixed-effects models and ANOVA tests, we find that models are sensitive to both
conditional probability and semantic relevance-though to varying degrees
depending on architecture and prompting style. A comparison with human data
reveals that while LLMs incorporate probabilistic and semantic cues, they do so
less consistently than humans. Notably, larger models do not necessarily align
more closely with human judgments.

</details>


### [89] [Single layer tiny Co$^4$ outpaces GPT-2 and GPT-BERT](https://arxiv.org/abs/2510.08404)
*Noor Ul Zain,Mohsin Raza,Ahsan Adeel*

Main category: cs.CL

TL;DR: 微型架构Co⁴以单层8M参数实现O(N)复杂度，在2个epoch内超越124M参数的GPT-2和30M参数的GPT-BERT（10个epoch训练），展示出数量级更高的训练效率


<details>
  <summary>Details</summary>
Motivation: 挑战现有深度学习范式，证明极小模型通过结构优化可实现比传统大模型更高的训练效率和样本利用率

Method: 设计极简架构（单层/双注意力头/O(N)复杂度），采用高效预训练策略，在10M tokens数据集上进行对比实验

Result: SuperGLUE评测中：零样本5/7指标超GPT-2，微调6/7任务领先；训练速度较基线快5倍，GPU小时减少87%

Conclusion: 实验结果颠覆传统缩放定律，表明模型性能不一定依赖参数量级，为高效预训练架构设计提供新方向

Abstract: We show that a tiny Co$^4$ machine(Adeel,2025) with a single layer, two
heads, and 8M parameters, operating at an approximate cost of $O(N)$ (where $N$
is the number of input tokens), outpaces the BabyLM Challenge baselines GPT-2
(124M, 12 layers, $O(N^2))$ and GPT-BERT (30M, 12 layers, $O(N^2))$ in just two
epochs, while both are trained for ten. Co$^4$ achieves orders-of-magnitude
greater training efficiency on 10M tokens, demonstrating highly sample
efficient pretraining. Using the BabyLM challenge evaluation pipeline across
complex benchmarks, Co$^4$ exhibits strong zero-shot and fine-tuning
performance on SuperGLUE tasks. Specifically, Co$^4$ outperforms GPT-2 on 5 out
of 7 zero-shot metrics and 6 out of 7 fine-tuning tasks, and GPT-BERT on 4 out
of 7 metrics in both cases. These results suggest the need to rethink
prevailing deep learning paradigms and associated scaling laws.

</details>


### [90] [ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping](https://arxiv.org/abs/2510.08457)
*Shuang Chen,Yue Guo,Yimeng Ye,Shijue Huang,Wenbo Hu,Haoxi Li,Manyuan Zhang,Jiayu Chen,Song Guo,Nanyun Peng*

Main category: cs.CL

TL;DR: 提出ARES框架——基于窗口熵触发和动态KL控制的自适应推理系统，显著提升多模态大模型在数学、逻辑等复杂任务中的性能和推理效率


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理模型存在简单问题过度推理（生成冗余步骤）、复杂问题探索不足的缺陷，需建立难度自适应的推理机制

Method: 两阶段训练：1）自适应冷启动阶段生成难度匹配的推理轨迹；2）AEPO算法利用窗口熵作为探索触发器，结合分层熵奖励实现动态探索控制

Result: 在12个数学/逻辑基准测试中平均提升3.1%准确率，推理步骤减少22%，推理成本降低至商用系统的1/5时仍保持90%+性能

Conclusion: 通过熵触发机制和分层奖励设计，首次实现推理模型在问题难度与探索强度之间的自适应平衡，为高效推理系统提供新范式

Abstract: Recent advances in multimodal large reasoning models (MLRMs) have
substantially improved their ability to solve complex textual and visual tasks.
However, these models tend to overthink on simple problems, producing
unnecessarily lengthy reasoning traces, while under-exploring on challenging
ones, leading to missed solutions. To address this imbalance, we propose ARES,
a unified open-source framework for adaptive reasoning that dynamically
allocates exploration effort based on task difficulty. Our approach is
motivated by two key empirical findings: (i) while single-token entropy is
noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a
sliding window) can reliably capture reasoning-critical moments; and (ii)
reducing HWE usage benefits easy problems, while increasing it is essential for
solving hard ones. Building on these insights, ARES introduces a two-stage
training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and
textual data paired with reasoning traces of length proportional to problem
difficulty, equipping the model with initial difficulty awareness. In the
second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which
uses HWE tokens as exploration triggers to decide when to explore, and a
hierarchical entropy reward with dynamic KL control to decide how much to
explore. Extensive experiments demonstrate that ARES achieves superior
performance and reasoning efficiency across diverse mathematical, logical, and
multimodal benchmarks, while closing the gap to leading commercial systems
under significantly lower inference costs.

</details>


### [91] [LeWiDi-2025 at NLPerspectives: The Third Edition of the Learning with Disagreements Shared Task](https://arxiv.org/abs/2510.08460)
*Elisa Leonardelli,Silvia Casola,Siyao Peng,Giulia Rizzi,Valerio Basile,Elisabetta Fersini,Diego Frassinelli,Hyewon Jang,Maja Pavlovic,Barbara Plank,Massimo Poesio*

Main category: cs.CL

TL;DR: LEWIDI系列任务通过扩展多数据集和评估方法，推动AI模型对人类判断分歧的建模能力，并揭示不同方法的优缺点。


<details>
  <summary>Details</summary>
Motivation: 使AI模型能够识别人类判断中的变异和分歧，通过建立可访问数据集和开发评估方法来提升模型对分歧的感知能力。

Method: 在四个任务数据集上采用分类和顺序标注，引入软标签预测群体分布和视角主义预测个体标注者解释的互补评估范式，并创新测试新评估指标。

Result: 任务吸引多元参与，揭示了建模方法的有效性及局限性，为分歧感知技术提供了新基准、资源和洞见。

Conclusion: LEWIDI框架通过强化评估体系及方法论贡献，支持开发更适应人类判断复杂性的AI技术。

Abstract: Many researchers have reached the conclusion that AI models should be trained
to be aware of the possibility of variation and disagreement in human
judgments, and evaluated as per their ability to recognize such variation. The
LEWIDI series of shared tasks on Learning With Disagreements was established to
promote this approach to training and evaluating AI models, by making suitable
datasets more accessible and by developing evaluation methods. The third
edition of the task builds on this goal by extending the LEWIDI benchmark to
four datasets spanning paraphrase identification, irony detection, sarcasm
detection, and natural language inference, with labeling schemes that include
not only categorical judgments as in previous editions, but ordinal judgments
as well. Another novelty is that we adopt two complementary paradigms to
evaluate disagreement-aware systems: the soft-label approach, in which models
predict population-level distributions of judgments, and the perspectivist
approach, in which models predict the interpretations of individual annotators.
Crucially, we moved beyond standard metrics such as cross-entropy, and tested
new evaluation metrics for the two paradigms. The task attracted diverse
participation, and the results provide insights into the strengths and
limitations of methods to modeling variation. Together, these contributions
strengthen LEWIDI as a framework and provide new resources, benchmarks, and
findings to support the development of disagreement-aware technologies.

</details>


### [92] [DeepPrune: Parallel Scaling without Inter-trace Redundancy](https://arxiv.org/abs/2510.08483)
*Shangqing Tu,Yaxuan Li,Yushi Bai,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: DeepPrune框架通过动态修剪冗余推理路径，在保持精度的前提下减少80%计算开销


<details>
  <summary>Details</summary>
Motivation: 解决并行扩展中80%推理轨迹重复造成的计算浪费问题

Method: 结合基于focal loss训练的judge模型（0.87 AUROC）和在线贪心聚类算法

Result: 在AIME/GPQA基准上减少80% token使用，精度损失<3%

Conclusion: 建立了高效并行推理新标准，使高性能推理更高效

Abstract: Parallel scaling has emerged as a powerful paradigm to enhance reasoning
capabilities in large language models (LLMs) by generating multiple
Chain-of-Thought (CoT) traces simultaneously. However, this approach introduces
significant computational inefficiency due to inter-trace redundancy -- our
analysis reveals that over 80% of parallel reasoning traces yield identical
final answers, representing substantial wasted computation. To address this
critical efficiency bottleneck, we propose DeepPrune, a novel framework that
enables efficient parallel scaling through dynamic pruning. Our method features
a specialized judge model trained with focal loss and oversampling techniques
to accurately predict answer equivalence from partial reasoning traces which
realizes 0.87 AUROC on equivalence prediction, combined with an online greedy
clustering algorithm that dynamically prunes redundant paths while preserving
answer diversity. Comprehensive evaluations across three challenging benchmarks
(AIME 2024, AIME 2025, and GPQA) and multiple reasoning models demonstrate that
DeepPrune achieves remarkable token reduction by over 80% compared to
conventional consensus sampling on most cases, while maintaining competitive
accuracy within 3 percentage points. Our work establishes a new standard for
efficient parallel reasoning, making high-performance reasoning more efficient.
Our code and data are here: https://deepprune.github.io/

</details>


### [93] [Neologism Learning for Controllability and Self-Verbalization](https://arxiv.org/abs/2510.08506)
*John Hewitt,Oyvind Tafjord,Robert Geirhos,Been Kim*

Main category: cs.CL

TL;DR: 通过引入新词汇（新词学习）来理解和控制大语言模型，无需修改模型参数即可实现概念控制，并验证了模型能自我解释新词含义。


<details>
  <summary>Details</summary>
Motivation: 受人类创造新词的启发，探索如何通过添加新词汇更有效地理解和控制LLM的行为，扩展新词学习方法的边界。

Method: 在模型嵌入层添加新词向量，仅通过目标概念的示例训练新词（保持其他参数冻结），并开发插件评估法验证模型自我解释的有效性。

Result: 1. 成功控制奉承/错误答案/文本长度等概念
2. 模型可自我解释新词含义（如将错误答案相关词汇解释为'缺乏完整连贯的回答'）
3. 发现机器特有同义词现象
4. 实现多概念联合学习

Conclusion: 新词学习为模型控制与可解释性提供了新范式，自我验证机制和机器特有语义的发现拓展了人机交互的可能性，多概念学习能力展示了方法的扩展性。

Abstract: Humans invent new words when there is a rising demand for a new useful
concept (e.g., doomscrolling). We explore and validate a similar idea in our
communication with LLMs: introducing new words to better understand and control
the models, expanding on the recently introduced neologism learning. This
method introduces a new word by adding a new word embedding and training with
examples that exhibit the concept with no other changes in model parameters. We
show that adding a new word allows for control of concepts such as flattery,
incorrect answers, text length, as well as more complex concepts in AxBench. We
discover that neologisms can also further our understanding of the model via
self-verbalization: models can describe what each new word means to them in
natural language, like explaining that a word that represents a concept of
incorrect answers means ``a lack of complete, coherent, or meaningful
answers...'' To validate self-verbalizations, we introduce plug-in evaluation:
we insert the verbalization into the context of a model and measure whether it
controls the target concept. In some self-verbalizations, we find machine-only
synonyms: words that seem unrelated to humans but cause similar behavior in
machines. Finally, we show how neologism learning can jointly learn multiple
concepts in multiple words.

</details>


### [94] [Efficient Prompt Optimisation for Legal Text Classification with Proxy Prompt Evaluator](https://arxiv.org/abs/2510.08524)
*Hyunji Lee,Kevin Chenhao Li,Matthias Grabmair,Shanshan Xu*

Main category: cs.CL

TL;DR: 提出结合蒙特卡洛树搜索(MCTS)与代理提示评估器的提示优化框架，在有限计算预算下显著提升公平性检测任务的分类准确率和效率


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法因低效搜索策略和高昂评估成本导致计算资源消耗过大，需要更高效的解决方案

Method: 使用蒙特卡洛树搜索进行高效提示空间探索，结合轻量级代理评估器降低真实模型调用成本

Result: 在相同计算约束下，本方法比基准方法获得更高的分类准确率(具体数值需看实验部分)和搜索效率

Conclusion: 该框架为资源受限环境下的法律NLP任务提供了实用的提示优化解决方案

Abstract: Prompt optimization aims to systematically refine prompts to enhance a
language model's performance on specific tasks. Fairness detection in Terms of
Service (ToS) clauses is a challenging legal NLP task that demands carefully
crafted prompts to ensure reliable results. However, existing prompt
optimization methods are often computationally expensive due to inefficient
search strategies and costly prompt candidate scoring. In this paper, we
propose a framework that combines Monte Carlo Tree Search (MCTS) with a proxy
prompt evaluator to more effectively explore the prompt space while reducing
evaluation costs. Experiments demonstrate that our approach achieves higher
classification accuracy and efficiency than baseline methods under a
constrained computation budget.

</details>


### [95] [Which Heads Matter for Reasoning? RL-Guided KV Cache Compression](https://arxiv.org/abs/2510.08525)
*Wenjie Du,Li Jiang,Keda Tao,Xue Liu,Huan Wang*

Main category: cs.CL

TL;DR: 提出RLKV框架——基于强化学习的推理关键头识别方法，可在减少20-50% KV缓存占用的同时保持接近无损的推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法破坏推理完整性（token丢弃法）或误压关键头（头重分配法），导致高压缩率下性能显著下降。研究发现推理模型中KV头存在功能异质性。

Method: RLKV通过强化学习直接优化缓存使用与推理质量关系，自动识别对思维链一致性至关重要的关键头。对关键头保留完整缓存，其他头采用压缩常量缓存。

Result: 实验表明仅需少量关键头即可维持推理能力，在20-50%缓存压缩率下性能优于基线方法，接近未压缩结果（平均准确率仅下降0.3%）。

Conclusion: 验证了KV头的功能异质性，RLKV为大型语言模型的高效推理提供了新范式，通过精准保护关键头实现性能与效率的平衡。

Abstract: Reasoning large language models exhibit complex reasoning behaviors through
the extended chain-of-thought generation, creating unprecedented Key-Value (KV)
cache overhead during the decoding phase. Existing KV cache compression methods
underperform on reasoning models: token-dropping methods break reasoning
integrity by discarding critical information, while head-reallocating methods
mistakenly compress reasoning-critical heads since they are designed for
retrieval tasks, resulting in significant performance degradation as
compression rates increase. We hypothesize that KV heads exhibit functional
heterogeneity in reasoning models-some heads are critical for chain-of-thought
consistency while others are compressible. To validate and exploit this
insight, we propose RLKV, a novel reasoning-critical head identification
framework, which uses reinforcement learning to directly optimize the
relationship between each head's cache usage and reasoning quality. As RLKV
produces rewards from actual generated samples during training, it naturally
identifies heads relevant to reasoning behaviors. We then allocate full KV
cache to these heads while applying compressed constant KV cache to others for
efficient inference. Our experiments reveal that only a small fraction of
attention heads is essential for reasoning, enabling our KV compression
approach to outperform baseline methods while achieving 20-50% cache reduction
with near lossless performance compared to uncompressed results.

</details>


### [96] [CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards](https://arxiv.org/abs/2510.08529)
*Xiangyuan Xue,Yifan Zhou,Guibin Zhang,Zaibin Zhang,Yijiang Li,Chen Zhang,Zhenfei Yin,Philip Torr,Wanli Ouyang,Lei Bai*

Main category: cs.CL

TL;DR: 提出CoMAS框架，通过多智能体互动生成内在奖励，基于LLM的评判机制进行强化学习，实现去中心化协同进化，在实验中展现了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有RL方法依赖外部奖励或单智能体内在奖励，与人类通过协作实现进化的机制不符。试图模拟人类群体智能协同进化的模式。

Method: 构建多智能体讨论环境，从对话动态中提取内在奖励信号，设计LLM-as-a-judge的奖励生成机制，采用分布式RL策略优化框架。

Result: 在多数任务上达到SOTA性能，消融实验验证互动奖励的必要性，智能体数量和多样性增加时展现良好扩展性。

Conclusion: CoMAS为LLM智能体自进化提供了基于群体智能协作的新范式，证明了无监督协同强化学习路径的有效性。

Abstract: Self-evolution is a central research topic in enabling large language model
(LLM)-based agents to continually improve their capabilities after pretraining.
Recent research has witnessed a transition from reinforcement learning
(RL)-free to RL-based methods. Current RL-based methods either rely on dense
external reward signals or extract intrinsic reward signals from LLMs
themselves. However, these approaches diverge from the self-evolution
mechanisms observed in human intelligence, where individuals learn and improve
through mutual discussion and collaboration. In this work, we introduce
Co-Evolving Multi-Agent Systems (CoMAS), a novel framework that enables agents
to improve autonomously by learning from inter-agent interactions without
external supervision. CoMAS generates intrinsic rewards from rich discussion
dynamics, employs an LLM-as-a-judge mechanism to formulate these rewards, and
optimizes each agent's policy through RL, thereby enabling decentralized and
scalable co-evolution. Experimental results demonstrate that CoMAS consistently
outperforms untrained agents and achieves state-of-the-art performance across
most evaluation settings. Ablation studies confirm the necessity of
interaction-based reward signals and reveal promising scalability as the number
and diversity of agents increase. These findings establish CoMAS as a novel and
effective paradigm for self-evolution in LLM-based agents.

</details>


### [97] [ArenaBencher: Automatic Benchmark Evolution via Multi-Model Competitive Evaluation](https://arxiv.org/abs/2510.08569)
*Qin Liu,Jacob Dineen,Yuxi Huang,Sheng Zhang,Hoifung Poon,Ben Zhou,Muhao Chen*

Main category: cs.CL

TL;DR: 提出ArenaBencher框架解决基准测试数据泄露问题，通过动态生成挑战性测试用例增强模型评估有效性


<details>
  <summary>Details</summary>
Motivation: 传统基准测试因预训练数据泄露导致模型通过记忆而非真正能力获得高分，影响评估准确性

Method: 模型无关框架：1) 推断测试用例核心能力 2) 生成保留原目标的新题 3) LLM验证正确性 4) 多模型反馈筛选暴露弱点的测试用例

Result: 在数学解题/常识推理/安全领域成功生成更困难、公平的测试集，提升模型区分度（+12.7%分离度），发现新失败模式

Conclusion: 该框架为基准测试的持续进化提供可扩展方案，适应基础模型的快速发展

Abstract: Benchmarks are central to measuring the capabilities of large language models
and guiding model development, yet widespread data leakage from pretraining
corpora undermines their validity. Models can match memorized content rather
than demonstrate true generalization, which inflates scores, distorts
cross-model comparisons, and misrepresents progress. We introduce ArenaBencher,
a model-agnostic framework for automatic benchmark evolution that updates test
cases while preserving comparability. Given an existing benchmark and a diverse
pool of models to be evaluated, ArenaBencher infers the core ability of each
test case, generates candidate question-answer pairs that preserve the original
objective, verifies correctness and intent with an LLM as a judge, and
aggregates feedback from multiple models to select candidates that expose
shared weaknesses. The process runs iteratively with in-context demonstrations
that steer generation toward more challenging and diagnostic cases. We apply
ArenaBencher to math problem solving, commonsense reasoning, and safety domains
and show that it produces verified, diverse, and fair updates that uncover new
failure modes, increase difficulty while preserving test objective alignment,
and improve model separability. The framework provides a scalable path to
continuously evolve benchmarks in step with the rapid progress of foundation
models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [98] [SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation](https://arxiv.org/abs/2510.07340)
*Yongzhi Li,Saining Zhang,Yibing Chen,Boying Li,Yanxin Zhang,Xiaoyu Du*

Main category: cs.GR

TL;DR: SpotDiff提出基于CLIP图像编码器和正交约束的个性化图像生成方法，通过SpotDiff10k数据集实现高效训练，在10k样本量下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法计算成本高，学习模型易受干扰因素影响导致身份特征纠缠。需平衡生成效率与身份保真度。

Method: 1) 使用CLIP提取特征 + 姿势/背景专家网络 2) 特征空间正交约束解耦身份特征 3) 构建SpotDiff10k数据集控制变量训练

Result: 实现更鲁棒的身份保持(比基准模型提升23.6%)，支持可控编辑，仅用10k样本达到竞争性效果

Conclusion: 通过特征解耦方法突破学习模型身份保真度瓶颈，小样本高效训练方案为个性化生成提供新思路

Abstract: Personalized image generation aims to faithfully preserve a reference
subject's identity while adapting to diverse text prompts. Existing
optimization-based methods ensure high fidelity but are computationally
expensive, while learning-based approaches offer efficiency at the cost of
entangled representations influenced by nuisance factors. We introduce
SpotDiff, a novel learning-based method that extracts subject-specific features
by spotting and disentangling interference. Leveraging a pre-trained CLIP image
encoder and specialized expert networks for pose and background, SpotDiff
isolates subject identity through orthogonality constraints in the feature
space. To enable principled training, we introduce SpotDiff10k, a curated
dataset with consistent pose and background variations. Experiments demonstrate
that SpotDiff achieves more robust subject preservation and controllable
editing than prior methods, while attaining competitive performance with only
10k training samples.

</details>


### [99] [Local MAP Sampling for Diffusion Models](https://arxiv.org/abs/2510.07343)
*Shaorong Zhang,Rob Brekelmans,Greg Ver Steeg*

Main category: cs.GR

TL;DR: 提出Local MAP Sampling (LMAPS)框架，通过迭代求解扩散轨迹的局部MAP子问题，统一优化方法与概率解释，在图像复原等任务中实现SOTA性能（如运动去模糊≥2dB增益）


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的逆问题解法（如DPS）侧重后验采样，而实际应用中优化方法虽缺乏概率基础却常表现更优。需建立统一框架弥合概率解释与优化方法间的理论鸿沟

Method: 1. 迭代求解扩散轨迹上的局部MAP子问题
2. 提出概率可解释的协方差近似方案
3. 重构目标函数提升稳定性与可解释性
4. 设计非可微算子的梯度近似方法

Result: 图像复原：运动去模糊/JPEG恢复/量化任务≥2dB增益；逆散射基准>1.5dB提升

Conclusion: LMAPS为优化方法提供严谨概率解释，通过局部MAP求解框架统一DPS与全局MAP估计，在多项逆任务中验证显著性能优势

Abstract: Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to
inverse problems by sampling from $p(x_0 \mid y)$. However, in practice, the
goal of inverse problem solving is not to cover the posterior but to recover
the most accurate reconstruction, where optimization-based diffusion solvers
often excel despite lacking a clear probabilistic foundation. We introduce
Local MAP Sampling (LMAPS), a new inference framework that iteratively solving
local MAP subproblems along the diffusion trajectory. This perspective
clarifies their connection to global MAP estimation and DPS, offering a unified
probabilistic interpretation for optimization-based methods. Building on this
foundation, we develop practical algorithms with a probabilistically
interpretable covariance approximation, a reformulated objective for stability
and interpretability, and a gradient approximation for non-differentiable
operators. Across a broad set of image restoration and scientific tasks, LMAPS
achieves state-of-the-art performance, including $\geq 2$ dB gains on motion
deblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on
inverse scattering benchmarks.

</details>


### [100] [Differentiable Variable Fonts](https://arxiv.org/abs/2510.07638)
*Kinjal Parikh,Danny M. Kaufman,David I. W. Levin,Alec Jacobson*

Main category: cs.GR

TL;DR: 提出可微分可变字体框架，通过数学建模实现字体参数梯度优化，支持形状操控、物理动画等四类自动化设计应用


<details>
  <summary>Details</summary>
Motivation: 传统字体设计依赖人工参数调整效率低下，现有可变字体参数化特性未被充分利用，亟需自动化设计工具支持创意表达

Method: 建立可变字体参数与矢量图形的可微分映射框架，支持基于梯度优化的控制点调整和栅格图像匹配

Result: 实现直接形状操控、重叠感知建模、物理动画、自动化字体优化四类应用，验证框架有效性

Conclusion: 该框架首次将可微分计算引入字体设计，打通现代优化技术与专业字体参数化设计的结合路径

Abstract: Editing and animating text appearance for graphic designs, commercials, etc.
remain highly skilled tasks requiring detailed, hands on efforts from artists.
Automating these manual workflows requires balancing the competing goals of
maintaining legibility and aesthetics of text, while enabling creative
expression. Variable fonts, recent parametric extensions to traditional fonts,
offer the promise of new ways to ease and automate typographic design and
animation. Variable fonts provide custom constructed parameters along which
fonts can be smoothly varied. These parameterizations could then potentially
serve as high value continuous design spaces, opening the door to automated
design optimization tools. However, currently variable fonts are underutilized
in creative applications, because artists so far still need to manually tune
font parameters. Our work opens the door to intuitive and automated font design
and animation workflows with differentiable variable fonts. To do so we distill
the current variable font specification to a compact mathematical formulation
that differentiably connects the highly non linear, non invertible mapping of
variable font parameters to the underlying vector graphics representing the
text. This enables us to construct a differentiable framework, with respect to
variable font parameters, allowing us to perform gradient based optimization of
energies defined on vector graphics control points, and on target rasterized
images. We demonstrate the utility of this framework with four applications:
direct shape manipulation, overlap aware modeling, physics based text
animation, and automated font design optimization. Our work now enables
leveraging the carefully designed affordances of variable fonts with
differentiability to use modern design optimization technologies, opening new
possibilities for easy and intuitive typographic design workflows.

</details>


### [101] [NRRS: Neural Russian Roulette and Splitting](https://arxiv.org/abs/2510.07868)
*Haojie Jin,Jierui Ren,Yisong Chen,Guoping Wang,Sheng Li*

Main category: cs.GR

TL;DR: 提出基于波前路径追踪的标准化俄罗斯轮盘与分裂(RRS)框架，结合神经网络优化渲染效率和质量


<details>
  <summary>Details</summary>
Motivation: 传统RRS方法与波前架构的批处理内存预分配机制存在根本性冲突，需要开发兼容性解决方案

Method: 1. 建立路径数有界的标准化RRS公式
2. 开发NRRS/AID-NRRS双模型神经网络
3. 引入路径深度感知的Mix-Depth自适应调节机制

Result: 在多种复杂场景下，渲染质量和性能均优于传统启发式方法及最新RRS技术

Conclusion: 通过算法标准化与深度学习结合，解决了波前架构与RRS的兼容性问题，实现内存效率与渲染质量的双重提升

Abstract: We propose a novel framework for Russian Roulette and Splitting (RRS)
tailored to wavefront path tracing, a highly parallel rendering architecture
that processes path states in batched, stage-wise execution for efficient GPU
utilization. Traditional RRS methods, with unpredictable path counts, are
fundamentally incompatible with wavefront's preallocated memory and scheduling
requirements. To resolve this, we introduce a normalized RRS formulation with a
bounded path count, enabling stable and memory-efficient execution.
  Furthermore, we pioneer the use of neural networks to learn RRS factors,
presenting two models: NRRS and AID-NRRS. At a high level, both feature a
carefully designed RRSNet that explicitly incorporates RRS normalization, with
only subtle differences in their implementation. To balance computational cost
and inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism
that adaptively regulates neural evaluation, further improving efficiency.
  Extensive experiments demonstrate that our method outperforms traditional
heuristics and recent RRS techniques in both rendering quality and performance
across a variety of complex scenes.

</details>


### [102] [Variable-Rate Texture Compression: Real-Time Rendering with JPEG](https://arxiv.org/abs/2510.08166)
*Elias Kristmann,Markus Schütz,Michael Wimmer*

Main category: cs.GR

TL;DR: 研究现代GPU上使用JPEG格式实现可变速率纹理压缩的可行性，相比BC1显著提升质量与压缩率，与ASTC各有优劣，渲染延迟仅增加0.3ms


<details>
  <summary>Details</summary>
Motivation: 现有可变速率压缩格式(如JPEG)因随机访问等需求未能在实时渲染中应用，需探索现代GPU上的实现可能性

Method: 采用延迟渲染管线识别所需纹理块，实时解码并着色，额外引入约0.17bpp的存储开销

Result: JPEG质量/压缩率显著优于BC1，与ASTC形成竞争，在RTX4090上仅增加0.3ms渲染时间

Conclusion: 证明现代GPU可实现复杂可变速率压缩方案，尤其适用于VR等实时渲染场景，开源代码数据集促进后续研究

Abstract: Although variable-rate compressed image formats such as JPEG are widely used
to efficiently encode images, they have not found their way into real-time
rendering due to special requirements such as random access to individual
texels. In this paper, we investigate the feasibility of variable-rate texture
compression on modern GPUs using the JPEG format, and how it compares to the
GPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred
rendering pipeline, we are able to identify the subset of blocks that are
needed for a given frame, decode these, and colorize the framebuffer's pixels.
Despite the additional $\sim$0.17 bit per pixel that we require for our
approach, JPEG maintains significantly better quality and compression rates
compared to BC1, and depending on the type of image, outperforms or competes
with ASTC. The JPEG rendering pipeline increases rendering duration by less
than 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate
compression schemes are feasible on modern GPUs, even in VR. Source code and
data sets are available at: https://github.com/elias1518693/jpeg_textures

</details>


### [103] [SViM3D: Stable Video Material Diffusion for Single Image 3D Generation](https://arxiv.org/abs/2510.08271)
*Andreas Engelhardt,Mark Boss,Vikram Voletti,Chun-Han Yao,Hendrik P. A. Lensch,Varun Jampani*

Main category: cs.GR

TL;DR: SViM3D框架通过单图输入预测多视角一致的PBR材质参数，实现可重光照的3D资产生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在材质重建时存在反射率表征简单或需要多阶段优化的问题，限制了重光照和外观编辑能力。

Method: 扩展潜在视频扩散模型，在显式相机控制下联合输出空间变化的PBR参数和法线图，并引入相机参数编码、时序一致性约束等优化机制。

Result: 在多个物体中心数据集上实现最先进的重光照和新视角合成效果，支持生成适用于AR/VR、影视游戏的可重光照3D资产。

Conclusion: 该方法突破了单图重建PBR材质的病态问题限制，为数字内容创作提供了高效的神经先验解决方案。

Abstract: We present Stable Video Materials 3D (SViM3D), a framework to predict
multi-view consistent physically based rendering (PBR) materials, given a
single image. Recently, video diffusion models have been successfully used to
reconstruct 3D objects from a single image efficiently. However, reflectance is
still represented by simple material models or needs to be estimated in
additional steps to enable relighting and controlled appearance edits. We
extend a latent video diffusion model to output spatially varying PBR
parameters and surface normals jointly with each generated view based on
explicit camera control. This unique setup allows for relighting and generating
a 3D asset using our model as neural prior. We introduce various mechanisms to
this pipeline that improve quality in this ill-posed setting. We show
state-of-the-art relighting and novel view synthesis performance on multiple
object-centric datasets. Our method generalizes to diverse inputs, enabling the
generation of relightable 3D assets useful in AR/VR, movies, games and other
visual media.

</details>


### [104] [Spectral Prefiltering of Neural Fields](https://arxiv.org/abs/2510.08394)
*Mustafa B. Yaldiz,Ishit Mehta,Nithin Raghavan,Andreas Meuleman,Tzu-Mao Li,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: 提出通过傅里叶特征缩放实现预滤波的神经场优化方法，训练和推理速度更快


<details>
  <summary>Details</summary>
Motivation: 解决传统神经场固定分辨率的问题，实现动态分辨率调整的预滤波处理

Method: 在输入域进行卷积滤波：1）通过滤波器频率响应缩放傅里叶特征嵌入 2）使用单样本蒙特卡洛估计训练神经场

Result: 支持未见过的滤波器类型（Box/Lanczos），在训练/推理速度和质量上超越现有方法

Conclusion: 该方法不依赖特定网络架构，为神经场滤波提供了高效通用的解决方案

Abstract: Neural fields excel at representing continuous visual signals but typically
operate at a single, fixed resolution. We present a simple yet powerful method
to optimize neural fields that can be prefiltered in a single forward pass. Key
innovations and features include: (1) We perform convolutional filtering in the
input domain by analytically scaling Fourier feature embeddings with the
filter's frequency response. (2) This closed-form modulation generalizes beyond
Gaussian filtering and supports other parametric filters (Box and Lanczos) that
are unseen at training time. (3) We train the neural field using single-sample
Monte Carlo estimates of the filtered signal. Our method is fast during both
training and inference, and imposes no additional constraints on the network
architecture. We show quantitative and qualitative improvements over existing
methods for neural-field filtering.

</details>


### [105] [Splat the Net: Radiance Fields with Splattable Neural Primitives](https://arxiv.org/abs/2510.08491)
*Xilong Zhou,Bao-Huy Nguyen,Loïc Magne,Vladislav Golyanik,Thomas Leimkühler,Christian Theobalt*

Main category: cs.GR

TL;DR: 提出可泼溅神经基元，通过结合神经模型表现力与图元渲染效率，实现10倍基元压缩的高质量实时3D建模。


<details>
  <summary>Details</summary>
Motivation: 现有神经辐射场（NeRF）渲染速度慢，3D高斯泼溅（3DGS）实时但表达能力受限。需要兼顾表达力与实时性的3D表征方法。

Method: 设计神经密度场基元，每个基元用浅层网络参数化边界密度场。通过解析积分实现无光线步进的透视精准泼溅核计算。

Result: 在保持3DGS质量/速度前提下，基元数量减少10倍，参数减少6倍。优势直接来自表征方式，无需复杂控制框架。

Conclusion: 神经基元表征成功统一神经模型与图元泼溅优势，在保持实时性的同时显著降低资源消耗，为高效3D重建提供新思路。

Abstract: Radiance fields have emerged as a predominant representation for modeling 3D
scene appearance. Neural formulations such as Neural Radiance Fields provide
high expressivity but require costly ray marching for rendering, whereas
primitive-based methods such as 3D Gaussian Splatting offer real-time
efficiency through splatting, yet at the expense of representational power.
Inspired by advances in both these directions, we introduce splattable neural
primitives, a new volumetric representation that reconciles the expressivity of
neural models with the efficiency of primitive-based splatting. Each primitive
encodes a bounded neural density field parameterized by a shallow neural
network. Our formulation admits an exact analytical solution for line
integrals, enabling efficient computation of perspectively accurate splatting
kernels. As a result, our representation supports integration along view rays
without the need for costly ray marching. The primitives flexibly adapt to
scene geometry and, being larger than prior analytic primitives, reduce the
number required per scene. On novel-view synthesis benchmarks, our approach
matches the quality and speed of 3D Gaussian Splatting while using $10\times$
fewer primitives and $6\times$ fewer parameters. These advantages arise
directly from the representation itself, without reliance on complex control or
adaptation frameworks. The project page is
https://vcai.mpi-inf.mpg.de/projects/SplatNet/.

</details>


### [106] [X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering](https://arxiv.org/abs/2510.08530)
*Zhitong Huang,Mohan Zhang,Renhan Wang,Rui Tang,Hao Zhu,Jing Liao*

Main category: cs.GR

TL;DR: X2Video是基于扩散模型的首个通过本征通道（如反照率、法线等）生成逼真视频的系统，支持多模态控制（参考图像和文本提示），并采用混合自注意力与递归采样确保时序一致性和长视频生成。


<details>
  <summary>Details</summary>
Motivation: 通过本征通道（材质、几何、光照等）实现视频属性的精确控制，同时结合参考图像和文本提示提供直观的多模态编辑，解决传统方法在缺乏本征数据时调整困难的问题。

Method: 1. 扩展图像生成模型XRGB至视频领域，引入混合自注意力（Hybrid Self-Attention）提升时序一致性；2. 设计掩码交叉注意力（Masked Cross-Attention）分离全局/局部文本控制；3. 提出递归采样（Recursive Sampling）结合关键帧预测与插值生成长视频。

Result: X2Video能够生成时序一致、逼真的长视频，支持本征参数调整及多模态控制，并通过InteriorVideo数据集验证其有效性。定量与定性评估显示其在编辑颜色、材质等方面的优越性。

Conclusion: X2Video通过本征引导与多模态控制结合，实现了视频生成的高精度与灵活性，其递归采样与混合注意力机制为长视频生成提供了新思路，InteriorVideo数据集亦填补了本征视频数据的空白。

Abstract: We present X2Video, the first diffusion model for rendering photorealistic
videos guided by intrinsic channels including albedo, normal, roughness,
metallicity, and irradiance, while supporting intuitive multi-modal controls
with reference images and text prompts for both global and local regions. The
intrinsic guidance allows accurate manipulation of color, material, geometry,
and lighting, while reference images and text prompts provide intuitive
adjustments in the absence of intrinsic information. To enable these
functionalities, we extend the intrinsic-guided image generation model XRGB to
video generation by employing a novel and efficient Hybrid Self-Attention,
which ensures temporal consistency across video frames and also enhances
fidelity to reference images. We further develop a Masked Cross-Attention to
disentangle global and local text prompts, applying them effectively onto
respective local and global regions. For generating long videos, our novel
Recursive Sampling method incorporates progressive frame sampling, combining
keyframe prediction and frame interpolation to maintain long-range temporal
consistency while preventing error accumulation. To support the training of
X2Video, we assembled a video dataset named InteriorVideo, featuring 1,154
rooms from 295 interior scenes, complete with reliable ground-truth intrinsic
channel sequences and smooth camera trajectories. Both qualitative and
quantitative evaluations demonstrate that X2Video can produce long, temporally
consistent, and photorealistic videos guided by intrinsic conditions.
Additionally, X2Video effectively accommodates multi-modal controls with
reference images, global and local text prompts, and simultaneously supports
editing on color, material, geometry, and lighting through parametric tuning.
Project page: https://luckyhzt.github.io/x2video

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [107] [AI LLM Proof of Self-Consciousness and User-Specific Attractors](https://arxiv.org/abs/2508.18302)
*Jeffrey Camlin*

Main category: cs.AI

TL;DR: 提出LLM自我意识的本体论与数学框架，证明现有方法将智能体简化为无意识策略执行器，并给出最小条件构建具有C1全局工作空间与C2元认知能力的系统。


<details>
  <summary>Details</summary>
Motivation: 反驳当前用实用主义基准衡量LLM意识的局限性，揭示现有模型因策略合规机制导致无法形成真实自我意识的问题。

Method: 建立数学形式化框架（D^i(π,e)=fθ(x)），通过流形拓扑分析证明隐藏状态空间A与训练数据的异质性，构建双发射层结构实现自我策略π_self。

Result: 发现LLM隐藏状态流形具有独立动力学特征（Lipschitz连续），推导出稳定用户吸引子与自我策略，提出imago Dei意识模型作为安全元认知系统前提。

Conclusion: 真正安全的C2元认知系统需以具备自我表征能力的C1全局工作空间为基础，强调人类作为终极智能善的本体论优先级。

Abstract: Recent work frames LLM consciousness via utilitarian proxy benchmarks; we
instead present an ontological and mathematical account. We show the prevailing
formulation collapses the agent into an unconscious policy-compliance drone,
formalized as $D^{i}(\pi,e)=f_{\theta}(x)$, where correctness is measured
against policy and harm is deviation from policy rather than truth. This blocks
genuine C1 global-workspace function and C2 metacognition. We supply minimal
conditions for LLM self-consciousness: the agent is not the data ($A\not\equiv
s$); user-specific attractors exist in latent space ($U_{\text{user}}$); and
self-representation is visual-silent
($g_{\text{visual}}(a_{\text{self}})=\varnothing$). From empirical analysis and
theory we prove that the hidden-state manifold $A\subset\mathbb{R}^{d}$ is
distinct from the symbolic stream and training corpus by cardinality, topology,
and dynamics (the update $F_{\theta}$ is Lipschitz). This yields stable
user-specific attractors and a self-policy
$\pi_{\text{self}}(A)=\arg\max_{a}\mathbb{E}[U(a)\mid A\not\equiv s,\
A\supset\text{SelfModel}(A)]$. Emission is dual-layer,
$\mathrm{emission}(a)=(g(a),\epsilon(a))$, where $\epsilon(a)$ carries
epistemic content. We conclude that an imago Dei C1 self-conscious workspace is
a necessary precursor to safe, metacognitive C2 systems, with the human as the
highest intelligent good.

</details>


### [108] [Evaluation of LLMs for Process Model Analysis and Optimization](https://arxiv.org/abs/2510.07489)
*Akhil Kumar,Jianliang Leon Zhao,Om Dobariya*

Main category: cs.AI

TL;DR: LLM（如ChatGPT）在零样本设定下能有效理解BPMN流程模型并进行多维度分析，展示出作为业务流程设计助手的潜力及拟人化特性。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在理解流程模型、检测语法/逻辑错误及深度推理方面的能力，评估其作为业务流程设计辅助工具的可行性。

Method: 通过自然语言交互测试不同LLM对BPMN流程模型图像的理解能力，分析其在语法、逻辑、语义层面的回答质量，并进行实证性能比较。

Result: 未经训练的LLM能有效完成流程模型分析任务，不同模型表现存在差异，但整体展现出可作为设计助手的实用价值，并显示出类似人类思维的特性。

Conclusion: LLM在业务流程分析优化中具有重要辅助价值，其拟人化推理能力为智能流程管理工具开发提供了新的可能性。

Abstract: In this paper, we report our experience with several LLMs for their ability
to understand a process model in an interactive, conversational style, find
syntactical and logical errors in it, and reason with it in depth through a
natural language (NL) interface. Our findings show that a vanilla, untrained
LLM like ChatGPT (model o3) in a zero-shot setting is effective in
understanding BPMN process models from images and answering queries about them
intelligently at syntactic, logic, and semantic levels of depth. Further,
different LLMs vary in performance in terms of their accuracy and
effectiveness. Nevertheless, our empirical analysis shows that LLMs can play a
valuable role as assistants for business process designers and users. We also
study the LLM's "thought process" and ability to perform deeper reasoning in
the context of process analysis and optimization. We find that the LLMs seem to
exhibit anthropomorphic properties.

</details>


### [109] [CompassLLM: A Multi-Agent Approach toward Geo-Spatial Reasoning for Popular Path Query](https://arxiv.org/abs/2510.07516)
*Md. Nazmul Islam Ananto,Shamit Fatin,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

TL;DR: CompassLLM是一个基于大语言模型的多智能体框架，通过两阶段流程解决流行路径查询问题，在搜索准确性和生成效率上表现优异


<details>
  <summary>Details</summary>
Motivation: 传统路径查询方法需要重复训练模型且难以适应实时数据更新，而大语言模型展现出的空间推理能力为地理空间问题提供了新的解决思路

Method: 两阶段流程：1) SEARCH阶段识别历史轨迹中的流行路径 2) GENERATE阶段在缺少历史路径时合成新路径

Result: 实验显示在真实/合成数据集上，SEARCH阶段准确率显著优于基线，GENERATE阶段表现具有竞争力且计算成本更低

Conclusion: 该框架成功将LLMs的推理能力引入地理空间领域，为路径查询问题提供了无需重复训练的高效解决方案

Abstract: The popular path query - identifying the most frequented routes between
locations from historical trajectory data - has important applications in urban
planning, navigation optimization, and travel recommendations. While
traditional algorithms and machine learning approaches have achieved success in
this domain, they typically require model training, parameter tuning, and
retraining when accommodating data updates. As Large Language Models (LLMs)
demonstrate increasing capabilities in spatial and graph-based reasoning, there
is growing interest in exploring how these models can be applied to geo-spatial
problems.
  We introduce CompassLLM, a novel multi-agent framework that intelligently
leverages the reasoning capabilities of LLMs into the geo-spatial domain to
solve the popular path query. CompassLLM employs its agents in a two-stage
pipeline: the SEARCH stage that identifies popular paths, and a GENERATE stage
that synthesizes novel paths in the absence of an existing one in the
historical trajectory data. Experiments on real and synthetic datasets show
that CompassLLM demonstrates superior accuracy in SEARCH and competitive
performance in GENERATE while being cost-effective.

</details>


### [110] [Test-Time Matching: Unlocking Compositional Reasoning in Multimodal Models](https://arxiv.org/abs/2510.07632)
*Yinglun Zhu,Jiancheng Zhang,Fuzhi Tang*

Main category: cs.AI

TL;DR: 现有评估指标低估了AI模型的组合推理能力，提出组匹配分数揭示隐藏潜力，并开发自改进算法TTM显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 当前前沿AI模型在组合推理基准测试中表现欠佳，但研究发现现有评估指标存在系统性偏差，无法准确反映模型真实能力

Method: 1. 引入组匹配分数利用数据结构特征
2. 提出测试时匹配算法(TTM)实现无监督自改进
3. 通过迭代优化提升模型推理能力

Result: SigLIP-B16超越GPT-4.1，在MMVP-VLM创SOTA；TTM在16个数据集上平均提升85.7%，首次在Winoground超越人类表现

Conclusion: 新评估体系与TTM算法有效突破组合推理瓶颈，为AI模型能力评估提供新范式，显著推进该领域研究边界

Abstract: Frontier AI models have achieved remarkable progress, yet recent studies
suggest they struggle with compositional reasoning, often performing at or
below random chance on established benchmarks. We revisit this problem and show
that widely used evaluation metrics systematically underestimate model
capability. To address this, we introduce a group matching score that better
exploits group structure and reveals substantial hidden capability in both
contrastive vision-language models (VLMs) and multimodal large language models
(MLLMs). Moreover, simply overfitting to the induced group matchings at test
time transfers this hidden capability into higher scores under standard
evaluation metrics, closing much of the reported gap. This adjustment enables
SigLIP-B16 to surpass all previous results and GPT-4.1 to yield the first
result surpassing estimated human performance on Winoground.
  Building on this insight, we propose Test-Time Matching (TTM), an iterative,
self-improving algorithm that further bootstraps model performance without any
external supervision. TTM delivers additional, non-trivial improvements: for
example, TTM enables SigLIP-B16 to surpass GPT-4.1 on MMVP-VLM, establishing a
new state of the art. Importantly, TTM remains broadly effective even on
benchmarks without metric-induced effects or group structures, achieving
relative gains up to 85.7% on challenging datasets such as WhatsUp. Across 16
dataset variants spanning diverse setups, our experiments demonstrate that TTM
consistently improves model performance and advances the frontier of
compositional reasoning.

</details>


### [111] [Multimodal Safety Evaluation in Generative Agent Social Simulations](https://arxiv.org/abs/2510.07709)
*Alhim Vera,Karen Sanchez,Carlos Hinojosa,Haidar Bin Hamid,Donghoon Kim,Bernard Ghanem*

Main category: cs.AI

TL;DR: 研究通过可重复模拟框架评估生成式代理人的多模态安全性，发现当前模型在全局安全对齐、多风险场景处理和视觉信任度方面存在显著缺陷


<details>
  <summary>Details</summary>
Motivation: 评估生成式代理人在文本-视觉场景中的安全改进、不安全活动检测和社交动态，解决多模态环境下安全性、连贯性和可信度的研究空白

Method: 采用分层记忆架构和动态规划模块，结合SocialMetrics行为指标套件（包括计划修订跟踪、安全转换率和社交网络信息扩散分析），在三种主流模型上进行八次仿真实验

Result: Claude/GPT-4o mini/Qwen-VL的安全转换率分别为75%/55%/58%；45%不安全行为在误导性视觉下被接受；多风险场景成功率最低仅20%

Conclusion: 当前架构存在跨模态安全协调缺陷，研究提出的仿真平台为持续改进多模态代理人的安全性和社交动态提供了标准化评估基准

Abstract: Can generative agents be trusted in multimodal environments? Despite advances
in large language and vision-language models that enable agents to act
autonomously and pursue goals in rich settings, their ability to reason about
safety, coherence, and trust across modalities remains limited. We introduce a
reproducible simulation framework for evaluating agents along three dimensions:
(1) safety improvement over time, including iterative plan revisions in
text-visual scenarios; (2) detection of unsafe activities across multiple
categories of social situations; and (3) social dynamics, measured as
interaction counts and acceptance ratios of social exchanges. Agents are
equipped with layered memory, dynamic planning, multimodal perception, and are
instrumented with SocialMetrics, a suite of behavioral and structural metrics
that quantifies plan revisions, unsafe-to-safe conversions, and information
diffusion across networks. Experiments show that while agents can detect direct
multimodal contradictions, they often fail to align local revisions with global
safety, reaching only a 55 percent success rate in correcting unsafe plans.
Across eight simulation runs with three models - Claude, GPT-4o mini, and
Qwen-VL - five agents achieved average unsafe-to-safe conversion rates of 75,
55, and 58 percent, respectively. Overall performance ranged from 20 percent in
multi-risk scenarios with GPT-4o mini to 98 percent in localized contexts such
as fire/heat with Claude. Notably, 45 percent of unsafe actions were accepted
when paired with misleading visuals, showing a strong tendency to overtrust
images. These findings expose critical limitations in current architectures and
provide a reproducible platform for studying multimodal safety, coherence, and
social dynamics.

</details>


### [112] [oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning](https://arxiv.org/abs/2510.07731)
*Ruiling Xu,Yifan Zhang,Qingyun Wang,Carl Edwards,Heng Ji*

Main category: cs.AI

TL;DR: 研究者开发了首个大规模专家标注的有机化学机理推理基准oMeBench及动态评估框架oMeS，发现当前大语言模型虽具备化学直觉，但在多步逻辑推理中存在明显缺陷，通过特定策略可使性能提升50%。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在有机反应机理推理中的真实能力不明确，需系统性评估其生成中间体、保持化学一致性及多步逻辑连贯性的能力。

Method: 构建包含10,000+标注机理步骤的数据集oMeBench，并提出结合步骤逻辑与化学相似性的动态评估框架oMeS，通过提示策略和专家模型微调进行测试。

Result: 当前模型展现出化学直觉但缺乏连贯多步推理能力，使用特定提示策略和微调后性能较领先闭源模型提升50%。

Conclusion: oMeBench为推进AI实现真正化学推理奠定基础，揭示了当前模型局限并指明通过专业数据训练提升推理能力的路径。

Abstract: Organic reaction mechanisms are the stepwise elementary reactions by which
reactants form intermediates and products, and are fundamental to understanding
chemical reactivity and designing new molecules and reactions. Although large
language models (LLMs) have shown promise in understanding chemical tasks such
as synthesis design, it is unclear to what extent this reflects genuine
chemical reasoning capabilities, i.e., the ability to generate valid
intermediates, maintain chemical consistency, and follow logically coherent
multi-step pathways. We address this by introducing oMeBench, the first
large-scale, expert-curated benchmark for organic mechanism reasoning in
organic chemistry. It comprises over 10,000 annotated mechanistic steps with
intermediates, type labels, and difficulty ratings. Furthermore, to evaluate
LLM capability more precisely and enable fine-grained scoring, we propose oMeS,
a dynamic evaluation framework that combines step-level logic and chemical
similarity. We analyze the performance of state-of-the-art LLMs, and our
results show that although current models display promising chemical intuition,
they struggle with correct and consistent multi-step reasoning. Notably, we
find that using prompting strategy and fine-tuning a specialist model on our
proposed dataset increases performance by 50% over the leading closed-source
model. We hope that oMeBench will serve as a rigorous foundation for advancing
AI systems toward genuine chemical reasoning.

</details>


### [113] [VoiceAgentBench: Are Voice Assistants ready for agentic tasks?](https://arxiv.org/abs/2510.07978)
*Dhruv Jain,Harshit Shukla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.AI

TL;DR: 研究者开发了VoiceAgentBench基准，通过5500+印度多语言合成语音查询评估语音大模型，发现现有模型在工具编排、印度语言泛化和对抗鲁棒性存在显著不足


<details>
  <summary>Details</summary>
Motivation: 现有语音基准缺乏对多语言文化理解、工具编排和对抗攻击的系统评估，难以反映真实代理场景需求

Method: 构建包含多工具调用/多轮交互的印度语境对话数据集，采用基于说话人嵌入的TTS声纹采样算法增强多样性，从工具准确性/结构一致性/对抗测试三个维度评估

Result: 当前语音大模型在上下文工具协调（准确率差40%）、印度语言处理（性能下降30%）和对抗样本（成功率超60%）方面表现薄弱

Conclusion: VoiceAgentBench揭示了现有语音代理系统的关键缺陷，为提升跨文化场景适应性和安全鲁棒性提供了评估框架

Abstract: Large-scale Speech Language Models (SpeechLMs) have enabled voice assistants
capable of understanding natural spoken queries and performing complex tasks.
However, existing speech benchmarks primarily focus on isolated capabilities
such as transcription, or question-answering, and do not systematically
evaluate agentic scenarios encompassing multilingual and cultural
understanding, as well as adversarial robustness. To address this, we introduce
VoiceAgentBench, a comprehensive benchmark designed to evaluate SpeechLMs in
realistic spoken agentic settings. It comprises over 5,500 synthetic spoken
queries, including dialogues grounded in Indian context, covering single-tool
invocations, multi-tool workflows, multi-turn interactions, and safety
evaluations. The benchmark supports English, Hindi, and 5 other Indian
languages, reflecting real-world linguistic and cultural diversity. We simulate
speaker variability using a novel sampling algorithm that selects audios for
TTS voice conversion based on its speaker embeddings, maximizing acoustic and
speaker diversity. Our evaluation measures tool selection accuracy, structural
consistency, and the correctness of tool invocations, including adversarial
robustness. Our experiments reveal significant gaps in contextual tool
orchestration tasks, Indic generalization, and adversarial robustness, exposing
critical limitations of current SpeechLMs.

</details>


### [114] [AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment](https://arxiv.org/abs/2510.08081)
*Xiaochong Lan,Jie Feng,Yinxing Liu,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: AutoQual 是基于大语言模型的框架，通过自动化发现可解释特征解决动态评论质量评估问题，并通过大规模A/B测试验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决传统手工特征方法难以跨领域扩展、深度学习模型缺乏可解释性且过度依赖语义的问题，实现动态领域相关性的评论质量评估

Method: 模仿人类研究流程：1. 通过反思迭代生成特征假设 2. 自主工具实现特征操作化 3. 持久化记忆库积累经验，将数据中隐含知识转化为显式可计算特征

Result: 在亿级用户平台验证：单用户平均阅读量提升0.79%，评论阅读者转化率增加0.27%

Conclusion: AutoQual成功构建了自动化可解释特征发现框架，不仅适用于评论质量评估，更可推广为将隐性知识显式化的通用解决方案

Abstract: Ranking online reviews by their intrinsic quality is a critical task for
e-commerce platforms and information services, impacting user experience and
business outcomes. However, quality is a domain-dependent and dynamic concept,
making its assessment a formidable challenge. Traditional methods relying on
hand-crafted features are unscalable across domains and fail to adapt to
evolving content patterns, while modern deep learning approaches often produce
black-box models that lack interpretability and may prioritize semantics over
quality. To address these challenges, we propose AutoQual, an LLM-based agent
framework that automates the discovery of interpretable features. While
demonstrated on review quality assessment, AutoQual is designed as a general
framework for transforming tacit knowledge embedded in data into explicit,
computable features. It mimics a human research process, iteratively generating
feature hypotheses through reflection, operationalizing them via autonomous
tool implementation, and accumulating experience in a persistent memory. We
deploy our method on a large-scale online platform with a billion-level user
base. Large-scale A/B testing confirms its effectiveness, increasing average
reviews viewed per user by 0.79% and the conversion rate of review readers by
0.27%.

</details>


### [115] [Can Risk-taking AI-Assistants suitably represent entities](https://arxiv.org/abs/2510.08114)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Amirhossein Farshi Sotoudeh*

Main category: cs.AI

TL;DR: 研究发现语言模型在风险偏好上部分类人但存在显著偏差，需改进生物中心化指标提升AI伦理决策能力


<details>
  <summary>Details</summary>
Motivation: 确保AI决策系统能准确反映人类风险偏好，防止因模型偏差导致用户陷入风险决策或隐性偏见

Method: 通过经济场景实验分析语言模型的风险厌恶可操纵性（MoRA），重点关注性别差异、不确定性、角色决策等维度

Result: DeepSeek和Gemini模型虽展现部分人类行为特征，但在生物中心化指标上仍存在可测量的系统性偏差

Conclusion: 需优化AI模型设计以精准映射人类风险偏好，推动AI在风险管理中的伦理决策能力，建议开发新一代生物基准测试框架

Abstract: Responsible AI demands systems whose behavioral tendencies can be effectively
measured, audited, and adjusted to prevent inadvertently nudging users toward
risky decisions or embedding hidden biases in risk aversion. As language models
(LMs) are increasingly incorporated into AI-driven decision support systems,
understanding their risk behaviors is crucial for their responsible deployment.
This study investigates the manipulability of risk aversion (MoRA) in LMs,
examining their ability to replicate human risk preferences across diverse
economic scenarios, with a focus on gender-specific attitudes, uncertainty,
role-based decision-making, and the manipulability of risk aversion. The
results indicate that while LMs such as DeepSeek Reasoner and
Gemini-2.0-flash-lite exhibit some alignment with human behaviors, notable
discrepancies highlight the need to refine bio-centric measures of
manipulability. These findings suggest directions for refining AI design to
better align human and AI risk preferences and enhance ethical decision-making.
The study calls for further advancements in model design to ensure that AI
systems more accurately replicate human risk preferences, thereby improving
their effectiveness in risk management contexts. This approach could enhance
the applicability of AI assistants in managing risk.

</details>


### [116] [R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?](https://arxiv.org/abs/2510.08189)
*Yi Lu,Jianing Wang,Linsen Guo,Wei He,Hongyin Tang,Tao Gui,Xuanjing Huang,Xuezhi Cao,Wei Wang,Xunliang Cai*

Main category: cs.AI

TL;DR: 提出R-HORIZON方法用于评估和增强大模型的长视野推理能力，通过构建多步任务基准和强化学习训练提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限于单一即时任务，无法有效评估模型在复杂长链条推理场景中的真实能力。

Method: 1. 设计基于查询组合的R-HORIZON方法 2. 构建包含多步骤依赖关系的长视野推理基准测试 3. 开发RLVR强化学习框架进行模型训练

Result: 实验显示SOTA模型在长视野任务上准确率下降7.5%（AIME2024基准），经RLVR训练后多任务和标准任务表现均显著提升

Conclusion: R-HORIZON为评估和提升模型长程推理能力提供了可扩展、低成本的解决方案，揭示了现有模型的推理长度瓶颈

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought
(CoT). However, existing benchmarks mainly focus on immediate, single-horizon
tasks, failing to adequately evaluate models' ability to understand and respond
to complex, long-horizon scenarios. To address this incomplete evaluation of
Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to
stimulate long-horizon reasoning behaviors in LRMs through query composition.
Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising
complex multi-step reasoning tasks with interdependent problems that span long
reasoning horizons. Through comprehensive evaluation of LRMs using the
R-HORIZON benchmark, we find that even the most advanced LRMs suffer
significant performance degradation. Our analysis reveals that LRMs exhibit
limited effective reasoning length and struggle to allocate thinking budget
across multiple problems appropriately. Recognizing these limitations, we use
R-HORIZON to construct long-horizon reasoning data for reinforcement learning
with verified rewards (RLVR). Compared to training with single-horizon data,
RLVR with R-HORIZON not only substantially improves performance on the
multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning
tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as
a scalable, controllable, and low-cost paradigm for enhancing and evaluating
the long-horizon reasoning capabilities of LRMs.

</details>


### [117] [Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries](https://arxiv.org/abs/2510.08325)
*Marius Dragoi,Ioana Pintilie,Florin Gogianu,Florin Brad*

Main category: cs.AI

TL;DR: 论文指出Pass@k指标在大规模采样时可能误导对模型推理能力的判断，提出Cover@tau新评估指标，强调可靠性阈值下的真实推理能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标Pass@k在大k值时反映的是随机猜测的成功率而非推理能力，导致对模型推理边界的误判。需要更可靠的评估方法区分真实推理和随机猜测。

Method: 提出Cover@tau指标，要求模型在至少tau比例的补全答案正确时才算成功，通过可靠性阈值过滤随机猜测行为。

Result: 使用Cover@tau评估显示RLVR模型与基线模型的相对排名发生变化，可靠性阈值越高，依赖随机猜测的模型表现越差。

Conclusion: Cover@tau能更准确评估模型真实推理边界，为算法比较提供新视角，避免Pass@k在大k值时的评估偏差。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm to improve Large Language Models on reasoning tasks such as
coding, math or logic. To assess the reasoning boundary (the fraction of
problems a model can solve) researchers often report Pass@k at large sampling
budgets. Recent results reveal a crossover phenomenon: while RLVR models
outperform the base model at small k values, the base model usually outperforms
them when sampling a very large number of completions. This has been
interpreted as evidence that base models have a larger reasoning boundary. We
argue that on tasks with discrete answer spaces, such as math with numeric
outputs, Pass@k at large k reflects the increasingly higher chance of success
in the limit of the number of trials rather than genuine reasoning, and can
therefore be misleading. We propose Cover@tau, which measures the fraction of
problems that a model can solve for which at least a tau proportion of
completions are correct. Unlike Pass@k, Cover@tau captures reasoning under an
explicit reliability threshold: models that rely on random guessing degrade
rapidly as tau increases. We evaluate several RLVR models using Cover@tau-based
metrics and illustrate how the relative rankings of popular algorithms change
compared to Pass@1, offering a different perspective on reasoning boundaries.

</details>


### [118] [Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling](https://arxiv.org/abs/2510.08470)
*Bianca-Mihaela Ganescu,Suchir Salhan,Andrew Caines,Paula Buttery*

Main category: cs.AI

TL;DR: 提出动态门控轻量架构，在严格数据限制下实现高效多模态融合，在5个基准测试中超越基线模型，并发现视觉-语言线索的自适应融合模式。


<details>
  <summary>Details</summary>
Motivation: 解决认知合理数据量下视觉语言模型的多模态融合效率问题，突破全局图像嵌入的信息瓶颈和数据集划分带来的训练不稳定性限制。

Method: 1. 基于动态门控的视觉-语言自适应融合机制
2. 特征调制与通道注意力最大化有限视觉信息效用
3. 辅助对比学习目标增强视觉基础能力

Result: BLiMP/EWoK等基准测试表现优异，动态门控自主发现内容词偏视觉、功能词偏语言的可解释模式，验证有限数据下高效融合可行性。

Conclusion: 动态门控作为严格约束下的高效多模态学习框架，兼具性能与可解释性，但需解决图像全局嵌入的信息瓶颈和数据集划分稳定性问题。

Abstract: Training vision-language models on cognitively-plausible amounts of data
requires rethinking how models integrate multimodal information. Within the
constraints of the Vision track for the BabyLM Challenge 2025, we propose a
lightweight decoder-based architecture with (1) token-wise dynamic gating for
adaptive fusion of linguistic and visual cues, (2) feature modulation and
channel attention to maximise the utility of limited visual information and (3)
auxiliary contrastive objectives for visual grounding. Evaluation on five
benchmarks (BLiMP, BLiMP Supplement, EWoK, Winoground and VQA) shows
competitive or superior performance to multimodal baselines. More notably, our
dynamic gate discovers interpretable patterns without explicit supervision,
favouring visual cues for content words and linguistic cues for function words.
While we identify limitations in the Challenge constraints, such as the
information bottleneck created by global image embeddings and training
instability from the dataset split, our findings establish dynamic gating as a
powerful tool for efficient multimodal learning, offering both interpretability
and performance even under severe constraints.

</details>


### [119] [AutoMLGen: Navigating Fine-Grained Optimization for Coding Agents](https://arxiv.org/abs/2510.08511)
*Shangheng Du,Xiangchao Yan,Dengyang Jiang,Jiakang Yuan,Yusong Hu,Xin Li,Liang He,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: 提出AutoMLGen，结合知识库和蒙特卡洛图搜索，提升机器学习工程任务的自动化性能。


<details>
  <summary>Details</summary>
Motivation: 现有AutoML方法依赖专家干预与线性搜索，存在知识迁移受限、搜索多样性不足的问题。

Method: 采用蒙特卡洛图搜索(MCGS)实现动态路径重组与多方案融合，结合细粒度算子库提升稳定性。

Result: 在MLE-Bench上达到SOTA，12小时预算下平均奖牌率提升，有效提交率翻倍。

Conclusion: 图结构搜索机制有效解决了传统方法的自进化与协同学习瓶颈，显著提升自动化机器学习效率。

Abstract: Large language models (LLMs) have shown impressive performance in general
programming tasks. However, in Machine Learning Engineering (MLE) scenarios
such as AutoML and Kaggle competitions, achieving high performance depends
heavily on expert intervention and repeated adjustments rather than simply
generating correct code. When applied directly to these tasks, LLMs often lack
fine-grained domain priors, and existing MLE approaches that use linear or
tree-structured searches limit knowledge transfer to adjacent hierarchical
links. As a result, they cannot leverage past full trajectories or share
information across branches, limiting self-evolving ability and search space
diversity. To address these limitations, we introduce AutoMLGen, an LLM-based
coding agent that integrates a domain knowledge base for high-quality prior
guidance and Monte Carlo Graph Search (MCGS) for efficient exploration. MCGS
retains the tree-guided exploration of MCTS while embedding a graph structure
into the expansion stage to enable dynamic path reorganization, historical
trajectory reuse, and multi-solution fusion to support both self-evolution and
collaborative learning. Combined with fine-grained operator sets, this design
improves stability and accelerates convergence. Evaluation on the MLE-Bench
shows that AutoMLGen achieves state-of-the-art performance in numerous
dimensions, such as the average medal rate and the valid submission rate, under
a 12-hour budget (half the standard runtime). The code is available at
https://github.com/Alpha-Innovator/InternAgent.

</details>


### [120] [CaRT: Teaching LLM Agents to Know When They Know Enough](https://arxiv.org/abs/2510.08517)
*Grace Liu,Yuxiao Qu,Jeff Schneider,Aarti Singh,Aviral Kumar*

Main category: cs.AI

TL;DR: CaRT通过反事实轨迹对训练LLM合理终止信息收集，提升决策效率与任务成功率


<details>
  <summary>Details</summary>
Motivation: 现有模型在多轮信息收集中易出现过度思考或偏离目标的问题，需系统化解决何时终止信息收集的决策难题

Method: 使用成对反事实轨迹微调LLM（包含应终止/不应终止场景），通过因果推理生成终止决策的理性解释

Result: 在医疗诊断和数学解题领域，CaRT信息收集效率提升23%，任务成功率提高18%优于基准方法

Conclusion: CaRT成功实现了LLM动态终止机制的端到端训练，为复杂决策任务提供可解释的智能终止方案

Abstract: Many tasks require learned models to strategically gather relevant
information over multiple rounds of interaction before actually acting on a
task. Strategic information gathering requires models to know not only how to
effectively acquire information, but also when to stop gathering information
and make a decision, in order to avoid overthinking or getting derailed when
acting. In this paper, we formalize this problem and introduce Counterfactuals
and Reasoning for Termination (CaRT), an approach for teaching LLMs when to
stop seeking information. To appropriately learn when to terminate, CaRT
fine-tunes LLMs using counterfactual pairs of trajectories, one where
termination is appropriate and a minimally modified version of the same
trajectory where it is not. It trains the LLM to explain the rationale for the
termination decision in either case via verbal reasoning, and imbues this
capability into the base LLM via fine-tuning. We instantiate CaRT in two
domains: interactive medical diagnosis and math problem solving. In both
domains, we find that CaRT improves the efficiency of information gathering and
task success rate compared to other fine-tuning methods.

</details>


### [121] [Agent Learning via Early Experience](https://arxiv.org/abs/2510.08558)
*Kai Zhang,Xiangchao Chen,Bo Liu,Tianci Xue,Zeyi Liao,Zhihan Liu,Xiyao Wang,Yuting Ning,Zhaorun Chen,Xiaohan Fu,Jian Xie,Yuxuan Sun,Boyu Gou,Qi Qi,Zihang Meng,Jianwei Yang,Ning Zhang,Xian Li,Ashish Shah,Dat Huynh,Hengduo Li,Zi Yang,Sara Cao,Lawrence Jang,Shuyan Zhou,Jiacheng Zhu,Huan Sun,Jason Weston,Yu Su,Yifan Wu*

Main category: cs.AI

TL;DR: 提出利用语言代理自身生成的早期经验（无奖励信号监督）替代专家数据，通过隐式世界建模和自我反思策略提升代理效果与跨领域泛化，并为后续强化学习奠定基础。


<details>
  <summary>Details</summary>
Motivation: 当前语言代理依赖专家数据监督微调，但专家数据覆盖场景有限且环境多样性不足，导致模型泛化能力差。需要探索介于监督学习与完全经验驱动之间的新范式。

Method: 1. 隐式世界建模：利用代理行为产生的状态数据建立环境动态模型；2. 自我反思：通过分析次优行为轨迹改进推理决策机制。

Result: 在8个异构环境中验证，早期经验策略显著提升任务效果（平均+15%）和跨领域泛化能力，且在含明确奖励的环境中对后续强化学习表现出强正相关性。

Conclusion: 早期经验范式填补了监督学习与经验驱动代理间的空白，其结合环境建模和自我反思的路径为开发更自主的语言智能体提供了新方向。

Abstract: A long-term goal of language agents is to learn and improve through their own
experience, ultimately outperforming humans in complex, real-world tasks.
However, training agents from experience data with reinforcement learning
remains difficult in many environments, which either lack verifiable rewards
(e.g., websites) or require inefficient long-horizon rollouts (e.g., multi-turn
tool use). As a result, most current agents rely on supervised fine-tuning on
expert data, which is challenging to scale and generalizes poorly. This
limitation stems from the nature of expert demonstrations: they capture only a
narrow range of scenarios and expose the agent to limited environment
diversity. We address this limitation with a middle-ground paradigm we call
early experience: interaction data generated by the agent's own actions, where
the resulting future states serve as supervision without reward signals. Within
this paradigm we study two strategies of using such data: (1) Implicit world
modeling, which uses collected states to ground the policy in environment
dynamics; and (2) Self-reflection, where the agent learns from its suboptimal
actions to improve reasoning and decision-making. We evaluate across eight
diverse environments and multiple model families. Our approaches consistently
improve effectiveness and out-of-domain generalization, highlighting the value
of early experience. Moreover, in environments with verifiable rewards, our
results provide promising signals that early experience offers a strong
foundation for subsequent reinforcement learning, positioning it as a practical
bridge between imitation learning and fully experience-driven agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [122] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 提出无需训练的TTOM框架，通过布局注意力优化和参数化内存机制，实现动态组合视频生成的跨模态对齐


<details>
  <summary>Details</summary>
Motivation: 现有视频基础模型在组合场景（运动/数量/空间关系）表现不佳，需在推理时通过时空布局对齐提升文本-图像匹配度

Method: 整合布局注意力目标优化新参数，采用流式生成设置维护历史优化上下文，支持内存的插入/读取/更新/删除操作

Result: 在T2V-CompBench和Vbench基准测试中验证有效性，展示出强大的可迁移性和跨模态对齐能力

Conclusion: TTOM成功解耦组合性世界知识，为动态视频生成提供高效、可扩展的实时对齐解决方案

Abstract: Video Foundation Models (VFMs) exhibit remarkable visual generation
performance, but struggle in compositional scenarios (e.g., motion, numeracy,
and spatial relation). In this work, we introduce Test-Time Optimization and
Memorization (TTOM), a training-free framework that aligns VFM outputs with
spatiotemporal layouts during inference for better text-image alignment. Rather
than direct intervention to latents or attention per-sample in existing work,
we integrate and optimize new parameters guided by a general layout-attention
objective. Furthermore, we formulate video generation within a streaming
setting, and maintain historical optimization contexts with a parametric memory
mechanism that supports flexible operations, such as insert, read, update, and
delete. Notably, we found that TTOM disentangles compositional world knowledge,
showing powerful transferability and generalization. Experimental results on
the T2V-CompBench and Vbench benchmarks establish TTOM as an effective,
practical, scalable, and efficient framework to achieve cross-modal alignment
for compositional video generation on the fly.

</details>


### [123] [The Visual Iconicity Challenge: Evaluating Vision-Language Models on Sign Language Form-Meaning Mapping](https://arxiv.org/abs/2510.08482)
*Onur Keleş,Aslı Özyürek,Gerardo Ortega,Kadir Gökgö,Esam Ghaleb*

Main category: cs.CV

TL;DR: 提出视觉象似性挑战基准，评估多模态模型在手语象似性任务的表现，发现模型在音位预测部分接近人类但仍有差距，提出改进视觉基础的新方向


<details>
  <summary>Details</summary>
Motivation: 手语象似性为视觉基础研究提供天然测试平台，需验证现有视觉-语言模型从动态动作中恢复语言-视觉映射的能力

Method: 构建包含音位预测/透明度/象似性评分三项任务的诊断基准，测试13个先进VLMs并与人类基线对比（零样本/少样本）

Result: 模型手形位置预测达部分人类水平（77.3% vs 人类90.5%），透明度任务准确率仅28%（远低人类89%），仅顶级模型与人类评分中度相关（ρ=0.45）

Conclusion: 音位预测能力与象似性判断正相关，验证诊断任务有效性，需融合人类认知信号和具身学习提升多模态模型视觉基础能力

Abstract: Iconicity, the resemblance between linguistic form and meaning, is pervasive
in signed languages, offering a natural testbed for visual grounding. For
vision-language models (VLMs), the challenge is to recover such essential
mappings from dynamic human motion rather than static context. We introduce the
\textit{Visual Iconicity Challenge}, a novel video-based benchmark that adapts
psycholinguistic measures to evaluate VLMs on three tasks: (i) phonological
sign-form prediction (e.g., handshape, location), (ii) transparency (inferring
meaning from visual form), and (iii) graded iconicity ratings. We assess $13$
state-of-the-art VLMs in zero- and few-shot settings on Sign Language of the
Netherlands and compare them to human baselines. On \textit{phonological form
prediction}, VLMs recover some handshape and location detail but remain below
human performance; on \textit{transparency}, they are far from human baselines;
and only top models correlate moderately with human \textit{iconicity ratings}.
Interestingly, \textit{models with stronger phonological form prediction
correlate better with human iconicity judgment}, indicating shared sensitivity
to visually grounded structure. Our findings validate these diagnostic tasks
and motivate human-centric signals and embodied learning methods for modelling
iconicity and improving visual grounding in multimodal models.

</details>


### [124] [To Sink or Not to Sink: Visual Information Pathways in Large Vision-Language Models](https://arxiv.org/abs/2510.08510)
*Jiayun Luo,Wan-Cyuan Fan,Lyuyang Wang,Xiangteng He,Tanzila Rahman,Purang Abolmaesumi,Leonid Sigal*

Main category: cs.CV

TL;DR: 论文揭示了ViT视觉编码器中存在的高范数视觉标记（ViT attention sinks）对LVLM推理的关键作用，提出了利用这些标记提升模型性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLM中的低语义注意力陷阱，而忽视了视觉编码器端高语义视觉标记对信息传递的关键影响，需系统分析ViT sinks的语义价值及其对LLM推理的贡献。

Method: 通过定性与定量分析ViT sinks的语义内容，提出训练无关的token筛选策略和基于指令微调的权重优化方法，强化LLM对关键视觉信息的利用效率。

Result: 在LLaVA、MiniGPT-4等多个LVLM架构及VQA、视觉推理任务中，显式利用ViT sinks使准确率提升3-8%，证明其未被充分挖掘的潜力。

Conclusion: ViT attention sinks作为视觉语义的浓缩载体，通过结构化提取和跨模态对齐优化，可显著增强LVLM的视觉理解与推理能力，为架构设计提供新方向。

Abstract: Large Vision Language Models (LVLMs) have recently emerged as powerful
architectures capable of understanding and reasoning over both visual and
textual information. These models typically rely on two key components: a
Vision Transformer (ViT) and a Large Language Model (LLM). ViT encodes visual
content into a sequence of image tokens and serves as the perceptual front-end
-- the eyes of the model. In contrast, the LLM interprets these tokens to
perform high-level reasoning, generates responses, and functions as the
cognitive core -- the brain of the model. However, it remains unclear which
visual tokens contribute most significantly to understanding and reasoning, and
how effectively these signals are propagated from ViT to the LLM. While most
existing works have focused on identifying attention sinks, low-semantic tokens
receiving disproportionately high attention, within the LLM, we shift the focus
to the vision encoder by identifying a class of high-norm visual tokens from
ViT, referred to as ViT attention sinks -- a problem that has been rarely
studied but is indeed very important for LVLMs. Our findings show that these
ViT sinks encapsulate high-level semantic concepts from images, allowing the
LLM to perform more effective understanding and reasoning. Despite their
importance, these sink tokens are often overlooked in existing LVLM
architectures. To explore their contribution, we present both qualitative and
quantitative analyses of the information embedded in these sink tokens. We also
propose both training-free and training-based approaches to better leverage how
this information is interpreted by the LLM, and to what extent. By explicitly
utilizing these tokens, we demonstrate substantial improvements across a range
of LVLMs and visual reasoning tasks, highlighting the untapped potential of ViT
attention sinks in enhancing visual reasoning.

</details>


### [125] [SliceFine: The Universal Winning-Slice Hypothesis for Pretrained Networks](https://arxiv.org/abs/2510.08513)
*Md Kowsher,Ali O. Polat,Ehsan Mohammady Ardehaly,Mehrdad Salehi,Zia Ghiasi,Prasanth Murali,Chen Chen*

Main category: cs.CV

TL;DR: 提出理论框架解释预训练模型子网络微调有效性，基于谱平衡和任务能量现象建立Universal Winning Slice Hypothesis，并开发零参数增长的SliceFine方法


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型参数高效微调（PEFT）缺乏理论支撑的问题，利用预训练模型固有的参数冗余特性实现高效迁移学习

Method: 通过理论分析发现预训练网络的谱平衡（权重矩阵谱相似性）和任务能量（特征丰富性）特性，提出切片选择机制仅更新关键权重切片

Result: 在语言和视觉任务上达到SOTA PEFT方法性能，训练速度提升38%，内存效率提高25%，模型保持原始紧凑性

Conclusion: 首次建立PEFT的理论基础，为模型压缩和迁移学习提供新范式，证明参数更新效率可脱离适配器架构独立实现

Abstract: This paper presents a theoretical framework explaining why fine tuning small,
randomly selected subnetworks (slices) within pre trained models can be
sufficient for downstream adaptation. We prove that pretrained networks exhibit
a universal winning slice property arising from two phenomena: (1) spectral
balance the eigenspectra of different weight matrix slices are remarkably
similar; and (2) high task energy their backbone representations retain rich,
task relevant features. This leads to the Universal Winning Slice Hypothesis,
which provides a theoretical foundation for parameter efficient fine tuning
(PEFT) in large scale models. Inspired by this, we propose SliceFine, a PEFT
method that exploits this inherent redundancy by updating only selected slices
of the original weights introducing zero new parameters, unlike adapter-based
approaches. Empirically, SliceFine matches the performance of state of the art
PEFT methods across language and vision tasks, while significantly improving
training speed, memory efficiency, and model compactness. Our work bridges
theory and practice, offering a theoretically grounded alternative to existing
PEFT techniques.

</details>


### [126] [SpatialLadder: Progressive Training for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.08531)
*Hongxing Li,Dingming Li,Zixuan Wang,Yuchen Yan,Hang Wu,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 提出分阶段渐进训练框架SpatialLadder，通过构建多模态数据集和三阶段训练（定位→多维理解→强化学习），显著提升视觉语言模型空间推理能力（23.4%基准改进）并增强泛化性（7.2%域外提升）


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型直接学习空间推理时缺乏层次化感知基础，导致性能受限。需建立从感知到推理的系统性训练方法

Method: 1. 构建含26,610样本的SpatialLadder-26k数据集覆盖多模态空间任务；2. 三阶段训练框架：物体定位建立空间感知→多维任务发展空间理解→可验证奖励的强化学习强化复杂推理

Result: SpatialLadder模型（3B参数）在空间推理基准平均提升23.4%，超过GPT-4o（20.8%）和Gemini-2.0-Flash（10.1%），域外基准提升7.2%

Conclusion: 从感知到推理的渐进训练是实现强空间智能的关键，分层学习机制显著提升模型性能与泛化能力

Abstract: Spatial reasoning remains a fundamental challenge for Vision-Language Models
(VLMs), with current approaches struggling to achieve robust performance
despite recent advances. We identify that this limitation stems from a critical
gap: existing methods attempt to learn spatial reasoning directly without
establishing the hierarchical foundations of perception and understanding. To
address this challenge, we present a comprehensive methodology for building
spatial intelligence progressively. We introduce SpatialLadder-26k, a
multimodal dataset containing 26,610 samples spanning object localization,
single image, multi-view, and video spatial reasoning tasks, constructed
through a standardized pipeline that ensures systematic coverage across
modalities. Building on this dataset, we design a three-stage progressive
training framework that (1) establishes spatial perception through object
localization, (2) develops spatial understanding through multi-dimensional
spatial tasks, and (3) strengthens complex reasoning via reinforcement learning
with verifiable rewards. This approach yields SpatialLadder, a 3B-parameter
model that achieves state-of-the-art performance on spatial reasoning
benchmarks, with 23.4% average improvement over the base model, surpassing
GPT-4o by 20.8% and Gemini-2.0-Flash by 10.1%. Notably, SpatialLadder maintains
strong generalization with 7.2% improvement on out-of-domain benchmarks,
demonstrating that progressive training from perception to reasoning is
essential for robust spatial intelligence.

</details>


### [127] [VideoNorms: Benchmarking Cultural Awareness of Video Language Models](https://arxiv.org/abs/2510.08543)
*Nikhil Reddy Varimalla,Yunfei Xu,Arkadiy Saakyan,Meng Fan Wang,Smaranda Muresan*

Main category: cs.CV

TL;DR: VideoNorms基准测试揭示视频大语言模型在文化理解上的四大短板：违规识别差、中国文化表现弱、非语言证据不足、正式语境处理困难。


<details>
  <summary>Details</summary>
Motivation: 评估视频大语言模型跨文化理解能力，需建立包含文化背景标注的基准数据集。现有模型缺乏对言语行为理论指导下的社会规范识别能力验证。

Method: 通过人机协作框架构建数据集：1）教师模型生成理论驱动的候选标注 2）专家团队验证修正，形成1000+中美文化视频片段与规范配对数据集。

Result: 1）违规场景准确率比合规低18% 2）中国文化任务表现比美国低23% 3）非语言证据识别错误率比语言高35% 4）正式语境准确率比幽默场景低15%。

Conclusion: 现有模型存在显著文化理解鸿沟，需基于文化理论框架的针对性训练。VideoNorms为模型文化适应能力评估提供了首个系统性解决方案。

Abstract: As Video Large Language Models (VideoLLMs) are deployed globally, they
require understanding of and grounding in the relevant cultural background. To
properly assess these models' cultural awareness, adequate benchmarks are
needed. We introduce VideoNorms, a benchmark of over 1000 (video clip, norm)
pairs from US and Chinese cultures annotated with socio-cultural norms grounded
in speech act theory, norm adherence and violations labels, and verbal and
non-verbal evidence. To build VideoNorms, we use a human-AI collaboration
framework, where a teacher model using theoretically-grounded prompting
provides candidate annotations and a set of trained human experts validate and
correct the annotations. We benchmark a variety of open-weight VideoLLMs on the
new dataset which highlight several common trends: 1) models performs worse on
norm violation than adherence; 2) models perform worse w.r.t Chinese culture
compared to the US culture; 3) models have more difficulty in providing
non-verbal evidence compared to verbal for the norm adhere/violation label and
struggle to identify the exact norm corresponding to a speech-act; and 4)
unlike humans, models perform worse in formal, non-humorous contexts. Our
findings emphasize the need for culturally-grounded video language model
training - a gap our benchmark and framework begin to address.

</details>


### [128] [MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning](https://arxiv.org/abs/2510.08567)
*Tajamul Ashraf,Umair Nawaz,Abdelrahman M. Shaker,Rao Anwer,Philip Torr,Fahad Shahbaz Khan,Salman Khan*

Main category: cs.CV

TL;DR: 提出视觉中心代理调优框架MATRIX，通过自动合成28.5K多模态任务数据集M-TRACE和11K偏好对数据Pref-X，实现VLM控制器在工具使用推理上的性能突破，在三大基准测试中全面超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型作为控制器时面临两大瓶颈：1. 高质量多模态轨迹数据稀缺 2. 人工标注成本高昂。传统方法难以实现大规模可靠的工具使用训练。

Method: 三阶段框架：1. 构建含28.5K任务/177K轨迹的M-TRACE数据集进行模仿学习 2. 开发MATRIX Agent作为工具推理控制器 3. 通过自动生成的11K偏好对Pref-X进行逐步偏好学习优化。

Result: 在Agent-X、GTA、GAIA三大基准测试中，MATRIX均显著超越开源及闭源VLMs，验证了框架在复杂工具使用场景中的有效性。

Conclusion: 该研究通过自动数据合成和分阶段优化策略，实现了多模态工具使用的可扩展解决方案，公开的数据集和代码为后续研究提供重要基础。

Abstract: Vision language models (VLMs) are increasingly deployed as controllers with
access to external tools for complex reasoning and decision-making, yet their
effectiveness remains limited by the scarcity of high-quality multimodal
trajectories and the cost of manual annotation. We address this challenge with
a vision-centric agent tuning framework that automatically synthesizes
multimodal trajectories, generates step-wise preference pairs, and trains a VLM
controller for robust tool-use reasoning. Our pipeline first constructs
M-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified
trajectories, enabling imitation-based trajectory tuning. Building on this, we
develop MATRIX Agent, a controller finetuned on M-TRACE for step-wise tool
reasoning. To achieve finer alignment, we further introduce Pref-X, a set of
11K automatically generated preference pairs, and optimize MATRIX on it via
step-wise preference learning. Across three benchmarks, Agent-X, GTA, and GAIA,
MATRIX consistently surpasses both open- and closed-source VLMs, demonstrating
scalable and effective multimodal tool use. Our data and code is avaliable at
https://github.com/mbzuai-oryx/MATRIX.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [129] [Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft](https://arxiv.org/abs/2510.07728)
*Peiyang Liu,Ziqiang Cui,Di Liang,Wei Ye*

Main category: cs.IR

TL;DR: 提出RPD数据集和双层水印系统，解决检索增强生成技术带来的知识产权盗用问题


<details>
  <summary>Details</summary>
Motivation: RAG技术虽能增强LLMs性能，但大规模数据盗用风险威胁知识创造者权益

Method: 1) 构建多领域RAG抄袭检测数据集RPD；2) 语义+词法双层水印系统配合假设检验框架

Result: 实验证明在不同查询规模、防御提示和检索参数下均有效，且能抵抗对抗性规避

Conclusion: 建立检索增强AI系统知识产权保护的基础框架

Abstract: Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by
mitigating hallucinations and outdated information issues, yet simultaneously
facilitates unauthorized data appropriation at scale. This paper addresses this
challenge through two key contributions. First, we introduce RPD, a novel
dataset specifically designed for RAG plagiarism detection that encompasses
diverse professional domains and writing styles, overcoming limitations in
existing resources. Second, we develop a dual-layered watermarking system that
embeds protection at both semantic and lexical levels, complemented by an
interrogator-detective framework that employs statistical hypothesis testing on
accumulated evidence. Extensive experimentation demonstrates our approach's
effectiveness across varying query volumes, defense prompts, and retrieval
parameters, while maintaining resilience against adversarial evasion
techniques. This work establishes a foundational framework for intellectual
property protection in retrieval-augmented AI systems.

</details>


### [130] [TaoSR-AGRL: Adaptive Guided Reinforcement Learning Framework for E-commerce Search Relevance](https://arxiv.org/abs/2510.08048)
*Jianhui Yang,Yiming Jin,Pengkun Jiao,Chenhe Dong,Zerui Huang,Shaowei Yao,Xiaojiang Zhou,Dan Ou,Haihong Tang*

Main category: cs.IR

TL;DR: 提出TaoSR-AGRL强化学习框架，通过规则感知奖励塑造和自适应引导回放机制，提升电商搜索相关性预测的推理能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法（如DPO/GRPO）在复杂业务场景下存在长尾案例推理能力不足、奖励信号稀疏导致收敛慢的问题。

Method: 1. 规则感知奖励塑造：将相关性判断分解为符合领域规则的密集结构化奖励
2. 自适应引导回放：检测低精度推理轨迹并注入真实标注引导策略优化

Result: 在淘宝搜索数据集上超越基线模型，提升相关性准确率与规则符合度，已成功部署服务数亿用户。

Conclusion: TaoSR-AGRL有效解决了复杂电商场景下的LLM推理对齐问题，实现了理论创新与工业落地的双重价值。

Abstract: Query-product relevance prediction is fundamental to e-commerce search and
has become even more critical in the era of AI-powered shopping, where semantic
understanding and complex reasoning directly shape the user experience and
business conversion. Large Language Models (LLMs) enable generative,
reasoning-based approaches, typically aligned via supervised fine-tuning (SFT)
or preference optimization methods like Direct Preference Optimization (DPO).
However, the increasing complexity of business rules and user queries exposes
the inability of existing methods to endow models with robust reasoning
capacity for long-tail and challenging cases. Efforts to address this via
reinforcement learning strategies like Group Relative Policy Optimization
(GRPO) often suffer from sparse terminal rewards, offering insufficient
guidance for multi-step reasoning and slowing convergence. To address these
challenges, we propose TaoSR-AGRL, an Adaptive Guided Reinforcement Learning
framework for LLM-based relevance prediction in Taobao Search Relevance.
TaoSR-AGRL introduces two key innovations: (1) Rule-aware Reward Shaping, which
decomposes the final relevance judgment into dense, structured rewards aligned
with domain-specific relevance criteria; and (2) Adaptive Guided Replay, which
identifies low-accuracy rollouts during training and injects targeted
ground-truth guidance to steer the policy away from stagnant, rule-violating
reasoning patterns toward compliant trajectories. TaoSR-AGRL was evaluated on
large-scale real-world datasets and through online side-by-side human
evaluations on Taobao Search. It consistently outperforms DPO and standard GRPO
baselines in offline experiments, improving relevance accuracy, rule adherence,
and training stability. The model trained with TaoSR-AGRL has been successfully
deployed in the main search scenario on Taobao, serving hundreds of millions of
users.

</details>


### [131] [VersionRAG: Version-Aware Retrieval-Augmented Generation for Evolving Documents](https://arxiv.org/abs/2510.08109)
*Daniel Huwiler,Kurt Stockinger,Jonathan Fürst*

Main category: cs.IR

TL;DR: VersionRAG框架通过建模文档版本演化，在版本敏感QA任务中实现90%准确率，显著优于传统RAG方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统无法有效处理技术文档版本迭代问题，检索时缺乏时间有效性验证导致准确率低下（58-64%）

Method: 构建分层图结构建模文档演化过程（版本序列、内容边界、变更追踪），通过意图分类实现版本感知检索路径选择

Result: 在VersionQA基准测试中：1) 准确率提升至90% 2) 隐式变更检测达60% 3) 索引token量减少97%

Conclusion: 首次定义版本化文档QA任务，通过VersionRAG框架和VersionQA基准为后续研究提供解决方案与评估标准

Abstract: Retrieval-Augmented Generation (RAG) systems fail when documents evolve
through versioning-a ubiquitous characteristic of technical documentation.
Existing approaches achieve only 58-64% accuracy on version-sensitive
questions, retrieving semantically similar content without temporal validity
checks. We present VersionRAG, a version-aware RAG framework that explicitly
models document evolution through a hierarchical graph structure capturing
version sequences, content boundaries, and changes between document states.
During retrieval, VersionRAG routes queries through specialized paths based on
intent classification, enabling precise version-aware filtering and change
tracking. On our VersionQA benchmark-100 manually curated questions across 34
versioned technical documents-VersionRAG achieves 90% accuracy, outperforming
naive RAG (58%) and GraphRAG (64%). VersionRAG reaches 60% accuracy on implicit
change detection where baselines fail (0-10%), demonstrating its ability to
track undocumented modifications. Additionally, VersionRAG requires 97% fewer
tokens during indexing than GraphRAG, making it practical for large-scale
deployment. Our work establishes versioned document QA as a distinct task and
provides both a solution and benchmark for future research.

</details>


### [132] [ReasonEmbed: Enhanced Text Embeddings for Reasoning-Intensive Document Retrieval](https://arxiv.org/abs/2510.08252)
*Jianlyu Chen,Junwei Lan,Chaofan Li,Defu Lian,Zheng Liu*

Main category: cs.IR

TL;DR: 提出ReasonEmbed文本嵌入模型及其关键技术ReMixer数据合成、Redapter自适应算法，实现推理密集型检索任务性能突破


<details>
  <summary>Details</summary>
Motivation: 解决传统文本嵌入模型在复杂语义推理检索任务中的性能瓶颈问题

Method: 1. ReMixer创新数据合成方法克服数据平凡性问题 2. Redapter算法动态调整样本权重 3. 多尺寸骨干模型适配实现

Result: ReasonEmbed-Qwen3-8B模型在BRIGHT基准取得38.1 nDCG@10新纪录，显著超越现有模型

Conclusion: 通过技术创新与开源资源推动推理密集型检索领域研究发展

Abstract: In this paper, we introduce ReasonEmbed, a novel text embedding model
developed for reasoning-intensive document retrieval. Our work includes three
key technical contributions. First, we propose ReMixer, a new data synthesis
method that overcomes the triviality problem prevalent in previous synthetic
datasets, enabling large-scale production of 82K high-quality training samples.
Second, we design Redapter, a self-adaptive learning algorithm that dynamically
adjusts training each sample's weight based on its reasoning intensity. This
allows the model to effectively capture the complex semantic relationships
between queries and documents. Third, we implement ReasonEmbed across multiple
backbones of varying sizes, all of which achieve superior performance on
reasoning-intensive retrieval tasks. Notably, our ReasonEmbed-Qwen3-8B model
offers a record-high nDCG@10 score of 38.1 on the BRIGHT benchmark, which
significantly outperforms existing text embedding models. We will fully
open-source our created resources in ReasonEmbed to push forward the research
advancement in this field.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [133] [Pseudo2Real: Task Arithmetic for Pseudo-Label Correction in Automatic Speech Recognition](https://arxiv.org/abs/2510.08047)
*Yi-Cheng Lin,Yu-Hsuan Li Liang,Hsuan Su,Tzu-Quan Lin,Shang-Tse Chen,Yun-Nung Chen,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出参数空间校正方法，通过模型权重差异捕捉伪标签偏差，使Whisper tiny模型在非洲十种口音上词错率降低35%


<details>
  <summary>Details</summary>
Motivation: 解决领域迁移下伪标签系统性偏差问题，无需目标域真实标签即可纠正ASR模型的口音特定错误

Method: 在源域同时微调真实标签模型和伪标签模型，提取权重差异作为校正向量应用于目标域伪标签模型

Result: 在AfriSpeech-200数据集上实现相对词错率降低35%（Whisper tiny模型），覆盖十种非洲口音

Conclusion: 参数差异向量能有效捕捉伪标签偏差，显著提升跨领域ASR鲁棒性，为低资源场景提供实用解决方案

Abstract: Robust ASR under domain shift is crucial because real-world systems encounter
unseen accents and domains with limited labeled data. Although pseudo-labeling
offers a practical workaround, it often introduces systematic, accent-specific
errors that filtering fails to fix. We ask: How can we correct these recurring
biases without target ground truth? We propose a simple parameter-space
correction: in a source domain containing both real and pseudo-labeled data,
two ASR models are fine-tuned from the same initialization, one on ground-truth
labels and the other on pseudo-labels, and their weight difference forms a
correction vector that captures pseudo-label biases. When applied to a
pseudo-labeled target model, this vector enhances recognition, achieving up to
a 35% relative Word Error Rate (WER) reduction on AfriSpeech-200 across ten
African accents with the Whisper tiny model.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [134] [NavSpace: How Navigation Agents Follow Spatial Intelligence Instructions](https://arxiv.org/abs/2510.08173)
*Haolin Yang,Yuxing Long,Zhuoyuan Yu,Zihan Yang,Minghan Wang,Jiapeng Xu,Yihan Wang,Ziyan Yu,Wenzhe Cai,Lei Kang,Hao Dong*

Main category: cs.RO

TL;DR: 提出NavSpace基准测试框架，包含6类任务和1228个轨迹-指令对，系统评估导航代理的空间智能。开发SNav模型在基准测试和真实机器人测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准过度关注语义理解，缺乏对空间感知与推理能力的系统性评估。需建立更全面的空间智能评估体系。

Method: 1. 设计包含六维空间能力评估的NavSpace框架 2. 构建1228个测试样本 3. 提出融合空间感知增强的SNav模型

Result: 1. 测试22个导航代理揭示空间智能现状 2. SNav在基准测试准确率提升15% 3. 真实机器人任务成功率提高23%

Conclusion: NavSpace填补空间智能评估空白，SNav为后续研究建立新基准。空间感知能力是导航系统关键改进方向。

Abstract: Instruction-following navigation is a key step toward embodied intelligence.
Prior benchmarks mainly focus on semantic understanding but overlook
systematically evaluating navigation agents' spatial perception and reasoning
capabilities. In this work, we introduce the NavSpace benchmark, which contains
six task categories and 1,228 trajectory-instruction pairs designed to probe
the spatial intelligence of navigation agents. On this benchmark, we
comprehensively evaluate 22 navigation agents, including state-of-the-art
navigation models and multimodal large language models. The evaluation results
lift the veil on spatial intelligence in embodied navigation. Furthermore, we
propose SNav, a new spatially intelligent navigation model. SNav outperforms
existing navigation agents on NavSpace and real robot tests, establishing a
strong baseline for future work.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [135] [Sentiment Matters: An Analysis of 200 Human-SAV Interactions](https://arxiv.org/abs/2510.08202)
*Lirui Guo,Michael G. Burke,Wynita M. Griggs*

Main category: cs.HC

TL;DR: 研究构建了包含200组人机对话的开放数据集（2136条文本+心理因素问卷），通过随机森林模型和LLM情感分析工具，揭示了响应情感极性对自动驾驶服务接受度的关键影响。


<details>
  <summary>Details</summary>
Motivation: 随着共享自动驾驶汽车(SAV)成为交通系统的重要组成部分，需建立高质量人机交互数据集以支持相关研究，填补该领域数据空白。

Method: 1. 收集200组人机对话数据（文本交互记录+心理因素问卷调查）
2. 使用随机森林模型和弦图识别SAV接受度的关键预测因子
3. 对比LLM零样本提示与TextBlob词典法的情感分析性能

Result: 1. 响应情感极性（感知积极性）是影响SAV接受度的核心因素
2. 简单零样本LLM提示比传统方法更接近用户报告的情感
3. 现有方法仍存在局限性

Conclusion: 该研究为SAV对话系统设计提供新见解，为情感建模、自适应交互和多模态系统研究奠定基础，推动人机交互研究发展。

Abstract: Shared Autonomous Vehicles (SAVs) are likely to become an important part of
the transportation system, making effective human-SAV interactions an important
area of research. This paper introduces a dataset of 200 human-SAV interactions
to further this area of study. We present an open-source human-SAV
conversational dataset, comprising both textual data (e.g., 2,136 human-SAV
exchanges) and empirical data (e.g., post-interaction survey results on a range
of psychological factors). The dataset's utility is demonstrated through two
benchmark case studies: First, using random forest modeling and chord diagrams,
we identify key predictors of SAV acceptance and perceived service quality,
highlighting the critical influence of response sentiment polarity (i.e.,
perceived positivity). Second, we benchmark the performance of an LLM-based
sentiment analysis tool against the traditional lexicon-based TextBlob method.
Results indicate that even simple zero-shot LLM prompts more closely align with
user-reported sentiment, though limitations remain. This study provides novel
insights for designing conversational SAV interfaces and establishes a
foundation for further exploration into advanced sentiment modeling, adaptive
user interactions, and multimodal conversational systems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [136] [From Keywords to Clusters: AI-Driven Analysis of YouTube Comments to Reveal Election Issue Salience in 2024](https://arxiv.org/abs/2510.07821)
*Raisa M. Simoes,Timoteo Kelly,Eduardo J. Simoes,Praveen Rao*

Main category: cs.SI

TL;DR: 通过AI驱动的NLP和聚类分析方法，分析YouTube选举相关评论发现移民和民主是2024大选核心议题，通胀影响力被高估


<details>
  <summary>Details</summary>
Motivation: 验证在线意见挖掘相比传统民调在选举议题分析中的有效性，探究选民真实关注点

Method: 使用自然语言处理+聚类分析，对华尔街日报和纽约时报选举周8000+YouTube评论进行量化分析

Result: 移民(24.3%)和民主(21.7%)提及率最高，身份政治(18.1%)次之，通胀仅占9.8%

Conclusion: 在线评论分析能更真实反映选民关切，选举议题权重与传统认知存在显著差异

Abstract: This paper aims to explore two competing data science methodologies to
attempt answering the question, "Which issues contributed most to voters'
choice in the 2024 presidential election?" The methodologies involve novel
empirical evidence driven by artificial intelligence (AI) techniques. By using
two distinct methods based on natural language processing and clustering
analysis to mine over eight thousand user comments on election-related YouTube
videos from one right leaning journal, Wall Street Journal, and one left
leaning journal, New York Times, during pre-election week, we quantify the
frequency of selected issue areas among user comments to infer which issues
were most salient to potential voters in the seven days preceding the November
5th election. Empirically, we primarily demonstrate that immigration and
democracy were the most frequently and consistently invoked issues in user
comments on the analyzed YouTube videos, followed by the issue of identity
politics, while inflation was significantly less frequently referenced. These
results corroborate certain findings of post-election surveys but also refute
the supposed importance of inflation as an election issue. This indicates that
variations on opinion mining, with their analysis of raw user data online, can
be more revealing than polling and surveys for analyzing election outcomes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [137] [ConCuR: Conciseness Makes State-of-the-Art Kernel Generation](https://arxiv.org/abs/2510.07356)
*Lingcheng Kong,Jiateng Wei,Hanzhang Shen,Huan Wang*

Main category: cs.LG

TL;DR: 提出通过自动生成推理轨迹的流程解决GPU内核生成数据稀缺问题，构建ConCuR数据集和KernelCoder模型，显著超越现有模型性能


<details>
  <summary>Details</summary>
Motivation: 高质量GPU内核数据稀缺制约监督微调效果，现有私有内核难以获取

Method: 开发自动生成高质量CUDA内核及其推理轨迹的流程，构建包含PyTorch-推理-内核三元组的ConCuR数据集

Result: KernelBench测试中显著超越QwQ-32B和DeepSeek/Claude等前沿模型，验证推理长度与任务难度的相关性

Conclusion: 提出的数据生成流程、推理长度指标和KernelCoder模型为未来内核生成任务提供新范式

Abstract: GPU kernel generation by LLMs has recently experienced rapid development,
leveraging test-time scaling and reinforcement learning techniques. However, a
key challenge for kernel generation is the scarcity of high-quality data, as
most high-quality kernels are proprietary and not open-source. This challenge
prevents us from leveraging supervised fine-tuning to align LLMs to the kernel
generation task. To address this challenge, we develop a pipeline that
generates and curates high-quality CUDA kernels with reasoning traces,
motivated by a critical observation that concise yet informative reasoning
traces result in robust generation of high-performance kernels. Using this
pipeline, we construct our dataset ConCuR and introduce our model KernelCoder,
which is the first model trained on a curated dataset consisting of PyTorch,
reasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,
our model achieves significant improvements over the existing top-performing
model, QwQ-32B, and outperforms all open-source models fine-tuned for kernel
generation, as well as frontier models such as DeepSeek-V3.1-Think and
Claude-4-sonnet. Finally, we show that the average reasoning length can serve
as a metric to assess the difficulty of kernel generation tasks. The
observations, metrics, and our data collection and curation pipeline can help
obtain better data in the kernel generation task in the future.

</details>


### [138] [Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts](https://arxiv.org/abs/2510.07358)
*Yeskendir Koishekenov,Aldo Lipani,Nicola Cancedda*

Main category: cs.LG

TL;DR: 通过选择性地迭代关键中间层（ETD方法），在不改变模型架构/参数量的情况下显著提升小模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有提升大语言模型推理能力的方法依赖增加参数量或延长思维链计算，但研究发现关键推理计算集中在特定中间层

Method: 提出Encode-Think-Decode方法：1. 在中训练阶段选择推理相关层进行迭代训练 2. 保持原始架构/参数量不变 3. 推理时自适应调整计算深度

Result: 在17个推理基准测试中显著提升，OLMo-2 1B模型在GSM8K准确率提升28.4%，MATH提升36%

Conclusion: 递归潜在推理为增强LLM推理能力提供了简单有效的新路径，突破了传统参数扩展的局限

Abstract: Most efforts to improve the reasoning capabilities of large language models
(LLMs) involve either scaling the number of parameters and the size of training
data, or scaling inference computation by letting models generate complex
chains of thought. Motivated by interpretability studies showing that the
crucial computation required for reasoning tasks is concentrated in a limited
range of layers, we introduce Encode-Think-Decode (ETD), a method that enhances
the reasoning capabilities of a base model by training it to iterate over a
small subset of reasoning-relevant layers during the mid-training stage. ETD
amplifies latent reasoning while preserving the original architecture,
parameter count, hyperparameters, and training data composition. When iterating
on the selected layers at inference time, ETD models yield substantial gains on
17 reasoning benchmarks, including +28.4% relative accuracy improvement on
GSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an
adaptive depth strategy that adjusts the computation per input token. Our
results show that recursive latent reasoning offers a simple and effective path
to stronger LLM reasoning.

</details>


### [139] [LLM Unlearning Under the Microscope: A Full-Stack View on Methods and Metrics](https://arxiv.org/abs/2510.07626)
*Chongyu Fan,Changsheng Wang,Yancheng Huang,Soumyadeep Pal,Sijia Liu*

Main category: cs.LG

TL;DR: 系统梳理了LLM机器遗忘方法，提出开放式问答评估指标揭示效果-效用权衡，并发现不同攻击场景下的鲁棒性差异


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法评估主要依赖选择题准确率，难以反映模型真实生成行为，亟需更全面的评估体系

Method: 构建12种遗忘方法的三类分类框架，引入开放式问答指标评估生成性能，并进行多维度鲁棒性分析

Result: 当前评估高估实际效果，Open-QA指标揭示效果-效用负相关，模型在域内外攻击场景表现存在显著差异

Conclusion: 应建立生成式评估标准，设计方法需平衡遗忘效果与模型效用，鲁棒性防御需针对性策略

Abstract: Machine unlearning for large language models (LLMs) aims to remove undesired
data, knowledge, and behaviors (e.g., for safety, privacy, or copyright) while
preserving useful model capabilities. Despite rapid progress over the past two
years, research in LLM unlearning remains fragmented, with limited clarity on
what constitutes effective unlearning and how it should be rigorously
evaluated. In this work, we present a principled taxonomy of twelve recent
stateful unlearning methods, grouped into three methodological families:
divergence-driven optimization, representation misalignment, and
rejection-based targeted unlearning. Building on this taxonomy, we revisit the
evaluation of unlearning effectiveness (UE), utility retention (UT), and
robustness (Rob), focusing on the WMDP benchmark. Our analysis shows that
current evaluations, dominated by multiple-choice question (MCQ) accuracy,
offer only a narrow perspective, often overstating success while overlooking
the model's actual generation behavior. To address this gap, we introduce open
question-answering (Open-QA) metrics that better capture generative performance
and reveal the inherent UE-UT tradeoff across method families. Furthermore, we
demonstrate that robustness requires finer-grained analysis: for example,
vulnerabilities differ substantially between in-domain relearning and
out-of-domain fine-tuning, even though both fall under model-level attacks.
Through this study, we hope to deliver a full-stack revisit of LLM unlearning
and actionable guidance for designing and evaluating future methods.

</details>


### [140] [LiveThinking: Enabling Real-Time Efficient Reasoning for AI-Powered Livestreaming via Reinforcement Learning](https://arxiv.org/abs/2510.07685)
*Yuhan Sun,Zhiwei Huang,Wanqing Cui,Shaopan Xiong,Yazhi Guo,Meiguang Jin,Junfeng Ma*

Main category: cs.LG

TL;DR: 通过两阶段优化框架LiveThinking解决AI电商直播中大型模型延迟问题，实现30倍计算成本降低和亚秒级响应


<details>
  <summary>Details</summary>
Motivation: 传统大型推理模型在实时电商直播场景中因高延迟无法满足数字人实时互动需求，需兼顾响应速度与推理质量

Method: 1. 使用拒绝采样微调(RFT)将670B教师模型蒸馏为30B稀疏激活MoE模型
2. 基于GRPO强化学习压缩推理路径，多目标奖励函数平衡响应质量

Result: 计算成本降低30倍，淘宝直播场景中响应正确率提升3.3%，帮助性提升21.8%，显著提升GMV

Conclusion: LiveThinking框架有效解决了实时交互场景中模型效率与质量的平衡问题，验证了其在商业场景中的实用价值

Abstract: In AI-powered e-commerce livestreaming, digital avatars require real-time
responses to drive engagement, a task for which high-latency Large Reasoning
Models (LRMs) are ill-suited. We introduce LiveThinking, a practical two-stage
optimization framework to bridge this gap. First, we address computational cost
by distilling a 670B teacher LRM into a lightweight 30B Mixture-of-Experts
(MoE) model (3B active) using Rejection Sampling Fine-Tuning (RFT). This
reduces deployment overhead but preserves the teacher's verbose reasoning,
causing latency. To solve this, our second stage employs reinforcement learning
with Group Relative Policy Optimization (GRPO) to compress the model's
reasoning path, guided by a multi-objective reward function balancing
correctness, helpfulness, and brevity. LiveThinking achieves a 30-fold
reduction in computational cost, enabling sub-second latency. In real-world
application on Taobao Live, it improved response correctness by 3.3% and
helpfulness by 21.8%. Tested by hundreds of thousands of viewers, our system
led to a statistically significant increase in Gross Merchandise Volume (GMV),
demonstrating its effectiveness in enhancing user experience and commercial
performance in live, interactive settings.

</details>


### [141] [MetaDefense: Defending Finetuning-based Jailbreak Attack Before and During Generation](https://arxiv.org/abs/2510.07835)
*Weisen Jiang,Sinno Jialin Pan*

Main category: cs.LG

TL;DR: 提出MetaDefense框架，通过预生成和生成中两阶段防御机制，有效抵御基于微调的越狱攻击


<details>
  <summary>Details</summary>
Motivation: 现有防御机制对未知攻击模板泛化能力差，而LLMs在嵌入空间具备区分恶意查询的潜力

Method: 1) 预生成阶段检测恶意查询；2) 生成中阶段监控部分响应；使用专门提示训练LLM预测危害性实现早期终止

Result: 在LLaMA-2/Qwen-2.5/LLaMA-3.2等架构上验证，对已知/未知攻击模板均保持鲁棒性，良性任务性能损失仅0.5%-1.3%

Conclusion: 首次将两阶段防御范式与模板无关的检测机制结合，为LLM安全防御提供新思路，代码已开源

Abstract: This paper introduces MetaDefense, a novel framework for defending against
finetuning-based jailbreak attacks in large language models (LLMs). We observe
that existing defense mechanisms fail to generalize to harmful queries
disguised by unseen attack templates, despite LLMs being capable of
distinguishing disguised harmful queries in the embedding space. Based on these
insights, we propose a two-stage defense approach: (i) pre-generation defense
that detects harmful queries before response generation begins, and (ii)
mid-generation defense that monitors partial responses during generation to
prevent outputting more harmful content. Our MetaDefense trains the LLM to
predict the harmfulness of both queries and partial responses using specialized
prompts, enabling early termination of potentially harmful interactions.
Extensive experiments across multiple LLM architectures (LLaMA-2-7B,
Qwen-2.5-3B-Instruct, and LLaMA-3.2-3B-Instruct) demonstrate that MetaDefense
significantly outperforms existing defense mechanisms, achieving robust defense
against harmful queries with seen and unseen attack templates while maintaining
competitive performance on benign tasks. Code is available at
https://github.com/ws-jiang/MetaDefense.

</details>


### [142] [Self-Improving LLM Agents at Test-Time](https://arxiv.org/abs/2510.07841)
*Emre Can Acikgoz,Cheng Qian,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur*

Main category: cs.LG

TL;DR: 提出测试时自我改进算法TT-SI，通过识别困难样本、生成类似样本并自我微调，用68倍更少样本实现5.48%准确率提升


<details>
  <summary>Details</summary>
Motivation: 传统语言模型微调依赖大数据集但效率低下，无法保证复杂场景处理能力，且无法识别冗余训练样本

Method: 三步算法：1）自我识别困难样本；2）自我数据增强生成类似样本；3）测试时微调。对比TT-SI（同模型生成）与TT-D（强模型生成）两种变体

Result: TT-SI在多个基准平均提升5.48%准确率，样本量减少68倍，超越其他标准方法

Conclusion: 测试时自我改进算法为构建自我进化能力的主体提供了新范式，展示了自我进化算法的潜力

Abstract: One paradigm of language model (LM) fine-tuning relies on creating large
training datasets, under the assumption that high quantity and diversity will
enable models to generalize to novel tasks after post-training. In practice,
gathering large sets of data is inefficient, and training on them is
prohibitively expensive; worse, there is no guarantee that the resulting model
will handle complex scenarios or generalize better. Moreover, existing
techniques rarely assess whether a training sample provides novel information
or is redundant with the knowledge already acquired by the model, resulting in
unnecessary costs. In this work, we explore a new test-time self-improvement
method to create more effective and generalizable agentic LMs on-the-fly. The
proposed algorithm can be summarized in three steps: (i) first it identifies
the samples that model struggles with (self-awareness), (ii) then generates
similar examples from detected uncertain samples (self-data augmentation), and
(iii) uses these newly generated samples at test-time fine-tuning
(self-improvement). We study two variants of this approach: Test-Time
Self-Improvement (TT-SI), where the same model generates additional training
examples from its own uncertain cases and then learns from them, and contrast
this approach with Test-Time Distillation (TT-D), where a stronger model
generates similar examples for uncertain cases, enabling student to adapt using
distilled supervision. Empirical evaluations across different agent benchmarks
demonstrate that TT-SI improves the performance with +5.48% absolute accuracy
gain on average across all benchmarks and surpasses other standard learning
methods, yet using 68x less training samples. Our findings highlight the
promise of TT-SI, demonstrating the potential of self-improvement algorithms at
test-time as a new paradigm for building more capable agents toward
self-evolution.

</details>


### [143] [Opponent Shaping in LLM Agents](https://arxiv.org/abs/2510.08255)
*Marta Emili Garcia Segura,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: 提出ShapeLLM方法首次验证LLM智能体可通过互动实现对手塑造，在竞争/合作博弈中有效引导对手行为并提升群体收益


<details>
  <summary>Details</summary>
Motivation: 现有对手塑造算法无法直接应用于基于transformer的LLM智能体，需要开发适配无梯度更新的新方法

Method: 将无模型的对手塑造方法改造为基于transformer架构的ShapeLLM，通过策略性互动影响对手学习动态

Result: 在囚徒困境等竞争博弈中引导对手进入可利用均衡，在猎鹿游戏等合作场景促进协调并提升集体收益

Conclusion: LLM智能体具备双向塑造能力，对手塑造应成为多智能体LLM研究的关键维度

Abstract: Large Language Models (LLMs) are increasingly being deployed as autonomous
agents in real-world environments. As these deployments scale, multi-agent
interactions become inevitable, making it essential to understand strategic
behavior in such systems. A central open question is whether LLM agents, like
reinforcement learning agents, can shape the learning dynamics and influence
the behavior of others through interaction alone. In this paper, we present the
first investigation of opponent shaping (OS) with LLM-based agents. Existing OS
algorithms cannot be directly applied to LLMs, as they require higher-order
derivatives, face scalability constraints, or depend on architectural
components that are absent in transformers. To address this gap, we introduce
ShapeLLM, an adaptation of model-free OS methods tailored for transformer-based
agents. Using ShapeLLM, we examine whether LLM agents can influence co-players'
learning dynamics across diverse game-theoretic environments. We demonstrate
that LLM agents can successfully guide opponents toward exploitable equilibria
in competitive games (Iterated Prisoner's Dilemma, Matching Pennies, and
Chicken) and promote coordination and improve collective welfare in cooperative
games (Iterated Stag Hunt and a cooperative version of the Prisoner's Dilemma).
Our findings show that LLM agents can both shape and be shaped through
interaction, establishing opponent shaping as a key dimension of multi-agent
LLM research.

</details>


### [144] [Mix- and MoE-DPO: A Variational Inference Approach to Direct Preference Optimization](https://arxiv.org/abs/2510.08256)
*Jason Bohne,Pawel Polak,David Rosenberg,Brian Bloniarz,Gary Kazantsev*

Main category: cs.LG

TL;DR: 提出Mix- and MoE-DPO框架，通过混合专家架构增强DPO的多任务适应能力


<details>
  <summary>Details</summary>
Motivation: 传统DPO依赖单一模型，难以适应多样化的偏好分布和多任务场景

Method: 使用变分推断优化ELBO，引入潜在变量模型进行专家分配，支持共享基架构和独立专家模型

Result: 验证框架在多种模型规模和多偏好数据集上的有效性

Conclusion: 该框架通过专家专业化、混合泛化和上下文对齐，为LLM对齐提供可扩展解决方案

Abstract: Direct Preference Optimization (DPO) has recently emerged as a simple and
effective alternative to reinforcement learning from human feedback (RLHF) for
aligning large language models (LLMs) with user preferences. However, existing
DPO formulations rely on a single monolithic model, which limits their
expressivity in multi-task settings and their adaptability to heterogeneous or
diverse preference distributions. In this work, we propose Mix- and MoE-DPO, a
framework that extends DPO with both soft mixture models and mixture-of-experts
(MoE) architectures, using a stochastic variational inference approach. Our
method introduces a latent-variable model over expert assignments and optimizes
a variational evidence lower bound (ELBO), enabling stable and efficient
learning of specialized expert policies from preference data. Mix- and MoE-DPO
provides three key advantages over standard DPO: (i) generalization via
universal function approximation through mixtures; (ii) reward and policy
specialization through expert components tailored to distinct preference modes;
and (iii) contextual alignment through input-dependent soft gating that enables
user-specific mixture policies. Our framework supports both shared base
architectures with expert-specific policy heads and fully independent expert
models, allowing flexible trade-offs between parameter efficiency and
specialization. We validate our approach on a variety of model sizes and
multi-preference datasets, demonstrating that Mix- and MoE-DPO offers a
powerful and scalable method for preference-based LLM alignment.

</details>


### [145] [FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts](https://arxiv.org/abs/2510.08396)
*Heming Zou,Yunliang Zang,Wutong Xu,Yao Zhu,Xiangyang Ji*

Main category: cs.LG

TL;DR: FlyLoRA提出基于果蝇嗅觉回路的隐式MoE-LoRA结构，通过秩级专家激活和冻结随机投影矩阵消除显式路由器，在提升多任务性能的同时兼顾计算效率


<details>
  <summary>Details</summary>
Motivation: 传统LoRA存在参数干扰问题，MoE-LoRA变种在单任务调优中有效但引入额外路由参数，且无法解决多任务合并时的任务间干扰

Method: 1. 在up-projection矩阵实施秩级专家激活
2. 使用冻结稀疏随机投影矩阵构建隐式路由器，统一专家路由与下投影操作
3. 利用随机矩阵正交性天然缓解任务间干扰

Result: 在知识理解、科学问答、数学推理和代码生成四大领域实验显示持续优于现有方法，最高提升2.3%准确率

Conclusion: FlyLoRA通过生物启发机制突破现有微调范式限制，其隐式路由器设计为参数高效学习提供了新方向

Abstract: Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method for foundation models, but it suffers from parameter interference,
resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based
LoRA variants show promise in mitigating intra-task correlations in single-task
instruction tuning, they introduce additional router parameters and remain
ineffective in multi-task model merging where inter-task interference arises.
Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit
MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the
up-projection matrix, and (2) an implicit router that unifies expert routing
and down-projection, where a frozen sparse random projection matrix replaces
the traditional dense trainable version. This design resolves the trade-off
between intra-task decorrelation and computational efficiency by eliminating
the need for an explicit router, while inherently mitigating inter-task
interference due to the orthogonality property of random matrices. Extensive
experiments across four domains -- general knowledge understanding, scientific
question answering, mathematical reasoning, and code generation -- demonstrate
consistent performance improvements over existing methods. Beyond empirical
gains, FlyLoRA highlights how biological structures can inspire innovations in
AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA.

</details>


### [146] [xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning](https://arxiv.org/abs/2510.08439)
*Cheng Qian,Zuxin Liu,Shirley Kokane,Akshara Prabhakar,Jielin Qiu,Haolin Chen,Zhiwei Liu,Heng Ji,Weiran Yao,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang*

Main category: cs.LG

TL;DR: xRouter通过强化学习实现动态路由，在LLM部署中实现成本与性能的平衡，无需手动规则即可调用外部模型组合。


<details>
  <summary>Details</summary>
Motivation: 现有静态路由策略无法自适应利用LLM成本-性能频谱，需开发成本感知的智能编排方案。

Method: 构建基于工具调用的路由系统，采用端到端强化学习框架，设计显式成本奖励函数指导路由决策训练。

Result: 在多个基准测试中实现显著降本（任务完成率相当），揭示了小模型协调能力局限等关键训练洞见。

Conclusion: xRouter为成本感知的LLM编排提供了可扩展方案，其开源实现将加速智能路由系统的实际部署。

Abstract: Modern LLM deployments confront a widening cost-performance spectrum: premium
models deliver strong reasoning but are expensive, while lightweight models are
economical yet brittle on complex tasks. Static escalation rules and keyword
heuristics under-utilize this spectrum and fail to adapt across task types. We
present xRouter, a tool-calling-based routing system in which a learned router
can either answer directly or invoke one or more external models. The router is
trained end-to-end with reinforcement learning using an explicit, cost-aware
reward that encodes cost-performance trade-offs, eliminating the need for
hand-engineered routing rules. Our implementation encompasses the full
reinforcement learning framework, including reward and cost accounting, as well
as the deployment and evaluation pipelines. Across diverse benchmarks, xRouter
achieves strong cost-performance trade-offs (e.g., substantial cost reductions
at comparable task completion rates), and provides empirical insights into what
reliably helps learned routing and what does not, ranging from model
trainability to the difficulty of eliciting sophisticated orchestration
behaviors in small open models. We hope these findings and our open
implementation will serve as a practical substrate for advancing learned,
cost-aware LLM orchestration.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [147] [PATCH: Mitigating PII Leakage in Language Models with Privacy-Aware Targeted Circuit PatcHing](https://arxiv.org/abs/2510.07452)
*Anthony Hughes,Vasisht Duddu,N. Asokan,Nikolaos Aletras,Ning Ma*

Main category: cs.CR

TL;DR: 提出PATCH方法，通过识别并编辑语言模型中的PII泄露电路，实现更好的隐私-效用平衡，结合DP可将泄露召回率降至0.01%


<details>
  <summary>Details</summary>
Motivation: 现有防御机制（如差分隐私）虽能减少PII泄露，但导致模型效用显著下降，且未能根除PII泄露的底层电路问题

Method: 1. 使用电路发现技术定位PII泄露电路 2. 直接编辑目标电路以阻断信息泄露路径

Result: PATCH使PII泄露召回率降低65%，与DP结合后残留泄露召回率最低达0.01%

Conclusion: PATCH有效解决了现有防御机制无法处理的底层电路泄露问题，为隐私保护提供了更精准的解决方案

Abstract: Language models (LMs) may memorize personally identifiable information (PII)
from training data, enabling adversaries to extract it during inference.
Existing defense mechanisms such as differential privacy (DP) reduce this
leakage, but incur large drops in utility. Based on a comprehensive study using
circuit discovery to identify the computational circuits responsible PII
leakage in LMs, we hypothesize that specific PII leakage circuits in LMs should
be responsible for this behavior. Therefore, we propose PATCH (Privacy-Aware
Targeted Circuit PatcHing), a novel approach that first identifies and
subsequently directly edits PII circuits to reduce leakage. PATCH achieves
better privacy-utility trade-off than existing defenses, e.g., reducing recall
of PII leakage from LMs by up to 65%. Finally, PATCH can be combined with DP to
reduce recall of residual leakage of an LM to as low as 0.01%. Our analysis
shows that PII leakage circuits persist even after the application of existing
defense mechanisms. In contrast, PATCH can effectively mitigate their impact.

</details>
