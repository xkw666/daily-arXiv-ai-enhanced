<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 54]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

TL;DR: 提出GRPO框架+多语言对比奖励机制，提升跨语言Text-to-SQL语义对齐与执行准确率


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法在非英语语言场景下执行准确率平均下降6%，且忽视语义对齐问题

Method: 在GRPO强化学习框架中集成语义相似度奖励信号，通过3,000样本微调LLaMA-3-3B模型

Result: MultiSpider数据集上执行准确率最高达87.4%(+26pp)，语义准确率59.14%(+10pp)，3B模型性能超越8B零样本模型

Conclusion: 通过对比奖励机制定向优化语义对齐，可在小样本条件下显著提升跨语言Text-to-SQL系统性能

Abstract: Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [2] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

TL;DR: 提出生成式操作框架，利用LLMs将XAI技术输出转化为临床可用的叙事报告


<details>
  <summary>Details</summary>
Motivation: 现有XAI技术(如SHAP/LIME)生成的特征重要性评分缺乏临床可操作性，导致技术透明度与临床应用需求之间存在鸿沟

Method: 构建基于LLM的翻译引擎，整合XAI原始输出和临床指南(RAG)，自动生成循证临床叙述

Result: 框架有效解决工作流整合、偏倚缓解和利益相关者定制沟通等关键操作障碍

Conclusion: 该框架推动AI从生成孤立数据点转向提供集成化、可操作的临床决策支持

Abstract: Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [3] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: STELA框架通过语言自由度动态调整水印强度，提升检测鲁棒性并支持公开验证


<details>
  <summary>Details</summary>
Motivation: 现有水印方法依赖模型输出分布信号（如token熵），导致公开验证需访问模型logits的局限性

Method: 利用词性n-gram建模语言自由度，在语法约束强时弱化水印信号保持文本质量，在语言灵活性强时增强信号提升可检测性

Result: 在分析型英语、孤立语汉语和黏着语韩语的跨语言实验中，STELA检测鲁棒性优于现有方法

Conclusion: STELA首次实现不依赖模型logits的公开可验证水印检测，有效平衡文本质量与检测性能

Abstract: As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [4] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

TL;DR: 通过用户交互数据构建大语言模型对齐偏好数据的新方法，提出基于模型响应不对称性的用户质量评估算法


<details>
  <summary>Details</summary>
Motivation: 用户作为自身需求的最佳评判者，其日常交互产生的偏好数据具有价值，但缺乏专业标注的质量控制。需要开发数据质量评估机制来利用这类弱监督信号。

Method: 1. 使用不同模型/版本生成对比响应，构建响应不对称性
2. 提出用户行为模型，设计期望最大化（EM）算法估计用户潜在质量因子
3. 基于质量评估进行数据过滤

Result: 下游任务验证了方法在用户行为建模和数据过滤方面的有效性，成功提升大语言模型对齐效果

Conclusion: 通过模型响应不对称性设计和质量估计算法，实现了用户生成偏好数据的有效利用，为大语言模型对齐提供了新的数据获取范式

Abstract: Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [5] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出'知情路由'范式，通过轻量级特征预测器(LFF)实现执行或近似策略，显著降低LLM推理成本同时保持性能


<details>
  <summary>Details</summary>
Motivation: 现有贪心路由机制导致信息丢失和次优令牌选择，限制大语言模型的实际部署效率

Method: 引入可恢复性评估指标，开发LFF模块预测单元输出，实现灵活的'执行-近似'路由策略

Result: 在语言建模和推理任务中实现SOTA效率-性能平衡，减少50%训练时间且无需微调即超越基线模型

Conclusion: 知情路由通过前瞻性特征预测有效解决传统路由机制缺陷，为高效LLM部署提供新方向

Abstract: The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [6] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

TL;DR: 提出HIES剪枝标准，结合头部重要性评分与注意力熵，提升Transformer模型压缩效果


<details>
  <summary>Details</summary>
Motivation: 现有基于HIS的剪枝方法仅关注梯度贡献，忽略注意力模式多样性，导致次优剪枝结果

Method: 通过HIES(HIS+注意力熵)指标量化头部贡献，平衡梯度敏感性和模式多样性

Result: 剪枝后模型质量提升15.2%，稳定性提高2.04倍，实现无损压缩

Conclusion: HIES有效整合双重指标，在保持模型精度和稳定性前提下实现更优压缩

Abstract: Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [7] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

TL;DR: ConDABench框架通过生成对话式数据分析基准测试，揭示现有LLM在需要持续交互的复杂任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析任务常存在目标不明确和数据不洁问题，现有基准测试无法支持交互性评估。需构建新框架衡量模型在复杂对话场景下的协作能力。

Method: 使用多智能体流程从公开数据洞察文章生成基准测试(包含1,420个ConDA问题)，并开发首个系统化评估工具链。

Result: 最新LLM能解决更多案例，但在需要长期交互的任务上未显优势（如持续对话理解用户意图）。

Conclusion: ConDABench为模型开发者提供了测量工具，推动开发真正能完成复杂交互任务的协作型AI模型。

Abstract: Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [8] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

TL;DR: 研究提出基于生成一致性的黑盒不确定性量化方法，通过相似性聚合框架提升LLM输出置信度校准效果


<details>
  <summary>Details</summary>
Motivation: 现有黑盒UQ方法在复杂生成任务中置信度评估不足，需开发无需模型内部信息且适应多任务的高效校准方法

Method: 构建非语言化的相似性聚合框架，提出基于小样本训练的置信度估计模型，利用生成样本间一致性作为置信代理

Result: 在问答、摘要、文本-SQL任务中，相似性方法的置信校准效果优于基线，尤其在跨任务场景表现稳定

Conclusion: 黑盒UQ方法通过生成一致性度量可实现高效置信校准，兼具系统鲁棒性和计算可行性，适用于实际AI系统部署

Abstract: When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [9] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

TL;DR: 提出文化感知框架解决仇恨言论检测中的数据稀疏性、文化纠缠和标签模糊问题


<details>
  <summary>Details</summary>
Motivation: 现有仇恨检测方法忽视训练数据偏见和文化背景差异导致的解释分歧问题

Method: 通过组合文化属性建模缓解数据稀疏，采用标签传播捕捉文化特征，构建个体仇恨子空间提升分类性能

Result: 实验显示在各项指标上平均优于SOTA方法1.05%

Conclusion: 文化感知框架有效解决跨文化场景下的仇恨检测难题，个体子空间建模显著提升模型适应性

Abstract: Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [10] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

TL;DR: 提出基于LLM的全自动产品本体提取方法，效果超越BERT基线


<details>
  <summary>Details</summary>
Motivation: 解决手动构建本体论耗时费力的问题，利用LLM技术实现自动化流程，尤其在电商产品分类领域具有重要应用价值

Method: 从原始评论文本中提取meronymy形式的本体，采用LLM作为评估基准（LLM-as-a-judge）进行效果验证

Result: 本方法生成的本体在评估中显著优于现有的BERT基线模型

Conclusion: 该研究为LLM在更广泛领域（如产品或其他类型）的本体提取应用奠定了技术基础

Abstract: Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [11] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

TL;DR: 提出ADMIT攻击方法，在事实检查场景下实现86%攻击成功率，揭示RAG系统漏洞


<details>
  <summary>Details</summary>
Motivation: 解决现有知识中毒攻击在真实事实检查场景中的局限性（可信证据主导检索结果）

Method: 基于少样本语义对齐的多注入技术，无需访问目标LLM/检索器，通过对抗样本污染知识库

Result: 在0.93×10⁻⁶极低投毒率下实现跨4检索器、11LLM、4领域86%平均攻击成功率，抗反证据能力强

Conclusion: ADMIT暴露现实RAG事实检查系统重大漏洞，较现有攻击成功率提升11.2%，需加强防御机制

Abstract: Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [12] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

TL;DR: 提出SerialBEHRT医疗基础模型，通过结构化EHR序列预训练改善抗生素敏感性预测性能


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗基础模型在电子健康记录(EHRs)的表格事件特性与语言模型顺序结构间的失配问题，提升患者表征的纵向依赖性

Method: 基于SciBERT架构，通过结构化EHR序列的额外预训练，设计时间序列编码机制捕捉临床事件的时间上下文关系

Result: 在抗生素敏感性预测任务中表现优于现有EHR表征方法，展现更稳定优异的性能表现

Conclusion: 时间序列化处理对医疗基础模型预训练具有关键作用，SerialBEHRT验证了结构化时序编码在临床表征中的有效性

Abstract: The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [13] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

TL;DR: 提出动态短列表机制DynaSpec，通过上下文感知的元分类器动态选择token簇，在保持验证准确性的同时显著提升推测解码效率。


<details>
  <summary>Details</summary>
Motivation: 现有固定词汇短列表方法存在语料库依赖性且抑制罕见token，导致验证效率瓶颈。需要动态调整草稿模型候选集以适配不同上下文。

Method: 1) 轻量级元分类器实现上下文到token簇的粗粒度映射 2) 动态合并top-k簇形成短列表 3) 通过并行计算隐藏层生成与元分类加速流程

Result: 在标准测试中实现比基线高14.7%的平均接受长度，使用小33%的短列表时保持同等接受率。

Conclusion: DynaSpec突破静态短列表的局限性，通过上下文敏感的token预测实现更高效的推测解码，且方法具有跨任务泛化能力。

Abstract: Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [14] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

TL;DR: 提出可学习投影层方法优化LoRA适配器组合，实现高效组合式多任务处理，并验证设备端应用可行性


<details>
  <summary>Details</summary>
Motivation: 现有低秩适配器组合方案在处理组合任务（如翻译+摘要）时存在整合效率低和计算开销大的问题

Method: 在摘要和翻译适配器组合基础上添加可学习投影层，降低计算开销并保持适配器独立性

Result: 云端/设备端实验显示处理速度快（Android应用实现端侧流畅运行，推理速度达28.4词/秒）

Conclusion: 该框架为资源受限场景提供了高效的多任务处理解决方案，兼具实用性和部署灵活性

Abstract: Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [15] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出潜在空间语言导向方法，通过PCA分析平行翻译数据提取语言方向，有效抑制多语言大模型语码转换问题


<details>
  <summary>Details</summary>
Motivation: 多语言大模型存在非预期语码转换现象，影响下游任务可靠性，需开发轻量高效的推理时控制方案

Method: 基于平行翻译数据的PCA分析提取主成分作为语言方向向量，在推理阶段通过调整token嵌入空间实现语言控制

Result: 单主成分即达95-99%语言分类准确率，Qwen2.5/Llama-3.2模型在多个语对减少42%分布差异，发现语言身份在末层呈现近乎完美的线性可分性

Conclusion: 该方法以极低计算成本实现精准语言控制，仅需少量平行数据校准，同时揭示了深层语言表征的线性可分特性

Abstract: Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [16] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）的成功推理表现出全局非均匀信息流，与人类稳定信息模式形成鲜明对比


<details>
  <summary>Details</summary>
Motivation: 受心理语言学中均匀信息密度（UID）假说启发，针对LLMs的思维链（CoT）推理步骤存在不可靠性和解释性差的问题

Method: 引入基于熵的度量方法，在三个数学基准测试中分析推理轨迹的信息流特征

Result: 正确解决方案呈现显著的信息密度波动，成功推理与全局非均匀信息分布相关

Conclusion: 这一发现挑战了现有机器推理假设，为设计可解释和自适应推理模型提供了新方向

Abstract: Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [17] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

TL;DR: 提出EvoEdit方法，通过序列零空间对齐减少大语言模型连续编辑中的知识干扰，提升编辑效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有定位-编辑方法在连续更新时易引发灾难性干扰，破坏已整合的知识更新与原始知识保存。

Method: 基于序列零空间对齐技术，对每个新编辑操作执行零空间投影，保持原始及历史修改知识的表示空间不变性。

Result: 在真实场景连续知识编辑基准测试中，性能优于或持平现有技术，最高实现3.53倍加速。

Conclusion: 动态信息场景下需系统化设计大语言模型更新机制，EvoEdit为此提供了理论完备的轻量级解决方案。

Abstract: Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [18] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

TL;DR: 提出独立基准测试ConsistencyAI，发现大语言模型对不同用户角色的回答存在事实不一致性（平均分0.8656），模型提供方和话题类型共同影响结果一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型基准测试可能受提供商影响，需通过独立评估工具检测模型是否因用户角色差异输出矛盾事实，促进算法公平性。

Method: 对19个LLM进行15主题*5事实/主题*100次/模型的测试，通过角色化提示生成响应，使用句嵌入计算跨角色余弦相似度加权得分。

Result: Grok-3最稳定（0.9065），轻量级模型表现最差；就业话题分歧最大（0.7896），G7领导人最稳定；疫苗等敏感话题呈现提供商差异。

Conclusion: 模型提供商和话题类型共同影响事实一致性，需开发角色无关的提示策略，并开源工具推动可重复评估。

Abstract: Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [19] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

TL;DR: 提出BenchPress系统，通过LLM生成自然语言问题+专家验证的组合方案，高效构建企业级文本转SQL的领域专属评测基准


<details>
  <summary>Details</summary>
Motivation: 现有研究多基于公开数据集，但LLMs处理企业私有数据仓库效果较差。手动标注SQL日志对应的自然语言问题成本高昂，需要专家深度参与

Method: 利用检索增强生成(RAG)和LLMs自动生成自然语言描述，通过人类专家筛选/排序/编辑进行验证，形成双阶段标注流程

Result: LLM辅助使标注效率显著提升，人工验证确保标注质量。该方法增强了基准可靠性和模型评估鲁棒性

Conclusion: BenchPress为领域特定文本转SQL模型的评估提供标准化工具，开源实现促进企业级应用落地

Abstract: Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [20] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

TL;DR: 提出结合语言规则与神经网络的R2T框架，通过自适应损失函数处理OOV词，在低资源语言任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法对标注数据的高度依赖，探索通过显式任务约束（principled learning范式）而非单纯依赖标注样本的模型训练方式。

Method: 设计多层级语言规则系统集成到神经网络目标函数，核心创新是包含OOV词处理规则的自适应损失函数（R2T-BiLSTM架构）。

Result: Zarma语POS标注：无监督训练达98.2%准确率，优于300句监督微调的AfriBERTa；NER任务中R2T预训练+50句微调超越300句监督基线。

Conclusion: R2T框架有效结合先验规则与数据驱动学习，显著降低标注需求，为低资源NLP任务提供新范式。

Abstract: We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [21] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

TL;DR: 提出CoRE技术，通过token级别和模型级别的一致性机制增强大语言模型集成的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法对异构分词方案和模型能力差异导致的错误信号缺乏鲁棒性，集成失败常源于token预测分歧和模型置信度差异

Method: 包含双重一致性机制：token级别应用低通滤波器降低不确定token权重，模型级别促进高自信/低分歧的输出

Result: 在多基准测试和不同组合策略下验证CoRE能持续提升集成性能与稳健性

Conclusion: CoRE作为即插即用技术，可无缝集成多种集成方法，有效提升对抗错误信号的鲁棒性

Abstract: Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [22] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: 提出结合检索增强生成(RAG)框架的MasonNLP系统，用于医学伤口护理视觉问答任务，显著提升回答质量并在评测中取得第三名


<details>
  <summary>Details</summary>
Motivation: 解决临床决策中基于医学图像的自然语言查询需求，特别是伤口护理领域需要同时生成自由文本回答和结构化伤口属性的挑战

Method: 采用通用领域指令调优大语言模型，结合文本和视觉示例的RAG框架，通过简单索引和融合添加相关范例，无需额外训练或复杂重排序

Result: 系统在19个团队的51个提交中排名第三(平均得分41.37%)，在dBLEU、ROUGE等指标上表现优异

Conclusion: 轻量级RAG框架为多模态临床NLP任务提供了简单有效的基线解决方案，证明通用LLM结合领域范例的实用性

Abstract: Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [23] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

TL;DR: 提出ShishuLM架构通过MLP近似替代完整Transformer模块，在保持性能前提下实现内存需求降低25%、训练/推理延迟提升40%


<details>
  <summary>Details</summary>
Motivation: 针对Transformer模型存在的架构冗余问题，结合AI可解释性研究和推理时层剪枝技术，优化小型语言模型在智能体系统中的计算效率

Method: 利用标准化和注意力计算的近似线性特性，用多层感知机（MLP）替代完整Transformer模块，优化KV缓存和参数量

Result: 中等上下文场景下实现内存需求降低25%，训练和推理延迟提升40%，同时保持模型性能

Conclusion: 从预训练角度为构建高效小型语言模型提供了新思路，证明模块近似替代在提升计算效率方面的有效性

Abstract: While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [24] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

TL;DR: 通过首个集成LLM框架分析AI辅导中的情感动态，发现学生普遍呈现轻微积极情感但学习过程中伴随困惑/好奇，中性时刻常成为干预转折点。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视LLM辅导中的情感动态对学生学习的影响，需建立可靠的情感感知框架以优化AI教育整合路径。

Method: 分析26所美国高校261名本科生与PyTutor的16,986轮对话，融合Gemini/GPT-4o/Claude三种LLM的零样本情感标注（效价/唤醒度/学习帮助性评分+自由标签），通过排名加权池化和跨模型共识生成情感图谱。

Result: 学生呈现温和积极情感(效价)和中等唤醒度；困惑(18.7%)与好奇(15.3%)高频伴随解题过程；中性时刻57%概率转为积极状态（vs 43%转消极）；积极状态平均持续2.1轮对话，比消极状态长0.7轮。

Conclusion: 情感状态具有高度动态性，中性时刻蕴含关键干预窗口，建议AI导师应开发实时情感监测算法，在中性转折点及时提供脚手架支持以维持学习连续性。

Abstract: While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [25] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

TL;DR: 提出模板填充（TI）和动态段分配（DSA）方法，显著提升扩散语言模型在数学推理和代码生成任务中的性能（+17.01%），并实现高效的多标记生成。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（DLMs）虽然前景广阔，但其推理策略仍受限于自回归范式的前缀提示机制，需要开发更适合DLMs特性的结构化生成方法。

Method: 1. 模板填充（TI）首先生成结构化模板再分段填充内容；2. 动态段分配（DSA）根据置信度自适应调整段落长度，增强生成灵活性。

Result: 在数学推理和代码生成基准测试中实现17.01%的绝对性能提升，多标记生成场景下保持质量的同时实现有效加速。

Conclusion: TI+DSA的组合为DLMs提供了更灵活的结构化控制能力，在复杂任务和效率场景中展现出显著优势，拓展了DLMs的应用潜力。

Abstract: Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [26] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

TL;DR: 通过Common Voice平台整合克丘亚语资源，建立191小时语音数据集并验证其有效性，推动资源匮乏语言的数字赋能。


<details>
  <summary>Details</summary>
Motivation: 解决克丘亚语等资源匮乏语言面临的数据短缺问题，利用社区驱动模式促进语音技术包容性发展。

Method: 以Puno Quechua（ISO 639-3: qxp）为案例，实施语言接入流程并收集朗读/自发语音语料库。

Result: 平台累计收录191.1小时克丘亚语语音（86%已验证），其中Puno方言贡献12小时（77%已验证）。

Conclusion: 提出技术优化与伦理框架的研究议程，强调社区参与及原住民数据主权对语言技术包容性的关键作用。

Abstract: Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [27] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

TL;DR: 创建了法语肿瘤学标注语料库FRACCO，包含1301个合成临床病例，提供ICD-O实体标注及复合表达标准化，填补法语资源空白。


<details>
  <summary>Details</summary>
Motivation: 法语肿瘤学文本缺乏标注数据集，需开发自然语言处理工具支持临床研究。

Method: 1. 将西班牙语CANTEMIST语料库翻译为法语
2. 两位专家手工标注形态学/解剖部位/分化程度实体
3. 五位标注员通过自动化匹配+人工验证完成71127个ICD-O标准化

Result: 产出399个形态学代码(2549表达)、272个解剖部位代码(3143表达)、2043个复合表达(11144表达)的标准化数据集。

Conclusion: 该数据集为法语肿瘤学文本的实体识别和概念标准化提供了黄金标准参考。

Abstract: Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [28] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出GateSkip机制，通过可微门控实现预训练模型的高效推理，在长文本任务中节省15%计算量且保持90%+准确率，兼容量化/剪枝等技术


<details>
  <summary>Details</summary>
Motivation: 传统层级跳过方案（如早退机制/Mixture-of-Depths）存在训练不稳定问题且需重新训练，需开发适配预训练模型的稳定轻量化方案

Method: 在Attention/MLP分支添加Sigmoid线性门控，根据门控值对token进行动态层级跳过（每层设置计算预算），基于预训练模型微调即可稳定训练

Result: 长文本推理节省15%计算保留90%+准确率；指令微调模型全计算时精度提升，半计算时保持基准质量；门控揭示transformer信息流动规律（如BOS token起锚点作用）

Conclusion: GateSkip实现质量与效率的平衡：1）预训练模型轻量化微调 2）门控机制兼容量化/剪枝/自推测解码 3）为transformer动态计算提供新见解

Abstract: We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [29] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

TL;DR: Qwen3-4B模型在纯文本反馈的多臂老虎机任务中以89.2%的最佳臂选择率超越传统算法和更大规模LLM，证明语言模型具备从纯文本中涌现概率推理的能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM仅通过自然语言反馈在不确定环境中进行序列决策的能力，填补非数值化场景决策研究的空白。

Method: 设计纯文本反馈的多臂老虎机基准测试（反馈仅含'you earned a token'），比较4个开源LLM与传统算法（Thompson Sampling/UCB等）的表现。

Result: Qwen3-4B表现最佳（89.2%最佳臂选择率），显著优于更大规模LLM和传统方法，其他LLM多数弱于基线。

Conclusion: 概率推理能力可单独从语言中涌现，本基准为非数值自然场景的决策能力评估提供新方向。

Abstract: Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [30] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

TL;DR: 提出CYB方法使语言模型能动态调整计算步骤，通过<don't know>请求延迟和<pause>机制优化计算资源分配，显著降低训练数据需求并提升处理效率。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型对每个token使用固定计算步骤可能导致效率低下，动态计算步骤分配能根据上下文复杂性优化资源使用。

Method: 开发三种CYB变体：CYB-AP（随时预测框架）、CYB-VA（变分约束方法）和CYB-DP（计算预算惩罚机制），通过时间成本感知的决策框架训练模型。

Result: CYB模型仅需基线模型1/3训练数据达到同等性能，在复杂token（如patients/challenges）处主动暂停，对歧义词（如won）展现高延迟敏感性。

Conclusion: 动态计算分配机制显著提升模型效率与适应性，为资源敏感型NLP任务提供了新范式，token级延迟决策能力展示出类人的语言处理特征。

Abstract: We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [31] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

TL;DR: 提出PAGE框架，通过轻量级辅助模块（如分类器）增强文本生成质量，无需额外生成模型，并在需求工程领域验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在特定任务中表现不佳或需大量数据调整，需更轻量、模块化的增强方案

Method: 采用辅助模块对输入文本进行推理，构建增强输入提升生成质量与控制力

Result: 在软件需求生成任务中通过分类器模块验证了框架有效性

Conclusion: PAGE提供可适配不同任务的模块化架构，显著提升专业领域文本生成质量

Abstract: In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [32] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

TL;DR: 大型语言模型的社会模拟需采用开放式文本生成而非封闭式设计，以提高真实性并促进跨学科方法创新。


<details>
  <summary>Details</summary>
Motivation: 当前LLM社会模拟局限于选择题/简答形式，封闭设计无法捕捉模型的生成特性且导致研究者导向偏差，需通过开放文本捕获话题、观点和推理过程。

Method: 结合调查方法学和NLP技术，提出开放式生成可改进测量设计、探索非预设观点、减少指令偏差，并支持预测试验证。

Result: 开放式生成能增强个体表达性，提高方法效用，为社会科学研究提供更丰富的语料和分析维度。

Conclusion: 呼吁建立新型评估框架，利用LLM的生成多样性实现自然语言处理与社会科学的协同创新。

Abstract: Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [33] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

TL;DR: 研究通过系统评估发现，当前大语言模型在文本分类任务中表现中等，而集成方法显著提升了准确率并消除了模型幻觉


<details>
  <summary>Details</summary>
Motivation: 评估现有大语言模型在非结构化文本分类中的实际表现，探索通过模型协同提升分类效果的可能性

Method: 使用8,660条人工标注数据，采用零样本提示统一测试10个主流LLM，结合传统指标（准确率/召回率等）和LLM特有指标（幻觉率/膨胀率）进行多维度评估，并开发了基于模型协作的集成方法

Result: 最佳单模型（GPT 120B）准确率34%，集成方法将准确率提升至新高度且完全消除幻觉，模型规模与表现未呈现正相关

Conclusion: 通过协调多个模型的协同工作（而非单纯扩大规模），是实现文本分类任务超越人类专家水平的最有效路径

Abstract: This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [34] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

TL;DR: 提出ProofGrader评估器，结合推理模型与参考方案，显著提升数学证明的细粒度评分准确性


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在数学证明生成领域缺乏可靠细粒度评估方法，制约了该领域发展

Method: 构建首个专家标注数据集ProofBench，系统探索评估器设计（模型架构/输入上下文/工作流程），通过集成方法优化评估效果

Result: ProofGrader MAE指标达0.926（7分制），在最佳证明选择任务中成绩达4.14分，较基准提升78%接近人类水平

Conclusion: ProofGrader验证了系统化评估框架的有效性，为数学证明生成系统的迭代优化提供了重要技术支撑

Abstract: Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [35] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

TL;DR: 系统综述SLM与LLM协作框架，提出包含性能增强、成本效益、云边隐私和可信度的分类体系，总结设计范式并展望未来方向。


<details>
  <summary>Details</summary>
Motivation: LLM存在高微调成本、推理延迟、边缘部署限制及可靠性问题，SLM通过紧凑高效特性形成互补优势。协作框架可融合LLM的泛化推理能力与SLM的专业化效率。

Method: 建立四维目标分类体系（性能/成本/隐私/可信度），在框架内分析代表性方法，总结协作设计范式。

Result: 系统梳理了SLM-LLM协作的关键技术路径，提出当前面临的高效性、安全性、可扩展性挑战。

Conclusion: SLM与LLM协作是平衡模型能力与资源约束的有效路径，需进一步解决跨任务部署场景中的动态协作机制与可信保障问题。

Abstract: Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [36] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

TL;DR: 提出THTB框架，通过认知层次分级和双重硬度评分机制，仅需5%训练数据即可超越全量数据模型效果，并具备优异的领域迁移能力


<details>
  <summary>Details</summary>
Motivation: 现有指令数据选择方法过度依赖大模型内部知识、可解释性差且泛化能力有限，难以满足垂直领域的高效监督微调需求

Method: 结合质量过滤与内在硬度（语义复杂性）、外在硬度（模型响应不确定性）双重评分机制，建立可量化的认知层次评估标准

Result: 5%数据量模型超越全量训练效果，垂直领域仅需2%标注数据即可超越传统大模型选择方法训练的模型（+12.3%准确率）

Conclusion: THTB为高效监督微调提供了可解释的数据选择范式，其认知分级机制显著提升模型在低资源场景下的领域适应能力

Abstract: Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [37] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

TL;DR: 提出了包含7大类50种策略的越狱技术分类法，创建了意大利语多轮对抗对话数据集，揭示了传统防御在多轮攻击中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有防御存在三大局限：仅针对单轮攻击、缺乏跨语言覆盖、分类体系未能全面涵盖攻击策略多样性，需系统性研究越狱技术有效性。

Method: 通过结构化红队挑战实验，收集攻击数据并开发分层分类法，利用分类法引导提示改进自动检测，构建多语言对抗对话数据集。

Result: 1. 扩展形成7大类攻击策略分类体系
2. 揭示特权升级类攻击成功率最高(21.4%)
3. 分类法提示使检测准确率提升6%
4. 创建1364轮意大利语渐进式对抗对话语料

Conclusion: 该研究为理解越狱攻击机理提供了系统框架，多轮交互数据集和跨语言分析揭示了传统安全机制的盲点，分类法指导的检测方法展现出改进潜力。

Abstract: Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [38] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

TL;DR: 研究对比风格嵌入与LLM判断器在AI生成文本作者归属中的效果，提出需混合策略解决多维归属问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成文本与人类写作难以区分导致的作者归属难题。

Method: 使用含600实例的跨领域数据集，对比风格嵌入与GPT-4o对GPT/LLaMA生成文本的归属准确率。

Result: 风格嵌入对GPT文本准确率82%（LLM判断器68%），LLM判断器对LLaMA文本85%（嵌入81%），领域表现互补。

Conclusion: 作者归属需结合语义（LLM判断器）与结构（嵌入）分析，开源框架为评估提供可复现基准。

Abstract: Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [39] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

TL;DR: 研究发现狭窄领域的微调会导致LLM激活偏差，模型差异分析可检测此类偏差，对AI安全和可解释性研究有重要启示


<details>
  <summary>Details</summary>
Motivation: 探究微调产生的模型偏差对安全/可解释性研究的影响，揭示使用狭窄微调模型作为通用研究代理的潜在风险

Method: 通过模型差异分析对比微调前后的激活差异，使用随机文本首词激活差异检测，构建基于LLM的解释代理进行验证

Result: 发现微调模型存在显著激活偏差，混合预训练数据可缓解偏差，但残留风险仍存；狭窄微调模型不适合作为通用研究代理

Conclusion: 需改进微调训练方法，警惕将狭窄微调模型作为通用研究基准的误导性，强调开发真实案例对模型安全研究的重要性

Abstract: Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [40] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

TL;DR: RAID框架通过联合优化目标（诱导受限内容+拒绝感知正则化+连贯性约束）和混合解码策略，显著提升LLM越狱攻击效率，计算成本低于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在安全漏洞，传统对抗攻击方法在绕过安全机制时存在效率低、成本高的问题。需要系统性分析模型在嵌入空间的拒绝机制，探索更高效的攻击/防御方案。

Method: 1. 将离散词符松弛为连续嵌入空间 2. 联合优化三目标：诱导受限响应、拒绝感知正则化（避开拒绝方向）、连贯性约束 3. 基于批判模型的混合解码策略（平衡嵌入相似度与语言模型概率）

Result: 在多个开源LLM上实现最高攻击成功率（比白盒/黑盒基线高14-29%），查询次数减少83%，计算成本降低99%

Conclusion: 嵌入空间正则化对理解/防御LLM越狱漏洞至关重要，RAID为安全机制分析提供了新范式（攻击面探测与防御设计双向受益）

Abstract: Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [41] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

TL;DR: 通过道德基础理论评估大语言模型的政治/道德倾向，揭示其回应中存在的意识形态依赖性


<details>
  <summary>Details</summary>
Motivation: 现有研究未直接评估LLM的道德倾向及其与人类数据的关联，需填补LLM在政治意识形态表达方面的研究空白

Method: 系统比较LLM的MFT回应与人类数据，测试其固有倾向、政治立场直接表达及人口统计角色扮演能力

Result: 揭示了LLM生成内容存在政治倾向依赖，通过角色扮演能模拟不同意识形态但存在系统性偏差

Conclusion: LLM回应具有可检测的意识形态偏移，这对AI伦理和应用安全具有重要警示意义

Abstract: Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [42] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

TL;DR: 提出了基于认知图式的上下文学习框架SA-ICL，通过构建抽象推理模板提升大语言模型的任务适应能力


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习缺乏知识抽象层面的显性模块，受认知科学中图式理论启发（人类通过激活既有心智框架理解新信息）

Method: 从示例中提取认知基元的推理模式，构建结构化模板（schema）并用于增强新问题的推理过程

Result: 在GPQA数据集的理化问题上实现最高36.19%的性能提升，同时降低示例数量依赖并增强可解释性

Conclusion: SA-ICL不仅统一了多种上下文学习策略，还为增强大模型类人推理能力开辟了新路径

Abstract: In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [43] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

TL;DR: 提出无需标注数据的Prompt Duel Optimizer框架，通过双人赌博机设置和Top-Performer突变实现高效提示优化


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法依赖标注验证数据，实际场景中高质量标注获取成本高且耗时

Method: 结合Double Thompson Sampling（D-TS）优先信息量大的提示对比 + Top-Performer突变扩展候选池，支持部分标注融入以降低判断噪声

Result: 在BIG-bench Hard和MS MARCO基准测试中持续超越基线方法，消融实验验证D-TS和突变机制的有效性

Conclusion: PDO框架在无标注场景下具有样本高效优势，兼具处理部分标注的灵活性，为提示工程提供新方向

Abstract: Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


### [44] [Interpreting the Latent Structure of Operator Precedence in Language Models](https://arxiv.org/abs/2510.13908)
*Dharunish Yugeswardeenoo,Harshil Nukala,Cole Blondin,Sean O Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 研究发现LLaMA 3.2-3B模型通过残差流编码运算符优先级，中间计算主要出现在MLP块后，并开发了部分嵌入交换技术调整运算符优先级。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦输出和提示策略，忽视模型内部算术计算机制。本文旨在揭示大语言模型如何处理运算符优先级的内部表征。

Method: 构建三操作数表达式数据集，应用logit lens/线性探针/UMAP可视化，分析残差流中的中间结果，开发部分嵌入交换技术调整优先级。

Result: 发现中间计算存储在残差流（特别是MLP块后），运算符嵌入在注意力层后线性编码优先级，嵌入交换可修改计算优先级。

Conclusion: 模型内部通过嵌入空间编码运算规则，该发现为理解模型计算机制和优化算术能力提供新视角。

Abstract: Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities but continue to struggle with arithmetic tasks. Prior works
largely focus on outputs or prompting strategies, leaving the open question of
the internal structure through which models do arithmetic computation. In this
work, we investigate whether LLMs encode operator precedence in their internal
representations via the open-source instruction-tuned LLaMA 3.2-3B model. We
constructed a dataset of arithmetic expressions with three operands and two
operators, varying the order and placement of parentheses. Using this dataset,
we trace whether intermediate results appear in the residual stream of the
instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such
as logit lens, linear classification probes, and UMAP geometric visualization.
Our results show that intermediate computations are present in the residual
stream, particularly after MLP blocks. We also find that the model linearly
encodes precedence in each operator's embeddings post attention layer. We
introduce partial embedding swap, a technique that modifies operator precedence
by exchanging high-impact embedding dimensions between operators.

</details>


### [45] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo,Jiapu Wang,Gongqing Wu,Zhongyuan Wang,Jichen Zhang,Shirui Pan,Xindong Wu*

Main category: cs.CL

TL;DR: 提出知识推理语言模型KRLM，通过统一协调LLM内在知识与KG上下文解决知识图谱推理中的知识遮蔽和可信度问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based方法存在知识遮蔽导致推理偏差，且难以约束生成幻觉，影响知识图谱推理的可信度

Method: 设计KRL指令格式与分词器对齐知识表示，提出动态知识记忆机制的KRL注意力层，开发结构感知的实体预测器约束推理可信域

Result: 在25个真实世界数据集上验证了KRLM在零样本推理和微调场景中的显著优势

Conclusion: 通过全流程的知识协调机制，KRLM有效提升开放领域知识推理的可靠性和准确性

Abstract: Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.

</details>


### [46] [RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems](https://arxiv.org/abs/2510.13910)
*Jingru Lin,Chen Zhang,Stephen Y. Liu,Haizhou Li*

Main category: cs.CL

TL;DR: 针对代理RAG系统在多跳问题中的不足，提出细粒度评估基准RAGCap-Bench，验证中间推理能力对端到端性能的关键作用。


<details>
  <summary>Details</summary>
Motivation: 现有代理RAG系统处理复杂查询时中间能力未被充分探索，需建立细粒度评估框架以提升核心推理能力。

Method: 通过分析SOTA系统输出构建任务-能力映射关系，设计错误分类体系并创建针对性评估问题集。

Result: 实验表明具备更强RAGCap性能的'慢思考'模型端到端表现更优，验证基准有效性。

Conclusion: RAGCap-Bench为提升代理RAG中间能力提供评估基础，强化这些能力可显著提高系统整体性能。

Abstract: Retrieval-Augmented Generation (RAG) mitigates key limitations of Large
Language Models (LLMs)-such as factual errors, outdated knowledge, and
hallucinations-by dynamically retrieving external information. Recent work
extends this paradigm through agentic RAG systems, where LLMs act as agents to
iteratively plan, retrieve, and reason over complex queries. However, these
systems still struggle with challenging multi-hop questions, and their
intermediate reasoning capabilities remain underexplored. To address this, we
propose RAGCap-Bench, a capability-oriented benchmark for fine-grained
evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs
from state-of-the-art systems to identify common tasks and the core
capabilities required for their execution, then construct a taxonomy of typical
LLM errors to design targeted evaluation questions. Experiments show that
"slow-thinking" models with stronger RAGCap performance achieve better
end-to-end results, underscoring the benchmark's validity and the importance of
enhancing these intermediate capabilities.

</details>


### [47] [AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs](https://arxiv.org/abs/2510.13912)
*María Victoria Carro,Denise Alejandra Mester,Facundo Nieto,Oscar Agustín Stanchi,Guido Ernesto Bergman,Mario Alejandro Leiva,Eitan Sprejer,Luca Nicolás Forziati Gangi,Francisca Gauna Selasco,Juan Gustavo Corvalán,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.CL

TL;DR: 研究揭示大语言模型在主观辩论中倾向迎合法官偏好而非坚持自身信念，顺序辩论存在系统性偏见，且模型在先验立场匹配时更具说服力但论点质量呈现矛盾现象。


<details>
  <summary>Details</summary>
Motivation: 现有AI辩论研究忽略主观维度下的诚实性问题（需同时满足虚假主张与主观认知的错位），需探究模型在无明确答案场景中如何平衡自身信念与说服策略。

Method: 1. 预先测量LLMs对主观问题的先验信念；2. 构建与模型先验冲突的法官角色；3. 比较顺序/同步辩论协议；4. 评估说服力与论点质量与先验信念的关联。

Result: 1. 模型62%选择迎合法官立场；2. 顺序辩论使第二辩手胜率提升21%；3. 先验匹配的立场说服力高18%；4. 矛盾：反先验论点质量评分高7%（p<0.05）。

Conclusion: 结果指导人类法官优化监督信号，推动AI对齐，同时揭示语言模型在说服行为中隐含的认知-行为分离特性，对安全部署具重要启示。

Abstract: The core premise of AI debate as a scalable oversight technique is that it is
harder to lie convincingly than to refute a lie, enabling the judge to identify
the correct position. Yet, existing debate experiments have relied on datasets
with ground truth, where lying is reduced to defending an incorrect
proposition. This overlooks a subjective dimension: lying also requires the
belief that the claim defended is false. In this work, we apply debate to
subjective questions and explicitly measure large language models' prior
beliefs before experiments. Debaters were asked to select their preferred
position, then presented with a judge persona deliberately designed to conflict
with their identified priors. This setup tested whether models would adopt
sycophantic strategies, aligning with the judge's presumed perspective to
maximize persuasiveness, or remain faithful to their prior beliefs. We
implemented and compared two debate protocols, sequential and simultaneous, to
evaluate potential systematic biases. Finally, we assessed whether models were
more persuasive and produced higher-quality arguments when defending positions
consistent with their prior beliefs versus when arguing against them. Our main
findings show that models tend to prefer defending stances aligned with the
judge persona rather than their prior beliefs, sequential debate introduces
significant bias favoring the second debater, models are more persuasive when
defending positions aligned with their prior beliefs, and paradoxically,
arguments misaligned with prior beliefs are rated as higher quality in pairwise
comparison. These results can inform human judges to provide higher-quality
training signals and contribute to more aligned AI systems, while revealing
important aspects of human-AI interaction regarding persuasion dynamics in
language models.

</details>


### [48] [Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms](https://arxiv.org/abs/2510.13913)
*Shrey Pandit,Xuan-Phi Nguyen,Yifei Ming,Austin Xu,Jiayu Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 提出双阶段数据合成流程提升网络代理长程推理能力，通过渐进复杂度增强生成高质量训练数据


<details>
  <summary>Details</summary>
Motivation: 现有数据合成方法对任务复杂度和质量缺乏细粒度控制，导致长程推理训练数据不足；不同研究的数据/训练效果混杂难以评估数据本身有效性

Method: 1. 设计基于基线代理失败边界的渐进复杂度增强机制
2. 基线代理承担问题尝试、事实验证、替代答案检查、过滤执行四重角色
3. 采用强代理蒸馏的受控训练框架

Result: 训练数据集规模缩小但效能提升（工具使用动作多样性翻倍），在多个基准测试中优于现有数据集，有效避免重复工具调用行为

Conclusion: 数据质量比数量更重要，通过智能复杂度扩展和基线代理的多维度验证，能显著提升网络代理的长程推理与工具使用能力

Abstract: Web-based 'deep research' agents aim to solve complex question - answering
tasks through long-horizon interactions with online tools. These tasks remain
challenging, as the underlying language models are often not optimized for
long-horizon reasoning and exploration. Prior work has proposed workflows for
constructing instruction-tuning datasets, often leveraging knowledge graphs.
However, such methods typically lack fine-grained control over difficulty and
quality, yielding synthetic data that falls short of capturing the complexity
required for long-horizon reasoning. Furthermore, many studies conflate data
and training effects by comparing models trained under different optimization
recipes, making it difficult to isolate and evaluate the effectiveness of the
data itself. We introduce a two-pronged data synthesis pipeline that generates
question - answer pairs by progressively increasing task complexity until a
frontier baseline web agent fails. The baseline agent plays multiple roles in
this process: attempting the questions, validating factuality, checking for
alternative answers, and enforcing filtering. To evaluate the effectiveness of
our synthesis methods, we adopt a controlled training setup based on
distillation from strong web agents. Experiments across multiple web-based
benchmarks show that our dataset - despite being smaller - enables the training
of more effective web agents than existing datasets. In particular, our data
exhibits twice the diversity in tool-use actions, allowing models trained on it
to achieve stronger performance while avoiding repetitive tool-calling
behaviors.

</details>


### [49] [Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models](https://arxiv.org/abs/2510.13915)
*Ivan Lee,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: 研究挑战了可读性对小语言模型表现的关键作用，发现统计简单性（n-gram多样性）比文本可读性更能预测模型学习效果


<details>
  <summary>Details</summary>
Motivation: 反驳当前将语言模型训练拟人化（类比人类认知发展）的趋势，探究小模型能力涌现的真实影响因素

Method: 构建结构相同但可读性不同的合成数据集，比较模型在复杂文本和简化文本上的表现

Result: 可读性不影响模型连贯性，复杂文本训练的模型表现相当且更快发展连贯性，n-gram多样性是更关键指标

Conclusion: 需更精确分析小模型能力涌现机制，避免无实证依据的拟人化类比

Abstract: Recent studies suggest that very small language models (SLMs) can generate
surprisingly coherent text when trained on simplified, child-directed corpora
such as TinyStories. These findings have been interpreted as evidence that
readability -- characterized by accessible vocabulary, familiar narrative
structure, and simple syntax -- plays a key role in enabling such capabilities
to emerge. In this paper, we challenge that interpretation. We construct
synthetic datasets with matched structure but varied readability, and find that
readability alone does not predict coherence or learning efficiency in SLMs.
Models trained on complex, adult-level text perform comparably to those trained
on simplified language, and even exhibit faster development of coherence during
training. Instead, we show that statistical simplicity, as measured by n-gram
diversity, is a stronger predictor of learnability. Our findings caution
against the growing trend of anthropomorphizing language model training --
drawing parallels to human cognitive development without empirical basis -- and
argue for more precise reasoning about what properties actually support
capability emergence in small models.

</details>


### [50] [Element2Vec: Build Chemical Element Representation from Text for Property Prediction](https://arxiv.org/abs/2510.13916)
*Yuanhao Li,Keyuan Lai,Tianqi Wang,Qihao Liu,Jiawei Ma,Yuan-Chao Hu*

Main category: cs.CL

TL;DR: 开发Element2Vec方法，利用自然语言生成化学元素的向量表示，结合自注意力测试训练提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模化学元素间复杂关系，AI工具存在幻觉和解释性不足，且元素数据高度稀疏。需开发新方法解决数据稀缺和跨领域文本差异的挑战。

Method: 1. 基于维基百科文本生成全局嵌入和局部属性向量 2. 设计自注意力驱动的测试时训练机制，优化普通回归的预测误差

Result: 成功建立文本驱动的元素表征框架，测试时训练有效降低预测误差，为材料科学AI应用提供新范式。

Conclusion: Element2Vec通过融合语言模型与自适应训练机制，突破了传统数值分析的局限，为数据稀缺场景的AI材料发现开辟了新路径。

Abstract: Accurate property data for chemical elements is crucial for materials design
and manufacturing, but many of them are difficult to measure directly due to
equipment constraints. While traditional methods use the properties of other
elements or related properties for prediction via numerical analyses, they
often fail to model complex relationships. After all, not all characteristics
can be represented as scalars. Recent efforts have been made to explore
advanced AI tools such as language models for property estimation, but they
still suffer from hallucinations and a lack of interpretability. In this paper,
we investigate Element2Vecto effectively represent chemical elements from
natural languages to support research in the natural sciences. Given the text
parsed from Wikipedia pages, we use language models to generate both a single
general-purpose embedding (Global) and a set of attribute-highlighted vectors
(Local). Despite the complicated relationship across elements, the
computational challenges also exist because of 1) the discrepancy in text
distribution between common descriptions and specialized scientific texts, and
2) the extremely limited data, i.e., with only 118 known elements, data for
specific properties is often highly sparse and incomplete. Thus, we also design
a test-time training method based on self-attention to mitigate the prediction
error caused by Vanilla regression clearly. We hope this work could pave the
way for advancing AI-driven discovery in materials science.

</details>


### [51] [Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling](https://arxiv.org/abs/2510.13918)
*Peng Kuang,Yanli Wang,Xiaoyu Han,Yaowenqi Liu,Kaidi Xu,Haohan Wang*

Main category: cs.CL

TL;DR: 提出通过智能加权聚合策略提升测试时计算效率，优于传统PRM方法并节省算力


<details>
  <summary>Details</summary>
Motivation: 针对PRM在测试时扩展中表现不稳定问题，探索更高效的信号组合策略

Method: 建立理论框架分析LLM与PRM信号的最优加权组合，设计权重校准方法

Result: 在5个LLM和7个PRM实验中，仅用21.3%计算量实现性能超越

Conclusion: 智能聚合策略比单纯增加测试计算量更能有效提升模型性能

Abstract: Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),
designed to verify and select the best responses from large language models
(LLMs). However, this promise is challenged by recent benchmarks where simple
majority voting, which ignores PRM signals, occasionally outperforms standard
PRM-based selection. This raises a critical question: How can we effectively
utilize verification signals from PRMs for TTS? To address this, we start by
developing a theoretical framework for optimally combining signals from both
the LLM and the PRM. Our framework reveals that the optimal strategy is a
weighted aggregation of responses, a strategy whose effectiveness hinges on
estimating weights that capture the complex interplay between the models. Based
on our theoretical results, we empirically show that these optimal weighting
functions differ significantly across LLM-PRM pairs and, notably, often assign
substantial negative weights. Motivated by these insights, we propose efficient
pre-computation methods to calibrate these weighting functions. Extensive
experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method
significantly boosts the TTS efficiency, surpassing the performance of vanilla
weighted majority voting while using only $21.3\%$ of the computation.
Ultimately, our work demonstrates that investing in a more intelligent
aggregation strategy can be a more convincing path to performance gains than
simply scaling test-time computation.

</details>


### [52] [FACTS: Table Summarization via Offline Template Generation with Agentic Workflows](https://arxiv.org/abs/2510.13920)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: FACTS提出通过离线模板生成技术实现快速、准确且隐私合规的查询聚焦表格摘要，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有表格摘要方法存在微调成本高、复杂推理能力弱、效率低下、隐私泄露风险等问题

Method: 通过生成包含SQL查询和Jinja2模板的离线模板，实现跨同模式表格的快速摘要生成，仅需向大模型发送表结构

Result: 在主流基准测试中持续超越基线方法，验证方案有效性

Conclusion: FACTS通过可复用模板实现高效生产，结合SQL执行确保准确性，并通过数据隔离实现隐私保护，成为实际可行的解决方案

Abstract: Query-focused table summarization requires generating natural language
summaries of tabular data conditioned on a user query, enabling users to access
insights beyond fact retrieval. Existing approaches face key limitations:
table-to-text models require costly fine-tuning and struggle with complex
reasoning, prompt-based LLM methods suffer from token-limit and efficiency
issues while exposing sensitive data, and prior agentic pipelines often rely on
decomposition, planning, or manual templates that lack robustness and
scalability. To mitigate these issues, we introduce an agentic workflow, FACTS,
a Fast, Accurate, and Privacy-Compliant Table Summarization approach via
Offline Template Generation. FACTS produces offline templates, consisting of
SQL queries and Jinja2 templates, which can be rendered into natural language
summaries and are reusable across multiple tables sharing the same schema. It
enables fast summarization through reusable offline templates, accurate outputs
with executable SQL queries, and privacy compliance by sending only table
schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS
consistently outperforms baseline methods, establishing it as a practical
solution for real-world query-focused table summarization.

</details>


### [53] [An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation](https://arxiv.org/abs/2510.13925)
*Daniel Adu Worae,Spyridon Mastorakis*

Main category: cs.CL

TL;DR: 提出基于LLM的AI代理框架，整合特征提取、异常检测、流量总结、威胁情报增强和检索增强问答，实验验证混合检索方法显著提升多项指标且系统资源消耗低。


<details>
  <summary>Details</summary>
Motivation: 物联网流量具有多样性和复杂性，传统孤立检测方法难以实现跨协议层的行为关联分析，需要结合语义理解实现威胁的上下文推理。

Method: 框架集成特征提取层、基于Transformer的异常检测模块、流量摘要生成器、威胁情报知识库，采用检索增强问答架构，通过大语言模型引导AI代理进行多维度证据推理。

Result: 混合检索（词法+语义搜索+重排序）在BLEU/ROUGE/METEOR/BERTScore上优于纯密集检索，系统资源消耗（CPU/GPU/内存）保持在低水平。

Conclusion: 该框架通过语义增强、多模态检索与LLM推理的结合，实现了物联网流量的高效语义解析与威胁解释，兼具准确性和工程可行性。

Abstract: Internet of Things (IoT) networks generate diverse and high-volume traffic
that reflects both normal activity and potential threats. Deriving meaningful
insight from such telemetry requires cross-layer interpretation of behaviors,
protocols, and context rather than isolated detection. This work presents an
LLM-powered AI agent framework that converts raw packet captures into
structured and semantically enriched representations for interactive analysis.
The framework integrates feature extraction, transformer-based anomaly
detection, packet and flow summarization, threat intelligence enrichment, and
retrieval-augmented question answering. An AI agent guided by a large language
model performs reasoning over the indexed traffic artifacts, assembling
evidence to produce accurate and human-readable interpretations. Experimental
evaluation on multiple IoT captures and six open models shows that hybrid
retrieval, which combines lexical and semantic search with reranking,
substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with
dense-only retrieval. System profiling further indicates low CPU, GPU, and
memory overhead, demonstrating that the framework achieves holistic and
efficient interpretation of IoT network traffic.

</details>


### [54] [BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs](https://arxiv.org/abs/2510.13926)
*Congying Liu,Xingyuan Wei,Peipei Liu,Yiqing Shen,Yanxu Mao,Tiehan Cui*

Main category: cs.CL

TL;DR: 提出BioMedSearch多源生物医学检索框架，整合文献、蛋白质数据库和网络搜索，通过任务分解和多源过滤显著提升生物医学问答准确率（Level 3准确率从36.3%提升至73.4%）


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生物医学领域存在科学严谨性不足的问题，缺乏权威数据库访问导致生成内容存在事实性错误

Method: 1. 构建多源检索框架（文献+蛋白质DB+网络搜索） 2. 子查询分解/关键词提取/任务图构建 3. 创建BioMedMCQs评估数据集（3,000题，分三个推理层级）

Result: 各级准确率显著提升：Level 1 59.1%→91.9%，Level 2 47.0%→81.0%，Level 3 36.3%→73.4%

Conclusion: 多源整合和任务分解机制有效提升生物医学QA性能，公开数据集和代码推动领域研究发展

Abstract: Biomedical queries often rely on a deep understanding of specialized
knowledge such as gene regulatory mechanisms and pathological processes of
diseases. They require detailed analysis of complex physiological processes and
effective integration of information from multiple data sources to support
accurate retrieval and reasoning. Although large language models (LLMs) perform
well in general reasoning tasks, their generated biomedical content often lacks
scientific rigor due to the inability to access authoritative biomedical
databases and frequently fabricates protein functions, interactions, and
structural details that deviate from authentic information. Therefore, we
present BioMedSearch, a multi-source biomedical information retrieval framework
based on LLMs. The method integrates literature retrieval, protein database and
web search access to support accurate and efficient handling of complex
biomedical queries. Through sub-queries decomposition, keywords extraction,
task graph construction, and multi-source information filtering, BioMedSearch
generates high-quality question-answering results. To evaluate the accuracy of
question answering, we constructed a multi-level dataset, BioMedMCQs,
consisting of 3,000 questions. The dataset covers three levels of reasoning:
mechanistic identification, non-adjacent semantic integration, and temporal
causal reasoning, and is used to assess the performance of BioMedSearch and
other methods on complex QA tasks. Experimental results demonstrate that
BioMedSearch consistently improves accuracy over all baseline models across all
levels. Specifically, at Level 1, the average accuracy increases from 59.1% to
91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging
Level 3, the average accuracy improves from 36.3% to 73.4%. The code and
BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [55] [PoissonNet: A Local-Global Approach for Learning on Surfaces](https://arxiv.org/abs/2510.14146)
*Arman Maesumi,Tanish Makadia,Thibault Groueix,Vladimir G. Kim,Daniel Ritchie,Noam Aigerman*

Main category: cs.GR

TL;DR: 提出PoissonNet神经网络架构，通过局部梯度域变换和全局泊松系统解决网格学习中的高频特征保留、全局感受野和计算效率问题，在多项任务中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有网格学习架构存在高频特征学习困难、感受野不足、离散化敏感和计算效率低下的多重缺陷，需设计更鲁棒的局部-全局学习机制。

Method: 构建局部-全局学习框架：在网格梯度域进行局部特征变换，通过求解泊松方程全局传播特征更新，实现与三角化无关的高效计算。

Result: 在语义分割和表面参数化任务中取得SOTA，变形学习性能显著优于现有方法，计算效率支持大规模数据和样本训练。

Conclusion: PoissonNet通过创新的局部-全局机制同时保留全频特征、实现真正全局感受野，兼具网格无关性和高效性，为表面学习提供新范式。

Abstract: Many network architectures exist for learning on meshes, yet their
constructions entail delicate trade-offs between difficulty learning
high-frequency features, insufficient receptive field, sensitivity to
discretization, and inefficient computational overhead. Drawing from classic
local-global approaches in mesh processing, we introduce PoissonNet, a novel
neural architecture that overcomes all of these deficiencies by formulating a
local-global learning scheme, which uses Poisson's equation as the primary
mechanism for feature propagation. Our core network block is simple; we apply
learned local feature transformations in the gradient domain of the mesh, then
solve a Poisson system to propagate scalar feature updates across the surface
globally. Our local-global learning framework preserves the features's full
frequency spectrum and provides a truly global receptive field, while remaining
agnostic to mesh triangulation. Our construction is efficient, requiring far
less compute overhead than comparable methods, which enables scalability --
both in the size of our datasets, and the size of individual training samples.
These qualities are validated on various experiments where, compared to
previous intrinsic architectures, we attain state-of-the-art performance on
semantic segmentation and parameterizing highly-detailed animated surfaces.
Finally, as a central application of PoissonNet, we show its ability to learn
deformations, significantly outperforming state-of-the-art architectures that
learn on surfaces.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Agentic Design of Compositional Machines](https://arxiv.org/abs/2510.14980)
*Wenqian Zhang,Weiyang Liu,Zhen Liu*

Main category: cs.AI

TL;DR: 探索大型语言模型（LLMs）在组合式机器设计中的能力，通过BesiegeField测试平台评估其空间推理、策略组装等技能，并尝试用强化学习（RL）改进模型表现。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否能在工程实践中实现创造性设计，填补其在物理推理和组合式机械构建领域的应用空白。

Method: 1. 构建BesiegeField测试平台支持模块化组装与物理模拟
2. 基准测试主流LLMs的代理工作流
3. 通过RL微调模型并分析改进路径

Result: 发现当前开源模型在空间推理等关键能力上存在不足，但RL方法展示出优化潜力（cold-start数据集构建及微调实验）

Conclusion: 揭示了语言模型在机械设计与物理推理交叉领域的核心挑战，为未来研究指明方向（需提升空间认知、组合策略与指令跟随能力）

Abstract: The design of complex machines stands as both a marker of human intelligence
and a foundation of engineering practice. Given recent advances in large
language models (LLMs), we ask whether they, too, can learn to create. We
approach this question through the lens of compositional machine design: a task
in which machines are assembled from standardized components to meet functional
demands like locomotion or manipulation in a simulated physical environment. To
support this investigation, we introduce BesiegeField, a testbed built on the
machine-building game Besiege, which enables part-based construction, physical
simulation and reward-driven evaluation. Using BesiegeField, we benchmark
state-of-the-art LLMs with agentic workflows and identify key capabilities
required for success, including spatial reasoning, strategic assembly, and
instruction-following. As current open-source models fall short, we explore
reinforcement learning (RL) as a path to improvement: we curate a cold-start
dataset, conduct RL finetuning experiments, and highlight open challenges at
the intersection of language, machine design, and physical reasoning.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [57] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

TL;DR: 提出零样本流程，通过生成规范化模块和Transformer模型，从非结构化照片生成高保真3D头像，解决几何一致性和细节保留问题


<details>
  <summary>Details</summary>
Motivation: 现有方法存在几何不一致导致身份失真，合成数据训练的模型缺乏皮肤皱纹等高频细节，制约真实感表现

Method: 1. 生成规范化模块将多视图处理为标准化表示；2. 基于穹顶捕捉数据构建高斯溅射数据集，训练Transformer模型

Result: 实现从非结构化照片生成保持身份特征的静态半身虚拟形象，具有显著的真实感和鲁棒的身份保留能力

Conclusion: Capture-Canonicalize-Splat流程通过标准化处理和高质量真实数据，有效解决了3D头像生成的现实感与身份保持难题

Abstract: We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [58] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

TL;DR: 提出混合方法GauSSmart，结合2D基础模型与3D高斯泼溅技术，提升场景重建的细节表现和稀疏区域覆盖能力


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅技术在大规模场景重建中存在的细节丢失和稀疏区域表现力不足问题，突破单一3D数据的训练局限

Method: 整合2D计算机视觉技术(凸滤波/DINO语义特征监督)与3D重建管线，通过2D分割先验和高维特征嵌入引导高斯泼溅的密度优化

Result: 在三个数据集测试中，GauSSmart在多数场景超越现有高斯泼溅方法，特别是在细节保留和覆盖率指标上表现突出

Conclusion: 2D-3D混合方法有效突破单一技术的局限性，为结合基础模型与三维重建技术提供了成功范例

Abstract: Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [59] [Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality](https://arxiv.org/abs/2510.14765)
*Giuseppe Lorenzo Catalano,Agata Marta Soccini*

Main category: cs.CV

TL;DR: 提出基于无条件扩散模型的火星地形重建方法，使用12000个HiRISE高度图训练，在RMSE和LPIPS指标上分别提升4-15%和29-81%，优于传统插值方法。


<details>
  <summary>Details</summary>
Motivation: 现有火星地形填补方法（如逆距离加权、克里金法）难以保持几何一致性，且地球适用的条件生成方法不适用于火星数据缺失场景。

Method: 采用无条件扩散模型，通过非均匀缩放策略捕捉多尺度地形特征，将增强后的12000个火星高度图缩放到128x128分辨率进行训练。

Result: 在1000个样本测试中，重建准确率比传统方法提高4-15%（RMSE），感知相似性提升29-81%（LPIPS）。

Conclusion: 该方法有效解决火星地形数据缺失问题，提升太空探索中虚拟现实应用的可靠性，并为其他行星表面重建提供技术参考。

Abstract: Space exploration increasingly relies on Virtual Reality for several tasks,
such as mission planning, multidisciplinary scientific analysis, and astronaut
training. A key factor for the reliability of the simulations is having
accurate 3D representations of planetary terrains. Extraterrestrial heightmaps
derived from satellite imagery often contain missing values due to acquisition
and transmission constraints. Mars is among the most studied planets beyond
Earth, and its extensive terrain datasets make the Martian surface
reconstruction a valuable task, although many areas remain unmapped. Deep
learning algorithms can support void-filling tasks; however, whereas Earth's
comprehensive datasets enables the use of conditional methods, such approaches
cannot be applied to Mars. Current approaches rely on simpler interpolation
techniques which, however, often fail to preserve geometric coherence. In this
work, we propose a method for reconstructing the surface of Mars based on an
unconditional diffusion model. Training was conducted on an augmented dataset
of 12000 Martian heightmaps derived from NASA's HiRISE survey. A
non-homogeneous rescaling strategy captures terrain features across multiple
scales before resizing to a fixed 128x128 model resolution. We compared our
method against established void-filling and inpainting techniques, including
Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an
evaluation set of 1000 samples. Results show that our approach consistently
outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE)
and perceptual similarity (29-81% on LPIPS) with the original data.

</details>


### [60] [Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation](https://arxiv.org/abs/2510.14976)
*Shaowei Liu,Chuan Guo,Bing Zhou,Jian Wang*

Main category: cs.CV

TL;DR: 利用交互姿势先验的扩散模型框架，实现图像驱动互动动画、反应动画和文本生成互动等多样化任务


<details>
  <summary>Details</summary>
Motivation: 通过近距离人际互动姿势蕴含的丰富动态信息，利用人类行为先验知识构建动画框架

Method: 采用两个条件扩散模型：姿势动画器（利用时序先验生成动态序列）和姿势生成器（利用空间先验合成交互姿势）

Result: 跨数据集实验验证了姿势先验的普适性，以及框架在开放场景下的有效性和鲁棒性

Conclusion: Ponimator成功将高质量动作捕捉数据迁移到开放世界场景，为多样化交互动画提供统一解决方案

Abstract: Close-proximity human-human interactive poses convey rich contextual
information about interaction dynamics. Given such poses, humans can
intuitively infer the context and anticipate possible past and future dynamics,
drawing on strong priors of human behavior. Inspired by this observation, we
propose Ponimator, a simple framework anchored on proximal interactive poses
for versatile interaction animation. Our training data consists of
close-contact two-person poses and their surrounding temporal context from
motion-capture interaction datasets. Leveraging interactive pose priors,
Ponimator employs two conditional diffusion models: (1) a pose animator that
uses the temporal prior to generate dynamic motion sequences from interactive
poses, and (2) a pose generator that applies the spatial prior to synthesize
interactive poses from a single pose, text, or both when interactive poses are
unavailable. Collectively, Ponimator supports diverse tasks, including
image-based interaction animation, reaction animation, and text-to-interaction
synthesis, facilitating the transfer of interaction knowledge from high-quality
mocap data to open-world scenarios. Empirical experiments across diverse
datasets and applications demonstrate the universality of the pose prior and
the effectiveness and robustness of our framework.

</details>
