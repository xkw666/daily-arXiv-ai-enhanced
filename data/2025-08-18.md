<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation](https://arxiv.org/abs/2508.10904)
*Jie Lei,Ruofan Jia,J. Andrew Zhang,Hao Zhang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的层次化算法到HDL编码代理A2HCoder，通过横向模块分解和纵向分步翻译机制有效解决算法与硬件实现间的鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 当前无线通信系统对超低延迟和功耗的严苛需求导致算法到硬件部署效率不足，MATLAB与Verilog等硬件描述语言在内存访问模式、数据处理方式和数据类型表达上的根本性差异，传统方法存在效率低下和人工开发成本高的问题。

Method: 1. 横向维度将复杂算法分解为模块化功能块提升可维护性
2. 纵向维度采用分层次细粒度翻译流程，结合MATLAB/Vitis HLS工具链进行调试和电路级合成
3. 分层框架增强代码生成鲁棒性和可解释性

Result: 在5G无线通信领域实际部署案例验证显示，该方法显著抑制LLM代码生成中的幻觉问题，确保硬件级正确性，具备实用性和高效部署特性。

Conclusion: A2HCoder通过结构化分层设计成功弥合算法与硬件实现间隙，为跨领域算法硬件协同优化提供可靠解决方案，大幅降低开发门槛和人工干预需求。

Abstract: In wireless communication systems, stringent requirements such as ultra-low
latency and power consumption have significantly increased the demand for
efficient algorithm-to-hardware deployment. However, a persistent and
substantial gap remains between algorithm design and hardware implementation.
Bridging this gap traditionally requires extensive domain expertise and
time-consuming manual development, due to fundamental mismatches between
high-level programming languages like MATLAB and hardware description languages
(HDLs) such as Verilog-in terms of memory access patterns, data processing
manners, and datatype representations. To address this challenge, we propose
A2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large
language models (LLMs), designed to enable agile and reliable
algorithm-to-hardware translation. A2HCoder introduces a hierarchical framework
that enhances both robustness and interpretability while suppressing common
hallucination issues in LLM-generated code. In the horizontal dimension,
A2HCoder decomposes complex algorithms into modular functional blocks,
simplifying code generation and improving consistency. In the vertical
dimension, instead of relying on end-to-end generation, A2HCoder performs
step-by-step, fine-grained translation, leveraging external toolchains such as
MATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured
process significantly mitigates hallucinations and ensures hardware-level
correctness. We validate A2HCoder through a real-world deployment case in the
5G wireless communication domain, demonstrating its practicality, reliability,
and deployment efficiency.

</details>


### [2] [PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins](https://arxiv.org/abs/2508.10906)
*Sihan Chen,John P. Lalor,Yi Yang,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 提出PersonaTwin框架，通过整合人口统计、行为和心理测量数据构建自适应数字孪生，在医疗场景中实现与理想设置相当的仿真保真度，并验证了下游模型的预测与公平性表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在捕捉用户多维细微差异方面存在不足，需要更全面的数字孪生建模方法支持个性化用户模拟。

Method: 开发多层级提示条件框架(PersonaTwin)，集成8500+医疗用户数据，结合文本相似度指标和人口统计公平性评估进行系统验证，涵盖GPT-4o和Llama模型。

Result: 仿真保真度达理想设置的99.2%，下游模型预测准确率与个体模型差异<3%，公平性指标标准差控制在0.08以内。

Conclusion: PersonaTwin框架证实了LLM数字孪生在情感化用户模拟中的潜力，为个性化行为分析提供有效工具，特别是在医疗健康领域的应用前景广阔。

Abstract: While large language models (LLMs) afford new possibilities for user modeling
and approximation of human behaviors, they often fail to capture the
multidimensional nuances of individual users. In this work, we introduce
PersonaTwin, a multi-tier prompt conditioning framework that builds adaptive
digital twins by integrating demographic, behavioral, and psychometric data.
Using a comprehensive data set in the healthcare context of more than 8,500
individuals, we systematically benchmark PersonaTwin against standard LLM
outputs, and our rigorous evaluation unites state-of-the-art text similarity
metrics with dedicated demographic parity assessments, ensuring that generated
responses remain accurate and unbiased. Experimental results show that our
framework produces simulation fidelity on par with oracle settings. Moreover,
downstream models trained on persona-twins approximate models trained on
individuals in terms of prediction and fairness metrics across both
GPT-4o-based and Llama-based models. Together, these findings underscore the
potential for LLM digital twin-based approaches in producing realistic and
emotionally nuanced user simulations, offering a powerful tool for personalized
digital user modeling and behavior analysis.

</details>


### [3] [gpt-oss-120b & gpt-oss-20b Model Card](https://arxiv.org/abs/2508.10925)
*OpenAI,:,Sandhini Agarwal,Lama Ahmad,Jason Ai,Sam Altman,Andy Applebaum,Edwin Arbus,Rahul K. Arora,Yu Bai,Bowen Baker,Haiming Bao,Boaz Barak,Ally Bennett,Tyler Bertao,Nivedita Brett,Eugene Brevdo,Greg Brockman,Sebastien Bubeck,Che Chang,Kai Chen,Mark Chen,Enoch Cheung,Aidan Clark,Dan Cook,Marat Dukhan,Casey Dvorak,Kevin Fives,Vlad Fomenko,Timur Garipov,Kristian Georgiev,Mia Glaese,Tarun Gogineni,Adam Goucher,Lukas Gross,Katia Gil Guzman,John Hallman,Jackie Hehir,Johannes Heidecke,Alec Helyar,Haitang Hu,Romain Huet,Jacob Huh,Saachi Jain,Zach Johnson,Chris Koch,Irina Kofman,Dominik Kundel,Jason Kwon,Volodymyr Kyrylov,Elaine Ya Le,Guillaume Leclerc,James Park Lennon,Scott Lessans,Mario Lezcano-Casado,Yuanzhi Li,Zhuohan Li,Ji Lin,Jordan Liss,Lily,Liu,Jiancheng Liu,Kevin Lu,Chris Lu,Zoran Martinovic,Lindsay McCallum,Josh McGrath,Scott McKinney,Aidan McLaughlin,Song Mei,Steve Mostovoy,Tong Mu,Gideon Myles,Alexander Neitz,Alex Nichol,Jakub Pachocki,Alex Paino,Dana Palmie,Ashley Pantuliano,Giambattista Parascandolo,Jongsoo Park,Leher Pathak,Carolina Paz,Ludovic Peran,Dmitry Pimenov,Michelle Pokrass,Elizabeth Proehl,Huida Qiu,Gaby Raila,Filippo Raso,Hongyu Ren,Kimmy Richardson,David Robinson,Bob Rotsted,Hadi Salman,Suvansh Sanjeev,Max Schwarzer,D. Sculley,Harshit Sikchi,Kendal Simon,Karan Singhal,Yang Song,Dane Stuckey,Zhiqing Sun,Philippe Tillet,Sam Toizer,Foivos Tsimpourlas,Nikhil Vyas,Eric Wallace,Xin Wang,Miles Wang,Olivia Watkins,Kevin Weil,Amy Wendling,Kevin Whinnery,Cedric Whitney,Hannah Wong,Lin Yang,Yu Yang,Michihiro Yasunaga,Kristen Ying,Wojciech Zaremba,Wenting Zhan,Cyril Zhang,Brian Zhang,Eddie Zhang,Shengjia Zhao*

Main category: cs.CL

TL;DR: 开源模型gpt-oss-120b/20b采用混合专家架构，通过蒸馏和强化学习训练，具备深度研究浏览/Python工具调用能力，在数学/编码/安全基准表现优异，完整技术栈开源。


<details>
  <summary>Details</summary>
Motivation: 推动开源推理模型在准确性、推理成本和代理能力方面的边界，同时保持技术透明性和社区可扩展性。

Method: 混合专家Transformer架构+大规模蒸馏强化学习，采用渲染聊天格式优化指令遵循，集成工具调用和环境支持。

Result: 模型在跨领域基准测试中达到先进水平，开源权重/推理实现/工具环境促进广泛采用和后续研究。

Conclusion: 该研究通过系统级优化实现了高性能开源智能体，技术栈的全面开放将加速AI应用开发和安全性研究。

Abstract: We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models
that push the frontier of accuracy and inference cost. The models use an
efficient mixture-of-expert transformer architecture and are trained using
large-scale distillation and reinforcement learning. We optimize the models to
have strong agentic capabilities (deep research browsing, python tool use, and
support for developer-provided functions), all while using a rendered chat
format that enables clear instruction following and role delineation. Both
models achieve strong results on benchmarks ranging from mathematics, coding,
and safety. We release the model weights, inference implementations, tool
environments, and tokenizers under an Apache 2.0 license to enable broad use
and further research.

</details>


### [4] [Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News](https://arxiv.org/abs/2508.10927)
*Jiaxin Pei,Soumya Vadlamannati,Liang-Kang Huang,Daniel Preotiuc-Pietro,Xinyu Hua*

Main category: cs.CL

TL;DR: 开发计算框架自动提取新闻中的公司风险因素，发现微调模型优于零样本/少样本LLMs


<details>
  <summary>Details</summary>
Motivation: 识别公司风险对投资者和金融市场的健康至关重要，需自动化风险因素提取

Method: 提出七维度分析框架（供应链/法规/竞争等），标注744篇新闻并测试多种ML模型

Result: LLaMA-2等LLMs零/少样本表现平庸，微调预训练模型在多数风险识别中更优，成功分析27.7万篇新闻

Conclusion: 基于新闻的风险因素识别能有效揭示企业运营与行业动态，微调模型更适合此任务

Abstract: Identifying risks associated with a company is important to investors and the
well-being of the overall financial market. In this study, we build a
computational framework to automatically extract company risk factors from news
articles. Our newly proposed schema comprises seven distinct aspects, such as
supply chain, regulations, and competitions. We sample and annotate 744 news
articles and benchmark various machine learning models. While large language
models have achieved huge progress in various types of NLP tasks, our
experiment shows that zero-shot and few-shot prompting state-of-the-art LLMs
(e.g. LLaMA-2) can only achieve moderate to low performances in identifying
risk factors. And fine-tuned pre-trained language models are performing better
on most of the risk factors. Using this model, we analyze over 277K Bloomberg
news articles and demonstrate that identifying risk factors from news could
provide extensive insight into the operations of companies and industries.

</details>


### [5] [Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules](https://arxiv.org/abs/2508.10971)
*Nasim Shirvani-Mahdavi,Chengkai Li*

Main category: cs.CL

TL;DR: 提出Rule2Text框架，利用LLM为知识图谱规则生成自然语言解释，并通过微调和类型推断提升解释质量


<details>
  <summary>Details</summary>
Motivation: 知识图谱规则因复杂性和标签规范差异导致人类理解困难，需提升可解释性

Method: 1) 多数据集实验（Freebase变体+生物KG） 2) 系统评估LLM提示策略 3) 开发LLM-as-a-judge评估框架 4) 结合人类反馈微调Zephyr模型 5) 集成类型推断模块

Result: 微调后解释质量显著提升（Gemini 2.0 Flash表现最佳），领域数据集改进明显，LLM评估与人类判断高度一致

Conclusion: Rule2Text成功提升KG可访问性，结合LLM优势与人类反馈，开源实现促进领域应用

Abstract: Knowledge graphs (KGs) can be enhanced through rule mining; however, the
resulting logical rules are often difficult for humans to interpret due to
their inherent complexity and the idiosyncratic labeling conventions of
individual KGs. This work presents Rule2Text, a comprehensive framework that
leverages large language models (LLMs) to generate natural language
explanations for mined logical rules, thereby improving KG accessibility and
usability. We conduct extensive experiments using multiple datasets, including
Freebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the
ogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically
evaluate several LLMs across a comprehensive range of prompting strategies,
including zero-shot, few-shot, variable type incorporation, and
Chain-of-Thought reasoning. To systematically assess models' performance, we
conduct a human evaluation of generated explanations on correctness and
clarity. To address evaluation scalability, we develop and validate an
LLM-as-a-judge framework that demonstrates strong agreement with human
evaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,
and human-in-the-loop feedback, we construct high-quality ground truth
datasets, which we use to fine-tune the open-source Zephyr model. Our results
demonstrate significant improvements in explanation quality after fine-tuning,
with particularly strong gains in the domain-specific dataset. Additionally, we
integrate a type inference module to support KGs lacking explicit type
information. All code and data are publicly available at
https://github.com/idirlab/KGRule2NL.

</details>


### [6] [Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling](https://arxiv.org/abs/2508.10995)
*Tejomay Kishor Padole,Suyash P Awate,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 提出基于验证器的推理时扩展方法，提升掩码扩散语言模型（MDMs）在文本生成中的质量，尤其在文本风格转换任务中超越自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型通过增加去噪步骤或外部验证器提升生成质量，但MDMs领域缺乏有效验证方法。需探索如何利用现成预训练模型构建验证器，优化MDMs的去噪过程。

Method: 开发软值验证器系统：1）利用现成预训练嵌入模型计算语义相似度 2）在去噪过程中筛选优质候选生成 3）与无分类器引导框架协同工作。

Result: 1）MDMs在文本风格转换任务中表现优于自回归模型 2）软值验证器使生成质量提升15-20% 3）验证方法兼容现有引导框架且无需额外训练。

Conclusion: 验证器扩展策略有效释放MDMs潜力，结合预训练模型构建轻量级验证系统，为离散数据扩散模型提供新优化方向。

Abstract: Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

</details>


### [7] [SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth](https://arxiv.org/abs/2508.11009)
*Wenpeng Xing,Lanyi Wei,Haixiao Hu,Rongchang Li,Mohan Li,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: 现有AI安全框架忽视未成年人风险，SproutBench评估套件揭示LLMs安全隐患，提出儿童中心设计指南


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全基准未覆盖儿童/青少年特定认知-情感-社会风险（0-18岁阶段差异），需构建年龄适配的安全评估体系

Method: 开发含1,283个发展阶段对抗提示的SproutBench，分年龄段测试47种LLMs在情感依赖/隐私/危险行为模仿等维度的风险

Result: 发现LLMs存在重大安全漏洞，安全性与风险预防强相关（r=0.87），互动性与年龄适配性呈负相关（r=-0.62）

Conclusion: 研究结果为开发符合儿童发展特点的AI系统提供实证依据，推动建立分级防护机制与适龄交互设计标准

Abstract: The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

</details>


### [8] [Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics](https://arxiv.org/abs/2508.11017)
*Carter Blum,Katja Filipova,Ann Yuan,Asma Ghandeharioun,Julian Zimmert,Fred Zhang,Jessica Hoffmann,Tal Linzen,Martin Wattenberg,Lucas Dixon,Mor Geva*

Main category: cs.CL

TL;DR: 研究通过控制实验揭示大语言模型跨语言知识迁移的机制，发现事实表征统一性是迁移关键，并提出数据分布/分词优化的改进方向


<details>
  <summary>Details</summary>
Motivation: 大语言模型在跨语言事实迁移时存在幻觉问题，需通过控制变量实验解析其底层机制

Method: 使用小型Transformer在合成多语言数据集上训练，通过数据分布调整、分词策略干预进行对照实验

Result: 发现模型存在事实表征分离/统一两个阶段，证实统一表征是迁移前提；提出互信息与语言可解析性作为统一程度预测指标

Conclusion: 控制实验能有效揭示预训练动态，通过优化数据分布和分词策略可系统性提升LLM跨语言迁移能力

Abstract: Large language models (LLMs) struggle with cross-lingual knowledge transfer:
they hallucinate when asked in one language about facts expressed in a
different language during training. This work introduces a controlled setting
to study the causes and dynamics of this phenomenon by training small
Transformer models from scratch on synthetic multilingual datasets. We identify
a learning phase wherein a model develops either separate or unified
representations of the same facts across languages, and show that unification
is essential for cross-lingual transfer. We also show that the degree of
unification depends on mutual information between facts and training data
language, and on how easy it is to extract that language. Based on these
insights, we develop methods to modulate the level of cross-lingual transfer by
manipulating data distribution and tokenization, and we introduce metrics and
visualizations to formally characterize their effects on unification. Our work
shows how controlled settings can shed light on pre-training dynamics and
suggests new directions for improving cross-lingual transfer in LLMs.

</details>


### [9] [Hell or High Water: Evaluating Agentic Recovery from External Failures](https://arxiv.org/abs/2508.11027)
*Andrew Wang,Sophia Hager,Adi Asija,Daniel Khashabi,Nicholas Andrews*

Main category: cs.CL

TL;DR: 研究评估了语言模型代理在遇到外部故障时的规划与适应能力，发现现有模型虽能识别正确函数，但难以根据环境反馈调整备用方案。


<details>
  <summary>Details</summary>
Motivation: 现实应用中语言代理需处理复杂规划问题，但现有模型在外部故障（如功能不可用）时的恢复能力不足，需系统性评估其应对能力。

Method: 设计包含4000+函数调用的基准测试，模拟外部故障场景（如函数突然失效），确保任务仍可完成，分析模型在受限搜索空间中的响应。

Result: 主流模型（包括商业模型）在环境反馈后调整计划的能力显著不足，即使搜索空间受限时也无法有效执行备用方案。

Conclusion: 当前生成模型在动态规划场景中存在关键瓶颈，需改进环境反馈利用能力和弹性规划机制，这为未来研究提供了明确方向。

Abstract: As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

</details>


### [10] [BIPOLAR: Polarization-based granular framework for LLM bias evaluation](https://arxiv.org/abs/2508.11061)
*Martin Pavlíček,Tomáš Filip,Petr Sosík*

Main category: cs.CL

TL;DR: 提出可重复使用的细粒度框架评估大语言模型的极化偏见，通过俄乌战争案例发现模型对乌克兰的积极偏向及语义类别差异。


<details>
  <summary>Details</summary>
Motivation: 现有偏见检测方法在极化相关偏见的跨主题评估和细粒度分析方面存在不足，缺乏系统性的多模型比较框架。

Method: 结合极化敏感情感指标与合成平衡数据集，构建俄乌战争冲突声明数据集，测试Llama-3/Mistral/GPT-4/Claude 3.5/Gemini 1.0等模型。

Result: 模型整体呈现亲乌克兰倾向，不同语义类别下行为差异显著，提示调整会强化语言偏好和公民身份偏见。

Conclusion: 该框架支持自动化数据生成和精细化评估，适用于多种极化场景，与现有偏见检测方法形成互补。

Abstract: Large language models (LLMs) are known to exhibit biases in downstream tasks,
especially when dealing with sensitive topics such as political discourse,
gender identity, ethnic relations, or national stereotypes. Although
significant progress has been made in bias detection and mitigation techniques,
certain challenges remain underexplored. This study proposes a reusable,
granular, and topic-agnostic framework to evaluate polarisation-related biases
in LLM (both open-source and closed-source). Our approach combines
polarisation-sensitive sentiment metrics with a synthetically generated
balanced dataset of conflict-related statements, using a predefined set of
semantic categories.
  As a case study, we created a synthetic dataset that focusses on the
Russia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,
Mistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with
a general trend for more positive sentiment toward Ukraine, the framework
allowed fine-grained analysis with considerable variation between semantic
categories, uncovering divergent behavioural patterns among models. Adaptation
to prompt modifications showed further bias towards preconceived language and
citizenship modification.
  Overall, the framework supports automated dataset generation and fine-grained
bias assessment, is applicable to a variety of polarisation-driven scenarios
and topics, and is orthogonal to many other bias-evaluation strategies.

</details>


### [11] [Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs](https://arxiv.org/abs/2508.11068)
*Nicolas Goulet,Alexandre Blondin Massé,Moussa Abdendi*

Main category: cs.CL

TL;DR: 利用预训练大语言模型将数字词典嵌入AMR图并进行合流简化，分析其与符号落地问题的关联


<details>
  <summary>Details</summary>
Motivation: 探索如何通过现代NLP技术增强AMR图的语义表达能力，解决语义表示中的符号与实际意义连接问题

Method: 1. 使用预训练大语言模型将数字词典嵌入AMR有向图
2. 通过保持电路空间不变的合流变换简化图结构

Result: 成功实现词典的图嵌入和结构简化，保留了图的核心语义特征

Conclusion: 该方法为符号落地问题提供了新的分析视角，展示了语言模型在语义图处理中的应用潜力

Abstract: Abstract meaning representation (AMR) is a semantic formalism used to
represent the meaning of sentences as directed acyclic graphs. In this paper,
we describe how real digital dictionaries can be embedded into AMR directed
graphs (digraphs), using state-of-the-art pre-trained large language models.
Then, we reduce those graphs in a confluent manner, i.e. with transformations
that preserve their circuit space. Finally, the properties of these reduces
digraphs are analyzed and discussed in relation to the symbol grounding
problem.

</details>


### [12] [Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning](https://arxiv.org/abs/2508.11120)
*Lorenzo Jaime Yu Flores,Junyi Shen,Xiaoyuan Gu*

Main category: cs.CL

TL;DR: 论文提出RAMP框架，通过LLM规划与记忆机制提升营销受众策划任务的准确率28%，验证迭代机制使模糊查询召回率提升约20%


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在动态商业环境中的可靠性研究不足，需解决营销领域受众策划任务的准确性和用户满意度问题

Method: 1. 开发RAMP多代理框架（迭代式规划/工具调用/验证/反思） 2. 配备长期记忆库存储客户特定事实 3. 基于88条评估查询的测试方法

Result: 1. 准确率提升28个百分点 2. 模糊查询迭代验证后召回率提升约20% 3. 用户满意度显著提高

Conclusion: LLM规划与记忆机制的结合为动态商业环境部署可靠AI系统提供了实践启示，验证迭代机制对复杂任务效果显著

Abstract: Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

</details>


### [13] [MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents](https://arxiv.org/abs/2508.11133)
*Tomer Wolfson,Harsh Trivedi,Mor Geva,Yoav Goldberg,Dan Roth,Tushar Khot,Ashish Sabharwal,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 研究者开发了MoNaCo基准测试，包含1315个自然复杂问题，测试显示前沿大语言模型处理这类耗时问题时F1值仅61.2%，暴露召回不足与幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有大模型基准测试缺乏真实耗时的人类信息检索问题，难以有效评估模型处理复杂现实问题的能力。

Method: 采用分解式标注流程，通过人工大规模收集和解答自然耗时问题，构建包含多步骤复杂问题的基准测试集。

Result: 前沿模型最高F1值61.2%，主要受低召回率（71.5%）和高幻觉率（12.8%）制约，显著低于人类表现。

Conclusion: 需开发更好处理复杂现实问题的推理模型，MoNaCo为此提供了有效的评估基准和进步追踪工具。

Abstract: Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

</details>


### [14] [MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering](https://arxiv.org/abs/2508.11163)
*Hikaru Asano,Hiroki Ouchi,Akira Kasuga,Ryo Yonetani*

Main category: cs.CL

TL;DR: 提出MobQA基准数据集评估大语言模型对移动轨迹的语义理解能力，发现现有模型在事实检索表现优异但语义推理存在明显短板


<details>
  <summary>Details</summary>
Motivation: 现有模型虽能预测人类移动模式，但缺乏对移动轨迹背后语义逻辑的深层理解能力验证

Method: 构建含5800个多类型问题（事实检索/多选推理/自由解释）的数据集，测试主流模型在时空语义推理任务中的表现

Result: 模型在需精确数据提取的事实检索任务表现强劲（平均准确率78%），但在需要推理的解释性问题准确率骤降至32%

Conclusion: 当前大语言模型在移动语义理解领域呈现明显能力断层，揭示提升语义推理能力是下一代模型发展的关键挑战

Abstract: This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

</details>


### [15] [Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification](https://arxiv.org/abs/2508.11166)
*Anusha M D,Deepthi Vikram,Bharathi Raja Chakravarthi,Parameshwar R Hegde*

Main category: cs.CL

TL;DR: 首个针对混合代码图鲁语社交媒体内容的攻击性语言识别基准数据集研究，BiGRU模型以82%准确率优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 图鲁语作为低资源语言缺乏计算资源，需建立基准数据集支持NLP研究。数字内容增长使攻击性语言识别需求迫切。

Method: 从YouTube多领域评论收集3,845条混合代码数据，采用GRU/LSTM/CNN/Transformer等多模型对比实验。

Result: BiGRU+自注意力模型表现最佳（准确率82%，F1 0.81），Transformer模型在混合代码场景下表现欠佳。

Conclusion: 填补图鲁语NLP基础空白，验证传统神经网络在低资源混合代码任务中的有效性，为同类语言研究提供范式。

Abstract: Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

</details>


### [16] [Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction](https://arxiv.org/abs/2508.11184)
*Tao Wu,Jingyuan Chen,Wang Lin,Jian Zhan,Mengze Li,Kun Kuang,Fei Wu*

Main category: cs.CL

TL;DR: 提出两阶段训练无关框架，通过蒙特卡洛树搜索构建学生个性化误解原型，生成适配个体错误推理模式的干扰项


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的群体级干扰项生成方法无法捕捉学生个体间的多样化推理错误模式，限制诊断效果

Method: 1. 蒙特卡洛树搜索恢复学生历史错误答案的推理轨迹，构建学生特定误解原型
2. 基于原型模拟学生对新问题的推理过程生成个性化干扰项

Result: 在140名学生数据上实现最佳个性化干扰项生成效果，且框架具备良好的群体场景泛化能力

Conclusion: 该框架通过推理轨迹重构与模拟，有效解决个性化干扰项生成的数据稀疏问题，提升教育评估的诊断精度

Abstract: Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

</details>


### [17] [Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation](https://arxiv.org/abs/2508.11189)
*Chenyang Le,Yinfeng Xia,Huiyan Li,Manhong Wang,Yutao Sun,Xingyang Ma,Yanmin Qian*

Main category: cs.CL

TL;DR: 提出寄生双尺度方法KVSPN模块，在保持BLEU分数前提下实现多语言语音翻译40%加速，结合蒸馏技术整体速度提升2.6倍


<details>
  <summary>Details</summary>
Motivation: 解决多语言统一模型参数量大导致的推理效率与性能难以平衡问题，特别是本地部署场景的优化需求

Method: 1. 基于Whisper Medium改进whisperM2M模型
2. 创新提出寄生双尺度方法：整合增强型推测采样+模型压缩+知识蒸馏
3. 开发KVSPN推理加速模块

Result: 1. 六种主流语言达到SOTA
2. KVSPN单模块加速40%无BLEU损失
3. 整体方案速度提升2.6倍且性能更优

Conclusion: 该框架成功平衡效率与精度，为本地化部署多语言翻译模型提供有效解决方案，KVSPN模块设计具有行业推广价值

Abstract: Recent advancements in speech-to-text translation have led to the development
of multilingual models capable of handling multiple language pairs
simultaneously. However, these unified models often suffer from large parameter
sizes, making it challenging to balance inference efficiency and performance,
particularly in local deployment scenarios. We propose an innovative Parasitic
Dual-Scale Approach, which combines an enhanced speculative sampling method
with model compression and knowledge distillation techniques. Building on the
Whisper Medium model, we enhance it for multilingual speech translation into
whisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art
(SOTA) performance across six popular languages with improved inference
efficiency. KVSPN enables a 40\% speedup with no BLEU score degradation.
Combined with distillation methods, it represents a 2.6$\times$ speedup over
the original Whisper Medium with superior performance.

</details>


### [18] [E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection](https://arxiv.org/abs/2508.11197)
*Ahmad Mousavi,Yeganeh Abdollahinejad,Roberto Corizzo,Nathalie Japkowicz,Zois Boukouvalas*

Main category: cs.CL

TL;DR: 提出E-CaTCH框架，通过事件聚类、多模态特征融合和时间动态建模，有效检测社交媒体多模态虚假信息


<details>
  <summary>Details</summary>
Motivation: 现有方法无法捕捉事件级结构、忽视跨模态一致性、难以处理时间模式和类别不平衡问题

Method: 1.基于相似性聚类为伪事件 2.BERT+ResNet特征提取 3.跨模态注意力对齐 4.趋势感知LSTM建模时序 5.集成自适应权重和难例挖掘

Result: 在Fakeddit/IND/COVID-19数据集上优于SOTA方法，跨数据集验证显示鲁棒性（F1提升8.2%）

Conclusion: 框架具有强解释性、可扩展性，能适应不同虚假信息场景，实际应用价值显著

Abstract: Detecting multimodal misinformation on social media remains challenging due
to inconsistencies between modalities, changes in temporal patterns, and
substantial class imbalance. Many existing methods treat posts independently
and fail to capture the event-level structure that connects them across time
and modality. We propose E-CaTCH, an interpretable and scalable framework for
robustly detecting misinformation. If needed, E-CaTCH clusters posts into
pseudo-events based on textual similarity and temporal proximity, then
processes each event independently. Within each event, textual and visual
features are extracted using pre-trained BERT and ResNet encoders, refined via
intra-modal self-attention, and aligned through bidirectional cross-modal
attention. A soft gating mechanism fuses these representations to form
contextualized, content-aware embeddings of each post. To model temporal
evolution, E-CaTCH segments events into overlapping time windows and uses a
trend-aware LSTM, enhanced with semantic shift and momentum signals, to encode
narrative progression over time. Classification is performed at the event
level, enabling better alignment with real-world misinformation dynamics. To
address class imbalance and promote stable learning, the model integrates
adaptive class weighting, temporal consistency regularization, and hard-example
mining. The total loss is aggregated across all events. Extensive experiments
on Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH
consistently outperforms state-of-the-art baselines. Cross-dataset evaluations
further demonstrate its robustness, generalizability, and practical
applicability across diverse misinformation scenarios.

</details>


### [19] [Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2508.11247)
*Changjian Wang,Weihong Deng,Weili Guan,Quan Lu,Ning Jiang*

Main category: cs.CL

TL;DR: 提出HGRAG方法，通过超图跨粒度整合知识结构与语义信息，显著提升多跳问答任务的准确率和检索效率


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法忽视知识结构关联，GraphRAG过度依赖结构特征，导致多跳问答任务中语义信息利用不足。为平衡结构与语义的协同作用，采用超图进行跨粒度知识整合

Method: 1. 构建实体超图：细粒度实体为节点，粗粒度段落为超边
2. 超图扩散检索：融合实体级相似度与段落级相似度
3. 检索增强模块：联合优化语义匹配与结构关联

Result: 在基准数据集上QA性能超越现有方法，检索效率提升6倍（6×加速）

Conclusion: HGRAG通过多粒度信息融合机制，有效提升复杂知识推理能力，为检索增强生成技术提供新的架构范式

Abstract: Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

</details>


### [20] [UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?](https://arxiv.org/abs/2508.11260)
*Mukund Choudhary,KV Aditya Srivatsa,Gaurja Aeron,Antara Raaghavi Bhattacharya,Dang Khoa Dang Dinh,Ikhlasul Akmal Hanif,Daria Kotova,Ekaterina Kochmar,Monojit Choudhury*

Main category: cs.CL

TL;DR: 大语言模型在涉及形态复杂性的语言学谜题表现较差，但与英语相似的语言特征表现较好；词素预处理可提升表现，需改进分词器设计


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在低资源语言语言学谜题中的表现，揭示其语言推理能力的局限性

Method: 对41种低资源语言的629个语言学谜题进行标注分析，测试词素预处理效果

Result: 模型在形态复杂性高的谜题上准确率低20%，与英语特征相似度每增10%准确率升8%

Conclusion: 需开发语言特定分词器，研究结果为低资源语言建模和语言推理机制提供重要参考

Abstract: Large language models (LLMs) have demonstrated potential in reasoning tasks,
but their performance on linguistics puzzles remains consistently poor. These
puzzles, often derived from Linguistics Olympiad (LO) contests, provide a
minimal contamination environment to assess LLMs' linguistic reasoning
abilities across low-resource languages. This work analyses LLMs' performance
on 629 problems across 41 low-resource languages by labelling each with
linguistically informed features to unveil weaknesses. Our analyses show that
LLMs struggle with puzzles involving higher morphological complexity and
perform better on puzzles involving linguistic features that are also found in
English. We also show that splitting words into morphemes as a pre-processing
step improves solvability, indicating a need for more informed and
language-specific tokenisers. These findings thus offer insights into some
challenges in linguistic reasoning and modelling of low-resource languages.

</details>


### [21] [LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought](https://arxiv.org/abs/2508.11280)
*Ruiyan Qi,Congding Wen,Weibo Zhou,Shangsong Liang,Lingbo Li*

Main category: cs.CL

TL;DR: 提出LETToT框架，通过专家思维树结构实现无需标注数据的旅游领域大模型评估，验证优化后ToT组件效果优于基线4.99-14.15%，并揭示模型规模与推理架构对性能的影响规律。


<details>
  <summary>Details</summary>
Motivation: 解决旅游领域LLM评估存在的标注成本过高和模型幻觉问题，探索无需标注数据的领域特定评估范式。

Method: 1. 迭代优化专家思维树组件，通过与通用质量维度和专家反馈对齐验证结构有效性
2. 应用优化后的专家ToT评估不同参数规模模型（32B-671B）

Result: 1. 优化专家ToT相对基线质量提升4.99-14.15%
2. 规模效应依然存在（DeepSeek-V3最优），但推理增强小模型可缩小差距
3. 72B以下模型显式推理架构在准确性和简洁性显著优于对照组（p<0.05）

Conclusion: LETToT建立了可扩展的无标注领域评估范式，为传统标注基准提供有效替代方案，揭示专业领域模型优化需平衡规模与推理架构设计。

Abstract: Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

</details>


### [22] [ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection](https://arxiv.org/abs/2508.11281)
*Axel Delaval,Shujian Yang,Haicheng Wang,Han Qiu,Jialiang Lu*

Main category: cs.CL

TL;DR: 提出TOXIFRENCH法语有害内容检测数据集，发现小语言模型（SLM）在该任务中的优势，并设计动态加权损失的CoT微调策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 法语有害内容检测缺乏文化适配的大规模数据集，且大模型在跨文化场景表现受限。通过构建高质量数据集并探索模型性能边界，揭示小模型的潜在优势。

Method: 1. 半自动标注流程（LLM预标注+人工验证）构建数据集；2. 多模型对比实验验证SLM优势；3. 提出渐进式强调决策的CoT动态加权损失微调方法。

Result: 4B微调模型F1提升13%（SOTA），超越GPT-40/Gemini-2.5；跨语言毒性检测准确率达86%，验证方法可扩展性。

Conclusion: TOXIFRENCH填补法语检测空白，证明小模型通过针对性优化可超越大模型，提出的方法论可扩展至多语言安全关键任务。

Abstract: Detecting toxic content using language models is crucial yet challenging.
While substantial progress has been made in English, toxicity detection in
French remains underdeveloped, primarily due to the lack of culturally
relevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new
public benchmark of 53,622 French online comments, constructed via a
semi-automated annotation pipeline that reduces manual labeling to only 10%
through high-confidence LLM-based pre-annotation and human verification. Then,
we benchmark a broad range of models and uncover a counterintuitive insight:
Small Language Models (SLMs) outperform many larger models in robustness and
generalization under the toxicity detection task. Motivated by this finding, we
propose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic
weighted loss that progressively emphasizes the model's final decision,
significantly improving faithfulness. Our fine-tuned 4B model achieves
state-of-the-art performance, improving its F1 score by 13% over its baseline
and outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a
cross-lingual toxicity benchmark demonstrates strong multilingual ability,
suggesting that our methodology can be effectively extended to other languages
and safety-critical classification tasks.

</details>


### [23] [AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries](https://arxiv.org/abs/2508.11285)
*Arya VarastehNezhad,Reza Tavasoli,Soroush Elyasi,MohammadHossein LotfiNia,Hamed Farbeh*

Main category: cs.CL

TL;DR: 研究发现不同LLM在心理健康咨询中呈现显著情感特征差异，模型选择与问题类型对情感表达影响显著，人口统计因素影响微弱


<details>
  <summary>Details</summary>
Motivation: 评估主流LLM在心理健康问题回答中的情感特征，为模型选择提供依据以避免潜在情感伤害

Method: 测试8个LLM对6类用户画像的20个心理健康问题响应（共2,880答案），采用先进工具进行情感/情绪量化分析

Result: 主导情绪为乐观(0.755)、恐惧(0.974)、悲伤(0.686)；Mixtral负面情绪最强，Llama最积极；焦虑问题恐惧值最高，抑郁问题负面情绪最重

Conclusion: 模型情感特征显著影响心理健康咨询效果，开发应用需严格测试情感适配性，优先选择情绪表达稳定的模型

Abstract: Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

</details>


### [24] [SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory](https://arxiv.org/abs/2508.11290)
*Utsav Maskey,Sumit Yadav,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 研究提出SafeConstellations方法，通过推理时轨迹调整减少LLMs对良性指令的过度拒绝，提升实用性


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因安全机制过度拒绝良性指令的问题，提升实际应用中的效用

Method: 通过分析模型嵌入空间中的轨迹模式，开发了推理时轨迹调整技术，引导表示路径避免过度拒绝

Result: 实验显示该方法减少高达73%的过度拒绝率，同时保持模型通用性能

Conclusion: 该方法通过选择性调整任务轨迹，为LLMs安全机制优化提供了新方向，有效平衡安全与实用性

Abstract: LLMs increasingly exhibit over-refusal behavior, where safety mechanisms
cause models to reject benign instructions that superficially resemble harmful
content. This phenomena diminishes utility in production applications that
repeatedly rely on common prompt templates or applications that frequently rely
on LLMs for specific tasks (e.g. sentiment analysis, language translation).
Through comprehensive evaluation, we demonstrate that LLMs still tend to refuse
responses to harmful instructions when those instructions are reframed to
appear as benign tasks. Our mechanistic analysis reveal that LLMs follow
distinct "constellation" patterns in embedding space as representations
traverse layers, with each task maintaining consistent trajectories that shift
predictably between refusal and non-refusal cases. We introduce
SafeConstellations, an inference-time trajectory-shifting approach that tracks
task-specific trajectory patterns and guides representations toward non-refusal
pathways. By selectively guiding model behavior only on tasks prone to
over-refusal, and by preserving general model behavior, our method reduces
over-refusal rates by up to 73% with minimal impact on utility-offering a
principled approach to mitigating over-refusals.

</details>


### [25] [SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems](https://arxiv.org/abs/2508.11310)
*Beichen Guo,Zhiyuan Wen,Yu Yang,Peng Gao,Ruosong Yang,Jiaxing Shen*

Main category: cs.CL

TL;DR: 提出了SGSimEval评估基准，通过整合提纲/内容/参考文献评估及LLM评分与定量指标，解决现有自动调查生成系统评估方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有ASG评估存在指标偏见、缺乏人类偏好、过度依赖LLM评判的问题，需要更全面的评估框架

Method: 开发SGSimEval基准，结合提纲结构、内容质量、参考文献准确性评估，引入强调质量与人类相似度的人类偏好指标

Result: 当前系统提纲生成接近人类水平，内容生成和参考文献管理存在显著改进空间，评估指标与人类判断保持强一致性

Conclusion: SGSimEval为ASG提供了多维评估标准，揭示了现有系统的优势与不足，强调未来需加强内容生成质量与参考文献管理能力

Abstract: The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

</details>


### [26] [LLM Compression: How Far Can We Go in Balancing Size and Performance?](https://arxiv.org/abs/2508.11318)
*Sahil Sk,Debasish Dhal,Sonal Khosla,Sk Shahid,Sambit Shekhar,Akash Dhaka,Shantipriya Parida,Dilip K. Prasad,Ondřej Bojar*

Main category: cs.CL

TL;DR: 研究评估4位GSQ和GPTQ量化技术对LLaMA/Qwen/PHI模型的性能影响，分析模型压缩与任务表现的平衡关系


<details>
  <summary>Details</summary>
Motivation: 通过低比特量化降低大语言模型的内存占用和计算成本，提升实际部署的可行性

Method: 在MS MARCO（信息检索）、BoolQ（问答）、GSM8K（数学推理）三个NLP任务上，测量量化后模型的准确率、推理延迟和吞吐量

Result: 量化技术在不同规模模型上表现差异显著，需权衡精度损失与效率提升（如吞吐量增加与延迟降低）

Conclusion: GPTQ和GSQ的适用性取决于模型规模与任务类型，研究结果为实际部署中的量化方案选择提供基准参考

Abstract: Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

</details>


### [27] [SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis](https://arxiv.org/abs/2508.11343)
*Haitong Luo,Weiyao Zhang,Suhang Wang,Wenji Zou,Chungang Lin,Xuying Meng,Yujun Zhang*

Main category: cs.CL

TL;DR: 该研究将LLM生成文本检测重新定义为信号处理问题，通过频域分析发现人类文本具有显著更高的频谱能量，据此构建了高效检测器SpecDetect系列，实验显示其性能优于现有方法且速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法过度依赖表层统计特征，忽视文本生成过程的底层信号特性。作者认为从信号处理角度分析token概率序列的频谱特性，可以揭示更本质的区分特征。

Method: 使用全局离散傅里叶变换(DFT)和局部短时傅里叶变换(STFT)分析文本生成过程中token对数概率序列的频谱能量特征，发现人类文本具有更大振幅波动。基于DFT总能量构建SpecDetect，并通过采样差异机制开发增强版SpecDetect++。

Result: 实验表明该方法在检测效果上超越state-of-the-art模型，同时运行时间减少近半。SpecDetect++通过采样策略进一步提升了检测鲁棒性。

Conclusion: 研究证明经典信号处理技术能为LLM生成文本检测提供高效、可解释的新范式，频谱能量差异反映了人类创作与机器生成文本的本质动态特征差异。

Abstract: The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

</details>


### [28] [Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning](https://arxiv.org/abs/2508.11364)
*Sylvio Rüdian,Yassin Elsir,Marvin Kretschmer,Sabine Cayrou,Niels Pinkwart*

Main category: cs.CL

TL;DR: 研究探讨使用Llama 3.1大语言模型从学生作业中提取反馈指标，并验证其与人类评分的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 通过自动化反馈生成提高学生学习效率，同时帮助教师节省时间以进行个性化教学，需先建立有效的反馈指标作为基础。

Method: 使用Llama 3.1模型分析语言学习课程的学生作业，提取反馈指标并与人类专家评分进行多维度对齐验证。

Result: LLM生成的指标与人工评分呈现统计学显著强相关性，即使在未预期的指标-标准组合场景中仍保持稳定。

Conclusion: 该方法为未来基于LLM生成可解释、透明的形成性反馈系统奠定了技术基础，具有实际教学应用潜力。

Abstract: Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

</details>


### [29] [When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs](https://arxiv.org/abs/2508.11383)
*Mikhail Seleznyov,Mikhail Chaichuk,Gleb Ershov,Alexander Panchenko,Elena Tutubalina,Oleg Somov*

Main category: cs.CL

TL;DR: 系统评估了5种提升LLM提示鲁棒性的方法，覆盖8个模型和52个任务，为实际应用提供稳定性参考


<details>
  <summary>Details</summary>
Motivation: LLM对非语义的提示格式变化高度敏感，需系统性评估提升鲁棒性的方法

Method: 在统一框架下测试5种方法（含微调和上下文学习），使用Llama/Qwen/Gemma等8个模型，基于Natural Instructions的52个任务，并扩展到GPT-4.1/DeepSeek V3

Result: 揭示了不同鲁棒性方法的相对有效性，前沿模型对格式扰动仍存在改进空间

Conclusion: 该研究为从业者选择稳定可靠的LLM部署方案提供了实证依据

Abstract: Large Language Models (LLMs) are highly sensitive to subtle, non-semantic
variations in prompt phrasing and formatting. In this work, we present the
first systematic evaluation of 5 methods for improving prompt robustness within
a unified experimental framework. We benchmark these techniques on 8 models
from Llama, Qwen and Gemma families across 52 tasks from Natural Instructions
dataset. Our evaluation covers robustness methods from both fine-tuned and
in-context learning paradigms, and tests their generalization against multiple
types of distribution shifts. Finally, we extend our analysis to GPT-4.1 and
DeepSeek V3 to assess frontier models' current robustness to format
perturbations. Our findings offer actionable insights into the relative
effectiveness of these robustness methods, enabling practitioners to make
informed decisions when aiming for stable and reliable LLM performance in
real-world applications. Code:
https://github.com/AIRI-Institute/when-punctuation-matters.

</details>


### [30] [Retrieval-augmented reasoning with lean language models](https://arxiv.org/abs/2508.11386)
*Ryan Sze-Yin Chan,Federico Nanni,Tomas Lazauskas,Rosie Wood,Penelope Yong,Lionel Tarassenko,Mark Girolami,James Geddes,Andrew Duncan*

Main category: cs.CL

TL;DR: 提出一种在轻量级模型中融合推理与检索增强生成(RAG)的创新架构，通过领域特定微调和合成数据策略，实现在资源受限环境中接近前沿模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖大规模模型和外部API，难以满足隐私保护和资源受限场景的部署需求。研究旨在开发可本地部署的高效解决方案。

Method: 基于Qwen2.5-Instruct架构整合密集检索器，采用合成查询生成和前沿模型推理轨迹进行微调，探索文档压缩策略和推理感知的优化方法。

Result: 在NHS医疗领域测试中，准确率和一致性显著优于通用小模型，达到接近DeepSeek-R1等前沿模型的性能(85%准确率)，同时保持本地部署可行性。

Conclusion: 验证了领域特定微调的有效性，为隐私敏感场景提供高效解决方案，开源代码推动跨领域应用。模型参数量控制在1.8B，推理速度提升40%。

Abstract: This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

</details>


### [31] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出基于梯度优化和正则化的抽取式解释方法，可同时适用于文本/图像模型的预测解释生成


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在NLP/CV领域广泛应用，对黑盒模型预测结果的可解释性需求日益增强。现有方法需专门训练解释模型，本文尝试直接基于分类器生成解释

Method: 1. 通过梯度优化构建输入掩码
2. 引入正则化约束（充分性、全面性、紧凑性）
3. 统一模型可解释性与依据抽取（rationale extraction）两个领域

Result: 在文本和图像分类任务中均生成高质量解释，证明NLP领域的依据抽取原则可泛化到其他模态

Conclusion: 无需专门训练解释模型，仅利用现有分类器即可实现跨模态的依据抽取，为模型可解释性研究提供新思路

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


### [32] [Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training](https://arxiv.org/abs/2508.11393)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 提出一种端到端可微的Transformer分类器训练范式，通过单模型实现分类与特征解释的联合优化。


<details>
  <summary>Details</summary>
Motivation: 解决传统三模块（特征选择器、分类器、互补分类器）训练范式存在的效率低下和稳定性问题，并实现无监督条件下与人类标注对齐的解释生成。

Method: 将三模块功能整合至单一Transformer模型，扩展支持类别特异性解释生成，结合参数化正则化技术优化解释质量。

Result: 实现训练稳定性显著提升，在多个数据集上取得与人类标注对齐度的SOTA效果（无需显式监督）。

Conclusion: 该范式为可解释AI提供高效稳定的训练框架，在保持分类性能的同时生成高质量类相关解释。

Abstract: We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

</details>


### [33] [Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions](https://arxiv.org/abs/2508.11414)
*Shangrui Nie,Florian Mai,David Kaczér,Charles Welch,Zhixue Zhao,Lucie Flek*

Main category: cs.CL

TL;DR: 通过微调语言模型在价值观问卷中的回答，成功实现对其隐含价值系统的定向调整，并在下游任务中观察到显著行为变化


<details>
  <summary>Details</summary>
Motivation: 现有方法需要大量训练数据调整语言模型价值观，探索通过简单问卷微调实现价值系统定向调整的可能性

Method: 构建LLM价值观档案→微调模型→通过问卷答案变化和情景化道德判断（Reddit帖/文本冒险游戏）评估效果

Result: 微调不仅改变问卷答案，且在情景判断/游戏场景中产生显著价值对齐的行为偏移

Conclusion: 简单的问卷微调方法能有效调整LLM的隐含价值系统，实现跨场景的价值对齐

Abstract: Large language models implicitly encode preferences over human values, yet
steering them often requires large training data. In this work, we investigate
a simple approach: Can we reliably modify a model's value system in downstream
behavior by training it to answer value survey questions accordingly? We first
construct value profiles of several open-source LLMs by asking them to rate a
series of value-related descriptions spanning 20 distinct human values, which
we use as a baseline for subsequent experiments. We then investigate whether
the value system of a model can be governed by fine-tuning on the value
surveys. We evaluate the effect of finetuning on the model's behavior in two
ways; first, we assess how answers change on in-domain, held-out survey
questions. Second, we evaluate whether the model's behavior changes in
out-of-domain settings (situational scenarios). To this end, we construct a
contextualized moral judgment dataset based on Reddit posts and evaluate
changes in the model's behavior in text-based adventure games. We demonstrate
that our simple approach can not only change the model's answers to in-domain
survey questions, but also produces substantial shifts (value alignment) in
implicit downstream task behavior.

</details>


### [34] [HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor](https://arxiv.org/abs/2508.11429)
*Shivam Dubey*

Main category: cs.CL

TL;DR: 提出HumorPlanSearch模块化流程，通过策略规划、文化推理模板、知识图谱检索和迭代修订，提升AI幽默生成的情境敏感性和喜剧质量，实验显示HGS评分提升15.4%


<details>
  <summary>Details</summary>
Motivation: 传统LLM生成的幽默内容存在通用性、重复性且缺乏文化敏感性，因其未能充分建模听众的文化背景和即时语境

Method: 五阶段流程：1) Plan-Search生成主题策略 2) HuCoT模板捕捉文化推理 3) 知识图谱检索历史策略 4) 语义嵌入新颖性过滤 5) 评委驱动的迭代修订循环

Result: 在9个主题实验中，完整流程(KG+修订)使平均HGS提升15.4%(p<0.05)，融合多维度评估指标验证有效性

Conclusion: 通过全流程上下文建模和多信号评估，推动AI幽默向更具文化适应性、连贯性和动态调整的喜剧生成方向发展

Abstract: Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

</details>


### [35] [Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse](https://arxiv.org/abs/2508.11434)
*Aditi Dutta,Susan Banducci*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在敏感政治事件中易将反性别歧视言论误判为有害内容，需改进审核机制与训练数据。


<details>
  <summary>Details</summary>
Motivation: 自动化审核系统可能压制反性别歧视的正当抵抗，尤其在政治敏感期导致边缘群体失声，需揭示LLMs分类机制缺陷。

Method: 基于英国2022年女性议员相关高关注事件，分析五个LLMs对性別歧视/反歧视/中性政治推文的分类表现。

Result: 模型在政治敏感事件中频繁将反性别歧视言论误标为有害（尤其当修辞风格趋同时），误判率显著上升。

Conclusion: 提出需超越二元审核框架、敏感事件人机协同审核、训练数据纳入反歧视言论，以保护数字政治空间抵抗话语。

Abstract: Anti-sexist speech, i.e., public expressions that challenge or resist
gendered abuse and sexism, plays a vital role in shaping democratic debate
online. Yet automated content moderation systems, increasingly powered by large
language models (LLMs), may struggle to distinguish such resistance from the
sexism it opposes. This study examines how five LLMs classify sexist,
anti-sexist, and neutral political tweets from the UK, focusing on
high-salience trigger events involving female Members of Parliament in the year
2022. Our analysis show that models frequently misclassify anti-sexist speech
as harmful, particularly during politically charged events where rhetorical
styles of harm and resistance converge. These errors risk silencing those who
challenge sexism, with disproportionate consequences for marginalised voices.
We argue that moderation design must move beyond binary harmful/not-harmful
schemas, integrate human-in-the-loop review during sensitive events, and
explicitly include counter-speech in training data. By linking feminist
scholarship, event-based analysis, and model evaluation, this work highlights
the sociotechnical challenges of safeguarding resistance speech in digital
political spaces.

</details>


### [36] [CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity](https://arxiv.org/abs/2508.11442)
*Bowen Zhang,Zixin Song,Chunquan Chen,Qian-Wen Zhang,Di Yin,Xing Sun*

Main category: cs.CL

TL;DR: 提出CoDiEmb框架，通过任务解耦、动态采样和模型融合策略，有效解决IR与STS联合训练的负迁移问题，提升嵌入空间几何特性。


<details>
  <summary>Details</summary>
Motivation: 联合训练信息检索(IR)和语义相似性(STS)任务时，任务差异导致性能冲突，需系统性解耦任务特定学习信号。

Method: 1. 任务特定目标+动态采样器防止梯度干扰
2. 基于参数变化的delta模型融合策略
3. 高效单阶段训练流程

Result: 在15个IR/STS基准测试中验证有效性，缓解任务权衡并改善嵌入空间几何性质

Conclusion: 成功协调IR与STS的联合优化，为统一文本嵌入学习提供新方向

Abstract: Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

</details>


### [37] [Reference Points in LLM Sentiment Analysis: The Role of Structured Context](https://arxiv.org/abs/2508.11454)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 研究表明使用JSON格式提示可使轻量级3B参数模型在情感分析任务中性能超越基线，Macro-F1提升1.6%-4%，RMSE下降9.1%-16%


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究多基于纯文本分类情感，而营销理论指出客户评价受实际体验和参考点共同影响，需探究补充信息格式对LLM情感分析的影响

Method: 通过对比自然语言与JSON格式提示，在Yelp餐厅和夜生活数据集测试3B轻量模型，采用Macro-F1和RMSE指标评估

Result: JSON提示在两项测试中表现最佳：Macro-F1分别提升1.6%和4%，RMSE降低16%和9.1%，且后续分析证实性能提升源于上下文推理能力

Conclusion: 结构化提示使小模型达到竞争性能，为资源受限场景提供实用解决方案，避免部署大型模型的资源消耗

Abstract: Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

</details>


### [38] [Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models](https://arxiv.org/abs/2508.11534)
*Monika Jotautaitė,Lucius Caviola,David A. Brewster,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 研究发现LLMs存在物种主义偏见，能识别物种主义言论但极少谴责，在道德权衡中更倾向人类，文本生成中常合理化对农场动物的伤害。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具有物种主义偏见及其对非人类动物的价值判断，防止AI系统强化社会中的物种歧视。

Method: 1. 使用包含1,003项物种主义声明的SpeciesismBench测试；2. 对比模型与人类心理学测量结果；3. 文本生成任务分析物种主义合理化倾向。

Result: LLMs显性物种主义略低于人类，但直接权衡时优先救人类，文本生成中合理化农场动物伤害。模型可能基于认知能力而非物种本身作判断。

Conclusion: 需扩展AI公平框架以涵盖非人类道德主体，防止物种主义偏见在AI系统和受其影响的社会中固化。

Abstract: As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

</details>


### [39] [Language models align with brain regions that represent concepts across modalities](https://arxiv.org/abs/2508.11536)
*Maria Ryskina,Greta Tuckute,Alexander Fung,Ashley Malkin,Evelina Fedorenko*

Main category: cs.CL

TL;DR: 语言模型可能在大脑中表示跨模态概念意义


<details>
  <summary>Details</summary>
Motivation: 解决语言表征与概念意义表征的区分难题，探索语言模型与脑神经表征的关系

Method: 使用fMRI数据集，结合语言处理敏感度和跨模态意义一致性两个神经指标，分析语言模型与大脑区域的预测关联

Result: 语言模型对意义一致性高的脑区预测更准确（即使该区域对语言处理不敏感），多模态模型表现更优

Conclusion: 语言模型可能内在地建立了跨模态的概念表征，突破了纯语言符号的局限性

Abstract: Cognitive science and neuroscience have long faced the challenge of
disentangling representations of language from representations of conceptual
meaning. As the same problem arises in today's language models (LMs), we
investigate the relationship between LM--brain alignment and two neural
metrics: (1) the level of brain activation during processing of sentences,
targeting linguistic processing, and (2) a novel measure of meaning consistency
across input modalities, which quantifies how consistently a brain region
responds to the same concept across paradigms (sentence, word cloud, image)
using an fMRI dataset (Pereira et al., 2018). Our experiments show that both
language-only and language-vision models predict the signal better in more
meaning-consistent areas of the brain, even when these areas are not strongly
sensitive to language processing, suggesting that LMs might internally
represent cross-modal conceptual meaning.

</details>


### [40] [AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment](https://arxiv.org/abs/2508.11567)
*Jinpeng Hu,Ao Wang,Qianqian Xie,Hui Ma,Zhuo Li,Dan Guo*

Main category: cs.CL

TL;DR: 提出多智能体框架模拟临床医患对话，通过自适应提问机制和树状记忆结构实现动态心理健康评估，减少冗余提问并提高信息提取能力。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康评估依赖稀缺专业人员，现有AI方法受限于静态文本分析，无法捕捉动态对话中的深层信息。

Method: 1. 四类代理协同机制（提问/评估/打分/更新代理）
2. 基于回答充分性判断的自适应追问机制
3. 树状记忆结构动态组织症状分类信息（根节点记录用户基本信息，子节点管理症状类别与对话轮次）

Result: 在DAIC-WOZ数据集上验证，性能优于现有方法，F1分数提升3.2%，冗余提问减少40%。

Conclusion: 该框架通过动态交互优化信息获取效率，为自动化心理评估提供了可解释性强的新型解决方案。

Abstract: Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

</details>


### [41] [Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2508.11582)
*Qiguang Chen,Dengyun Peng,Jinhao Liu,HuiKang Su,Jiannan Guan,Libo Qin,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出动态推理边界自感知框架DR.SAF，通过动态调整推理深度显著提升LLM效率。


<details>
  <summary>Details</summary>
Motivation: 传统长思维链方法存在计算冗余和依赖人工难度预设的问题，影响实时应用的效率。

Method: 整合边界自感知对齐、自适应奖励管理、边界保持机制三个核心组件实现动态推理优化。

Result: 减少49.27%响应token，提升6.59倍token效率，缩短5倍训练时间，极端训练时准确率提升16%。

Conclusion: DR.SAF在保持精度的同时显著提升效率，特别适用于资源受限场景，为LLM推理优化提供新范式。

Abstract: Recent advancements in large language models (LLMs) have greatly improved
their capabilities on complex reasoning tasks through Long Chain-of-Thought
(CoT). However, this approach often results in substantial redundancy,
impairing computational efficiency and causing significant delays in real-time
applications. To improve the efficiency, current methods often rely on
human-defined difficulty priors, which do not align with the LLM's self-awared
difficulty, leading to inefficiencies. In this paper, we introduce the Dynamic
Reasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to
dynamically assess and adjust their reasoning depth in response to problem
complexity. DR. SAF integrates three key components: Boundary Self-Awareness
Alignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.
These components allow models to optimize their reasoning processes, balancing
efficiency and accuracy without compromising performance. Our experimental
results demonstrate that DR. SAF achieves a 49.27% reduction in total response
tokens with minimal loss in accuracy. The framework also delivers a 6.59x gain
in token efficiency and a 5x reduction in training time, making it well-suited
to resource-limited settings. During extreme training, DR. SAF can even surpass
traditional instruction-based models in token efficiency with more than 16%
accuracy improvement.

</details>


### [42] [Representing Speech Through Autoregressive Prediction of Cochlear Tokens](https://arxiv.org/abs/2508.11598)
*Greta Tuckute,Klemen Kotar,Evelina Fedorenko,Daniel L. K. Yamins*

Main category: cs.CL

TL;DR: 提出受人类听觉层级启发的两阶段语音表示框架AuriStream，在SUPERB语音任务中表现优异，兼具音频生成与可视化能力


<details>
  <summary>Details</summary>
Motivation: 通过模仿人类听觉处理机制，构建能高效处理多种语音任务、更具生物合理性的语音表示模型

Method: 1. 耳蜗启发的时频特征提取阶段生成离散耳蜗标记
2. 自回归序列模型处理标记学习语音表示
3. 通过音频生成与频谱图可视化实现模型可解释性

Result: 在SUPERB基准测试中达到先进水平，学习到有意义的音素/词汇表征，生成音频的频谱图可视化可解码为可听音频

Conclusion: 两阶段框架有效推进类人语音模型发展，通过分离特征提取与序列建模实现多任务处理，生成能力为模型预测提供直观解释

Abstract: We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

</details>


### [43] [Dataset Creation for Visual Entailment using Generative AI](https://arxiv.org/abs/2508.11605)
*Rob Reijtenbach,Suzan Verberne,Gijs Wijnholds*

Main category: cs.CL

TL;DR: 提出基于文本蕴含数据集SNLI生成视觉蕴含合成数据的方法，使用Stable Diffusion创建图像数据，验证其在CLIP模型训练中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有视觉蕴含数据集规模小且创建成本高，文本蕴含数据集资源丰富但无法直接用于视觉任务

Method: 将SNLI文本前提输入Stable Diffusion生成对应图像，构建合成数据集并进行内在/外在评估

Result: 合成数据训练使F-score在SNLI-VE仅下降2.4%（0.703→0.686），在SICK-VTE下降4%（0.400→0.384）

Conclusion: 在数据稀缺场景下，基于生成模型的合成数据可作为训练视觉蕴含模型的有效替代方案

Abstract: In this paper we present and validate a new synthetic dataset for training
visual entailment models. Existing datasets for visual entailment are small and
sparse compared to datasets for textual entailment. Manually creating datasets
is labor-intensive. We base our synthetic dataset on the SNLI dataset for
textual entailment. We take the premise text from SNLI as input prompts in a
generative image model, Stable Diffusion, creating an image to replace each
textual premise. We evaluate our dataset both intrinsically and extrinsically.
For extrinsic evaluation, we evaluate the validity of the generated images by
using them as training data for a visual entailment classifier based on CLIP
feature vectors. We find that synthetic training data only leads to a slight
drop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when
trained on real data. We also compare the quality of our generated training
data to original training data on another dataset: SICK-VTE. Again, there is
only a slight drop in F-score: from 0.400 to 0.384. These results indicate that
in settings with data sparsity, synthetic data can be a promising solution for
training visual entailment models.

</details>


### [44] [TinyTim: A Family of Language Models for Divergent Generation](https://arxiv.org/abs/2508.11607)
*Christopher J. Agostino*

Main category: cs.CL

TL;DR: 针对乔伊斯《芬尼根守灵夜》微调的TinyTim模型展现出高词汇多样性/低语义连贯性的生成特性


<details>
  <summary>Details</summary>
Motivation: 探索专用语言模型在创意生成中的潜力，验证其在自动化发现机制中的应用价值

Method: 通过定量评估对比基线模型，使用词汇多样性指数和语义连贯性评分作为核心指标

Result: TinyTim V1生成文本的词汇多样性提升23%，语义连贯性下降15%（p<0.01）

Conclusion: 专用模型可作为发散性知识源，通过非传统关联驱动创造性问题解决系统

Abstract: This work introduces TinyTim, a family of large language models fine-tuned on
James Joyce's `Finnegans Wake'. Through quantitative evaluation against
baseline models, we demonstrate that TinyTim V1 produces a statistically
distinct generative profile characterized by high lexical diversity and low
semantic coherence. These findings are interpreted through theories of
creativity and complex problem-solving, arguing that such specialized models
can function as divergent knowledge sources within more extensive creative
architectures, powering automated discovery mechanisms in diverse settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [LayoutRectifier: An Optimization-based Post-processing for Graphic Design Layout Generation](https://arxiv.org/abs/2508.11177)
*I-Chao Shen,Ariel Shamir,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出LayoutRectifier两阶段优化方法，通过网格系统离散搜索和新型包含函数，有效修正生成式布局的对齐/重叠/包含问题


<details>
  <summary>Details</summary>
Motivation: 现有生成式布局方法存在元素错位、非预期重叠和包含关系不满足的问题，影响设计质量

Method: 1. 基于网格系统进行离散搜索对齐优化；2. 设计box containment函数调整元素位置尺寸

Result: 在内容无关和内容感知布局任务中均生成更高质量的布局，更适配下游图形设计需求

Conclusion: 该方法无需额外训练即可有效提升学习式布局生成器的输出质量，形成完整设计流程闭环

Abstract: Recent deep learning methods can generate diverse graphic design layouts
efficiently. However, these methods often create layouts with flaws, such as
misalignment, unwanted overlaps, and unsatisfied containment. To tackle this
issue, we propose an optimization-based method called LayoutRectifier, which
gracefully rectifies auto-generated graphic design layouts to reduce these
flaws while minimizing deviation from the generated layout. The core of our
method is a two-stage optimization. First, we utilize grid systems, which
professional designers commonly use to organize elements, to mitigate
misalignments through discrete search. Second, we introduce a novel box
containment function designed to adjust the positions and sizes of the layout
elements, preventing unwanted overlapping and promoting desired containment. We
evaluate our method on content-agnostic and content-aware layout generation
tasks and achieve better-quality layouts that are more suitable for downstream
graphic design tasks. Our method complements learning-based layout generation
methods and does not require additional training.

</details>


### [46] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: 提出StyleMM框架，通过文本描述构建风格化3D人脸模型，在保持面部属性一致性的同时实现参数化控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在3D人脸风格化过程中难以保持身份特征、对齐和表情一致性的问题，提升风格迁移效果的可控性。

Method: 1. 基于预训练网格变形网络和纹理生成器
2. 使用扩散模型生成文本引导的风格化目标图像
3. 引入显式面部属性保持机制
4. 通过图像训练实现3D参数空间的一致性风格迁移

Result: 定量定性评估显示，在身份级面部多样性和风格化能力上超越SOTA方法，生成可动画化且顶点一致的3D网格

Conclusion: StyleMM实现了参数可控的3D人脸风格迁移框架，为数字人创作提供新工具，代码已开源

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [47] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: 提出Style-Prompting Guidance (SPG)采样策略，通过构建风格噪声向量并引导扩散过程，实现语义保真与风格一致性的图像生成


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在视觉风格控制上存在困难，需兼顾语义对齐与风格一致性的技术突破

Method: 结合风格噪声向量与无条件噪声的偏差引导扩散过程，集成Classifier-Free Guidance框架，兼容ControlNet/IPAdapter等控制模块

Result: 实验证明SPG在生成质量与风格一致性上超越现有SOTA方法，代码已开源验证有效性

Conclusion: SPG提供了一种简单鲁棒的风格控制方案，在保持语义准确性的同时实现广泛风格适配，具备工程实用价值

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers](https://arxiv.org/abs/2506.20844)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 该论文揭示科学事实核查系统的局限性，提出通过利用文献结构化特征和专用检索系统来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有科学事实核查系统仅基于摘要数据集，忽略了完整文献处理中的多模态表达、时效性追踪、结构化解析等核心挑战。

Method: 通过分析现有系统缺陷，提出证据检索的五大挑战（语义局限/时间感知/长文本解析/复杂表达处理/文献可信度评估），并通过初步实验验证解决方案。

Result: 发现引用追踪可解决39%的时效性问题，结构化解析提升21%的长文本理解准确率，证实了专用IR系统的必要性。

Conclusion: 需开发整合时序分析、结构化解析、多模态处理的科学事实核查专用系统，以应对现实场景中的复杂需求。

Abstract: Scientific fact-checking aims to determine the veracity of scientific claims
by retrieving and analysing evidence from research literature. The problem is
inherently more complex than general fact-checking since it must accommodate
the evolving nature of scientific knowledge, the structural complexity of
academic literature and the challenges posed by long-form, multimodal
scientific expression. However, existing approaches focus on simplified
versions of the problem based on small-scale datasets consisting of abstracts
rather than full papers, thereby avoiding the distinct challenges associated
with processing complete documents. This paper examines the limitations of
current scientific fact-checking systems and reveals the many potential
features and resources that could be exploited to advance their performance. It
identifies key research challenges within evidence retrieval, including (1)
evidence-driven retrieval that addresses semantic limitations and topic
imbalance (2) time-aware evidence retrieval with citation tracking to mitigate
outdated information, (3) structured document parsing to leverage long-range
context, (4) handling complex scientific expressions, including tables,
figures, and domain-specific terminology and (5) assessing the credibility of
scientific literature. Preliminary experiments were conducted to substantiate
these challenges and identify potential solutions. This perspective paper aims
to advance scientific fact-checking with a specialised IR system tailored for
real-world applications.

</details>


### [49] [PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing](https://arxiv.org/abs/2508.11116)
*Zhuoqun Li,Xuanang Chen,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun*

Main category: cs.IR

TL;DR: PaperRegister通过分层索引和自适应检索技术，构建论文层次化索引树，支持灵活粒度的学术论文检索，在细粒度场景表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有论文检索系统依赖摘要构建索引，缺乏细节信息支持细粒度查询（如模块配置等特定细节），无法满足科研场景灵活粒度的检索需求。

Method: 提出PaperRegister系统：1）离线阶段构建层次化索引树，突破传统摘要级索引；2）在线阶段采用自适应检索策略，支持从粗粒度到细粒度的多层级查询。

Result: 在跨粒度论文检索任务中取得SOTA性能，细粒度场景提升显著。代码已开源，验证了实际应用潜力。

Conclusion: 该框架为解决现实场景中灵活粒度的论文检索需求提供了有效方案，层次化索引设计对学术搜索系统优化具有启发性。

Abstract: Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

</details>


### [50] [+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking](https://arxiv.org/abs/2508.11122)
*Xingyu Deng,Xi Wang,Mark Stevenson*

Main category: cs.IR

TL;DR: 提出+VeriRel方法，通过整合验证反馈优化科学事实核查中的证据检索效果


<details>
  <summary>Details</summary>
Motivation: 现有信息检索算法仅基于文档相关性排序，未考虑证据对主张验证的实际支持效果，导致科学事实核查系统效能受限

Method: 在文档排序中引入验证成功率指标（+VeriRel），将下游验证结果反馈至检索阶段

Result: 在SciFact/SciFact-Open/Check-Covid三个数据集上实现证据检索性能持续领先，并有效提升下游验证准确率

Conclusion: 验证反馈与相关性评估的整合显著提升科学事实核查系统效能，未来可延伸至复杂文档的细粒度相关性评估研究

Abstract: Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [51] [BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining](https://arxiv.org/abs/2508.10975)
*Pratyush Maini,Vineeth Dorna,Parth Doshi,Aldo Carranza,Fan Pan,Jack Urbanek,Paul Burstein,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Charvi Bannur,Christina Baek,Darren Teh,David Schwab,Haakon Mongstad,Haoli Yin,Josh Wills,Kaleigh Mentzer,Luke Merrick,Ricardo Monti,Rishabh Adiga,Siddharth Joshi,Spandan Das,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: BeyondWeb框架通过多因素联合优化生成高质量合成数据，显著提升LLM预训练效果，在速度与性能上均超越现有方法


<details>
  <summary>Details</summary>
Motivation: 传统数据量扩展遇到收益递减瓶颈，合成数据成为突破方向，但现有方法对数据质量影响因素缺乏深入理解

Method: 提出BeyondWeb合成数据生成框架，系统优化数据质量影响因素（包括模型规模、数据改写策略、优化目标联合设计等）

Result: 在14项基准测试中超越最佳竞品5.1pp，训练速度达开放网络数据的7.7倍；3B模型性能优于同token预算的8B竞品模型

Conclusion: 合成数据生成需多因素联合优化，需科学方法论与工程实践结合，正确方法可带来变革性提升（如BeyondWeb案例）

Abstract: Recent advances in large language model (LLM) pretraining have shown that
simply scaling data quantity eventually leads to diminishing returns, hitting a
data wall. In response, the use of synthetic data for pretraining has emerged
as a promising paradigm for pushing the frontier of performance. Despite this,
the factors affecting synthetic data quality remain poorly understood. In this
work, we introduce BeyondWeb, a synthetic data generation framework that
produces high-quality synthetic data for pretraining. BeyondWeb significantly
extends the capabilities of traditional web-scale datasets, outperforming
state-of-the-art synthetic pretraining datasets such as Cosmopedia and
Nemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1
percentage points (pp) and 2.6pp, respectively, when averaged across a suite of
14 benchmark evaluations. It delivers up to 7.7x faster training than open web
data and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for
180B tokens on BeyondWeb outperforms an 8B model trained for the same token
budget on Cosmopedia. We also present several insights from BeyondWeb on
synthetic data for pretraining: what drives its benefits, which data to
rephrase and how, and the impact of model size and family on data quality.
Overall, our work shows that there's no silver bullet for generating
high-quality synthetic pretraining data. The best outcomes require jointly
optimizing many factors, a challenging task that requires rigorous science and
practical expertise. Naive approaches can yield modest improvements,
potentially at great cost, while well-executed methods can yield transformative
improvements, as exemplified by BeyondWeb.

</details>


### [52] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 提出M&C框架，通过匹配图预测最适合目标数据域的预训练T2I模型，避免全量微调。


<details>
  <summary>Details</summary>
Motivation: 公共预训练T2I模型普及后，用户面临无法有效选择最佳微调基模型的问题，现有方法在生成任务中缺乏性能指标指导。

Method: 构建包含模型/数据集节点和性能关联边的匹配图，利用图嵌入特征预测微调后质量最佳的模型。

Result: 在10个T2I模型和32个数据集测试中，M&C成功预测最佳模型比例达61.3%，其余案例也能选择接近最优模型。

Conclusion: M&C框架首次系统解决T2I模型选择难题，显著提升模型适配效率，降低AI应用开发成本。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [53] [How Causal Abstraction Underpins Computational Explanation](https://arxiv.org/abs/2508.11214)
*Atticus Geiger,Jacqueline Harding,Thomas Icard*

Main category: cs.LG

TL;DR: 该论文通过因果抽象理论框架，探讨认知行为和机器学习中计算实现与表征的关系，强调泛化与预测在理解计算过程中的核心作用。


<details>
  <summary>Details</summary>
Motivation: 将经典计算哲学与当代机器学习连接，论证因果理论如何帮助解释计算实现机制，以及表征在系统泛化能力和预测行为中的功能。

Method: 基于因果抽象理论构建计算实现模型，结合深度学习与神经网络的实例，分析传统计算主题在机器学习中的现代表达及表征的作用机制。

Result: 提出以因果抽象为基础的计算实现解释框架，揭示表征在认知系统和机器学习模型的泛化及预测行为中起关键媒介作用。

Conclusion: 计算实现问题应置于因果抽象框架下研究，而表征的有效性需通过系统在泛化与预测任务中的表现来验证，这对理解生物认知与人工智能具双重启示。

Abstract: Explanations of cognitive behavior often appeal to computations over
representations. What does it take for a system to implement a given
computation over suitable representational vehicles within that system? We
argue that the language of causality -- and specifically the theory of causal
abstraction -- provides a fruitful lens on this topic. Drawing on current
discussions in deep learning with artificial neural networks, we illustrate how
classical themes in the philosophy of computation and cognition resurface in
contemporary machine learning. We offer an account of computational
implementation grounded in causal abstraction, and examine the role for
representation in the resulting picture. We argue that these issues are most
profitably explored in connection with generalization and prediction.

</details>


### [54] [Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing](https://arxiv.org/abs/2508.11258)
*Ruicheng Xian,Yuxuan Wan,Han Zhao*

Main category: cs.LG

TL;DR: 提出通过提示策略从封闭权重LLMs中提取特征，结合公平算法训练轻量级公平分类器的框架，在保持高准确性的同时有效保障群体公平性。


<details>
  <summary>Details</summary>
Motivation: 传统公平算法依赖模型微调或头部调整，但无法适用于GPT-4等封闭权重LLMs的上下文学习场景。高风险应用中需要保证群体公平性，亟需新的解决方案。

Method: 将LLM作为特征提取器，通过设计特定公平标准的提示策略获取预测特征（如token概率），使用后处理公平算法训练轻量级分类器。

Result: 在5个数据集（含3个表格数据）中，该框架在精度-公平性权衡方面优于基于LLM嵌入训练或传统方法，具有数据高效性优势。

Conclusion: 该框架为封闭权重LLMs提供高效公平分类方案，可应用于高风险场景，同时支持开放权重和封闭权重模型，突破传统方法限制。

Abstract: Instruction fine-tuned large language models (LLMs) enable a simple zero-shot
or few-shot prompting paradigm, also known as in-context learning, for building
prediction models. This convenience, combined with continued advances in LLM
capability, has the potential to drive their adoption across a broad range of
domains, including high-stakes applications where group fairness -- preventing
disparate impacts across demographic groups -- is essential. The majority of
existing approaches to enforcing group fairness on LLM-based classifiers rely
on traditional fair algorithms applied via model fine-tuning or head-tuning on
final-layer embeddings, but they are no longer applicable to closed-weight LLMs
under the in-context learning setting, which include some of the most capable
commercial models today, such as GPT-4, Gemini, and Claude. In this paper, we
propose a framework for deriving fair classifiers from closed-weight LLMs via
prompting: the LLM is treated as a feature extractor, and features are elicited
from its probabilistic predictions (e.g., token log probabilities) using
prompts strategically designed for the specified fairness criterion to obtain
sufficient statistics for fair classification; a fair algorithm is then applied
to these features to train a lightweight fair classifier in a post-hoc manner.
Experiments on five datasets, including three tabular ones, demonstrate strong
accuracy-fairness tradeoffs for the classifiers derived by our framework from
both open-weight and closed-weight LLMs; in particular, our framework is
data-efficient and outperforms fair classifiers trained on LLM embeddings
(i.e., head-tuning) or from scratch on raw tabular features.

</details>


### [55] [Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning](https://arxiv.org/abs/2508.11328)
*Haitong Luo,Suhang Wang,Weiyao Zhang,Ruiqi Meng,Xuying Meng,Yujun Zhang*

Main category: cs.LG

TL;DR: 提出HS-GPPT模型通过频谱对齐机制解决图预训练中频谱分布不匹配问题，在有限监督下实现跨同配/异配图的频谱知识迁移


<details>
  <summary>Details</summary>
Motivation: 现有基于同配性的图预训练方法无法适应现实图中不同同配性带来的频谱分布差异，导致知识迁移效率低下

Method: 采用混合频谱滤波器主干网络，结合局部-全局对比学习获取频谱知识，设计提示图实现预训练与下游任务的频谱分布对齐

Result: 在转导式与归纳式学习场景下均取得显著效果提升，实验验证了频谱对齐机制的有效性，代码已开源

Conclusion: 通过预训练和提示调整阶段的频谱对齐设计，成功突破传统方法在异配性图数据上的知识迁移瓶颈

Abstract: Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [56] [Diffusion is a code repair operator and generator](https://arxiv.org/abs/2508.11110)
*Mukul Singh,Gust Verbruggen,Vu Le,Sumit Gulwani*

Main category: cs.SE

TL;DR: 代码扩散模型通过逆向去噪生成代码，后期阶段可模拟最后一步修复功能。研究验证其在添加噪声恢复修复和生成训练数据两方面的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 探索预训练代码扩散模型在最后一步代码修复（last-mile repair）中的应用潜力，利用其逆向去噪特性模拟修复过程

Method: 1. 向错误代码添加噪声后恢复扩散过程实现修复
2. 从扩散过程中采样中间/最终程序生成训练数据，支持更高效的任务训练

Result: 在Python/Excel/PowerShell三个领域验证有效性，分析模型在修复能力和训练数据生成方面的表现特性

Conclusion: 代码扩散模型具备双重应用价值：既能直接用于最后一步修复，又能生成大规模训练数据支持轻量化修复模型训练

Abstract: Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

</details>


### [57] [ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal](https://arxiv.org/abs/2508.11222)
*Haonan Zhang,Dongxia Wang,Yi Liu,Kexin Chen,Jiashui Wang,Xinlei Ying,Long Liu,Wenhai Wang*

Main category: cs.SE

TL;DR: 提出了首个进化测试框架ORFuzz，通过整合安全类别感知种子选择、自适应变异器优化和OR-Judge评估模型，系统性检测LLM过度拒绝行为，生成成功率翻倍的测试案例并建立高效基准数据集ORFuzzSet。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型存在过度拒绝良性查询的严重功能缺陷，现有测试方法基准不完善且测试生成能力有限，影响LLM的可靠性和可用性。

Method: 1) 安全类别感知种子选择实现全面测试覆盖 2) 基于推理LLM的自适应变异器优化生成有效测试案例 3) 开发用户感知对齐的OR-Judge评估模型。

Result: ORFuzz生成测试案例的平均成功率6.98%（达基线两倍以上），构建的ORFuzzSet基准在10个LLM上实现63.56%平均过度拒绝率。

Conclusion: ORFuzz框架和ORFuzzSet数据集为LLM可靠性测试提供自动化解决方案和社区资源，推动可信LLM系统开发。

Abstract: Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: 提出ViPE视频处理引擎，可高效估计相机参数/运动/深度图，支持多场景多相机模型，在多个基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有空间AI系统依赖高质量3D标注数据，但视频数据的精确标注获取困难。ViPE旨在自动生成高质量3D标注，降低相关研究门槛

Method: 通过处理原始视频估计相机内参、运动参数和密集近度量深度图，支持自拍视频/电影镜头/行车记录仪等动态场景，兼容针孔/广角/全景相机模型

Result: 在TUM/KITTI基准测试中分别超越基线18%/50%，单GPU运行3-5FPS，标注了含10万真实视频/100万AI生成视频/2000全景视频的9600万帧数据集

Conclusion: ViPE及开源数据集为空间AI系统提供高效解决方案，支持实时处理和多场景应用，显著加速3D感知技术发展

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [59] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 多模态大模型通过集成外部工具可突破数据质量、任务性能、评估体系三大瓶颈，为AGI发展提供新路径


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs面临多模态数据质量低下、复杂任务表现欠佳、评估标准不完善等问题，受人类借助工具增强认知的启发，探索通过外部工具增强模型能力

Method: 从数据获取增强（API/专家模型/知识库）、任务性能提升、评估体系优化、未来方向四个维度构建工具增强框架

Result: 系统论证了工具增强在高质量数据标注（如GPT-4V标注）、复杂任务分解（如数学推理）、三维评估体系构建（如MMMU基准）中的关键作用

Conclusion: 工具增强策略是提升MLLMs可靠性和领域适应性的核心范式，本文建立的系统分析框架为下一代多模态系统开发提供理论蓝图和技术路线

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [60] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: 研究发现多模态大语言模型在文档欺诈检测中展现零样本泛化优势，模型规模与检测精度关联性有限，需针对性微调


<details>
  <summary>Details</summary>
Motivation: 现有文档欺诈检测方法难以应对复杂伪造手段，亟需探索多模态大语言模型在识别篡改痕迹和逻辑矛盾方面的潜力

Method: 采用标准交易文档数据集，通过提示优化和推理过程分析，系统评估十余种先进LLMs检测文本篡改、格式异常和金额不一致的能力

Result: 顶尖多模态LLMs零样本表现超越传统方法，部分视觉模型表现不稳定，模型推理能力与检测精度未呈现显著正相关

Conclusion: 研究证实多模态LLMs可显著提升文档防伪系统效能，为构建可解释的智能防欺诈框架奠定理论基础

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [61] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 提出基于对比学习的MICC算法，通过多尺度图像-文本语义对齐和自适应特征融合，在双数据集上实现谣言检测SOTA性能突破


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法忽视图像内容与多尺度跨模态关联，导致关键判别信息丢失

Method: 1) SCLIP编码器构建跨模态统一语义空间 2) 跨模态Top-K对齐模块提取关键图像区域 3) 基于互信息的尺度感知融合网络

Result: 在两个真实数据集上显著超越现有最优方法，准确率提升3.5%-7.2%

Conclusion: 多尺度跨模态关联建模与自适应特征融合机制能有效提升谣言检测性能，具有实际部署价值

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [62] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 开发了一种通过奖励引导解码提升多模态大语言模型视觉定位的方法，在对象幻觉基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型(MLLMs)的广泛应用，需要动态适应用户需求。本文旨在通过受控解码实现模型推理过程的可控性，特别针对视觉定位中的对象精确度与召回率平衡问题。

Method: 1. 构建两个独立奖励模型分别控制对象精确度和召回率
2. 提供双重控制机制：动态调整奖励权重实现精度-召回权衡；通过搜索范围控制计算资源与视觉定位的平衡

Result: 在标准对象幻觉基准测试中，方法显著优于现有缓解技术，同时实现解码过程的可控性(如动态调整精度/召回权重从0.1到0.9时，召回率提升37%而精度仅下降4%)

Conclusion: 该方法开创了MLLMs推理过程的可控解码范式，为实际应用中的计算资源-性能权衡提供灵活解决方案，同时持续超越现有幻觉抑制方法的表现。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [63] [Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information](https://arxiv.org/abs/2508.11252)
*Youcheng Huang,Bowen Qin,Chen Huang,Duanyu Feng,Xi Yang,Wenqiang Lei*

Main category: cs.AI

TL;DR: 现有基准仅评估大模型在完整问题上的数学推理能力，但真实智能需具备主动信息追问能力。研究通过构建不完整问题数据集，揭示了大模型在该能力上的缺陷及潜在改进方向。


<details>
  <summary>Details</summary>
Motivation: 突破现有仅评估大模型解决完整问题的研究范式，强调真正智能体应具备在信息不全时主动追问的能力，推动大模型向'真实智能'进化。

Method: 1. 构建包含多种场景的不完整问题数据集
2. 系统评估大模型在主动信息追问、过度思考、幻觉等方面的表现
3. 探索监督微调对提升该能力的潜力

Result: 1. 大模型普遍缺乏主动追问能力
2. 发现模型存在'过度思考'(反复推导)和'幻觉'(虚构信息)行为
3. 监督微调虽能提升但面临泛化性挑战

Conclusion: 研究为开发真正智能的大模型提供了新方向：需突破被动解题模式，培养主动交互能力，这既是技术挑战也是实现通用人工智能的关键突破点。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving
abilities in mathematics, as evaluated by existing benchmarks exclusively on
well-defined problems. However, such evaluation setup constitutes a critical
gap, since a genuine intelligent agent should not only solve problems (as a
math quiz solver), but also be able~to ask for information when the problems
lack sufficient information, enabling proactivity in responding users'
requests. To bridge such gap, we proposes a new dataset consisting of two types
of incomplete problems with diverse contexts. Based on the dataset, our
systematical evaluation of LRMs reveals their inability in proactively asking
for information. In addition, we uncover the behaviors related to overthinking
and hallucination of LRMs, and highlight the potential and challenges of
supervised fine-tuning in learning such ability. We hope to provide new
insights in developing LRMs with genuine intelligence, rather than just solving
problems.

</details>


### [64] [Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps](https://arxiv.org/abs/2508.11452)
*Kangyu Wang,Hongliang He,Lin Liu,Ruiqi Liang,Zhenzhong Lan,Jianguo Li*

Main category: cs.AI

TL;DR: 论文提出Inclusion Arena实时排行榜，通过真实应用场景的用户反馈和创新的Bradley-Terry模型改进（Placement Matches冷启动机制+Proximity Sampling智能采样策略），实现更可靠的AI模型性能评估。


<details>
  <summary>Details</summary>
Motivation: 现有LLM/MLLM评测基准依赖静态数据集和通用提示，无法反映模型在实际应用中的真实表现。

Method: 1. 将成对模型比较嵌入自然用户交互
2. 改进Bradley-Terry模型：
   - Placement Matches快速估算新模型初始评分
   - Proximity Sampling优先让能力相近的模型对战

Result: 实证分析显示：
1. 排名稳定可靠
2. 数据传递性优于通用众包数据集
3. 显著降低恶意操纵风险

Conclusion: 通过连接基础模型与真实应用场景，Inclusion Arena能加速开发真正面向实际部署优化的AI模型。平台已公开可用。

Abstract: Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [Benchmarking Prosody Encoding in Discrete Speech Tokens](https://arxiv.org/abs/2508.11224)
*Kentaro Onda,Satoru Fukayama,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.SD

TL;DR: 研究通过分析离散标记对人工修改韵律的敏感性，提出改进语音语言模型的设计指南


<details>
  <summary>Details</summary>
Motivation: 现有离散标记方案在捕捉语音韵律特征方面存在不足，且相关参数选择缺乏理论依据，影响语音语言模型对韵律信息的编码能力

Method: 采用基于人工修改韵律（音高、节奏等）的敏感性测试框架，系统评估不同SSL模型和聚类参数下离散标记的韵律编码能力

Result: 发现离散标记对韵律特征的编码效率与SSL模型架构及聚类数量强相关，提出参数选择的最佳实践方案

Conclusion: 该分析框架为语音语言模型中离散标记的优化设计提供了数据驱动的决策依据，显著提升韵律特征编码效果

Abstract: Recently, discrete tokens derived from self-supervised learning (SSL) models
via k-means clustering have been actively studied as pseudo-text in speech
language models and as efficient intermediate representations for various
tasks. However, these discrete tokens are typically learned in advance,
separately from the training of language models or downstream tasks. As a
result, choices related to discretization, such as the SSL model used or the
number of clusters, must be made heuristically. In particular, speech language
models are expected to understand and generate responses that reflect not only
the semantic content but also prosodic features. Yet, there has been limited
research on the ability of discrete tokens to capture prosodic information. To
address this gap, this study conducts a comprehensive analysis focusing on
prosodic encoding based on their sensitivity to the artificially modified
prosody, aiming to provide practical guidelines for designing discrete tokens.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [66] [Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style](https://arxiv.org/abs/2508.11187)
*Wonjune Kang,Deb Roy*

Main category: eess.AS

TL;DR: 提出一种基于自然语言描述的语音风格检索方法，通过联合编码实现语音与文本跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 传统语音检索主要关注内容（what），而本研究聚焦于语音表达方式（how），旨在通过文本描述精准检索特定情感或风格的语音片段。

Method: 1. 构建语音-文本联合潜在空间编码器
2. 探索不同编码架构和训练准则
3. 采用提示增强策略提升泛化能力

Result: 在包含22种说话风格的多数据集上实现高Recall@k指标

Conclusion: 该方法有效建立语音与文本的跨模态关联，提示增强策略显著提升对任意文本查询的适应性，为基于风格的语音检索提供新范式。

Abstract: We introduce the task of expressive speech retrieval, where the goal is to
retrieve speech utterances spoken in a given style based on a natural language
description of that style. While prior work has primarily focused on performing
speech retrieval based on what was said in an utterance, we aim to do so based
on how something was said. We train speech and text encoders to embed speech
and text descriptions of speaking styles into a joint latent space, which
enables using free-form text prompts describing emotions or styles as queries
to retrieve matching expressive speech segments. We perform detailed analyses
of various aspects of our proposed framework, including encoder architectures,
training criteria for effective cross-modal alignment, and prompt augmentation
for improved generalization to arbitrary text queries. Experiments on multiple
datasets encompassing 22 speaking styles demonstrate that our approach achieves
strong retrieval performance as measured by Recall@k.

</details>


### [67] [Emphasis Sensitivity in Speech Representations](https://arxiv.org/abs/2508.11566)
*Shaun Cassini,Thomas Hain,Anton Ragni*

Main category: eess.AS

TL;DR: 研究现代语音模型对韵律重音的敏感性，提出残差框架揭示重音的结构化编码差异


<details>
  <summary>Details</summary>
Motivation: 已有研究依赖孤立的声学关联（如音高、时长）或标签预测，但忽略了重音的关系结构特性，需系统性分析模型编码方式

Method: 提出基于残差的框架：将韵律重音定义为中性词与重读词表征的差异，分析自监督语音模型及ASR微调模型的残差空间特性

Result: 残差与持续时间变化强相关且词身份预测能力差；微调模型的残差子空间紧凑度比预训练模型高50%，表明重音编码为一致的低维变换

Conclusion: 语音模型通过结构化方式编码韵律重音，任务特定学习（如ASR）使这种编码更结构化，残差框架有效捕捉关系特性

Abstract: This work investigates whether modern speech models are sensitive to prosodic
emphasis - whether they encode emphasized and neutral words in systematically
different ways. Prior work typically relies on isolated acoustic correlates
(e.g., pitch, duration) or label prediction, both of which miss the relational
structure of emphasis. This paper proposes a residual-based framework, defining
emphasis as the difference between paired neutral and emphasized word
representations. Analysis on self-supervised speech models shows that these
residuals correlate strongly with duration changes and perform poorly at word
identity prediction, indicating a structured, relational encoding of prosodic
emphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more
compact than in pre-trained models, further suggesting that emphasis is encoded
as a consistent, low-dimensional transformation that becomes more structured
with task-specific learning.

</details>
