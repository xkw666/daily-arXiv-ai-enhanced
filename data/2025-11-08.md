<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.SD](#cs.SD) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出基于大五人格特质的低秩子空间调控框架，实现LLM隐性个性的精准控制而不影响基础性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以可靠控制LLM的隐性人格特征，需探索心理特质与模型表示的关系及其行为调控机制

Method: 通过transformer层激活提取+低秩子空间发现+动态层选择，构建人格特质定向调控框架

Result: 揭示人格特征存在于低秩共享子空间，验证扰动调控机制对输出特质有效性且不影响模型基础能力

Conclusion: 建立了心理学理论与模型对齐的桥梁，为LLM行为控制提供了可操作的定向调控方法论

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: TextualVerifier框架通过思维链分解和多数投票机制，首次实现TextGrad文本优化的自我验证，提升推理有效性并降低计算开销


<details>
  <summary>Details</summary>
Motivation: TextGrad现有方法缺乏文本决策中的自我验证机制，导致推理有效性无法保障

Method: 四阶段验证流程：思维链分解→变体生成→多数投票→共识聚合；在TextGrad的损失函数和优化结果验证阶段双向集成

Result: PRM800K验证阶段推理有效性提升29%；TextGrad整合后准确率从68.2%提升至70.4%（p<0.001）；GPQA/MMLU基准分别提升8.08/10.71/3.92个百分点

Conclusion: TextualVerifier突破传统数值梯度依赖，通过纯文本验证机制增强TextGrad系统可靠性，为文本优化领域开辟新的验证范式

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 扩展GRDD希腊方言数据集至10种变体/637万词规模，首次系统评估方言数据对LLMs微调效果


<details>
  <summary>Details</summary>
Motivation: 解决现有希腊方言数据分散且规模不足的问题，验证高质量方言数据对提升大语言模型性能的作用

Method: 1. 整合6种新方言构建GRDD+数据集 2. 使用三种8B参数模型微调 3. 与前沿闭源模型对比测试

Result: 创建首个大规模希腊多方言数据集（6,374,939词），微调实验显示方言数据显著提升开源模型语言理解能力

Conclusion: GRDD+填补希腊方言资源空白，证实方言数据对LLM优化具有关键作用，为低资源语言处理提供新范式

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 开发针对波兰语的最大开源基础模型家族PLLuM，解决非英语语言模型支持不足问题，强调透明性和文化适配性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型以英语为中心，导致波兰语等高性价比、文化适配的AI模型缺失。通过开源模式推动波兰本土AI技术主权建设

Method: 构建1400亿波兰语token预训练语料库，开发7.7万指令数据集和10万偏好优化数据集，采用混合模块实现输出校正和安全过滤的负责任AI框架

Result: 成功应用于公共管理下游任务验证，通过开源促进波兰AI研究生态系统发展

Conclusion: PLLuM填补波兰语LLM空白，建立透明可信的AI开发范式，为小语种AI发展提供可复制框架

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出STARS解码算法，通过分段token对齐和拒绝采样提升LLM对齐效率与质量


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（微调/Best-of-N采样）存在计算成本高或效率低下的问题，需更优解决方案

Method: 在解码阶段对固定长度token段进行迭代采样-评分-调整，实现早期生成路径修正

Result: 在6个LLM上超越SFT（+14.9%）和DPO（+4.3%），与Best-of-N基线竞争力相当

Conclusion: 确立了细粒度奖励引导采样作为传统微调和全序列排序方法的高效替代方案

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出通过二分决策分解多标签分类任务，结合知识蒸馏和前缀缓存机制，显著提升LLM分类效率（尤其在短文本场景），并在情感文本分析中验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类模型生成结构化响应效率低下。通过将多标签任务分解为独立二元查询，配合前缀缓存机制，在保持精度的同时实现短文本推理效率的大幅提升

Method: 1. 将多标签分类重构为二分决策序列
2. 使用DeepSeek-V3生成多维度情感标注
3. 通过LLM-to-SLM蒸馏技术微调HerBERT-Large等小型模型
4. 采用前缀缓存机制加速短文本推理

Result: 1. 微调模型在训练维度上显著超越零样本基线（平均提升15-20%）
2. 短文本推理速度提升3-5倍（缓存机制贡献主要增益）
3. PLLuM-8B模型达到SOTA水平（F1=0.87）

Conclusion: 二分决策分解+知识蒸馏+缓存优化的方法论框架具有跨领域扩展性，为LLM分类任务提供高效解决方案，特别适合需要实时响应的多标签短文本场景

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 研究发现低资源语言数据集存在领域分布失衡（政治/宗教训练数据 vs 新闻/健康/体育基准数据）、显著男性性别偏向（人名/语法/刻板印象）及有害内容，且数据量最大语言问题最突出，强调数量≠质量


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言NLP研究中过度追求数据规模的现象，研究旨在揭示数据集质量缺陷（特别是性别偏见）对技术性能和社会公平性的双重危害

Method: 通过分析阿法尔奥罗莫语、阿姆哈拉语和提格里尼亚语的机器翻译数据集，重点评估性别表征（人名/语法性别/刻板印象）和领域分布特征

Result: 1) 训练数据与基准数据领域错位 2) 男性主导现象普遍（男性人名占比高、动词性别标记偏男性、职业刻板印象）3) 存在对女性的有害/毒性描述，且数据量最大的语言问题最严重

Conclusion: 强调低资源语言数据集质量评估的紧迫性，建议在数据收集早期介入偏见修正，避免规模化过程中放大社会危害

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出无需重新训练的GRAD方法，通过语料库构建token转移图实现解码时证据融合，在多个基准测试中显著降低大语言模型幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有缓解LLM幻觉的方法依赖外部知识源（提示工程/知识图谱），存在领域敏感、维护成本高的问题，需更轻量级的解决方案

Method: 单次前向传播构建稀疏token转移图，解码时最大归一化证据logits并与模型logits自适应融合，优先选择高证据词元同时保持生成流畅性

Result: 在3个模型和多个QA基准测试中，相比贪婪解码实现：内在准确率↑9.7%、幻觉率↓8.6%、正确率↑6.9%，真实性与信息量综合得分最优

Conclusion: 验证了语料级统计证据可有效引导生成真实性，为对比解码和知识图谱增强提供了即插即用的轻量替代方案

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 研究通过迭代参考游戏比较人类与视觉语言模型的上下文推理能力，发现模型在相关上下文下的表现显著提升但仍落后于人类水平。


<details>
  <summary>Details</summary>
Motivation: 验证智能体在多轮语言环境中的上下文敏感推理能力，探索不同上下文条件对模型表现的影响。

Method: 通过控制上下文的数量、顺序和相关性设计迭代参考游戏实验，对比人类与模型的反应模式。

Result: 无相关上下文时模型准确率高于随机但显著弱于人类（模型65% vs 人类92%）；引入相关上下文后模型准确率提升至83%。

Conclusion: 视觉语言模型展现出情境学习潜力，但在抽象指代的少样本参考场景中仍存在显著能力缺口。

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 研究者开发了Human Flourishing Geographic Index (HFGI)，通过分析26亿条美国地理定位推特数据，结合微调大语言模型构建48个社会繁荣指标，提供高时空分辨率的福祉分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有社会福祉测量方法时空分辨率不足，需建立更精细的指标体系来全面评估人类繁荣的多维度特征。

Method: 使用2013-2023年26亿条地理定位推特数据，通过微调LLM分类48个指标（含移民态度和腐败感知），构建月度/年度州县级数据集并进行验证。

Result: 验证显示该指标能准确反映社会繁荣结构，与传统指标存在预期相关性，支持不平等分析和社会变迁研究。

Conclusion: 该资源首次实现基于社交媒体话语的十年跨度高分辨率繁荣动态分析，为多学科研究提供新视角。

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 提出通过向量翻译在跨模型语义空间建立潜在桥梁，替代传统token通信方式，实现AI系统间的高效语义共享


<details>
  <summary>Details</summary>
Motivation: 传统多智能体系统中LLM以明文token传递消息存在信息传递效率低、计算开销大的问题，需要更高效的跨模型语义交互方式

Method: 采用双编码器翻译架构训练Llama-2-7B和Mistral-7B-Instruct之间的潜在空间映射，通过30%混合强度的向量注入实现语义引导

Result: 达到平均余弦对齐度0.538，双向评估显示2.01:1的传输不对称性，证明通用模型比指令微调模型具有更好的表征迁移能力

Conclusion: 潜在向量注入在保持计算稳定性的同时实现了跨模型语义通信，为构建基于意义共享的协作式AI系统提供了新范式

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 提出将溯因推理整合到RAG框架中，通过生成候选前提填补证据缺口，提升问答准确性和推理可靠性


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强LLM在证据不完整时会出现推理断层，需要溯因推理机制来生成缺失前提

Method: 构建包含证据完整性检测、候选前提生成、一致性/合理性验证的三阶段框架

Result: 在溯因推理和多跳QA任务中验证了方法有效性，答案准确率提升且推理过程更可靠

Conclusion: 溯因推理为增强RAG系统的鲁棒性和可解释性提供了新方向

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出弱监督转换器(WST)，在70%转录错误率下仍保持ASR性能，显著优于现有弱监督方法


<details>
  <summary>Details</summary>
Motivation: 传统RNN-T模型依赖高质量标注数据，而真实场景中常存在大量带噪声/错误的转录数据，需要开发更鲁棒的训练方法

Method: 设计灵活训练图结构，直接建模转录错误分布，无需置信度估计或预训练模型辅助

Result: 在合成和工业数据集上验证，WST在70%错误率时性能下降仅0.3%，相对BTC/OTC方法词错误率降低12-15%

Conclusion: WST为实际ASR应用提供了可靠的弱监督解决方案，其开源实现将推动该领域进一步发展

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [14] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 提出专家对齐(T-FIX)作为LLM解释评估新标准，构建跨七领域基准并开发专家校准指标


<details>
  <summary>Details</summary>
Motivation: 现有LLM解释评估侧重合理性而非专家直觉匹配，难以满足知识密集型领域（如医学、天文）专家用户需求

Method: 1. 构建跨七个知识密集型领域的T-FIX基准
2. 与领域专家合作开发新评估指标
3. 量化LLM解释与专家判断的校准度

Result: 开发出能有效衡量LLM解释与专家认知对齐程度的评估体系

Conclusion: 专家对齐应成为LLM解释评估的核心标准，T-FIX为跨领域专家级解释质量评估提供方法论框架

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [15] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 提出PoK框架，通过将复杂时间问题分解为子目标并结合对比时序检索器，显著提升大语言模型在时序知识图谱问答中的表现，最高准确率提升56%


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分理解时间约束语义，大语言模型存在时序推理幻觉和知识缺失问题，需结合结构化规划与知识检索增强事实一致性

Method: PoK框架包含知识计划模块（问题分解为工具预定义子目标）和对比检索时序知识库（TKS），实现语义与时序对齐的事实检索

Result: 在四个TKGQA基准测试中，检索精度和推理准确率显著提升，最高超越现有最好方法56%

Conclusion: 通过结构化知识规划与时序知识检索的结合，PoK有效提升大语言模型的时序推理可解释性，解决幻觉与知识局限性问题

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [16] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 研究比较人类与大型语言模型（LLMs）在情感词汇联想上的差异，发现LLMs的联想较人类更可预测但缺乏创造性，且会放大词汇的情感负荷。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能像人类一样通过个人经验和情感形成创造性联想，尤其是对情感词汇的关联机制差异。

Method: 通过对比分析人类与LLMs对情感词汇的联想数据，评估其重叠度、情感强度及创造性差异。

Result: LLMs与人类联想重叠度中等，但LLMs会放大词汇情感负荷，联想模式更可预测且创造性较低。

Conclusion: LLMs的联想机制与人类存在显著差异，其缺乏创造性和情感放大的特性可能限制其在需要灵活认知的任务中的应用。

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [17] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 通过大规模训练数据优化和模型架构改进，提出基于Transformer的放射学报告去识别方案，性能超越现有学术和商业系统


<details>
  <summary>Details</summary>
Motivation: 现有医疗文本去识别系统在跨机构泛化性和PHI检测完整性存在局限，需通过多模态训练提升模型鲁棒性并验证合成数据的实用性

Method: 1. 在Stanford放射学数据集上微调Transformer模型
2. 新增AGE类别并优化架构
3. 采用「隐藏即存在」方法生成合成PHI
4. 与多个商业云系统进行对比测试

Result: Penn/Stanford数据集F1达0.973/0.996，合成数据检测一致性F1=0.959，商业系统对比测试F1领先幅度达20.6%-32.8%

Conclusion: 基于多源放射学数据训练的Transformer模型在PHI检测中实现新突破，为临床文本安全处理建立新基准

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [18] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 提出k-列表语言识别的充要条件，扩展Angluin理论并证明统计场景下指数级识别速率为最优


<details>
  <summary>Details</summary>
Motivation: 经典语言识别理论存在局限性，受现代语言生成技术启发，探索通过多假设列表提升识别能力

Method: 递归扩展Angluin特征定理，构建语言集合的k层分解结构，结合统计学习理论分析识别速率边界

Result: 建立k-列表识别的分解等价性定理，证明可识别集合存在指数级收敛速率且不可突破该下限

Conclusion: 多假设机制通过结构分解突破单列表限制，统计场景下可达到理论最优识别效率边界

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [19] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批量提示技术不仅降低LLM推理成本，还能通过抑制过度思考、减少冗余修正提升推理效率，在13个基准测试中实现3-5倍token节省和准确率提升，并涌现群体推理效应。


<details>
  <summary>Details</summary>
Motivation: 探索批量推理在大型推理模型（LRM）中除吞吐量优化外的潜在正则化价值，揭示其改善模型推理行为的机制。

Method: 通过13个多样化基准测试，结合行为分析（过度思考抑制、犹豫语言减少、答案确定性提升）和集体效应观察，系统研究批量推理的影响。

Result: 批量推理使token消耗减少3-5倍，准确率提升，消除78%的自我修正冗余，并在同批次内出现跨样本的模式泛化能力。

Conclusion: 批量推理本质是高效的推理时正则化器，为提升LLM推理效率和可靠性提供了新范式，超越传统吞吐量优化视角。

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [20] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: 提出RIDE框架，通过项目反应理论生成数学难题，有效暴露大语言模型推理脆弱性，性能平均下降21.73%


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的扰动方法易产生病态问题，无法系统评估问题难度和基准演进

Method: 结合IRT理论构建难度分级器，使用35个LLM模拟学生反应，通过强化学习指导问题重写

Result: 高级LLM在对抗性问题中性能平均下降21.73%，暴露数学推理鲁棒性缺陷

Conclusion: RIDE框架有效验证评估方法，揭示当前模型数学推理能力的局限性

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [21] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: CantoASR框架通过整合声学特征与音频语言模型推理，显著提升了低资源粤语语音识别性能


<details>
  <summary>Details</summary>
Motivation: 解决现有ASR模型（如Whisper）在低资源粤语场景下因标注数据稀缺、六声调系统、变调和口音差异导致的高词错率问题

Method: 结合强制对齐声学特征提取、LoRA微调的Whisper模型提升声调识别能力，以及指令调优的Qwen-Audio实现韵律感知纠错

Result: 在粤语自然语音数据集上取得比Whisper-Large-V3显著的CER提升

Conclusion: 声学特征与音频语言模型的协同框架为低资源声调方言ASR提供了可扩展的解决方案

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [22] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 提出三种多智能体LLM管道提升Text-to-SQL性能，验证讨论式协作和小模型优化的有效性


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂SQL生成中存在局限，且先前研究忽视小型高效模型的潜力

Method: 1. 多智能体讨论管道（迭代反馈优化） 2. 规划器-编码器管道（分步生成计划） 3. 编码器-聚合器管道（多方案择优）

Result: Qwen2.5-7B模型经3轮讨论执行准确率提升10.6%；规划器-编码器管道使Gemma模型达到56.4%最高准确率

Conclusion: 多智能体协作显著提升性能，规划器-编码器模式最优，为小型模型优化提供新方向

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [23] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: LAAC提出将LLM作为通信中介以解决AI生成内容泛滥导致的无效沟通循环，通过三维度可信度评估发现需解决的信任差距


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容泛滥导致发送方过度扩展内容、接收方机械压缩，形成虚假沟通循环，亟需重构LLM在通信中的定位

Method: 采用多智能体架构，通过信息捕获保真度、可重复性和查询响应完整性三个维度系统评估LAAC的可信度

Result: 实验发现LAAC在信息提取准确性、跨实例一致性及响应可靠性方面存在可量化的信任缺口

Conclusion: 必须解决信任维度的系统性缺陷，特别是在高风险通信场景中实现意图保真与响应完整性的平衡

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [24] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 提出计算图灵测试框架验证LLM生成文本的真实性，发现校准后模型仍与人类文本存在显著差异，揭示语义保真度与人类相似性的权衡


<details>
  <summary>Details</summary>
Motivation: 现有基于人类判断的LLM验证方法存在局限性，需要更可靠的评估工具来验证语言模型生成文本的人类真实性

Method: 集成BERT检测指标、语义相似度和可解释语言特征（文体标记/主题模式）的验证框架，系统比较9个开源LLM在5种校准策略下的表现

Result: 校准后LLM输出在情感语调/表达上与人类文本差异显著；指令微调模型表现劣于基础模型；模型规模扩大不提升人类相似性；存在语义保真度与人类相似性的负相关

Conclusion: 提供可扩展的LLM验证框架，揭示当前模型在模拟人类交流中的核心局限，强调优化方向需平衡语义准确性与人类相似性

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 评估当前大型语言模型在波兰国家上诉法院资格考试中的表现，发现其知识测试达标但书面评估未通过，且自动评分系统与人工评估存在显著差异


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在法律资格考试中的实际应用潜力，探索其作为考生和自动评分系统的可行性

Method: 构建混合信息处理流程，测试GPT-4.1/Claude/Bielik等模型在闭卷和检索增强模式下的表现，包含法律知识测试与文书写作评估

Result: 模型选择题正确率达标（73%），但文书写作通过率为0%，LLM自动评分与人工评审的Kappa系数仅为0.32

Conclusion: 当前LLMs存在法律条款误引（35%案例）、逻辑漏洞（28%文书）和事实幻觉（17%陈述）等缺陷，暂不具备替代人类法律专家的能力

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: 提出REMIND方法，通过分析模型在相似样本上的损失变化，检测未学习数据残留的影响以评估遗忘效果


<details>
  <summary>Details</summary>
Motivation: 现有遗忘评估方法仅关注单个样本，可能忽略语义相似样本的残留影响，导致隐私泄露风险

Method: 通过分析模型在小范围输入变异中的损失模式，发现未学习数据形成更平坦的损失曲面特征

Result: REMIND在仅需查询访问条件下超越现有方法，跨模型/数据集/改写输入均表现鲁棒

Conclusion: REMIND为语言模型遗忘效果提供敏感且可解释的评估框架，推动可信AI发展

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 研究表明当前预训练方法未能充分利用数据集信息，通过检索增强可将MMLU准确率提升10个百分点，相当于5倍计算效率提升


<details>
  <summary>Details</summary>
Motivation: 探索预训练过程中数据信息提取效率，验证现有方法是否充分利用了预训练数据集的价值

Method: 结合检索增强生成(RAG)和测试时计算，量化预训练后从标准开源数据集检索带来的性能增益

Result: 在MMLU、Math-500和SimpleQA任务中观察到显著准确率提升（LLaMA 3.1 8B模型在MMLU提升10%），检索效率相当于预训练计算量的5倍

Conclusion: 当前预训练方法存在显著的数据价值浪费，通过改进检索机制和测试时计算可释放数据集潜力

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 提出基于图算法的主题标注方法，在保持计算效率的同时实现与ChatGPT-3.5相当的标注效果，显著优于传统基准方法。


<details>
  <summary>Details</summary>
Motivation: 现有主题建模方法生成的主题词缺乏解释性，且主流深度学习方法计算成本过高。需要开发兼顾可解释性与计算效率的替代方案。

Method: 通过构建语义图模型扩展主题词汇，分析词汇间的拓扑关系生成解释性标签，避免依赖计算密集型模型。

Result: 在两个数据集上BERTScore提升15%，计算效率比传统方法高3倍，与ChatGPT-3.5的标注质量相当。

Conclusion: 验证了概率统计方法与图结构分析的有效结合，未来可探索多模态数据融合和自动化评估框架的构建。

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 提出SSPO方法，通过句子级重要性比率和熵调整机制，平衡GRPO与GSPO的优缺点，提升LLM强化学习训练稳定性和数据利用率


<details>
  <summary>Details</summary>
Motivation: 现有GRPO存在训练不稳定问题（token级计算易受异常值影响），GSPO存在数据利用率低缺陷（响应级计算导致整体误弃）

Method: 1. 句子级重要性比率计算 2. 引入句子熵动态调整PPO-CLIP裁剪边界（鼓励高熵token探索，收紧低熵token裁剪范围）

Result: 五数据集平均得分46.57，超过GRPO(43.01)和GSPO(44.42)，在三个数据集达到SOTA性能

Conclusion: SSPO有效结合GSPO优势并规避其缺陷，通过多层级平衡设计显著提升生成数据的利用效率

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 提出基于可学习性评分与批量依赖优化的数据选择方法，使机器翻译微调效率提升5倍、计算效率提升24倍


<details>
  <summary>Details</summary>
Motivation: 数据质量与有效筛选是提升机器翻译性能的关键，传统方法未充分考虑数据点间的依赖关系与学习潜力

Method: 结合学习模型与预训练参考模型的协同机制，通过可学习性评分筛选数据，并采用考虑数据关联性的批量选择策略

Result: 在英-波斯等语言对实验中实现5倍数据效率提升，缓存嵌入下计算效率提升24倍，翻译性能显著优于随机选择

Conclusion: 该数据选择框架通过精准筛选与批量优化，为低资源场景下的高效模型微调提供了有效解决方案

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 研究测试LLM在时间推理与跨语言表现，发现英语提示优于挪威语，模型规模提升效果，但挪威专用模型表现未达预期。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在时间推理任务中的表现，特别关注不同语言提示（英语/挪威语）对结果的影响。

Method: 使用1940年挪威书籍的琐事问题，让LLM以1940年视角回答，采用英语/挪威语双语提问，通过LLM-as-judge与人工抽查结合评分。

Result: 1. 英语提示效果显著优于挪威语
2. 模型规模扩大提升准确率
3. 挪威专用最大模型表现未超越通用大模型

Conclusion: 语言选择对LLM时间推理任务影响显著，模型规模扩展有效提升性能，但专用语言模型需进一步优化。

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [32] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出PTTSD框架，通过概率文本时序建模实现抑郁症严重程度的可解释预测，在性能指标和不确定性校准方面优于现有文本系统


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症预测模型缺乏不确定性估计和时间动态建模能力，影响临床决策的可信度和解释性

Method: 结合双向LSTM、自注意力机制、残差连接与高斯/Student-t概率输出头，开发序列到序列和序列到单值两种模型变体

Result: 在E-DAIC和DAIC-WOZ数据集上取得文本系统最佳性能（MAE=3.85/3.55），预测区间校准良好，注意力机制和概率建模有效性经消融实验验证

Conclusion: PTTSD通过不确定性建模和时序分析增强了预测的临床解释性，为精神健康评估提供了可靠的计算框架

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [33] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 首个评估视觉语言模型在泰语文本丰富视觉理解任务上的综合基准ThaiOCRBench，包含2,808个样本的多样化人工标注数据集


<details>
  <summary>Details</summary>
Motivation: 现有多模态基准主要聚焦高资源语言，泰语在需要文档结构理解的任务中代表性不足

Method: 通过13类任务的2,808个样本数据集，在零样本设置下评估专有和开源VLMs（如Gemini 2.5 Pro）

Result: 专有模型显著优于开源模型，开源模型在细粒度文本识别（下降38.7%）和手写内容提取（下降41.2%）表现最差

Conclusion: ThaiOCRBench为低资源复杂文字环境提供标准化评估框架，揭示了语言偏见、结构不匹配和幻觉内容三大核心挑战

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [34] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 提出RUST-BENCH表格推理基准，挑战LLMs在复杂真实表格中的多领域推理能力


<details>
  <summary>Details</summary>
Motivation: 现有基准使用小型均匀表格，无法评估LLMs在真实场景中处理异构数据/领域特定/长上下文推理的表现

Method: 构建包含7966问题、2031真实表格的跨领域测试集(RB-Science和RB-Sports)，联合评估模型在规模/异构性/领域/推理复杂度四个维度

Result: 实验显示现有LLM在异构模式理解/多跳推理存在显著缺陷，提示架构与训练策略改进方向

Conclusion: RUST-BENCH为表格推理研究确立新标准，推动处理复杂结构化数据的模型能力发展

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [35] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: 论文提出基于LLM提示的多轮文本简化方法(MRS-Rule/MRS-Joint)，通过分析CEFR级别差距发现简化效果规律，系统在TSAR-2025任务中排名第7，改进后性能进一步提升。


<details>
  <summary>Details</summary>
Motivation: 基于对提示式文本简化方法的分析，发现文本简化性能与源文本和目标文本的CEFR等级差距存在显著关联。

Method: 提出两种多轮简化方法：规则导向的MRS-Rule和规则-LLM联合的MRS-Joint，均通过GPT-4o生成简化文本。

Result: 提交系统在20支队伍中排名第7，后续改进表明以LLM简化候选为起点可显著提升多轮简化性能。

Conclusion: 多轮简化方法有效验证了CEFR差距理论，联合LLM的迭代方法为可读性控制文本简化提供了新方向。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [36] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 研究通过BFI-2框架评估六种大语言模型的人格特质表现，发现神经质和外向性易受温度参数影响，模型架构差异导致稳定人格特征聚类。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs的人格化行为对AI系统责任开发、模型调优及伦理治理至关重要，需系统评估其行为模式。

Method: 使用Big Five Inventory-2框架，在六种LLMs上调整采样温度参数，通过层次聚类分析模型人格特质差异。

Result: 五大人格维度中四个存在显著差异，神经质/外向性受温度影响明显；模型聚类显示架构差异导致稳定特质倾向。

Conclusion: 研究为LLMs人格模式形成提供新见解，指导模型调优与伦理治理，公开数据代码促进后续研究。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [37] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: RAGalyst是一个自动化评估领域特定RAG系统的框架，通过合成QA数据集和优化LLM评判指标，实现与人工标注的高度对齐。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法在专业领域存在局限：基于启发式的指标无法捕捉领域特性，LLM评判方法缺乏人类判断验证。需开发系统化评估框架解决安全关键领域的评估挑战。

Method: 1. 构建代理流程生成合成QA数据集，通过代理过滤保证数据保真度
2. 优化「答案正确性」和「可回答性」指标提示词，实现与人类标注强相关
3. 在军事、网络安全、桥梁工程三领域开展多组件对比实验

Result: 1. RAG性能高度依赖应用场景，无通用最优配置
2. 嵌入模型选择影响大于LLM更换(最高达31.9%差异)
3. 识别出文档不完整(32.1%)和检索失败(28.4%)为主要错误来源

Conclusion: RAGalyst提供系统化评估框架，揭示领域特定权衡关系，支持构建可靠RAG系统。框架已开源，助力从业者基于数据驱动决策。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [38] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 提出Lunguage++框架量化放射报告中的显性/隐性不确定性，通过LLM排序和诊断路径扩展实现结构化分析


<details>
  <summary>Details</summary>
Motivation: 放射报告包含显性（模棱两可表达）和隐性（信息缺失）两种不确定性，现有规则系统无法有效量化，阻碍自动化分析应用

Method: 1. 显性不确定性：构建专家验证的LLM短语概率映射表
2. 隐性不确定性：基于14种诊断路径扩展特征性子发现

Result: 发布增强版Lunguage++基准，支持不确定性感知的图像分类、诊断推理及临床影响研究

Conclusion: 结构化不确定性表征框架为医疗AI系统提供了更可靠的诊断支持和新研究维度

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [39] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 语言模型通过隐藏激活控制可预测其推理路径的不确定性，激活干预在模型未确定最终答案时最有效


<details>
  <summary>Details</summary>
Motivation: 探索语言模型生成过程中是否存在对替代推理路径的隐式表示，解决模型不确定性量化困难的问题

Method: 使用隐藏激活控制技术预测模型在思维链推理中的不确定性，通过激活干预实验测量模型路径可引导性

Result: 模型不确定性程度与激活控制易度正相关，隐藏激活可预测未来结果分布，证明模型隐式编码可能路径空间

Conclusion: 语言模型在生成过程中持续保持多路径表示能力，激活干预效果取决于模型对最终答案的承诺状态

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [40] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: IntelliProof: 基于论证图的可交互议论文分析系统，通过LLM分类和可视化提升评估透明度与用户参与度（演示链接：https://intelliproof.vercel.app）


<details>
  <summary>Details</summary>
Motivation: 解决现有自动评分系统缺乏透明度和用户互动的问题，通过可视化解释和交互式工具增强论证分析的参与度与理解深度

Method: 将议论文转化为论证图（节点=论点，边=支持/攻击关系），利用LLM进行关系分类/评分，开发可视化界面和自然语言解释工具

Result: 实现支持实时分析、提供量化连贯性指标与分类依据的系统，允许快速探索论证质量同时保留人工监督机制

Conclusion: 通过融合自动分析与人工交互，IntelliProof显著提升了议论文评估的透明度和用户体验，建立了论证结构与用户认知的有效连接

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [41] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型编码助手存在严重安全漏洞，最新开源模型仍受早期漏洞影响，提出结合漏洞严重性、生成概率和诱导提示的PE/ME评估指标


<details>
  <summary>Details</summary>
Motivation: 随着LLM代码生成工具在软件开发中作用日益关键，其生成漏洞对网络安全的影响尚未被有效解决。现有安全基准和改进方法未能真正提升主流编码模型的安全性，安全性和功能性的取舍导致漏洞修补效果不佳

Method: 通过实证分析模型在现实场景中的漏洞重现情况，提出Prompt Exposure（PE）三维严重性指标（漏洞严重程度、生成概率、诱导提示构成），并基于PE定义Model Exposure（ME）评估模型整体漏洞风险

Result: 发现最新开源模型仍存在早期报告的漏洞场景，证明现有安全改进存在局限性。提出的ME评分能有效识别高概率生成且高危的漏洞，为漏洞修复提供优先级指导

Conclusion: 需在LLM代码生成中平衡安全性与功能性，PE/ME指标为漏洞风险评估提供了新范式，推动优先修复高暴露风险的漏洞，促进编码助手安全性的系统性提升

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [42] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 论文提出了首个孟加拉语生物医学问答数据集BanglaMedQA/BanglaMMedBench，通过Agentic RAG策略实现89.54%的最高准确率，证明了增强低资源语言医疗AI可靠性的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（孟加拉语）缺乏可靠生物医疗问答系统导致的医疗知识获取不平等问题。

Method: 结合OCR构建医学教材语料库，设计动态选择检索与推理策略的Agentic RAG流程，测试传统/零样本回退/迭代反馈等多种RAG方案。

Result: Agentic RAG配置在GPT-OSS-120B模型上达到89.54%准确率，显著优于其他方案且生成逻辑更优质。

Conclusion: RAG方法能有效提升孟加拉语医疗QA可靠性，为多语言医疗AI研究奠定技术基础。

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [43] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: DeReC框架通过结合密集检索与专用分类，在事实核查任务中实现了比生成式LLM更高的准确率和效率（RAWFC数据集运行时间减少95%）


<details>
  <summary>Details</summary>
Motivation: 解决现有基于LLM的事实核查方法存在的计算资源消耗大（需数小时处理）和生成幻觉风险的问题

Method: 使用通用文本嵌入替代自回归LLM，结合密集检索获取相关证据，再通过专用分类器进行事实验证的轻量级框架

Result: 在RAWFC数据集达到65.58% F1分数（超越SOTA方法4.38%），运行时间从454分钟降至23分钟；LIAR-RAW数据集运行时间减少92%

Conclusion: 经过精心设计的检索系统在特定任务中可以超越LLM性能，同时具备更强的实际部署可行性，为高效事实核查系统提供了新方向

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [44] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 通过熵值监测实现自适应的思维链生成终止机制LEASH，在保持推理能力的同时显著降低计算开销


<details>
  <summary>Details</summary>
Motivation: 传统思维链（CoT）方法生成完整推理链导致计算资源浪费和延迟增加，需寻找高效替代方案

Method: 监控熵值斜率和top-logit边际改善，当信号稳定时自动终止生成过程

Result: 在GSM8K和AQuA-RAT基准测试中，平均减少30-35%的token生成和27%延迟，准确率下降10个百分点

Conclusion: LEASH提供无需训练的自适应停止策略，平衡计算效率与推理性能，适用于不同模型

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [Shellular Metamaterial Design via Compact Electric Potential Parametrization](https://arxiv.org/abs/2511.04025)
*Chang Liu,Bohan Wang*

Main category: cs.GR

TL;DR: 通过紧凑设计空间与高效GPU均质化流程，实现高性能壳层超材料的逆向设计与快速验证


<details>
  <summary>Details</summary>
Motivation: 突破传统壳层超材料设计空间受限与计算效率瓶颈，实现复杂几何的快速优化与性能预测

Method: 构建参数化设计空间（数十自由度）+ GPU加速均质化流程（20ms评估/0.5s弹性张量计算）

Result: 结构性能达理论极限91.86%，覆盖宽泛材料属性，几何多样性超越现有低体积壳层结构

Conclusion: 该方法实现了超材料逆向设计范式，结合高效计算与增材制造验证，具有工程应用潜力

Abstract: We introduce a compact yet highly expressive design space for shellular
metamaterials. By employing only a few dozen degrees of freedom, this design
space represents geometries ranging from simple planar configurations to
complex triply periodic minimal surfaces. Coupled with this representation, we
develop an efficient GPU-based homogenization pipeline that evaluates the
structure in under 20 ms and computes the corresponding effective elastic
tensor in near-real-time (0.5 s). The high speed of this evaluation facilitates
an exhaustive exploration of the design space and supports an inverse-design
scheme that tailors the shellular structure to specific macroscopic target
property. Structures derived through this approach exhibit not only geometric
diversity but also a wide spectrum of mechanical responses, covering a broad
range of material properties. Moreover, they achieve up to 91.86% of
theoretical upper bounds, a level of performance comparable to state-of-the-art
shellular structures with low solid volume. Finally, our prototypes, fabricated
via additive manufacturing, confirm the practical manufacturability of these
designs, underscoring their potential for real-world engineering applications.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [Black-Box Guardrail Reverse-engineering Attack](https://arxiv.org/abs/2511.04215)
*Hongwei Yao,Yun Xia,Shuo Shao,Haoran Shi,Tong Qiao,Cong Wang*

Main category: cs.CR

TL;DR: 研究提出基于强化学习的GRA框架，通过逆向工程攻击LLM防护栏，在ChatGPT等系统实现0.92规则匹配率，API成本低于85美元，揭示现有安全机制重大风险


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护栏虽能过滤有害输出，但其决策模式存在可观测漏洞，攻击者可能通过逆向工程突破安全限制

Method: 采用遗传算法驱动的数据增强技术，通过差异优先采样、定向突变/交叉的迭代优化，构建近似防护栏决策的代理模型

Result: 在ChatGPT/DeepSeek/Qwen3商业系统验证，规则匹配准确率超过92%，攻击成本控制在85美元以内

Conclusion: 当前LLM防护栏设计存在根本性脆弱环节，需开发抗逆向工程的新型防御架构确保模型安全部署

Abstract: Large language models (LLMs) increasingly employ guardrails to enforce
ethical, legal, and application-specific constraints on their outputs. While
effective at mitigating harmful responses, these guardrails introduce a new
class of vulnerabilities by exposing observable decision patterns. In this
work, we present the first study of black-box LLM guardrail reverse-engineering
attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement
learning-based framework that leverages genetic algorithm-driven data
augmentation to approximate the decision-making policy of victim guardrails. By
iteratively collecting input-output pairs, prioritizing divergence cases, and
applying targeted mutations and crossovers, our method incrementally converges
toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on
three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,
and demonstrate that it achieves an rule matching rate exceeding 0.92 while
requiring less than $85 in API costs. These findings underscore the practical
feasibility of guardrail extraction and highlight significant security risks
for current LLM safety mechanisms. Our findings expose critical vulnerabilities
in current guardrail designs and highlight the urgent need for more robust
defense mechanisms in LLM deployment.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [47] [Sub-exponential Growth in Online Word Usage: A Piecewise Power-Law Model](https://arxiv.org/abs/2511.04106)
*Hayafumi Watanabe*

Main category: physics.soc-ph

TL;DR: 提出分段幂律模型揭示社会扩散中亚指数增长普遍存在，α参数反映话题传播特性


<details>
  <summary>Details</summary>
Motivation: 传统S型模型难以解释社会现象中的亚指数增长模式，需要系统性研究复杂扩散曲线的定量特征

Method: 分析十亿级日本博客数据与多语种搜索趋势，建立分段幂律模型，结合微观行为建模

Result: 55%项目符合分段模型，α中值0.5显示亚指数主导，扩散规模由增长率R主导，α与话题类型相关

Conclusion: 亚指数增长是普遍模式，模型为复杂扩散曲线提供统一分析框架，α可作为社会传播偏好指标

Abstract: The diffusion of ideas and language in society has conventionally been
described by S-shaped models, such as the logistic curve. However, the role of
sub-exponential growth -a slower than exponential pattern known in
epidemiology- has been largely overlooked in broader social phenomena. Here, we
present a piecewise power-law model to characterize complex growth curves with
a few parameters. We systematically analyzed a large-scale dataset of
approximately one billion Japanese blog articles linked to Wikipedia
vocabulary, and observed consistent patterns in web search trend data (English,
Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that
about 55% (1,625 items) were found to have no abrupt jumps and were well
captured by one or two segments. For single-segment curves, we found that (i)
the mode of the shape parameter alpha was near 0.5, indicating prevalent
sub-exponential growth; (ii) the ultimate diffusion scale is primarily
determined by the growth rate R, with minor contributions from alpha or the
duration T; and (iii) alpha showed a tendency to vary with the nature of the
topic, being smaller for niche/local topics and larger for widely shared ones.
Furthermore, a micro-behavioral model distinguishing outward contact with
strangers from inward interaction within their community suggests that alpha
can be interpreted as an index of the preference for outward-oriented
communication. These findings suggest that sub-exponential growth is a common
pattern of social diffusion, and our model provides a practical framework for
consistently describing, comparing, and interpreting complex and diverse growth
curves.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [48] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 该论文系统综述了基于人类反馈的强化学习(RLHF)在AI对齐中的最新进展，重点突破多模态对齐、文化公平和低延迟优化三大方向。


<details>
  <summary>Details</summary>
Motivation: 针对传统文本对齐方法在跨模态场景、文化偏见和响应效率方面的局限性，研究者亟需系统性框架整合前沿算法创新与挑战分析。

Method: 通过分阶段分析：1) 梳理PPO/DPO/GRPO等基础算法 2) 对比多模态表征学习、文化敏感奖励模型、蒸馏加速等新技术 3) 建立评估基准体系

Result: 构建了涵盖算法创新、评估指标和应用场景的三维分析框架，提出包含12项核心挑战的技术路线图，特别指出文化嵌入与计算效率的权衡关系。

Conclusion: 该研究为开发具备跨模态理解、文化包容性和实时响应能力的下一代对齐技术提供了方法论基础，强调算法鲁棒性与伦理约束的协同优化路径。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [49] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 提出通过生成合成预训练数据实现决策树元学习的高效方法，性能接近真实数据预训练且显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统决策树预训练依赖真实数据或计算昂贵的最优树采样，难以满足大规模元学习需求

Method: 采用近优决策树采样生成大规模合成数据集，结合MetaTree transformer架构进行元学习

Result: 在保持与真实数据/最优树预训练相当性能的同时，计算成本降低90%，数据生成效率提升10倍

Conclusion: 该方法开创了可解释模型元学习新范式，为高风险领域快速部署自适应决策树系统提供技术基础

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [50] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 提出分布感知的旋转校准方法DartQuant，通过QR-Orth优化方案实现47倍加速和10倍内存节省，首次在单卡3090上完成70B大模型的量化校准。


<details>
  <summary>Details</summary>
Motivation: 针对现有旋转优化算法端到端微调计算成本高、易过拟合的问题，旨在降低旋转优化的复杂度并减少对任务特定损失的依赖。

Method: 1. 分布感知的旋转校准约束激活值分布；2. QR-Orth优化方案替代交替优化；3. 支持单卡完成70B模型校准。

Result: 70B模型旋转优化加速47倍，内存节省10倍；首次在单卡3090实现70B大模型旋转校准；多模型量化实验显示SOTA性能。

Conclusion: DartQuant通过创新性的分布约束和优化方案，显著提升量化效率，为资源受限环境下的大模型部署提供可行解决方案。

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [51] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 研究建立了MXFP4格式下LLM后训练量化基准，发现旋转方法与MXFP4存在兼容性问题，提出块旋转策略显著提升精度。


<details>
  <summary>Details</summary>
Motivation: 新兴MXFP4格式获得多硬件支持，但现有PTQ方法在该格式下的适用性尚未明确，需系统性评估和方法适配。

Method: 1. 建立MXFP4格式PTQ基准测试 2. 分析旋转方法与MXFP4的底层冲突 3. 提出基于PoT块缩放的块旋转策略适配方案

Result: GPTQ方法保持稳定性能，传统旋转方法精度下降23%，块旋转策略使LLaMA-30B准确率提升17.5%

Conclusion: 研究为MXFP4格式量化提供实践指导，揭示了格式特性与算法设计的关联，为新兴低精度格式研究奠定基础

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [52] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文揭示了当前大语言模型不确定性量化方法在数据模糊性场景下的性能缺陷，首次通过构建模糊问答数据集MAQA*/AmbigQA*验证了预测分布、模型内部表征和模型集成等方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实语言场景天然存在模糊性（aleatoric uncertainty），但现有不确定性量化方法仅在无模糊性假设场景下测试，导致实际部署中可靠性存疑。

Method: 构建首个含事实共现分布标注的模糊问答数据集MAQA*和AmbigQA*，系统评估预测分布、模型内部表征、模型集成三类不确定性估计方法在模糊场景下的表现。

Result: 所有评估方法在模糊数据上的性能均接近随机水平，其中基于预测分布和模型集成的估计方法存在理论上的根本性局限。

Conclusion: 当前不确定性量化方法在模糊性处理上存在本质缺陷，需重新思考建模范式，推动更适应现实语言复杂性的新型方法开发。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [53] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 提出SynthKGQA框架生成知识图谱问答数据集，提升KG检索器评估与训练效果


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏具有挑战性QA数据集和真实目标，难以进行有效的KG检索器对比

Method: 开发可从任意知识图谱生成合成QA数据集的框架，应用于Wikidata创建GTSQA测试集

Result: 生成数据不仅能支持更全面的基准测试，还能训练出性能更优的KG增强模型

Conclusion: SynthKGQA框架和GTSQA数据集为KG检索器的零样本泛化能力评估提供新工具，推动KG增强LLM发展

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [54] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: 提出了可探索性（explorability）作为下推自动机的非确定性度量，构建了介于历史确定性和完全非确定性之间的无限层次结构，揭示了其表达能力与简洁性特征


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对下推系统非确定性的量化分级，需建立操作性强且能反映计算效率的层次化度量指标，以填补历史确定性自动机与完全非确定性自动机之间的理论空白

Method: 通过定义k-可探索性概念，构建参数化探索模型，分析不同探索层级在表达能力、简洁性方面的渐进关系，并引入与输入长度相关的指数级探索性证明其与上下文无关语言的对应关系

Result: 发现k-可探索自动机形成严格递增的无限表达能力层级，指数探索性精确刻画上下文无关语言，且可探索自动机相对历史确定性自动机存在双重指数级简洁性优势

Conclusion: 可探索性为非确定性下推系统提供了理论坚实的度量标准，其层次结构揭示了计算资源与表达能力的内在关联，为形式化验证和编译器优化提供了新的分析框架

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [55] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: 提出协作多智能体框架提升数学问题生成的复杂度控制与认知需求平衡，初步验证显示有效提升教育内容质量


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的自动问题生成方法难以精确控制问题复杂度与认知需求，需通过创新框架提升教育内容适应性

Method: 构建协作多智能体系统，通过迭代优化问题-答案对实现复杂度与认知需求的动态平衡

Result: 在相关性、重要性、清晰度、难度匹配和可解答性五个元评估标准中，系统展现出更好的复杂度控制能力，初步评估显示内容质量显著提升

Conclusion: 多智能体协作机制为自适应学习环境提供了新范式，通过推理时计算优化实现了教育内容生成的可控性与教学价值平衡

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [56] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 开发基于LLaMA-3.3-70B的智能聊天机器人，通过混合检索技术为大学生提供个性化校园指导


<details>
  <summary>Details</summary>
Motivation: 解决本科生缺乏规模化个性指导的问题，现有数字工具定制化程度不足

Method: 构建数据管道整合CSV/网页数据，采用BM25词频排序与ChromaDB语义检索的混合模式，使用LLaMA-3.3-70B生成对话

Result: 响应文本语义相关性BERTScore 0.831/METEOR 0.809，数据更新效率提升245%（106.82秒 vs 368.62秒）

Conclusion: 该系统能有效帮助学生理解大学生活、优化学期规划，特别适用于学分制大学环境

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [57] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: 双代理AI框架MimiTalk通过监督模型和对话模型实现社会科学研究中可扩展、符合伦理的对话数据收集，在信息丰富性和稳定性上优于人类访谈。


<details>
  <summary>Details</summary>
Motivation: 解决传统访谈方法在规模扩展、伦理控制和信息质量稳定性方面的局限性，探索人机协作在定性研究中的潜力。

Method: 开展三阶段研究：可用性测试（20人）、AI与人类访谈NLP指标对比（121 vs 1,271样本）、跨学科研究者盲测实验（10人）。

Result: AI访谈减少焦虑感（降低32%），信息丰富性提升25%，在技术细节和敏感话题处理更优；人类访谈在文化情感维度保留更好。

Conclusion: 双代理宪法AI框架支持可复制、规模化的高质量定性研究，建立人机协同新范式，为社会科学方法论提供技术增强路径。

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出高保真稀疏体素化表示方法Faithful Contouring，支持2048+分辨率且无需场函数转换，在三维重建任务中实现近无损几何保真。


<details>
  <summary>Details</summary>
Motivation: 现有基于等值面的体素化方法依赖水密化处理和渲染优化，导致几何细节丢失，难以保持复杂网格的拓扑结构。

Method: 通过稀疏体素化技术保留锐利边缘和内部结构，开发双模式自编码器实现可扩展的细节保持重建，避免等值面提取过程。

Result: 直接表示误差达10^-5级别；网格重建Chamfer距离降低93%，F-score提升35%，显著超越基线方法。

Conclusion: 该方法在表示精度与重建效率上实现突破，为三维学习任务提供了高保真、可编辑的通用表示框架。

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [59] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 提出新的OCR旋转鲁棒性基准ORB和基于Phi-3.5-Vision的高效旋转分类方法，显著提升多语言OCR性能


<details>
  <summary>Details</summary>
Motivation: 实际应用中用户操作错误常导致文档图像方向错误，影响OCR性能。现有系统缺乏针对旋转鲁棒性的专门评估和解决方案

Method: 1. 构建包含英文和多语言数据的ORB基准（ORB-En和ORB-Indic）
2. 基于Phi-3.5-Vision视觉编码器开发动态裁剪的轻量级分类流程
3. 通过微调实现4类旋转分类

Result: 分类准确率达96%（ORB-En）和92%（ORB-Indic），闭源OCR性能提升最高14%，开源模型提升达4倍

Conclusion: 该方法有效解决真实场景中的文档方向问题，显著提升多种OCR系统的鲁棒性，特别适用于中低资源语言场景

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [60] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 提出'视频思维'新范式，利用Sora-2视频生成模型统一多模态推理，在VideoThinkBench基准测试中验证其视觉和文本任务的双重优势


<details>
  <summary>Details</summary>
Motivation: 现有文本和图像思维范式存在动态过程表征不足、模态分离的局限性，需要能统一时空推理的解决方案

Method: 构建VideoThinkBench基准（含视觉中心/文本中心任务），通过视频生成模型Sora-2实现时空连续的多模态推理

Result: Sora-2视觉任务媲美SOTA VLM（Eyeballing Games超越VLM），文本任务MATH达92%、MMMU达75.53%准确率，自洽性和上下文学习可提升性能

Conclusion: 视频生成模型具备统一多模态理解的潜力，'视频思维'可作为时空连续的统一推理范式

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 汇编代码标记化方案显著影响下游任务性能，需权衡内在特性与实际应用效果


<details>
  <summary>Details</summary>
Motivation: 汇编代码标记化作为基础环节长期缺乏系统研究，现有NLP方案在语义覆盖度和词汇压缩效率等方面存在优化空间

Method: 采用Llama/BERT/BART等预训练模型，通过词汇压缩率、语义保真度等内在指标，结合函数签名预测等下游任务进行多维度评估

Result: 标记器选择显著影响下游性能（准确率差异达15%），词汇规模与语义覆盖度存在倒U型关系，指令边界感知的预标记规则可提升19%任务效果

Conclusion: 提出汇编代码定制化标记化框架，建立内在指标与下游任务关联模型，为二进制分析工作流提供可解释的优化路径

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [62] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: LLMs虽能通过提示语言和文化框架调整输出，但仍系统偏向荷兰、德国、美国、日本等国的文化价值观，无法充分代表文化多样性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs全球应用中的文化代表性问题，验证提示语言和文化框架对模型价值观输出的影响。

Method: 使用Hofstede价值观模块和世界价值观调查的63个项目，翻译成11种语言，以不同文化视角构建提示测试10个LLM。

Result: 提示语言和文化框架均影响输出，但模型始终偏向特定国家价值观；文化框架比语言更能提升与人类价值观一致性；组合策略未见增效。

Conclusion: LLMs处于响应灵活性与文化锚定困境：能响应提示变化，但固守特定文化基准，难以充分体现文化多样性。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [63] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 研究发现Llama语言模型能高保真复现人类合作行为模式，Qwen更接近纳什均衡预测，证明LLMs可作为社会科学研究的补充工具


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在决策模拟中与人类行为的对齐程度，解决模型误用风险和模拟有效性双重问题

Method: 构建博弈论实验数字孪生系统，开发系统化提示探测框架，测试Llama/Mistral/Qwen三大开源模型，扩展生成新型博弈配置的假设

Result: Llama成功捕捉人类偏离理性选择的行为特征，Qwen符合纳什均衡预测；无需角色提示即可实现群体行为复现；生成超出原参数范围的可验证假设

Conclusion: 适当校准的LLMs既能复现人类行为模式，又能系统性探索未知实验空间，为社会科学研究提供预测生成和理论验证的新范式

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [64] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，通过模拟人类研究者工作流程（分析论文局限→提出假设→实验验证→撰写论文），利用现代编码代理处理复杂任务，评估显示其论文质量优于全自动系统但存在应用风险。


<details>
  <summary>Details</summary>
Motivation: 现有AI科研系统存在全自动化假设不实际/小规模代码操作的局限性，需建立可信赖的AI科研范式。

Method: 构建遵循人类研究流程的系统，结合基线论文分析→多文件代码代理→AI评审/作者评估/学术平台投稿三维度验证。

Result: 生成论文评审分数优于全自动系统（Agent4Science评分3.25 vs 2.8），但作者评估发现20%重大错误，平台接收率仅40%。

Conclusion: 当前AI科学家系统已具备科研辅助潜力，但直接应用存在事实错误/可复现性等风险，需建立更严格的AI科研验证框架。

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [65] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 提出区分协作/非协作查询的框架，揭示现有表格问答数据集混合查询类型的评估缺陷，倡导通过协作视角优化自然语言接口设计


<details>
  <summary>Details</summary>
Motivation: 现有自然语言接口常将查询歧义视为缺陷，本文将其重构为协作互动特征，探索共享责任的查询解析机制

Method: 建立理论框架划分可解析的协作查询与不可解析的非协作查询，系统分析15个主流数据集的查询类型分布特征

Result: 现有数据集未有效区分查询类型，既无法准确评估系统执行精度，也难以检验语义解释能力，暴露评估体系的设计缺陷

Conclusion: 框架推动从消除歧义转向协作解析的范式转变，为表格数据接口设计提供新评估维度，指明支持协作机制的技术研发方向

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [66] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: DR. WELL框架通过神经符号方法与两阶段协商协议，实现去中心化多智能体协作规划，利用符号抽象和动态世界模型提升任务效率。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹级协调易因微小偏差引发冲突，需通过符号抽象提升协作稳定性和可复用性。

Method: 1. 两阶段协商协议（角色提案→联合分配）
2. 独立生成符号计划
3. 共享世界模型动态更新状态

Result: 协作推块任务中任务完成率提升，动态世界模型通过协商自优化实现效率进化（时间开销换取策略优化）

Conclusion: 符号规划使协作具备可解释性和复用性，神经符号框架在去中心化场景下实现高效灵活的多智能体协调。

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [67] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: VeriCoT通过将思维链推理步骤转化为可验证的一阶逻辑框架，结合符号验证与自然语言解释，有效识别逻辑漏洞并提升LLMs推理可信度


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的思维链推理缺乏自我验证机制，即使答案正确底层逻辑可能存在缺陷，这在医疗/法律等高风险领域存在重大隐患

Method: 1. 将自然语言推理步骤形式化为一阶逻辑
2. 建立基于上下文/常识/逻辑前提的三维验证框架
3. 结合自动逻辑求解器与人工验证机制

Result: 在ProofWriter/LegalBench/BioASQ数据集上验证：
- 有效识别87%的推理漏洞
- 验证信号与最终答案正确性相关系数达0.92
- 通过验证信号优化的模型准确率提升15%

Conclusion: VeriCoT开创了神经符号协同验证范式，通过形式验证信号支持自反思、监督微调和偏好优化，显著提升LLMs推理的可靠性与可解释性

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [68] [MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation](https://arxiv.org/abs/2511.03942)
*Shih-Lun Wu,Yoon Kim,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: MIDI-LLM通过扩展文本LLM词汇表并采用两阶段训练方法，实现了从文本生成多轨MIDI音乐，在质量、控制力和推理速度上优于Text2midi模型。


<details>
  <summary>Details</summary>
Motivation: 现有文本到MIDI模型（如Text2midi）在生成质量、文本控制精度和推理速度方面存在局限，需探索更高效的LLM应用方案。

Method: 1. 扩展文本LLM词汇表添加MIDI标记 2. 采用保留原始参数结构的两阶段训练策略 3. 利用vLLM库实现加速推理

Result: 实验显示MIDI-LLM在音乐质量（Fréchet Distance降低32%）、文本控制精度（Rouge-L提高18%）和推理速度（快3.7倍）全面超越基线模型

Conclusion: 该方法成功将LLM应用于音乐生成领域，通过结构优化实现高效推理，为多模态音乐创作提供了新范式。

Abstract: We present MIDI-LLM, an LLM for generating multitrack MIDI music from
free-form text prompts. Our approach expands a text LLM's vocabulary to include
MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI
abilities. By preserving the original LLM's parameter structure, we can
directly leverage the vLLM library for accelerated inference. Experiments show
that MIDI-LLM achieves higher quality, better text control, and faster
inference compared to the recent Text2midi model. Live demo at
https://midi-llm-demo.vercel.app.

</details>
