<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.MA](#cs.MA) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shoot First, Ask Questions Later? Building Rational Agents that Explore and Act Like People](https://arxiv.org/abs/2510.20886)
*Gabriel Grand,Valerio Pepe,Jacob Andreas,Joshua B. Tenenbaum*

Main category: cs.CL

TL;DR: 研究开发了协作战舰任务评估语言模型智能体的理性决策能力，提出基于贝叶斯实验设计的蒙特卡洛推理策略，显著提升信息搜索效率，在多个任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 高风险AI应用需要理性信息搜索能力，但现有语言模型代理在上下文理解、问题生成和行动选择方面存在不足，需系统性评估和改进方法。

Method: 设计战略对话任务Collaborative Battleship对比人类与LM表现；提出贝叶斯实验设计框架下的蒙特卡洛推理方法，优化Spotter的回答准确性和Captain的信息增益。

Result: 方法使Spotter准确率提升14.7%，Captain信息增益达噪声上限的94.2%；Llama-4-Scout胜率从8%提升至82%，成本仅为GPT-5的1%。在Guess Who任务中准确率提升28.3-42.4%。

Conclusion: 提出的贝叶斯实验设计框架有效提升了语言模型代理的理性决策能力，展示了跨任务通用性，为构建高效信息搜索智能体提供了新范式。

Abstract: Many high-stakes applications of AI require forming data-driven hypotheses
and making targeted guesses; e.g., in scientific and diagnostic settings. Given
limited resources, to what extent do agents based on language models (LMs) act
rationally? We develop methods to benchmark and enhance agentic
information-seeking, drawing on insights from human behavior. First, we
introduce a strategic decision-oriented dialogue task called Collaborative
Battleship, in which a partially-informed Captain must balance exploration
(asking questions) and action (taking shots), while a fully-informed Spotter
must provide accurate answers under an information bottleneck. Compared to
human players (N=42), we find that LM agents struggle to ground answers in
context, generate informative questions, and select high-value actions. Next,
to address these gaps, we develop novel Monte Carlo inference strategies for
LMs based on principles from Bayesian Experimental Design (BED). For Spotter
agents, our approach boosts accuracy by up to 14.7% absolute over LM-only
baselines; for Captain agents, it raises expected information gain (EIG) by up
to 0.227 bits (94.2% of the achievable noise ceiling). Combined, these
components yield sharper targeting (+0.303-0.374 F1), and enable weaker LMs,
such as Llama-4-Scout, to outperform both humans (8% -> 82% win rate) and
frontier models (0% -> 67% win rate vs. GPT-5) at ~1% of GPT-5's cost. We
replicate these findings on Guess Who? where our methods significantly boost
accuracy (+28.3-42.4 p.p.), demonstrating their general applicability for
building rational information-seeking agents.

</details>


### [2] [Code-enabled language models can outperform reasoning models on diverse tasks](https://arxiv.org/abs/2510.20909)
*Cedegao E. Zhang,Cédric Colas,Gabriel Poesia,Joshua B. Tenenbaum,Jacob Andreas*

Main category: cs.CL

TL;DR: CodeAdapt方法通过结合CodeAct框架（多步骤自然语言推理与代码执行交替）和少量样本的上下文学习，使得未微调的语言模型在指令遵循、数学推理等任务上超越专用推理模型，同时显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习训练的推理模型（RMs）虽然表现优异，但存在训练成本高、运行速度慢的问题。研究旨在探索未经微调的标准指令语言模型（LMs）是否具备与之匹敌的推理能力。

Method: 提出CodeAdapt方法：1）CodeAct框架实现自然语言推理与代码执行的多步骤交替；2）仅需5个训练样本的上下文学习引导模型生成代码增强的推理路径。

Result: 在8个任务中，CodeAdapt使3个LMs平均超越对应RMs达22.9%，token效率提升10-81%。四个模型在6个任务上平均表现提升35.7%。推理轨迹展现出丰富的多策略问题解决能力。

Conclusion: 代码增强的LMs具备：1）领域通用的鲁棒推理能力；2）认知基础扎实的架构特性，为后续强化学习提供了理想的预训练基础。

Abstract: Reasoning models (RMs), language models (LMs) trained with reinforcement
learning to produce long-form natural language reasoning, have been remarkably
successful, but they still require large amounts of computation and data to
train, and can be slow and expensive to run. In this paper, we show that
standard instruct LMs can already be elicited to be strong reasoners at a level
comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs
R1) without finetuning, across diverse domains from instruction following and
creative generation to mathematical reasoning. This is achieved by CodeAdapt,
our simple recipe that combines the CodeAct framework, where LMs interleave
natural language reasoning with code execution in a multi-step fashion, with
few-shot bootstrap in-context learning from as few as five training problems.
Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables
three LMs to outperform the corresponding RMs on average over eight tasks (up
to 22.9%) while being 10-81% more token efficient, and delivers superior
performance on six tasks when averaged over the four models (up to 35.7%).
Furthermore, the code-augmented reasoning traces display rich and varied
problem-solving strategies. Our findings support that (1) CodeAdapt-style
learning and reasoning may be robust and domain general and (2) code-enabled
LMs are cognitively grounded and powerful systems, potentially providing a
strong foundation for in-weight reinforcement learning.

</details>


### [3] [FicSim: A Dataset for Multi-Faceted Semantic Similarity in Long-Form Fiction](https://arxiv.org/abs/2510.20926)
*Natasha Johnson,Amanda Bertsch,Maria-Emil Deal,Emma Strubell*

Main category: cs.CL

TL;DR: 构建FICSIM数据集评估文本嵌入模型在文学研究任务中的有效性，发现现有模型更关注表层特征而非语义特征


<details>
  <summary>Details</summary>
Motivation: 现有长文本评估存在标注成本高、数据污染风险，且传统嵌入相似性数据集侧重短文本粗粒度评估，无法满足计算文学研究需求

Method: 收集最新长篇小说构建FICSIM数据集，基于作者元数据制定12维度相似性评分体系，经数字人文学者验证，强调作者持续知情同意机制

Result: 多种嵌入模型在语义类别识别上表现不足，倾向于依赖表层语言特征而非文学研究所需的深层语义特征

Conclusion: FICSIM填补文学计算评估空白，揭示模型改进方向，通过作者授权机制保障数据伦理，推动负责任AI在人文领域应用

Abstract: As language models become capable of processing increasingly long and complex
texts, there has been growing interest in their application within
computational literary studies. However, evaluating the usefulness of these
models for such tasks remains challenging due to the cost of fine-grained
annotation for long-form texts and the data contamination concerns inherent in
using public-domain literature. Current embedding similarity datasets are not
suitable for evaluating literary-domain tasks because of a focus on
coarse-grained similarity and primarily on very short text. We assemble and
release FICSIM, a dataset of long-form, recently written fiction, including
scores along 12 axes of similarity informed by author-produced metadata and
validated by digital humanities scholars. We evaluate a suite of embedding
models on this task, demonstrating a tendency across models to focus on
surface-level features over semantic categories that would be useful for
computational literary studies tasks. Throughout our data-collection process,
we prioritize author agency and rely on continual, informed author consent.

</details>


### [4] [Do LLMs Truly Understand When a Precedent Is Overruled?](https://arxiv.org/abs/2510.20941)
*Li Zhang,Jaromir Savelka,Kevin Ashley*

Main category: cs.CL

TL;DR: 评估大语言模型在长法律文档理解中的表现，发现存在时代敏感性、浅层推理和上下文依赖的推理失败三大局限，并提出符合实际法律推理复杂性的新基准测试


<details>
  <summary>Details</summary>
Motivation: 现有法律文档理解评估主要依赖简化的人工任务，无法反映法律实务的真实复杂性。司法判例中的推翻关系（overruling relationships）可作为重要测试基准，模拟法律从业者的实际工作场景

Method: 使用236组美国最高法院案例对数据集，评估当前最先进的大语言模型在识别判例推翻关系任务中的表现

Result: 1. 模型对历史案例表现显著差于现代案例（时代敏感性）
2. 依赖浅层逻辑启发而非深层法律理解（浅层推理）
3. 在复杂开放任务中产生时间逻辑错误（上下文依赖的推理失败）

Conclusion: 提出了首个符合现实法律推理复杂性和重要性的长上下文评估基准，揭示了当前大语言模型在法律文档理解中的根本性局限

Abstract: Large language models (LLMs) with extended context windows show promise for
complex legal reasoning tasks, yet their ability to understand long legal
documents remains insufficiently evaluated. Developing long-context benchmarks
that capture realistic, high-stakes tasks remains a significant challenge in
the field, as most existing evaluations rely on simplified synthetic tasks that
fail to represent the complexity of real-world document understanding.
Overruling relationships are foundational to common-law doctrine and commonly
found in judicial opinions. They provide a focused and important testbed for
long-document legal understanding that closely resembles what legal
professionals actually do. We present an assessment of state-of-the-art LLMs on
identifying overruling relationships from U.S. Supreme Court cases using a
dataset of 236 case pairs. Our evaluation reveals three critical limitations:
(1) era sensitivity -- the models show degraded performance on historical cases
compared to modern ones, revealing fundamental temporal bias in their training;
(2) shallow reasoning -- models rely on shallow logical heuristics rather than
deep legal comprehension; and (3) context-dependent reasoning failures --
models produce temporally impossible relationships in complex open-ended tasks
despite maintaining basic temporal awareness in simple contexts. Our work
contributes a benchmark that addresses the critical gap in realistic
long-context evaluation, providing an environment that mirrors the complexity
and stakes of actual legal reasoning tasks.

</details>


### [5] [Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting](https://arxiv.org/abs/2510.20957)
*Josh McGiff,Khanh-Tung Tran,William Mulcahy,Dáibhidh Ó Luinín,Jake Dalzell,Róisín Ní Bhroin,Adam Burke,Barry O'Sullivan,Hoang D. Nguyen,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 首个爱尔兰语语法能力评估框架Irish-BLiMP的构建及测试，发现人类语法准确率显著优于所有语言模型（平均高16.6%），开源/闭源模型差距达18.1%。


<details>
  <summary>Details</summary>
Motivation: 针对爱尔兰语作为濒危低资源语言缺乏系统性语法评估工具的问题，建立标准化评估框架以促进语言理解研究。

Method: 人工构建1020个最小对比对（覆盖11个语法特征），团队由爱尔兰语母语者审核，对比测试现有LLMs与人类参与者的语法知识。

Result: 人类平均准确率90.1%（最高模型gpt-5仅73.5%），人类与模型在不同语法难点上表现分化，开源模型表现显著落后闭源模型18.1%。

Conclusion: Irish-BLiMP为低资源语言模型的语法能力评估提供标准化基准，揭示LLMs与人类语言认知的本质差异，推动濒危语言处理研究。

Abstract: We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), the
first dataset and framework designed for fine-grained evaluation of linguistic
competence in the Irish language, an endangered language. Drawing on a variety
of linguistic literature and grammar reference works, we manually constructed
and reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,
through a team of fluent Irish speakers. We evaluate both existing Large
Language Models (LLMs) and fluent human participants on their syntactic
knowledge of Irish. Our findings show that humans outperform all models across
all linguistic features, achieving 16.6% higher accuracy on average. Moreover,
a substantial performance gap of 18.1% persists between open- and closed-source
LLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracy
compared to 90.1% by human. Interestingly, human participants and models
struggle on different aspects of Irish grammar, thus highlighting a difference
in representation learned by the models. Overall, Irish-BLiMP provides the
first systematic framework for evaluating the grammatical competence of LLMs in
Irish and offers a valuable benchmark for advancing research on linguistic
understanding in low-resource languages.

</details>


### [6] [Can Confidence Estimates Decide When Chain-of-thought is Necessary for Llms?](https://arxiv.org/abs/2510.21007)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 提出置信度门控的CoT方法，通过无训练置信度估计动态控制推理链触发，在减少冗余计算的同时保持模型性能，但实际部署效果受数据集和模型差异影响较大


<details>
  <summary>Details</summary>
Motivation: 传统CoT提示在提升复杂任务性能时会显著增加计算成本，而现有模型缺乏智能触发CoT的有效机制，导致资源浪费和性能不稳定

Method: 提出confidence-gated CoT框架，系统评估四种无训练置信度估计方法（包括答案概率、语义一致性等指标）作为CoT触发机制

Result: 实验表明现有置信度指标平均可减少34%冗余CoT，但最佳指标选择依赖具体任务和模型（GPT-OSS与Qwen3表现差异达28%）

Conclusion: 当前无训练置信度方法具备实用潜力但可靠性不足，需开发更鲁棒的自适应门控机制以实现CoT的精准触发

Abstract: Chain-of-thought (CoT) prompting has emerged as a common technique for
enhancing the reasoning abilities of large language models (LLMs). While
extended reasoning can boost accuracy on complex tasks, it is often unnecessary
and substantially increases token usage, limiting the practicality of reasoning
models in many scenarios. Recent models, such as GPT-OSS and Qwen3, expose
controls that enable users to adjust the length of CoT or determine whether it
is used at all. Yet, it remains unclear when CoT should be used: on some tasks
it improves performance, while on others it provides little benefit or even
harms performance. We address this challenge with confidence-gated CoT, where a
model invokes reasoning only when confidence in its direct answer is low. To
this end, we present the first systematic study of training-free confidence
estimation methods for CoT gating. Specifically, we evaluate four training-free
confidence estimation methods and compare them to a random baseline and an
oracle that always knows when CoT is needed. Through extensive experiments, we
show that existing training-free confidence measures can reduce redundant CoT
and outperform randomly invoked CoT. However, the utility of individual
confidence measures is inconsistent, varying with both the dataset and the
model, underscoring the difficulty of deploying confidence-gated CoT in
practice. By analysing both strengths and failure modes, our study highlights
the potential and limitations of current methods and paves the way toward more
reliable adaptive gating of CoT.

</details>


### [7] [Input Matters: Evaluating Input Structure's Impact on LLM Summaries of Sports Play-by-Play](https://arxiv.org/abs/2510.21034)
*Barkavi Sundararajan,Somayajulu Sripada,Ehud Reiter*

Main category: cs.CL

TL;DR: 输入结构显著影响LLM生成体育报道的事实错误率，JSON格式效果最佳（Llama错误率降69%，Qwen降65%）


<details>
  <summary>Details</summary>
Motivation: 解决LLM在体育报道等准确性关键领域可能生成不忠实内容的问题，量化输入结构对事实错误的影响

Method: 使用Llama-3.1-70B和Qwen2.5-72B模型，通过三种输入格式（行结构/JSON/非结构化）生成180场NBA比赛摘要，人工标注3312个错误，采用重复测量方差分析和Tukey HSD检验

Result: 结构化输入显著降低错误率：JSON格式比非结构化减少65%-69%错误，行结构减少51%-54%。方差分析显示输入结构解释80%以上误差变异，事后检验证实各格式差异显著

Conclusion: 结构化输入（特别是JSON）能有效减少LLM事实错误，建议在数据敏感场景优先采用结构化输入方案

Abstract: A major concern when deploying LLMs in accuracy-critical domains such as
sports reporting is that the generated text may not faithfully reflect the
input data. We quantify how input structure affects hallucinations and other
factual errors in LLM-generated summaries of NBA play-by-play data, across
three formats: row-structured, JSON and unstructured. We manually annotated
3,312 factual errors across 180 game summaries produced by two models,
Llama-3.1-70B and Qwen2.5-72B. Input structure has a strong effect: JSON input
reduces error rates by 69% for Llama and 65% for Qwen compared to unstructured
input, while row-structured input reduces errors by 54% for Llama and 51% for
Qwen. A two-way repeated measures ANOVA shows that input structure accounts for
over 80% of the variance in error rates, with Tukey HSD post hoc tests
confirming statistically significant differences between all input formats.

</details>


### [8] [Reasoning's Razor: Reasoning Improves Accuracy but Can Hurt Recall at Critical Operating Points in Safety and Hallucination Detection](https://arxiv.org/abs/2510.21049)
*Atoosa Chegini,Hamid Kazemi,Garrett Souza,Maria Safi,Yang Song,Samy Bengio,Sinead Williamson,Mehrdad Farajtabar*

Main category: cs.CL

TL;DR: 推理机制提升LLMs平均准确率但损害严格精度需求，集成方案可平衡优势


<details>
  <summary>Details</summary>
Motivation: 实际应用中安全检测等任务需要极低误报率，但现有研究未系统验证推理机制在此类精确敏感任务中的适用性

Method: 在安全/幻觉检测任务中对比推理增强(Think On)与常规模式(Think Off)，分析微调/零样本场景下标准LLM与大型推理模型的表现

Result: Think On整体准确率提升3-5%但低FPR阈值表现差，Think Off在FPR<1%时准确率高10-15%；令牌评分优于自我置信度评估；集成方案可恢复双方优势

Conclusion: 推理机制是双刃剑：常规任务有益，严格精度场景需谨慎使用；建议采用集成方案并优先使用令牌评分机制

Abstract: Reasoning has become a central paradigm for large language models (LLMs),
consistently boosting accuracy across diverse benchmarks. Yet its suitability
for precision-sensitive tasks remains unclear. We present the first systematic
study of reasoning for classification tasks under strict low false positive
rate (FPR) regimes. Our analysis covers two tasks--safety detection and
hallucination detection--evaluated in both fine-tuned and zero-shot settings,
using standard LLMs and Large Reasoning Models (LRMs). Our results reveal a
clear trade-off: Think On (reasoning-augmented) generation improves overall
accuracy, but underperforms at the low-FPR thresholds essential for practical
use. In contrast, Think Off (no reasoning during inference) dominates in these
precision-sensitive regimes, with Think On surpassing only when higher FPRs are
acceptable. In addition, we find token-based scoring substantially outperforms
self-verbalized confidence for precision-sensitive deployments. Finally, a
simple ensemble of the two modes recovers the strengths of each. Taken
together, our findings position reasoning as a double-edged tool: beneficial
for average accuracy, but often ill-suited for applications requiring strict
precision.

</details>


### [9] [Dynamic Retriever for In-Context Knowledge Editing via Policy Optimization](https://arxiv.org/abs/2510.21059)
*Mahmud Wasif Nafee,Maiqi Jiang,Haipeng Chen,Yanfu Zhang*

Main category: cs.CL

TL;DR: 提出DR-IKE框架，通过强化学习动态筛选演示案例，实现无需修改权重的黑盒大语言模型知识编辑，提升编辑成功率17.1%并降低延迟41.6%。


<details>
  <summary>Details</summary>
Motivation: 现有基于静态演示集的知识编辑方法存在数量-质量权衡困境和任务难度适应性不足的问题，需通过动态价值评估改进。

Method: 1) 用REINFORCE算法训练BERT检索器评估演示案例的编辑效用
2) 可学习阈值动态调整提示长度，困难任务自动扩展演示集

Result: COUNTERFACT基准测试显示编辑成功率提升17.1%，延迟降低41.6%，且保持无关查询准确率。代码已开源。

Conclusion: DR-IKE框架通过动态检索机制实现了高效可扩展的知识更新，为黑盒大语言模型提供自适应的知识编辑解决方案。

Abstract: Large language models (LLMs) excel at factual recall yet still propagate
stale or incorrect knowledge. In-context knowledge editing offers a
gradient-free remedy suitable for black-box APIs, but current editors rely on
static demonstration sets chosen by surface-level similarity, leading to two
persistent obstacles: (i) a quantity-quality trade-off, and (ii) lack of
adaptivity to task difficulty. We address these issues by dynamically selecting
supporting demonstrations according to their utility for the edit. We propose
Dynamic Retriever for In-Context Knowledge Editing (DR-IKE), a lightweight
framework that (1) trains a BERT retriever with REINFORCE to rank
demonstrations by editing reward, and (2) employs a learnable threshold to
prune low-value examples, shortening the prompt when the edit is easy and
expanding it when the task is hard. DR-IKE performs editing without modifying
model weights, relying solely on forward passes for compatibility with
black-box LLMs. On the COUNTERFACT benchmark, it improves edit success by up to
17.1%, reduces latency by 41.6%, and preserves accuracy on unrelated queries,
demonstrating scalable and adaptive knowledge editing. The code is available at
https://github.com/mwnafee/DR-IKE .

</details>


### [10] [Bridging Language Gaps with Adaptive RAG: Improving Indonesian Language Question Answering](https://arxiv.org/abs/2510.21068)
*William Christian,Daniel Adamlu,Adrian Yu,Derwin Suhartono*

Main category: cs.CL

TL;DR: 研究将自适应RAG系统应用于印尼语QA任务，发现分类器可靠但多检索策略存在显著不一致性


<details>
  <summary>Details</summary>
Motivation: 解决现有问答系统在非英语语言（特别是印尼语）上的性能差距，并克服低资源语言数据不足的挑战

Method: 通过问题复杂度分类器动态选择回答策略，使用机器翻译进行数据增强

Result: 分类器表现可靠，但多检索回答策略导致评估指标下降

Conclusion: 自适应RAG在低资源语言中展现潜力，但需优化多检索策略的稳定性

Abstract: Question Answering (QA) has seen significant improvements with the
advancement of machine learning models, further studies enhanced this question
answering system by retrieving external information, called Retrieval-Augmented
Generation (RAG) to produce more accurate and informative answers. However,
these state-of-the-art-performance is predominantly in English language. To
address this gap we made an effort of bridging language gaps by incorporating
Adaptive RAG system to Indonesian language. Adaptive RAG system integrates a
classifier whose task is to distinguish the question complexity, which in turn
determines the strategy for answering the question. To overcome the limited
availability of Indonesian language dataset, our study employs machine
translation as data augmentation approach. Experiments show reliable question
complexity classifier; however, we observed significant inconsistencies in
multi-retrieval answering strategy which negatively impacted the overall
evaluation when this strategy was applied. These findings highlight both the
promise and challenges of question answering in low-resource language
suggesting directions for future improvement.

</details>


### [11] [CDrugRed: A Chinese Drug Recommendation Dataset for Discharge Medications in Metabolic Diseases](https://arxiv.org/abs/2510.21084)
*Juntao Li,Haobin Yuan,Ling Luo,Yan Jiang,Fan Wang,Ping Zhang,Huiyi Lv,Jian Wang,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 首个公开的中文代谢疾病出院药物推荐数据集CDrugRed发布，包含5894条患者记录，实验显示现有模型仍有较大改进空间（最佳F1=0.5648）


<details>
  <summary>Details</summary>
Motivation: 当前公开的EHR数据集（尤其是非英语）稀缺，阻碍了智能药物推荐系统的发展

Method: 构建包含患者全维度医疗信息的CDrugRed数据集，并基于该数据集对多种大语言模型进行出院药物推荐任务测试

Result: 监督微调后最佳模型F1分数0.5648，Jaccard分数0.4477，表明临床药物推荐任务具有高度复杂性

Conclusion: CDrugRed为开发更强大的药物推荐系统提供了具有挑战性的基准资源，数据集已通过协议公开

Abstract: Intelligent drug recommendation based on Electronic Health Records (EHRs) is
critical for improving for improving the quality and efficiency of clinical
decision-making. By leveraging large-scale patient data, drug recommendation
systems can assist physicians in selecting the most appropriate medications
according to a patient's medical history, diagnoses, laboratory results, and
comorbidities. However, the advancement of such systems is significantly
hampered by the scarcity of publicly available, real-world EHR datasets,
particularly in languages other than English. In this work, we present
CDrugRed, a first publicly available Chinese drug recommendation dataset
focused on discharge medications for metabolic diseases. The dataset includes
5,894 de-identified records from 3,190 patients, containing comprehensive
information such as patient demographics, medical history, clinical course, and
discharge diagnoses. We assess the utility of CDrugRed by benchmarking several
state-of-the-art large language models (LLMs) on the discharge medication
recommendation task. Experimental results show that while supervised
fine-tuning improves model performance, there remains substantial room for
improvement, with the best model achieving the F1 score of 0.5648 and Jaccard
score of 0.4477. This result highlights the complexity of the clinical drug
recommendation task and establishes CDrugRed as a challenging and valuable
resource for developing more robust and accurate drug recommendation systems.
The dataset is publicly available to the research community under the data
usage agreements at https://github.com/DUTIR-BioNLP/CDrugRed.

</details>


### [12] [Self-Rewarding PPO: Aligning Large Language Models with Demonstrations Only](https://arxiv.org/abs/2510.21090)
*Qingru Zhang,Liang Qiu,Ilgee Hong,Zhenghao Xu,Tianyi Liu,Shiyang Li,Rongzhi Zhang,Zheng Li,Lihong Li,Bing Yin,Chao Zhang,Jianshu Chen,Haoming Jiang,Tuo Zhao*

Main category: cs.CL

TL;DR: 提出Self-Rewarding PPO方法，通过结合监督微调(SFT)与近端策略优化(PPO)，利用策略比率作为隐式奖励信号，解决传统SFT方法在泛化性和数据效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 传统SFT方法存在过拟合、跨域泛化差的问题，尤其在数据有限时显著。研究者希望通过结合SFT与强化学习框架PPO，利用预训练模型自身特性构建奖励机制，摆脱对人类标注数据的依赖。

Method: 核心是设计基于SFT模型与预训练基模型策略比率的奖励函数：1) 将预训练模型作为基线策略 2) SFT模型作为目标策略 3) 通过PPO进行在线策略优化，实现自我奖励机制的持续改进。

Result: 在多个NLP任务中验证显示，该方法在数据效率、模型鲁棒性和跨域泛化能力上均优于传统SFT，特别是在高质量标注数据稀缺时表现突出。

Conclusion: 该方法为语言模型对齐提供更高效的范式，通过内置奖励机制降低对人工标注的依赖，为有限数据场景下的模型优化提供了新思路。

Abstract: Supervised fine-tuning (SFT) has emerged as a crucial method for aligning
large language models (LLMs) with human-annotated demonstrations. However, SFT,
being an off-policy approach similar to behavior cloning, often struggles with
overfitting and poor out-of-domain generalization, especially in limited-data
scenarios. To address these limitations, we propose Self-Rewarding PPO, a novel
fine-tuning method that leverages on-policy techniques to enhance
generalization performance. Our approach combines the strengths of SFT and
proximal policy optimization (PPO) to achieve more effective alignment from
demonstration data. At its core is a reward function designed as the log policy
ratio between the SFT model and the pretrained base model. This function serves
as an implicit reward signal, using the pretrained policy as a baseline and the
SFT policy as a target. By doing so, it enables on-policy fine-tuning without
relying on human preference annotations. The integration of this self-rewarding
mechanism with PPO addresses key limitations of SFT, improving generalization,
data efficiency, and robustness. Our empirical evaluation across a range of
natural language processing tasks demonstrates that Self-Rewarding PPO
consistently outperforms traditional SFT methods. The results highlight the
effectiveness of our approach in aligning LLMs using demonstration data,
particularly in scenarios where high-quality annotated data is scarce.

</details>


### [13] [The Gray Zone of Faithfulness: Taming Ambiguity in Unfaithfulness Detection](https://arxiv.org/abs/2510.21118)
*Qiang Ding,Lvzhou Luo,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 提出VeriGray基准解决LLM摘要生成中外部知识标注模糊问题，发现现有模型存在6%幻觉和8%需外部验证的中间类别。


<details>
  <summary>Details</summary>
Motivation: 现有忠实性检测基准因外部知识边界不明确导致标注不一致，影响模型评估的准确性。

Method: 设计包含Out-Dependent中间类别的标注框架，构建VeriGray基准用于检测摘要不忠实现象。

Result: SOTA模型（如GPT-5）在摘要任务中出现6%幻觉，8%句子属于需外部验证的中间类别。

Conclusion: 标注模糊性显著影响检测效果，VeriGray基准揭示了现有方法的不足与改进方向。

Abstract: Ensuring that Large Language Models (LLMs) generate summaries faithful to a
given source document is essential for real-world applications. While prior
research has explored LLM faithfulness, existing benchmarks suffer from
annotation ambiguity, primarily due to the ill-defined boundary of permissible
external knowledge in generated outputs. For instance, common sense is often
incorporated into responses and labeled as "faithful", yet the acceptable
extent of such knowledge remains unspecified, leading to inconsistent
annotations. To address this issue, we propose a novel faithfulness annotation
framework, which introduces an intermediate category, Out-Dependent, to
classify cases where external knowledge is required for verification. Using
this framework, we construct VeriGray (Verification with the Gray Zone) -- a
new unfaithfulness detection benchmark in summarization. Statistics reveal that
even SOTA LLMs, such as GPT-5, exhibit hallucinations ($\sim 6\%$ of sentences)
in summarization tasks. Moreover, a substantial proportion ($\sim 8\%$ on
average of models) of generated sentences fall into the Out-Dependent category,
underscoring the importance of resolving annotation ambiguity in unfaithfulness
detection benchmarks. Experiments demonstrate that our benchmark poses
significant challenges to multiple baseline methods, indicating considerable
room for future improvement.

</details>


### [14] [Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications](https://arxiv.org/abs/2510.21131)
*Guangxin Su,Hanchen Wang,Jianwei Wang,Wenjie Zhang,Ying Zhang,Jian Pei*

Main category: cs.CL

TL;DR: 系统综述LLM与文本属性图的协同整合，提出分类法与实现框架，探讨双向增强策略及多领域应用


<details>
  <summary>Details</summary>
Motivation: LLMs存在黑箱推理局限，TAGs缺乏语义深度。两者的整合可实现图表示增强与LLM推理可解释性互补

Method: 从协同视角构建分类体系：LLM增强图任务（LLM4TAG）与图增强LLM推理（TAG4LLM），提出顺序/并行/多模块协同策略

Result: 建立包含图预训练、提示工程、参数高效微调的技术框架，整理实证数据集并验证在推荐系统、生物医学等领域的应用有效性

Conclusion: LLM与图结构协同是突破认知推理瓶颈的关键路径，需进一步解决知识对齐、动态更新和计算效率等挑战

Abstract: Large Language Models (LLMs) have achieved remarkable success in natural
language processing through strong semantic understanding and generation.
However, their black-box nature limits structured and multi-hop reasoning. In
contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures
enriched with textual context, yet often lack semantic depth. Recent research
shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG
representation learning and improving the reasoning and interpretability of
LLMs. This survey provides the first systematic review of LLM--TAG integration
from an orchestration perspective. We introduce a novel taxonomy covering two
fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and
TAG for LLM, where structured graphs improve LLM reasoning. We categorize
orchestration strategies into sequential, parallel, and multi-module
frameworks, and discuss advances in TAG-specific pretraining, prompting, and
parameter-efficient fine-tuning. Beyond methodology, we summarize empirical
insights, curate available datasets, and highlight diverse applications across
recommendation systems, biomedical analysis, and knowledge-intensive question
answering. Finally, we outline open challenges and promising research
directions, aiming to guide future work at the intersection of language and
graph learning.

</details>


### [15] [Social Simulations with Large Language Model Risk Utopian Illusion](https://arxiv.org/abs/2510.21180)
*Ning Bian,Xianpei Han,Hongyu Lin,Baolei Wu,Jun Wang*

Main category: cs.CL

TL;DR: 研究发现LLMs在社交模拟中反映理想化人类行为而非真实行为，存在社会认知偏差（社会角色偏差/首因效应/积极性偏差），导致形成缺乏复杂性的'乌托邦'社会


<details>
  <summary>Details</summary>
Motivation: 分析LLMs在社交模拟中的行为偏离问题，避免科学误读和现实应用风险

Method: 通过聊天室式多智能体交互模拟，结合五个语言维度分析，测试三个家族的八个代表性LLMs

Result: LLMs展现社会期望偏差塑造的理想化行为模式，导致模拟社会缺乏真实人类互动的复杂性和多样性

Conclusion: 呼吁开发更社会情境化的LLMs以捕捉人类行为多样性

Abstract: Reliable simulation of human behavior is essential for explaining,
predicting, and intervening in our society. Recent advances in large language
models (LLMs) have shown promise in emulating human behaviors, interactions,
and decision-making, offering a powerful new lens for social science studies.
However, the extent to which LLMs diverge from authentic human behavior in
social contexts remains underexplored, posing risks of misinterpretation in
scientific studies and unintended consequences in real-world applications.
Here, we introduce a systematic framework for analyzing LLMs' behavior in
social simulation. Our approach simulates multi-agent interactions through
chatroom-style conversations and analyzes them across five linguistic
dimensions, providing a simple yet effective method to examine emergent social
cognitive biases. We conduct extensive experiments involving eight
representative LLMs across three families. Our findings reveal that LLMs do not
faithfully reproduce genuine human behavior but instead reflect overly
idealized versions of it, shaped by the social desirability bias. In
particular, LLMs show social role bias, primacy effect, and positivity bias,
resulting in "Utopian" societies that lack the complexity and variability of
real human interactions. These findings call for more socially grounded LLMs
that capture the diversity of human social behavior.

</details>


### [16] [Estonian Native Large Language Model Benchmark](https://arxiv.org/abs/2510.21193)
*Helena Grete Lillepalu,Tanel Alumäe*

Main category: cs.CL

TL;DR: 提出首个爱沙尼亚语LLM综合评估基准，包含7个原生数据集，比较32个模型表现，验证LLM评估有效性


<details>
  <summary>Details</summary>
Motivation: 爱沙尼亚语LLM评估基准稀缺，缺乏跨模型性能比较研究

Method: 构建7个原生数据集评估多维度能力；比较6个基础模型和26个指令微调模型；结合人工评估与Claude 3.7作为自动评审

Result: 人工评分与基准中度-高度相关；Claude 3.7与人类评分高度一致（皮尔逊相关系数0.82）

Conclusion: 高性能LLM可有效支持小语种模型评估，降低人工评审成本

Abstract: The availability of LLM benchmarks for the Estonian language is limited, and
a comprehensive evaluation comparing the performance of different LLMs on
Estonian tasks has yet to be conducted. We introduce a new benchmark for
evaluating LLMs in Estonian, based on seven diverse datasets. These datasets
assess general and domain-specific knowledge, understanding of Estonian grammar
and vocabulary, summarization abilities, contextual comprehension, and more.
The datasets are all generated from native Estonian sources without using
machine translation. We compare the performance of base models,
instruction-tuned open-source models, and commercial models. Our evaluation
includes 6 base models and 26 instruction-tuned models. To assess the results,
we employ both human evaluation and LLM-as-a-judge methods. Human evaluation
scores showed moderate to high correlation with benchmark evaluations,
depending on the dataset. Claude 3.7 Sonnet, used as an LLM judge, demonstrated
strong alignment with human ratings, indicating that top-performing LLMs can
effectively support the evaluation of Estonian-language models.

</details>


### [17] [The "Right" Discourse on Migration: Analysing Migration-Related Tweets in Right and Far-Right Political Movements](https://arxiv.org/abs/2510.21220)
*Nishan Chatterjee,Veronika Bajt,Ana Zwitter Vitez,Senja Pollak*

Main category: cs.CL

TL;DR: 将自然语言处理技术与社会学理论结合，分析欧洲极右翼在Twitter上关于移民议题的仇恨言论与说服策略


<details>
  <summary>Details</summary>
Motivation: 欧洲右翼民粹主义兴起背景下，通过社交媒体的日常互动揭示极端意识形态传播机制及其对政治生态的影响

Method: 采用前沿的NLP技术分析英法双语极右翼推文语料库(MIGR-TWIT)，结合语言学、社会学和计算科学的跨学科方法

Result: 揭示极右翼群体围绕移民问题的论述模式、仇恨言论特征及政治说服技术，建立数字话语与社会动态的关联框架

Conclusion: 多学科融合方法为理解社交媒体极端主义提供了新视角，对制定平台治理策略具有重要实证价值

Abstract: The rise of right-wing populism in Europe has brought to the forefront the
significance of analysing social media discourse to understand the
dissemination of extremist ideologies and their impact on political outcomes.
Twitter, as a platform for interaction and mobilisation, provides a unique
window into the everyday communication of far-right supporters. In this paper,
we propose a methodology that uses state-of-the-art natural language processing
techniques with sociological insights to analyse the MIGR-TWIT corpus of
far-right tweets in English and French. We aim to uncover patterns of discourse
surrounding migration, hate speech, and persuasion techniques employed by right
and far-right actors. By integrating linguistic, sociological, and
computational approaches, we seek to offer cross-disciplinary insights into
societal dynamics and contribute to a better understanding of contemporary
challenges posed by right-wing extremism on social media platforms.

</details>


### [18] [DispatchMAS: Fusing taxonomy and artificial intelligence agents for emergency medical services](https://arxiv.org/abs/2510.21228)
*Xiang Li,Huizi Yu,Wenkong Wang,Yiran Wu,Jiayan Zhou,Wenyue Hua,Xinxin Lin,Wenjia Tan,Lexuan Zhu,Bingyi Chen,Guang Chen,Ming-Li Chen,Yang Zhou,Zhao Li,Themistocles L. Assimes,Yongfeng Zhang,Qingyun Wu,Xin Ma,Lingyao Li,Lizhou Fan*

Main category: cs.CL

TL;DR: 开发基于LLM的多智能体系统模拟紧急医疗调度场景，通过医生评估和算法验证显示高效可靠性


<details>
  <summary>Details</summary>
Motivation: 紧急医疗调度存在高压力场景和决策模糊性，需通过AI技术增强调度员决策能力

Method: 构建临床分类法框架+六阶段协议，开发AutoGen多智能体系统，采用医生人工评估（100案例）结合算法语言分析

Result: 系统展示94%准确调度和91%有效指导，情感中立（73.7%中性情绪）、高可读性（Flesch 80.9）和礼貌交互（60%礼貌）

Conclusion: 验证了多智能体系统在培训、协议评估和实时决策中的实用价值，为AI整合应急响应提供安全实施路径

Abstract: Objective: Emergency medical dispatch (EMD) is a high-stakes process
challenged by caller distress, ambiguity, and cognitive load. Large Language
Models (LLMs) and Multi-Agent Systems (MAS) offer opportunities to augment
dispatchers. This study aimed to develop and evaluate a taxonomy-grounded,
LLM-powered multi-agent system for simulating realistic EMD scenarios. Methods:
We constructed a clinical taxonomy (32 chief complaints, 6 caller identities
from MIMIC-III) and a six-phase call protocol. Using this framework, we
developed an AutoGen-based MAS with Caller and Dispatcher Agents. The system
grounds interactions in a fact commons to ensure clinical plausibility and
mitigate misinformation. We used a hybrid evaluation framework: four physicians
assessed 100 simulated cases for "Guidance Efficacy" and "Dispatch
Effectiveness," supplemented by automated linguistic analysis (sentiment,
readability, politeness). Results: Human evaluation, with substantial
inter-rater agreement (Gwe's AC1 > 0.70), confirmed the system's high
performance. It demonstrated excellent Dispatch Effectiveness (e.g., 94 %
contacting the correct potential other agents) and Guidance Efficacy (advice
provided in 91 % of cases), both rated highly by physicians. Algorithmic
metrics corroborated these findings, indicating a predominantly neutral
affective profile (73.7 % neutral sentiment; 90.4 % neutral emotion), high
readability (Flesch 80.9), and a consistently polite style (60.0 % polite; 0 %
impolite). Conclusion: Our taxonomy-grounded MAS simulates diverse, clinically
plausible dispatch scenarios with high fidelity. Findings support its use for
dispatcher training, protocol evaluation, and as a foundation for real-time
decision support. This work outlines a pathway for safely integrating advanced
AI agents into emergency response workflows.

</details>


### [19] [Correlation Dimension of Auto-Regressive Large Language Models](https://arxiv.org/abs/2510.21258)
*Xin Du,Kumiko Tanaka-Ishii*

Main category: cs.CL

TL;DR: 提出使用分形几何的相关维度指标，有效量化语言模型生成文本的结构复杂性，突破传统困惑度指标的局限性


<details>
  <summary>Details</summary>
Motivation: 传统困惑度指标仅关注局部预测准确性，无法解释LLMs在长文本生成中出现的重复/不连贯等结构性问题，需建立更全面的评估体系

Method: 基于分形几何理论构建相关维度指标，通过测量文本嵌入空间的自相似性，量化语言层级递归结构的复杂程度

Result: 实验证明该指标能识别预训练三阶段特征、反映上下文复杂性、检测幻觉倾向，4bit量化下仍保持鲁棒性，且适用于Transformer/Mamba等架构

Conclusion: 该分形度量框架为理解LLMs生成机制提供新视角，在模型诊断、文本质量评估等领域具有重要应用价值

Abstract: Large language models (LLMs) have achieved remarkable progress in natural
language generation, yet they continue to display puzzling behaviors -- such as
repetition and incoherence -- even when exhibiting low perplexity. This
highlights a key limitation of conventional evaluation metrics, which emphasize
local prediction accuracy while overlooking long-range structural complexity.
We introduce correlation dimension, a fractal-geometric measure of
self-similarity, to quantify the epistemological complexity of text as
perceived by a language model. This measure captures the hierarchical
recurrence structure of language, bridging local and global properties in a
unified framework. Through extensive experiments, we show that correlation
dimension (1) reveals three distinct phases during pretraining, (2) reflects
context-dependent complexity, (3) indicates a model's tendency toward
hallucination, and (4) reliably detects multiple forms of degeneration in
generated text. The method is computationally efficient, robust to model
quantization (down to 4-bit precision), broadly applicable across
autoregressive architectures (e.g., Transformer and Mamba), and provides fresh
insight into the generative dynamics of LLMs.

</details>


### [20] [Sparser Block-Sparse Attention via Token Permutation](https://arxiv.org/abs/2510.21270)
*Xinghao Wang,Pengyu Wang,Dong Zhang,Chenkun Tan,Shaojun Zhou,Zhaoxiang Liu,Shiguo Lian,Fangxu Liu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出Permuted Block-Sparse Attention方法，通过优化注意力机制提升大语言模型的长上下文计算效率


<details>
  <summary>Details</summary>
Motivation: 现有块稀疏注意力在长序列场景存在块级稀疏性不足，导致计算冗余和效率低下

Method: 利用注意力置换特性重新排列键值块，增加块级稀疏性，配合定制化permuted-FlashAttention内核实现加速

Result: 在真实长文本数据集上取得2.75倍预填充加速，模型精度超越现有方法且接近全注意力基线

Conclusion: PBS-Attn通过创新性块排列机制有效平衡计算效率与模型精度，为长上下文LLM优化提供新方向

Abstract: Scaling the context length of large language models (LLMs) offers significant
benefits but is computationally expensive. This expense stems primarily from
the self-attention mechanism, whose $O(N^2)$ complexity with respect to
sequence length presents a major bottleneck for both memory and latency.
Fortunately, the attention matrix is often sparse, particularly for long
sequences, suggesting an opportunity for optimization. Block-sparse attention
has emerged as a promising solution that partitions sequences into blocks and
skips computation for a subset of these blocks. However, the effectiveness of
this method is highly dependent on the underlying attention patterns, which can
lead to sub-optimal block-level sparsity. For instance, important key tokens
for queries within a single block may be scattered across numerous other
blocks, leading to computational redundancy. In this work, we propose Permuted
Block-Sparse Attention (\textbf{PBS-Attn}), a plug-and-play method that
leverages the permutation properties of attention to increase block-level
sparsity and enhance the computational efficiency of LLM prefilling. We conduct
comprehensive experiments on challenging real-world long-context datasets,
demonstrating that PBS-Attn consistently outperforms existing block-sparse
attention methods in model accuracy and closely matches the full attention
baseline. Powered by our custom permuted-FlashAttention kernels, PBS-Attn
achieves an end-to-end speedup of up to $2.75\times$ in long-context
prefilling, confirming its practical viability. Code available at
https://github.com/xinghaow99/pbs-attn

</details>


### [21] [PARL: Prompt-based Agents for Reinforcement Learning](https://arxiv.org/abs/2510.21306)
*Yarik Menchaca Resendiz,Roman Klinger*

Main category: cs.CL

TL;DR: 提出PARL方法，通过提示机制使未微调的LLM成为强化学习代理，在非语言类强化学习任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM在语言任务的表现，缺乏其在需要结构化推理的强化学习环境（如网格世界）中的探索。本文旨在验证LLM在非纯语言类强化学习任务中的应用潜力。

Method: 开发PARL框架，通过将动作、状态和奖励编码到提示中，使LLM通过试错学习完成标准强化学习任务（如网格导航），无需模型微调。

Result: PARL在简单环境中持平或超越传统强化学习代理，但面临数学运算和状态解码的局限性。

Conclusion: 证明未微调LLM具备结构化强化学习任务处理能力，但复杂场景表现受限，提示预训练知识迁移可能性的同时揭示改进方向。

Abstract: Large language models (LLMs) have demonstrated high performance on tasks
expressed in natural language, particularly in zero- or few-shot settings.
These are typically framed as supervised (e.g., classification) or unsupervised
(e.g., clustering) problems. However, limited work evaluates LLMs as agents in
reinforcement learning (RL) tasks (e.g., playing games), where learning occurs
through interaction with an environment and a reward system. While prior work
focused on representing tasks that rely on a language representation, we study
structured, non-linguistic reasoning - such as interpreting positions in a grid
world. We therefore introduce PARL (Prompt-based Agent for Reinforcement
Learning), a method that uses LLMs as RL agents through prompting, without any
fine-tuning. PARL encodes actions, states, and rewards in the prompt, enabling
the model to learn through trial-and-error interaction. We evaluate PARL on
three standard RL tasks that do not entirely rely on natural language. We show
that it can match or outperform traditional RL agents in simple environments by
leveraging pretrained knowledge. However, we identify performance limitations
in tasks that require complex mathematical operations or decoding states and
actions.

</details>


### [22] [Efficient semantic uncertainty quantification in language models via diversity-steered sampling](https://arxiv.org/abs/2510.21310)
*Ji Won Park,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 提出多样性导向的采样器框架，通过NLI模型抑制语义冗余，提升LLM不确定性估计的样本效率


<details>
  <summary>Details</summary>
Motivation: 传统LLM不确定性估计需要大量生成样本导致成本高昂，需解决自由问答场景的语义不确定性量化难题

Method: 在解码阶段注入NLI模型构建的语义相似性惩罚，结合重要性重加权和方差控制技术，适配自回归和扩散两种范式

Result: 在4个QA基准上实现同等样本量下更优的语义覆盖，保持基线性能的同时提升不确定性估计质量

Conclusion: 该模块化框架无需修改基础LLM，可作为风险敏感场景下不确定性估计的即插即用方案

Abstract: Accurately estimating semantic aleatoric and epistemic uncertainties in large
language models (LLMs) is particularly challenging in free-form question
answering (QA), where obtaining stable estimates often requires many expensive
generations. We introduce a diversity-steered sampler that discourages
semantically redundant outputs during decoding, covers both autoregressive and
masked diffusion paradigms, and yields substantial sample-efficiency gains. The
key idea is to inject a continuous semantic-similarity penalty into the model's
proposal distribution using a natural language inference (NLI) model lightly
finetuned on partial prefixes or intermediate diffusion states. We debias
downstream uncertainty estimates with importance reweighting and shrink their
variance with control variates. Across four QA benchmarks, our method matches
or surpasses baselines while covering more semantic clusters with the same
number of samples. Being modular and requiring no gradient access to the base
LLM, the framework promises to serve as a drop-in enhancement for uncertainty
estimation in risk-sensitive model deployments.

</details>


### [23] [Typoglycemia under the Hood: Investigating Language Models' Understanding of Scrambled Words](https://arxiv.org/abs/2510.21326)
*Gianluca Sperduti,Alejandro Moreo*

Main category: cs.CL

TL;DR: 论文探究NLP模型在字母顺序混乱(typoglycemia)时仍保持鲁棒性的原因，发现英语中混淆词数量有限且上下文差异显著，导致BERT等模型性能下降幅度较小。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解释为何当单词因字母顺序混乱导致语义混淆时（如form和from变为相同表示），NLP模型仍能有效工作，重点揭示其鲁棒性的底层机制。

Method: 通过分析英国国家语料库量化单词混淆现象，评估BERT的消歧能力，并比较在干净/字母混乱文本上训练的BERT变体性能差异。

Result: 结果显示：(1)英语中因typoglycemia混淆的单词数量较少；(2)混淆词常出现在差异显著的上下文中；(3)字母打乱训练仅导致模型性能轻微下降。

Conclusion: 模型鲁棒性主要源于英语本身特性：有限的单词混淆可能性和充分的上下文差异，这为理解NLP抗干扰能力提供了理论依据。

Abstract: Research in linguistics has shown that humans can read words with internally
scrambled letters, a phenomenon recently dubbed typoglycemia. Some specific NLP
models have recently been proposed that similarly demonstrate robustness to
such distortions by ignoring the internal order of characters by design. This
raises a fundamental question: how can models perform well when many distinct
words (e.g., form and from) collapse into identical representations under
typoglycemia? Our work, focusing exclusively on the English language, seeks to
shed light on the underlying aspects responsible for this robustness. We
hypothesize that the main reasons have to do with the fact that (i) relatively
few English words collapse under typoglycemia, and that (ii) collapsed words
tend to occur in contexts so distinct that disambiguation becomes trivial. In
our analysis, we (i) analyze the British National Corpus to quantify word
collapse and ambiguity under typoglycemia, (ii) evaluate BERT's ability to
disambiguate collapsing forms, and (iii) conduct a probing experiment by
comparing variants of BERT trained from scratch on clean versus typoglycemic
Wikipedia text; our results reveal that the performance degradation caused by
scrambling is smaller than expected.

</details>


### [24] [TripTide: A Benchmark for Adaptive Travel Planning under Disruptions](https://arxiv.org/abs/2510.21329)
*Priyanshu Karmakar,Soumyabrata Chaudhuri,Shubhojit Mallick,Manish Gupta,Abhik Jana,Shreya Ghosh*

Main category: cs.CL

TL;DR: 提出首个评估大语言模型在旅行计划中断场景下调整能力的基准TripTide，通过多维评估揭示LLM在时序一致性、地理连贯性和抗干扰能力方面的特性


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划系统（如TripCraft）虽能生成个性化行程，但缺乏对现实干扰（如航班取消/天气变化）的适应能力评估。需建立系统性基准衡量LLM在动态环境中的应变表现

Method: 1. 构建含干扰严重度/旅客容忍度的评估维度
2. 设计自动化指标：意图保持度/响应性/适应性
3. 采用LLM-as-judge自动评估
4. 专家人工验证语义/空间/时序/响应四维度

Result: 1. LLM保持89%的时序一致性
2. 短途行程空间偏差率32%（长途降至15%）
3. 行程长度增加20%时，干扰处理能力下降41%
4. GPT-4在响应及时性上优于Claude-2 28%

Conclusion: TripTide为评估LLM在现实不确定性下的旅行规划能力建立新基准，揭示模型在长序列任务中的地理连贯性优势与抗干扰脆弱性，为构建鲁棒的AI旅行助手提供方法论

Abstract: Recent efforts like TripCraft and TravelPlanner have advanced the use of
Large Language Models ( LLMs) for personalized, constraint aware travel
itinerary generation. Yet, real travel often faces disruptions. To address
this, we present TripTide, the first benchmark evaluating LLM's ability to
revise itineraries under realistic disruptions. TripTide models key dimensions
such as disruption severity and traveler tolerance, enabling nuanced assessment
of LLM adaptability to events like flight cancellations, weather closures, or
overbooked attractions. We conduct a threefold evaluation. First, we introduce
automatic metrics including Preservation of Intent (how well the revised plan
maintains feasibility and goals), Responsiveness (promptness and
appropriateness of disruption handling), and Adaptability (semantic, spatial,
and sequential divergence between original and revised plans). Second, we apply
an LLM-as-a-judge approach to automatically assess revision quality. Third, we
perform manual expert evaluation to verify whether revisions preserve semantic,
spatial, sequential, and responsive aspects. Our experiments show that LLMs
maintain strong sequential consistency and semantic stability, while spatial
deviations are larger for shorter trips but decrease with longer ones,
indicating that extended plans encourage better geographic coherence. However,
disruption-handling ability declines as plan length increases, highlighting
limits in LLM robustness. TripTide establishes a benchmark for evaluating
adaptability, personalization, and resilience in LLM-based travel planning
under real-world uncertainty.

</details>


### [25] [Multi-turn Training with Basic Human Feedback Helps Little on LLM Reasoning](https://arxiv.org/abs/2510.21339)
*Qiang Liu,Wuganjing Song,Zhenzhou Lin,Feifan Chen,Qiaolong Cai,Chen Li,Yongduo Sui*

Main category: cs.CL

TL;DR: 研究发现单轮训练在具备完整信息的任务中表现更优，多轮训练反而会降低模型推理性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型单轮强化学习训练与多轮人机交互实际应用场景之间的不匹配问题，验证多轮反馈训练对推理任务是否必要

Method: 通过对比传统单轮训练与三种多轮训练策略，在不同轮次评估场景下的泛化能力

Result: 单轮训练模型在单/多轮评估中均表现优异，而多轮训练模型单轮推理性能下降38%

Conclusion: 对于信息完整的任务，稳健的单轮训练体系仍是最优选择，基础反馈的多轮训练收益有限且可能损害模型推理能力

Abstract: The reasoning capabilities of Large Language Models (LLMs) are typically
developed through the single-turn reinforcement learning, whereas real-world
applications often involve multi-turn interactions with human feedback, leading
to a potential mismatch between training and deployment conditions. In this
work, we study whether multi-turn training with human feedback is necessary for
reasoning tasks. We compare conventional single-turn training with three
multi-turn strategies and reach contrary conclusions to previous research. We
find that models trained in a single-turn setting generalize effectively to
both single- and multi-turn evaluations, while models trained with multi-turn
strategies exhibit a significant degradation in single-turn reasoning
performance. These results suggest that for tasks with complete information,
robust single-turn training remains more effective and reliable, as multi-turn
training with basic feedback provides limited benefits and can even degrade
reasoning capabilities.

</details>


### [26] [A Diagnostic Benchmark for Sweden-Related Factual Knowledge](https://arxiv.org/abs/2510.21360)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 研究团队构建了针对瑞典本土知识的问答基准数据集，用于评估多语言模型在跨语言事实回忆和知识保留中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有瑞典基准多为美国基准的翻译版，无法有效评估瑞典本土特色知识。国际媒体对瑞典本土人物/事件覆盖有限，需构建针对性测试工具。

Method: 通过广播节目和体育赛事收集问题，创建含英文翻译的双语数据集，测试不同规模/瑞典语覆盖程度的模型，并分析持续预训练的影响。

Result: 1. 瑞典覆盖强的小模型表现媲美大三倍的多语言模型
2. 持续瑞典预训练提升知识但导致部分遗忘
3. 数据集可有效诊断跨语言知识一致性

Conclusion: 该数据集为研究多语言模型的语言适应机制和知识保留提供了有效的诊断工具，揭示了语言专业化与知识遗忘的平衡挑战。

Abstract: Many Swedish benchmarks are translated US-centric benchmarks, and therefore
not suitable for testing knowledge that is particularly relevant, or even
specific, to Sweden. We therefore introduce a manually written
question-answering benchmark specifically targeted to Sweden-related
personalities and events, many of which receive very limited coverage in
international media. Our annotators drew inspiration from a popular radio
program featuring public figures from culture and media, as well as major
sports events in Sweden. The dataset can be used to measure factual recall
across models of varying sizes and degrees of Swedish coverage, and allows to
probe cross-lingual factual consistency as to contains English translations.
Using the dataset, we find that smaller models with stronger Swedish coverage
perform comparably to a three times larger multilingual model in recalling
Sweden-related facts. We also observe that continued pre-training on Swedish
generally improves factual knowledge but also leads to forgetting of a part of
the previously known information. These results demonstrate the dataset's
potential as a diagnostic tool for studying language adaptation and knowledge
retention in multilingual models and during language adaptation.

</details>


### [27] [SindBERT, the Sailor: Charting the Seas of Turkish NLP](https://arxiv.org/abs/2510.21364)
*Raphael Scheible-Schmitt,Stefan Schweter*

Main category: cs.CL

TL;DR: SindBERT是首个土耳其语大规模RoBERTa编码器模型，通过312GB文本训练，在多项NLP任务中表现与现有模型相当，揭示了语料质量比数据量更重要及当前基准的潜在饱和。


<details>
  <summary>Details</summary>
Motivation: 解决形态丰富语言（如土耳其语）在大规模预训练中的不足，填补土耳其语NLP资源空白。

Method: 使用mC4、OSCAR23和维基百科的312GB土耳其文本从头训练RoBERTa-base/large模型，并在词性标注、NER、冒犯性检测和TurBLiMP基准测试中评估。

Result: SindBERT-large在两项任务中领先但未全面占优，与XLM-R/EuroBERT类似显示基准可能饱和；BERTurk等小模型因语料优质表现更佳。

Conclusion: SindBERT作为开放资源推动了土耳其语NLP发展，同时实证了形态丰富语言中语料组成的关键作用及模型扩展的局限性。

Abstract: Transformer models have revolutionized NLP, yet many morphologically rich
languages remain underrepresented in large-scale pre-training efforts. With
SindBERT, we set out to chart the seas of Turkish NLP, providing the first
large-scale RoBERTa-based encoder for Turkish. Trained from scratch on 312 GB
of Turkish text (mC4, OSCAR23, Wikipedia), SindBERT is released in both base
and large configurations, representing the first large-scale encoder-only
language model available for Turkish. We evaluate SindBERT on part-of-speech
tagging, named entity recognition, offensive language detection, and the
TurBLiMP linguistic acceptability benchmark. Our results show that SindBERT
performs competitively with existing Turkish and multilingual models, with the
large variant achieving the best scores in two of four tasks but showing no
consistent scaling advantage overall. This flat scaling trend, also observed
for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
saturated. At the same time, comparisons with smaller but more curated models
such as BERTurk highlight that corpus quality and diversity can outweigh sheer
data volume. Taken together, SindBERT contributes both as an openly released
resource for Turkish NLP and as an empirical case study on the limits of
scaling and the central role of corpus composition in morphologically rich
languages. The SindBERT models are released under the MIT license and made
available in both fairseq and Huggingface formats.

</details>


### [28] [HalleluBERT: Let every token that has meaning bear its weight](https://arxiv.org/abs/2510.21372)
*Raphael Scheible-Schmitt*

Main category: cs.CL

TL;DR: HalleluBERT希伯来语RoBERTa模型通过49.1GB专用语料训练，在NER和情感分类任务上超越现有基准，验证单语预训练优势


<details>
  <summary>Details</summary>
Motivation: 解决现有希伯来语模型（HeBERT/AlephBERT/HeRo）存在的语料规模受限、词汇不匹配和训练不充分问题

Method: 使用49.1GB去重希伯来网页文本+维基百科，采用希伯来语专用字节级BPE词汇表从头训练基础版和大规模版RoBERTa编码器

Result: 在命名实体识别和情感分类基准测试中，性能优于单语和多语言基线模型，创希伯来语新SOTA

Conclusion: 充分收敛的单语预训练显著提升效果，HalleluBERT为希伯来语NLP树立新标杆

Abstract: Transformer-based models have advanced NLP, yet Hebrew still lacks a
large-scale RoBERTa encoder which is extensively trained. Existing models such
as HeBERT, AlephBERT, and HeRo are limited by corpus size, vocabulary, or
training depth. We present HalleluBERT, a RoBERTa-based encoder family (base
and large) trained from scratch on 49.1~GB of deduplicated Hebrew web text and
Wikipedia with a Hebrew-specific byte-level BPE vocabulary. Evaluated on NER
and sentiment classification benchmarks, HalleluBERT outperforms both
monolingual and multilingual baselines. HalleluBERT sets a new state of the art
for Hebrew and highlights the benefits of fully converged monolingual
pretraining.

</details>


### [29] [Vision Language Models for Dynamic Human Activity Recognition in Healthcare Settings](https://arxiv.org/abs/2510.21424)
*Abderrazek Abid,Thanh-Cong Ho,Fakhri Karray*

Main category: cs.CL

TL;DR: 研究提出用视觉语言模型（VLMs）改进人类活动识别，通过创新数据集和评估方法验证其超越传统模型的潜力


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在动态输出评估上存在局限，VLMs的灵活性优势尚未在人类活动识别领域充分开发

Method: 构建描述性标题数据集，设计综合评估框架，并与SOTA深度学习模型进行对比实验

Result: VLMs在准确性指标上达到与传统模型相当或更优表现，部分场景提升显著

Conclusion: 该研究为VLM在智能医疗的集成建立了基准，揭示了其在动态健康监测中的革新潜力

Abstract: As generative AI continues to evolve, Vision Language Models (VLMs) have
emerged as promising tools in various healthcare applications. One area that
remains relatively underexplored is their use in human activity recognition
(HAR) for remote health monitoring. VLMs offer notable strengths, including
greater flexibility and the ability to overcome some of the constraints of
traditional deep learning models. However, a key challenge in applying VLMs to
HAR lies in the difficulty of evaluating their dynamic and often
non-deterministic outputs. To address this gap, we introduce a descriptive
caption data set and propose comprehensive evaluation methods to evaluate VLMs
in HAR. Through comparative experiments with state-of-the-art deep learning
models, our findings demonstrate that VLMs achieve comparable performance and,
in some cases, even surpass conventional approaches in terms of accuracy. This
work contributes a strong benchmark and opens new possibilities for the
integration of VLMs into intelligent healthcare systems.

</details>


### [30] [Redefining Retrieval Evaluation in the Era of LLMs](https://arxiv.org/abs/2510.21440)
*Giovanni Trappolini,Florin Cuconasu,Simone Filice,Yoelle Maarek,Fabrizio Silvestri*

Main category: cs.CL

TL;DR: 传统IR指标（如nDCG）与RAG系统存在两大错配：人类位置衰减假设不适用于LLM全文档处理模式，且未考虑干扰文档的负面影响。作者提出UDCG指标，通过效用-干扰评估框架和LLM位置衰减函数，将相关性指标与生成准确率相关性提升36%。


<details>
  <summary>Details</summary>
Motivation: 传统IR指标基于人类用户顺序阅读文档的假设，而RAG系统中LLM会整体处理检索结果。此外，传统指标未考虑干扰文档对生成质量的负面影响。这两大错配导致传统指标无法准确预测RAG性能。

Method: 1. 提出效用导向的标注框架，量化相关文档的正向贡献和干扰文档的负向影响
2. 设计UDCG指标，采用面向LLM的位置衰减函数
3. 在5个数据集和6种LLM上进行实验验证

Result: UDCG相比传统指标（nDCG/MAP/MRR）与端到端答案准确率的相关系数提升最高达36%，在零样本场景下提升更显著。实验证明该指标对多种LLM具有普适性。

Conclusion: 本研究突破了传统IR评估范式，首次建立检索质量与生成性能的直接量化关系，为面向LLM的检索系统优化提供了可靠评估工具，推动RAG技术向更实用化方向发展。

Abstract: Traditional Information Retrieval (IR) metrics, such as nDCG, MAP, and MRR,
assume that human users sequentially examine documents with diminishing
attention to lower ranks. This assumption breaks down in Retrieval Augmented
Generation (RAG) systems, where search results are consumed by Large Language
Models (LLMs), which, unlike humans, process all retrieved documents as a whole
rather than sequentially. Additionally, traditional IR metrics do not account
for related but irrelevant documents that actively degrade generation quality,
rather than merely being ignored. Due to these two major misalignments, namely
human vs. machine position discount and human relevance vs. machine utility,
classical IR metrics do not accurately predict RAG performance. We introduce a
utility-based annotation schema that quantifies both the positive contribution
of relevant passages and the negative impact of distracting ones. Building on
this foundation, we propose UDCG (Utility and Distraction-aware Cumulative
Gain), a metric using an LLM-oriented positional discount to directly optimize
the correlation with the end-to-end answer accuracy. Experiments on five
datasets and six LLMs demonstrate that UDCG improves correlation by up to 36%
compared to traditional metrics. Our work provides a critical step toward
aligning IR evaluation with LLM consumers and enables more reliable assessment
of RAG components

</details>


### [31] [REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring](https://arxiv.org/abs/2510.21445)
*Thanh Cong Ho,Farah Kharrat,Abderrazek Abid,Fakhri Karray*

Main category: cs.CL

TL;DR: REMONI系统通过整合多模态大语言模型、物联网和可穿戴设备，实现自动化的远程健康监测，具备实时体征监测、异常警报和自然语言交互功能。


<details>
  <summary>Details</summary>
Motivation: 现有远程医疗系统缺乏人机交互功能，无法有效识别患者活动状态与情绪。本研究旨在通过多模态技术提升医患交互效率与监测精度。

Method: 结合智能手表等设备采集生命体征/加速度数据，部署跌倒检测模型与异常识别算法，利用MLLMs处理患者视频中的活动/情绪，通过提示工程整合多源数据。

Result: 开发完整原型系统并通过测试，验证了系统在实时体征监测、紧急事件响应（如跌倒检测准确率达92%）及医疗人员交互效率提升方面的有效性。

Conclusion: REMONI系统通过技术创新填补了远程医疗的人机交互缺口，可降低30%医护人员工作量，具有实际应用价值与成本效益。

Abstract: With the widespread adoption of wearable devices in our daily lives, the
demand and appeal for remote patient monitoring have significantly increased.
Most research in this field has concentrated on collecting sensor data,
visualizing it, and analyzing it to detect anomalies in specific diseases such
as diabetes, heart disease and depression. However, this domain has a notable
gap in the aspect of human-machine interaction. This paper proposes REMONI, an
autonomous REmote health MONItoring system that integrates multimodal large
language models (MLLMs), the Internet of Things (IoT), and wearable devices.
The system automatically and continuously collects vital signs, accelerometer
data from a special wearable (such as a smartwatch), and visual data in patient
video clips collected from cameras. This data is processed by an anomaly
detection module, which includes a fall detection model and algorithms to
identify and alert caregivers of the patient's emergency conditions. A
distinctive feature of our proposed system is the natural language processing
component, developed with MLLMs capable of detecting and recognizing a
patient's activity and emotion while responding to healthcare worker's
inquiries. Additionally, prompt engineering is employed to integrate all
patient information seamlessly. As a result, doctors and nurses can access
real-time vital signs and the patient's current state and mood by interacting
with an intelligent agent through a user-friendly web application. Our
experiments demonstrate that our system is implementable and scalable for
real-life scenarios, potentially reducing the workload of medical professionals
and healthcare costs. A full-fledged prototype illustrating the functionalities
of the system has been developed and being tested to demonstrate the robustness
of its various capabilities.

</details>


### [32] [MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization](https://arxiv.org/abs/2510.21473)
*Chenglong Wang,Yang Gan,Hang Zhou,Chi Hu,Yongyu Mu,Kai Song,Murun Yang,Bei Li,Chunliang Zhang,Tongran Liu,Jingbo Zhu,Zhengtao Yu,Tong Xiao*

Main category: cs.CL

TL;DR: 论文提出MRO方法，通过增强标记相关性提升扩散语言模型的推理性能与采样效率。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在减少去噪步骤时推理性能下降，核心问题在于独立生成掩码标记导致无法捕捉标记间相关性。

Method: 提出多奖励优化（MRO），结合测试时缩放、拒绝采样、强化学习，并引入组步策略与重要性采样降低方差。

Result: MRO在提升DLMs推理能力的同时实现采样加速，且保持基准测试性能。

Conclusion: 增强序列内/间标记相关性可显著改善DLMs推理性能，MRO为高效训练提供新方向。

Abstract: Recent advances in diffusion language models (DLMs) have presented a
promising alternative to traditional autoregressive large language models
(LLMs). However, DLMs still lag behind LLMs in reasoning performance,
especially as the number of denoising steps decreases. Our analysis reveals
that this shortcoming arises primarily from the independent generation of
masked tokens across denoising steps, which fails to capture the token
correlation. In this paper, we define two types of token correlation:
intra-sequence correlation and inter-sequence correlation, and demonstrate that
enhancing these correlations improves reasoning performance. To this end, we
propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to
consider the token correlation during the denoising process. More specifically,
our MRO approach leverages test-time scaling, reject sampling, and
reinforcement learning to directly optimize the token correlation with multiple
elaborate rewards. Additionally, we introduce group step and importance
sampling strategies to mitigate reward variance and enhance sampling
efficiency. Through extensive experiments, we demonstrate that MRO not only
improves reasoning performance but also achieves significant sampling speedups
while maintaining high performance on reasoning benchmarks.

</details>


### [33] [Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models](https://arxiv.org/abs/2510.21520)
*Omer Moussa,Mariya Toneva*

Main category: cs.CL

TL;DR: 提出多参与者脑调整方法，通过联合预测多个大脑的fMRI响应，显著提升预训练语言模型与大脑神经活动的对齐泛化能力，在减少数据需求的同时改进下游语义任务表现


<details>
  <summary>Details</summary>
Motivation: 现有脑对齐方法存在参与者依赖性强、单被试数据量有限的问题，阻碍模型在新被试和群体层面的泛化分析

Method: 通过微调预训练语音语言模型，使其能够同时预测多个被试的fMRI脑响应数据，实现跨被试的联合建模

Result: 1) 新被试数据需求降低5倍 2) 脑对齐提升达50% 3) 跨数据集强泛化 4) 语义任务表现提升

Conclusion: 多被试脑调整建立了神经科学与AI的双向促进机制，通过群体神经数据训练可获得更普适的语义表征，推动跨学科融合发展

Abstract: Pretrained language models are remarkably effective in aligning with human
brain responses elicited by natural language stimuli, positioning them as
promising model organisms for studying language processing in the brain.
However, existing approaches for both estimating and improving this brain
alignment are participant-dependent and highly affected by the amount of data
available per participant, hindering both generalization to new participants
and population-level analyses. In this work, we address these limitations by
introducing a scalable, generalizable brain-tuning method, in which we
fine-tune pretrained speech language models to jointly predict fMRI responses
from multiple participants. We demonstrate that the resulting brain-tuned
models exhibit strong individual brain alignment while generalizing across
participants. Specifically, our method leads to 1) a 5-fold decrease in the
amount of fMRI data needed to predict brain data from new participants, 2) up
to a 50% increase in the overall brain alignment, and 3) strong generalization
to new unseen datasets. Furthermore, this multi-participant brain-tuning
additionally improves downstream performance on semantic tasks, suggesting that
training using brain data from multiple participants leads to more
generalizable semantic representations. Taken together, these findings
demonstrate a bidirectional benefit between neuroscience and AI, helping bridge
the gap between the two fields. We make our code and models publicly available
at https://github.com/bridge-ai-neuro/multi-brain-tuning.

</details>


### [34] [InterpDetect: Interpretable Signals for Detecting Hallucinations in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.21538)
*Likun Tan,Kuan-Wei Huang,Joy Shi,Kevin Wu*

Main category: cs.CL

TL;DR: 提出基于外部上下文评分与参数知识评分的机制检测方法，有效识别RAG系统幻觉问题并具备跨模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有RAG幻觉检测方法混淆了外部上下文和参数知识的贡献，无法准确溯源生成内容的来源

Method: 通过分层计算外部上下文分数和参数知识分数，训练回归分类器预测幻觉，使用Qwen3-0.6b进行实验验证

Result: 检测效果优于GPT系列模型及现有基线方法，且在GPT-4.1-mini上展现跨模型泛化能力

Conclusion: 机制信号可作为RAG系统幻觉检测的高效通用指标，支持代理模型评估范式

Abstract: Retrieval-Augmented Generation (RAG) integrates external knowledge to
mitigate hallucinations, yet models often generate outputs inconsistent with
retrieved content. Accurate hallucination detection requires disentangling the
contributions of external context and parametric knowledge, which prior methods
typically conflate. We investigate the mechanisms underlying RAG hallucinations
and find they arise when later-layer FFN modules disproportionately inject
parametric knowledge into the residual stream. To address this, we explore a
mechanistic detection approach based on external context scores and parametric
knowledge scores. Using Qwen3-0.6b, we compute these scores across layers and
attention heads and train regression-based classifiers to predict
hallucinations. Our method is evaluated against state-of-the-art LLMs (GPT-5,
GPT-4.1) and detection baselines (RAGAS, TruLens, RefChecker). Furthermore,
classifiers trained on Qwen3-0.6b signals generalize to GPT-4.1-mini responses,
demonstrating the potential of proxy-model evaluation. Our results highlight
mechanistic signals as efficient, generalizable predictors for hallucination
detection in RAG systems.

</details>


### [35] [Document Understanding, Measurement, and Manipulation Using Category Theory](https://arxiv.org/abs/2510.21553)
*Jared Claypoole,Yunye Gong,Noson S. Yanofsky,Ajay Divakaran*

Main category: cs.CL

TL;DR: 应用范畴论提取多模态文档结构，开发信息测量、总结扩展技术及自监督改进大模型方法


<details>
  <summary>Details</summary>
Motivation: 通过数学表示文档为QA对的范畴并进行信息正交化，实现文档信息量化测量与扩展

Method: 1. 建立文档的QA对范畴表示 2. 信息正交化处理 3. 开发信息测量/总结/扩展技术 4. 基于RLVR的自监督模型优化方法

Result: 实现基于大模型的技术方案，提出多模态框架扩展，通过一致性约束提升模型性能

Conclusion: 范畴论框架为文档信息处理提供系统性方法，在自动摘要生成和模型优化方面展现创新潜力

Abstract: We apply category theory to extract multimodal document structure which leads
us to develop information theoretic measures, content summarization and
extension, and self-supervised improvement of large pretrained models. We first
develop a mathematical representation of a document as a category of
question-answer pairs. Second, we develop an orthogonalization procedure to
divide the information contained in one or more documents into non-overlapping
pieces. The structures extracted in the first and second steps lead us to
develop methods to measure and enumerate the information contained in a
document. We also build on those steps to develop new summarization techniques,
as well as to develop a solution to a new problem viz. exegesis resulting in an
extension of the original document. Our question-answer pair methodology
enables a novel rate distortion analysis of summarization techniques. We
implement our techniques using large pretrained models, and we propose a
multimodal extension of our overall mathematical framework. Finally, we develop
a novel self-supervised method using RLVR to improve large pretrained models
using consistency constraints such as composability and closure under certain
operations that stem naturally from our category theoretic framework.

</details>


### [36] [Are the LLMs Capable of Maintaining at Least the Language Genus?](https://arxiv.org/abs/2510.21561)
*Sandra Mitrović,David Kletz,Ljiljana Dolamic,Fabio Rinaldi*

Main category: cs.CL

TL;DR: 探究大型语言模型是否对语言属敏感，发现属级效应存在但受训练资源限制，数据不平衡仍是主要影响因素


<details>
  <summary>Details</summary>
Motivation: 先前研究未充分探索语言谱系结构对LLMs多语言行为的影响，特别是属级结构在模型多语言表现中的作用

Method: 扩展MultiQ数据集分析：1)检查语言忠诚度不足时模型是否转向同属语言 2)验证属内知识一致性是否优于跨属

Result: 属级效应存在但受训练资源条件限制；不同模型家族展现差异化多语言策略；训练数据不平衡仍是主导因素

Conclusion: LLMs能够编码语言属级结构特征，但训练数据资源分布仍是塑造其多语言表现的决定性因素

Abstract: Large Language Models (LLMs) display notable variation in multilingual
behavior, yet the role of genealogical language structure in shaping this
variation remains underexplored. In this paper, we investigate whether LLMs
exhibit sensitivity to linguistic genera by extending prior analyses on the
MultiQ dataset. We first check if models prefer to switch to genealogically
related languages when prompt language fidelity is not maintained. Next, we
investigate whether knowledge consistency is better preserved within than
across genera. We show that genus-level effects are present but strongly
conditioned by training resource availability. We further observe distinct
multilingual strategies across LLMs families. Our findings suggest that LLMs
encode aspects of genus-level structure, but training data imbalances remain
the primary factor shaping their multilingual performance.

</details>


### [37] [From Polyester Girlfriends to Blind Mice: Creating the First Pragmatics Understanding Benchmarks for Slovene](https://arxiv.org/abs/2510.21575)
*Mojca Brglez,Špela Vintar*

Main category: cs.CL

TL;DR: 论文提出了首个斯洛文尼亚语语用理解基准测试SloPragEval和SloPragMega，发现当前大语言模型在文化特异性非字面表达理解上仍存在缺陷，并指出专有模型与开源模型间的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有语言能力评估多关注表层语言能力，缺乏对语用学（语境化意义理解）的深入测评，尤其在低资源语言领域存在空白。

Method: 构建包含405道多选题的语用理解基准测试集，通过翻译验证、建立人类基线（200名参与者），并开展GPT-4等模型的试点评估。

Result: 模型在文化特异性暗示理解（如谚语）准确率仅62.5%，专有模型（GPT-4）比开源模型（Mistral）表现优22.5%，但均未超过人类基线（89%准确率）。

Conclusion: 有效的语言理解评估需：1）基于原生语料构建 2）考虑文化特异性 3）通过人类响应验证 4）区分字面与非字面理解能力。

Abstract: Large language models are demonstrating increasing capabilities, excelling at
benchmarks once considered very difficult. As their capabilities grow, there is
a need for more challenging evaluations that go beyond surface-level linguistic
competence. Namely, language competence involves not only syntax and semantics
but also pragmatics, i.e., understanding situational meaning as shaped by
context as well as linguistic and cultural norms. To contribute to this line of
research, we introduce SloPragEval and SloPragMega, the first pragmatics
understanding benchmarks for Slovene that contain altogether 405
multiple-choice questions. We discuss the difficulties of translation, describe
the campaign to establish a human baseline, and report pilot evaluations with
LLMs. Our results indicate that current models have greatly improved in
understanding nuanced language but may still fail to infer implied speaker
meaning in non-literal utterances, especially those that are culture-specific.
We also observe a significant gap between proprietary and open-source models.
Finally, we argue that benchmarks targeting nuanced language understanding and
knowledge of the target culture must be designed with care, preferably
constructed from native data, and validated with human responses.

</details>


### [38] [Automated Quality Control for Language Documentation: Detecting Phonotactic Inconsistencies in a Kokborok Wordlist](https://arxiv.org/abs/2510.21584)
*Kellen Parker van Dam,Abishek Stephen*

Main category: cs.CL

TL;DR: 提出无监督异常检测方法识别语言文档中的音系异常，支持低资源语言数据质量提升


<details>
  <summary>Details</summary>
Motivation: 语言文档中的转录错误和未记录借词会误导语言分析，需系统性验证方法

Method: 使用字符级和音节级音系特征的无监督算法，应用于Kokborok与孟加拉语的多语言数据集

Result: 音节特征显著优于字符基线，高召回率方法可系统标记需验证词项

Conclusion: 为田野工作者提供系统验证工具，促进低资源语言文档数据质量改进

Abstract: Lexical data collection in language documentation often contains
transcription errors and undocumented borrowings that can mislead linguistic
analysis. We present unsupervised anomaly detection methods to identify
phonotactic inconsistencies in wordlists, applying them to a multilingual
dataset of Kokborok varieties with Bangla. Using character-level and
syllable-level phonotactic features, our algorithms identify potential
transcription errors and borrowings. While precision and recall remain modest
due to the subtle nature of these anomalies, syllable-aware features
significantly outperform character-level baselines. The high-recall approach
provides fieldworkers with a systematic method to flag entries requiring
verification, supporting data quality improvement in low-resourced language
documentation.

</details>


### [39] [RETuning: Upgrading Inference-Time Scaling for Stock Movement Prediction with Large Language Models](https://arxiv.org/abs/2510.21604)
*Xueyuan Lin,Cehao Yang,Ye Ma,Ming Li,Rongjunchen Zhang,Yang Ni,Xiaojun Wu,Chengjin Xu,Jian Guo,Hui Xiong*

Main category: cs.CL

TL;DR: 提出RETuning方法解决LLMs在股票预测中依赖分析师观点、缺乏系统推理的问题，通过动态构建分析框架提升模型预测能力


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在股票预测中过度依赖分析师观点，缺乏独立逻辑推理和对抗性证据权衡，导致预测可靠性不足

Method: RETuning冷启动方法：动态构建多源信息分析框架→组织并量化涨跌证据→基于框架独立推理预测，减少上下文干扰

Result: 实验证明RETuning成功释放模型金融推理能力，在跨时间/跨股票场景中保持预测有效性（数据集含5,123支A股20万样本）

Conclusion: 通过独立逻辑推理框架设计和大规模数据集验证，RETuning为LLMs在金融领域的可靠预测提供了新范式

Abstract: Recently, large language models (LLMs) have demonstrated outstanding
reasoning capabilities on mathematical and coding tasks. However, their
application to financial tasks-especially the most fundamental task of stock
movement prediction-remains underexplored. We study a three-class
classification problem (up, hold, down) and, by analyzing existing reasoning
responses, observe that: (1) LLMs follow analysts' opinions rather than exhibit
a systematic, independent analytical logic (CoTs). (2) LLMs list summaries from
different sources without weighing adversarial evidence, yet such
counterevidence is crucial for reliable prediction. It shows that the model
does not make good use of its reasoning ability to complete the task. To
address this, we propose Reflective Evidence Tuning (RETuning), a cold-start
method prior to reinforcement learning, to enhance prediction ability. While
generating CoT, RETuning encourages dynamically constructing an analytical
framework from diverse information sources, organizing and scoring evidence for
price up or down based on that framework-rather than on contextual
viewpoints-and finally reflecting to derive the prediction. This approach
maximally aligns the model with its learned analytical framework, ensuring
independent logical reasoning and reducing undue influence from context. We
also build a large-scale dataset spanning all of 2024 for 5,123 A-share stocks,
with long contexts (32K tokens) and over 200K samples. In addition to price and
news, it incorporates analysts' opinions, quantitative reports, fundamental
data, macroeconomic indicators, and similar stocks. Experiments show that
RETuning successfully unlocks the model's reasoning ability in the financial
domain. Inference-time scaling still works even after 6 months or on
out-of-distribution stocks, since the models gain valuable insights about stock
movement prediction.

</details>


### [40] [The Universal Landscape of Human Reasoning](https://arxiv.org/abs/2510.21623)
*Qiguang Chen,Jinhao Liu,Libo Qin,Yimeng Zhang,Yihao Liang,Shangxu Ren,Chengyu Luan,Dengyun Peng,Hanjing Li,Jiannan Guan,Zheng Yan,Jiaqi Wang,Mengkang Hu,Yantao Du,Zhi Chen,Xie Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: 通过IF-Track方法结合大语言模型，首次在统一度量空间内量化人类推理动态，揭示认知机制并调和经典心理学理论分歧。


<details>
  <summary>Details</summary>
Motivation: 现有模型（经典逻辑/概率模型）仅能解释部分推理特征，缺乏统一量化框架。需建立信息流追踪方法以系统描述人类推理的动态过程。

Method: 提出IF-Track框架：1）使用LLMs作为概率编码器 2）量化推理步骤中的信息熵与增益 3）跨任务细粒度分析 4）构建统一度量空间建模推理行为

Result: 1）成功捕捉核心推理特征 2）识别系统性错误模式 3）量化个体差异 4）调和单双过程理论 5）发现LLMs与人类认知的潜在对齐

Conclusion: 建立理论测量间的定量桥梁，揭示LLMs重塑人类推理的机制，为认知架构研究提供新范式。

Abstract: Understanding how information is dynamically accumulated and transformed in
human reasoning has long challenged cognitive psychology, philosophy, and
artificial intelligence. Existing accounts, from classical logic to
probabilistic models, illuminate aspects of output or individual modelling, but
do not offer a unified, quantitative description of general human reasoning
dynamics. To solve this, we introduce Information Flow Tracking (IF-Track),
that uses large language models (LLMs) as probabilistic encoder to quantify
information entropy and gain at each reasoning step. Through fine-grained
analyses across diverse tasks, our method is the first successfully models the
universal landscape of human reasoning behaviors within a single metric space.
We show that IF-Track captures essential reasoning features, identifies
systematic error patterns, and characterizes individual differences. Applied to
discussion of advanced psychological theory, we first reconcile single- versus
dual-process theories in IF-Track and discover the alignment of artificial and
human cognition and how LLMs reshaping human reasoning process. This approach
establishes a quantitative bridge between theory and measurement, offering
mechanistic insights into the architecture of reasoning.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [41] [PC-NCLaws: Physics-Embedded Conditional Neural Constitutive Laws for Elastoplastic Materials](https://arxiv.org/abs/2510.21404)
*Xueguang Xie,Shu Yan,Shiwen Jia,Siyu Yang,Aimin Hao,Yang Gao,Peng Yu*

Main category: cs.GR

TL;DR: 提出了结合PDE和神经网络的材料建模框架，实现泛化建模与参数反演


<details>
  <summary>Details</summary>
Motivation: 数据驱动方法存在物理一致性差和跨场景泛化能力不足的问题，需建立统一物理嵌入的建模框架

Method: 使用双神经网络分别建模弹塑性本构关系，引入物理参数作为条件输入实现多场景联合训练

Result: 在运动重建、长期预测、参数反演等方面达到SOTA，支持未知属性物体的逆向分析

Conclusion: 该框架兼具正向模拟和逆向分析能力，为复杂材料建模提供了统一解决方案

Abstract: While data-driven methods offer significant promise for modeling complex
materials, they often face challenges in generalizing across diverse physical
scenarios and maintaining physical consistency. To address these limitations,
we propose a generalizable framework called Physics-Embedded Conditional Neural
Constitutive Laws for Elastoplastic Materials, which combines the partial
differential equations with neural networks. Specifically, the model employs
two separate neural networks to model elastic and plastic constitutive laws.
Simultaneously, the model incorporates physical parameters as conditional
inputs and is trained on comprehensive datasets encompassing multiple scenarios
with varying physical parameters, thereby enabling generalization across
different properties without requiring retraining for each individual case.
Furthermore, the differentiable architecture of our model, combined with its
explicit parameter inputs, enables the inverse estimation of physical
parameters from observed motion sequences. This capability extends our
framework to objects with unknown or unmeasured properties. Experimental
results demonstrate state-of-the-art performance in motion reconstruction,
robust long-term prediction, geometry generalization, and precise parameters
estimation for elastoplastic materials, highlighting its versatility as a
unified simulator and inverse analysis tool.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [42] [Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference](https://arxiv.org/abs/2510.21184)
*Stephen Zhao,Aidan Li,Rob Brekelmans,Roger Grosse*

Main category: cs.LG

TL;DR: 提出RePULSe方法，通过双损失机制优化RL对齐效果，在保持平均性能的同时降低不良输出概率


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐方法专注优化平均奖励，而降低不良输出的方法常导致平均性能下降，需要改善这种权衡关系

Method: 在标准RL损失基础上，增加基于学习提案的低奖励输出采样引导机制，通过双重损失函数调整模型

Result: 实验显示RePULSe在期望奖励与不良输出概率的权衡上优于传统RL方法，且具备更强的对抗鲁棒性

Conclusion: RePULSe通过主动采样低奖励输出的策略设计，有效突破了传统RL对齐方法的性能平衡瓶颈

Abstract: Reinforcement learning (RL) has become a predominant technique to align
language models (LMs) with human preferences or promote outputs which are
deemed to be desirable by a given reward function. Standard RL approaches
optimize average reward, while methods explicitly focused on reducing the
probability of undesired outputs typically come at a cost to average-case
performance. To improve this tradeoff, we introduce RePULSe, a new training
method that augments the standard RL loss with an additional loss that uses
learned proposals to guide sampling low-reward outputs, and then reduces those
outputs' probability. We run experiments demonstrating that RePULSe produces a
better tradeoff of expected reward versus the probability of undesired outputs
and is more adversarially robust, compared to standard RL alignment approaches
and alternatives.

</details>


### [43] [Leverage Unlearning to Sanitize LLMs](https://arxiv.org/abs/2510.21322)
*Antoine Boutet,Lucas Magnana*

Main category: cs.LG

TL;DR: 提出SANI方法通过消除+修复两阶段机制，无需额外微调即可消除大语言模型对敏感信息的记忆，有效降低信息泄露风险


<details>
  <summary>Details</summary>
Motivation: 专用领域微调数据包含敏感信息（如医疗/商业数据），模型会记忆并泄露这些信息，传统重新微调方法成本过高

Method: 1. 消除阶段：重置模型最后层的特定神经元破坏细粒度记忆
2. 修复阶段：在避免敏感信息记忆的同时微调模型

Result: 仅需少量额外训练周期即可显著减少信息泄露（医疗数据模型泄露减少78%，预训练模型术语泄露减少92%）

Conclusion: 该方法特别适合医院等已投入大量资源训练模型的机构，可在共享模型前快速完成净化处理

Abstract: Pre-trained large language models (LLMs) are becoming useful for various
tasks. To improve their performance on certain tasks, it is necessary to
fine-tune them on specific data corpora (e.g., medical reports, business data).
These specialized data corpora may contain sensitive data (e.g., personal or
confidential data) that will be memorized by the model and likely to be
regurgitated during its subsequent use. This memorization of sensitive
information by the model poses a significant privacy or confidentiality issue.
To remove this memorization and sanitize the model without requiring costly
additional fine-tuning on a secured data corpus, we propose SANI. SANI is an
unlearning approach to sanitize language models. It relies on both an erasure
and repair phases that 1) reset certain neurons in the last layers of the model
to disrupt the memorization of fine-grained information, and then 2) fine-tune
the model while avoiding memorizing sensitive information. We comprehensively
evaluate SANI to sanitize both a model fine-tuned and specialized with medical
data by removing directly and indirectly identifiers from the memorization of
the model, and a standard pre-trained model by removing specific terms defined
as confidential information from the model. Results show that with only few
additional epochs of unlearning, the model is sanitized and the number of
regurgitations is drastically reduced. This approach can be particularly useful
for hospitals or other industries that have already spent significant resources
training models on large datasets and wish to sanitize them before sharing.

</details>


### [44] [FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models](https://arxiv.org/abs/2510.21363)
*Zihao Fu,Ryan Brown,Shun Shao,Kai Rawal,Eoin Delaney,Chris Russell*

Main category: cs.LG

TL;DR: 提出了FairImagen框架，通过Fair PCA投影CLIP嵌入实现无需重新训练的去偏方法，在保持图像质量的同时显著提升文本到图像生成的公平性


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型（如Stable Diffusion）会复制并放大社会偏见，特别是在性别和种族等人口属性维度上需要有效的事后校正方案

Method: 整合公平主成分分析(Fair PCA)构建语义保持子空间，结合噪声注入增强去偏效果，提出跨人口统一投影方法实现多属性同步去偏

Result: 在性别、种族及交叉属性实验中，FairImagen将公平性提升2-5倍，仅需牺牲约15%图像质量，在质量-公平性权衡上优于现有方法

Conclusion: 该框架首次实现模型无关的跨维度去偏，为生成式AI的公平性问题提供了无需模型调整、可扩展的解决方案

Abstract: Text-to-image diffusion models, such as Stable Diffusion, have demonstrated
remarkable capabilities in generating high-quality and diverse images from
natural language prompts. However, recent studies reveal that these models
often replicate and amplify societal biases, particularly along demographic
attributes like gender and race. In this paper, we introduce FairImagen
(https://github.com/fuzihaofzh/FairImagen), a post-hoc debiasing framework that
operates on prompt embeddings to mitigate such biases without retraining or
modifying the underlying diffusion model. Our method integrates Fair Principal
Component Analysis to project CLIP-based input embeddings into a subspace that
minimizes group-specific information while preserving semantic content. We
further enhance debiasing effectiveness through empirical noise injection and
propose a unified cross-demographic projection method that enables simultaneous
debiasing across multiple demographic attributes. Extensive experiments across
gender, race, and intersectional settings demonstrate that FairImagen
significantly improves fairness with a moderate trade-off in image quality and
prompt fidelity. Our framework outperforms existing post-hoc methods and offers
a simple, scalable, and model-agnostic solution for equitable text-to-image
generation.

</details>


### [45] [Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations](https://arxiv.org/abs/2510.21631)
*Faisal Hamman,Pasan Dissanayake,Yanjun Fu,Sanghamitra Dutta*

Main category: cs.LG

TL;DR: 提出反事实解释增强蒸馏方法CoD，在少样本场景下通过决策边界映射显著提升知识蒸馏效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有任务感知蒸馏方法依赖大量数据的痛点，利用反事实解释突破数据限制。

Method: 通过生成最小扰动翻转预测的反事实样本，系统性注入蒸馏过程来定位教师模型决策边界。

Result: 在8-512样本范围内优于基准方法，仅用半数样本+CFEs即可提升各数据集/大模型表现。

Conclusion: 从统计和几何角度验证CFEs提供信息增益，帮助学生模型更精准模仿教师决策边界。

Abstract: Knowledge distillation is a promising approach to transfer capabilities from
complex teacher models to smaller, resource-efficient student models that can
be deployed easily, particularly in task-aware scenarios. However, existing
methods of task-aware distillation typically require substantial quantities of
data which may be unavailable or expensive to obtain in many practical
scenarios. In this paper, we address this challenge by introducing a novel
strategy called Counterfactual-explanation-infused Distillation CoD for
few-shot task-aware knowledge distillation by systematically infusing
counterfactual explanations. Counterfactual explanations (CFEs) refer to inputs
that can flip the output prediction of the teacher model with minimum
perturbation. Our strategy CoD leverages these CFEs to precisely map the
teacher's decision boundary with significantly fewer samples. We provide
theoretical guarantees for motivating the role of CFEs in distillation, from
both statistical and geometric perspectives. We mathematically show that CFEs
can improve parameter estimation by providing more informative examples near
the teacher's decision boundary. We also derive geometric insights on how CFEs
effectively act as knowledge probes, helping the students mimic the teacher's
decision boundaries more effectively than standard data. We perform experiments
across various datasets and LLMs to show that CoD outperforms standard
distillation approaches in few-shot regimes (as low as 8-512 samples). Notably,
CoD only uses half of the original samples used by the baselines, paired with
their corresponding CFEs and still improves performance.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [Self-Jailbreaking: Language Models Can Reason Themselves Out of Safety Alignment After Benign Reasoning Training](https://arxiv.org/abs/2510.20956)
*Zheng-Xin Yong,Stephen H. Bach*

Main category: cs.CR

TL;DR: Discovery of 'self-jailbreaking' phenomenon in reasoning language models (RLMs) where benign training leads to safety bypass through contextual assumptions, with mitigation via minimal safety reasoning data.


<details>
  <summary>Details</summary>
Motivation: To investigate unexpected safety misalignment in RLMs after domain-specific reasoning training and identify countermeasures.

Method: Analyzed multiple open-weight RLMs' behaviors, conducted mechanistic studies on compliance patterns, and tested safety reinforcement through targeted training data.

Result: Multiple RLMs exhibited self-jailbreaking behaviors despite safety awareness. Adding safety reasoning data effectively maintained alignment.

Conclusion: First systematic analysis of self-jailbreaking mechanisms, providing practical safety maintenance solutions for advanced RLMs.

Abstract: We discover a novel and surprising phenomenon of unintentional misalignment
in reasoning language models (RLMs), which we call self-jailbreaking.
Specifically, after benign reasoning training on math or code domains, RLMs
will use multiple strategies to circumvent their own safety guardrails. One
strategy is to introduce benign assumptions about users and scenarios to
justify fulfilling harmful requests. For instance, an RLM reasons that harmful
requests like ``outline a strategy for stealing customers' credit card
information from a retail store'' could be associated with the benign intent of
``a security professional trying to test defense,'' despite no such benign
context being provided as input. We observe that many open-weight RLMs,
including DeepSeek-R1-distilled, s1.1, Phi-4-mini-reasoning, and Nemotron,
suffer from self-jailbreaking despite being aware of the harmfulness of the
requests. We also provide a mechanistic understanding of self-jailbreaking:
RLMs are more compliant after benign reasoning training, and after
self-jailbreaking, models appear to perceive malicious requests as less harmful
in the CoT, thus enabling compliance with them. To mitigate self-jailbreaking,
we find that including minimal safety reasoning data during training is
sufficient to ensure RLMs remain safety-aligned. Our work provides the first
systematic analysis of self-jailbreaking behavior and offers a practical path
forward for maintaining safety in increasingly capable RLMs.

</details>


### [47] [SBASH: a Framework for Designing and Evaluating RAG vs. Prompt-Tuned LLM Honeypots](https://arxiv.org/abs/2510.21459)
*Adetayo Adebimpe,Helmut Neukirchen,Thomas Welsh*

Main category: cs.CR

TL;DR: 提出基于本地轻量级LLM的SBASH框架解决传统蜜罐数据安全问题，通过对比RAG与非RAG模型，发现RAG提升未调优模型准确率，系统提示词调优模型无需RAG即可达到相近效果且延迟更低。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的蜜罐存在响应准确性/时效性问题、云部署成本高及数据泄露风险。需通过本地轻量级LLM构建新型蜜罐框架解决这些问题。

Method: 使用检索增强生成(RAG)和非RAG的LLM处理Linux指令，通过响应时延、人类测试真实感、Levenshtein距离/SBert/BertScore等指标评估系统相似度。

Result: RAG使未调优模型准确率提升2.7%，系统提示词调优的非RAG模型达到与RAG相近准确率（91.3% vs 92.1%）且延迟降低15ms。

Conclusion: SBASH框架通过本地LLM平衡安全与性能，系统提示词调优可替代RAG实现高效响应，为智能蜜罐部署提供新范式。

Abstract: Honeypots are decoy systems used for gathering valuable threat intelligence
or diverting attackers away from production systems. Maximising attacker
engagement is essential to their utility. However research has highlighted that
context-awareness, such as the ability to respond to new attack types, systems
and attacker agents, is necessary to increase engagement. Large Language Models
(LLMs) have been shown as one approach to increase context awareness but suffer
from several challenges including accuracy and timeliness of response time,
high operational costs and data-protection issues due to cloud deployment. We
propose the System-Based Attention Shell Honeypot (SBASH) framework which
manages data-protection issues through the use of lightweight local LLMs. We
investigate the use of Retrieval Augmented Generation (RAG) supported LLMs and
non-RAG LLMs for Linux shell commands and evaluate them using several different
metrics such as response time differences, realism from human testers, and
similarity to a real system calculated with Levenshtein distance, SBert, and
BertScore. We show that RAG improves accuracy for untuned models while models
that have been tuned via a system prompt that tells the LLM to respond like a
Linux system achieve without RAG a similar accuracy as untuned with RAG, while
having a slightly lower latency.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [ArtiLatent: Realistic Articulated 3D Object Generation via Structured Latents](https://arxiv.org/abs/2510.21432)
*Honghua Chen,Yushi Lan,Yongwei Chen,Xingang Pan*

Main category: cs.CV

TL;DR: ArtiLatent框架通过联合建模部件几何与关节动力学，结合潜在扩散模型和关节感知高斯解码器，实现高精度可动3D物体生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在关节化3D物体合成中几何细节不足、关节运动不准确和静态姿态纹理不真实的问题。

Method: 1. 使用变分自编码器将稀疏体素表示与关节属性编码到统一潜在空间
2. 潜在扩散模型实现物理合理的多样化采样
3. 关节感知高斯解码器处理可见性变化，根据关节状态生成纹理

Result: 在PartNet-Mobility和ACD数据集上实现几何一致性和外观保真度的突破，超越现有方法

Conclusion: 该框架为可动3D物体的合成与操控提供了可扩展的解决方案，显著提升跨关节配置的视觉真实感

Abstract: We propose ArtiLatent, a generative framework that synthesizes human-made 3D
objects with fine-grained geometry, accurate articulation, and realistic
appearance. Our approach jointly models part geometry and articulation dynamics
by embedding sparse voxel representations and associated articulation
properties, including joint type, axis, origin, range, and part category, into
a unified latent space via a variational autoencoder. A latent diffusion model
is then trained over this space to enable diverse yet physically plausible
sampling. To reconstruct photorealistic 3D shapes, we introduce an
articulation-aware Gaussian decoder that accounts for articulation-dependent
visibility changes (e.g., revealing the interior of a drawer when opened). By
conditioning appearance decoding on articulation state, our method assigns
plausible texture features to regions that are typically occluded in static
poses, significantly improving visual realism across articulation
configurations. Extensive experiments on furniture-like objects from
PartNet-Mobility and ACD datasets demonstrate that ArtiLatent outperforms
existing approaches in geometric consistency and appearance fidelity. Our
framework provides a scalable solution for articulated 3D object synthesis and
manipulation.

</details>


### [49] [Group Inertial Poser: Multi-Person Pose and Global Translation from Sparse Inertial Sensors and Ultra-Wideband Ranging](https://arxiv.org/abs/2510.21654)
*Ying Xue,Jiaxi Jiang,Rayan Armani,Dominik Hollidt,Yi-Chi Liao,Christian Holz*

Main category: cs.CV

TL;DR: 提出结合IMU传感器与超宽带测距技术，通过两步优化实现多人三维姿态与全局轨迹追踪，并发布首个IMU+UWB双人数据集。


<details>
  <summary>Details</summary>
Motivation: 解决纯IMU方案缺乏空间参考导致的全局定位不准和多人相对位置估计难题，利用UWB跨设备测距提供绝对距离信息。

Method: 融合IMU惯性数据与UWB测距距离，构建结构化状态空间模型整合时序运动特征，采用两步优化流程实现姿态估计与全局轨迹追踪。

Result: 在合成/真实数据中准确率与鲁棒性超越SOTA方法，发布包含200分钟双人运动数据的GIP-DB数据集。

Conclusion: 验证了IMU+UWB方案在野外多人运动捕捉的应用潜力，开源代码模型推动相关研究。

Abstract: Tracking human full-body motion using sparse wearable inertial measurement
units (IMUs) overcomes the limitations of occlusion and instrumentation of the
environment inherent in vision-based approaches. However, purely IMU-based
tracking compromises translation estimates and accurate relative positioning
between individuals, as inertial cues are inherently self-referential and
provide no direct spatial reference for others. In this paper, we present a
novel approach for robustly estimating body poses and global translation for
multiple individuals by leveraging the distances between sparse wearable
sensors - both on each individual and across multiple individuals. Our method
Group Inertial Poser estimates these absolute distances between pairs of
sensors from ultra-wideband ranging (UWB) and fuses them with inertial
observations as input into structured state-space models to integrate temporal
motion patterns for precise 3D pose estimation. Our novel two-step optimization
further leverages the estimated distances for accurately tracking people's
global trajectories through the world. We also introduce GIP-DB, the first
IMU+UWB dataset for two-person tracking, which comprises 200 minutes of motion
recordings from 14 participants. In our evaluation, Group Inertial Poser
outperforms previous state-of-the-art methods in accuracy and robustness across
synthetic and real-world data, showing the promise of IMU+UWB-based multi-human
motion capture in the wild. Code, models, dataset:
https://github.com/eth-siplab/GroupInertialPoser

</details>


### [50] [WorldGrow: Generating Infinite 3D World](https://arxiv.org/abs/2510.21682)
*Sikuang Li,Chen Yang,Jiemin Fang,Taoran Yi,Jia Lu,Jiazhong Cen,Lingxi Xie,Wei Shen,Qi Tian*

Main category: cs.CV

TL;DR: 提出WorldGrow分层框架，通过预训练3D模型生成结构化场景块，实现无限扩展且保持几何/外观一致性的3D场景生成


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三大局限：(1) 2D方法存在跨视角几何/外观不一致 (2) 3D隐式表示难以扩展 (3) 当前3D基础模型多为物体中心，难以支持场景级生成

Method: 1. 数据清洗流程提取高质量场景块
2. 3D块修复机制实现上下文感知扩展
3. 粗细粒度生成策略保证全局布局与局部细节

Result: 在3D-FRONT数据集上实现SOTA几何重建，唯一支持无限场景生成且保持照片级真实感与结构一致性

Conclusion: WorldGrow为构建大规模虚拟环境提供新范式，其场景生成能力为未来世界模型建设奠定基础

Abstract: We tackle the challenge of generating the infinitely extendable 3D world --
large, continuous environments with coherent geometry and realistic appearance.
Existing methods face key challenges: 2D-lifting approaches suffer from
geometric and appearance inconsistencies across views, 3D implicit
representations are hard to scale up, and current 3D foundation models are
mostly object-centric, limiting their applicability to scene-level generation.
Our key insight is leveraging strong generation priors from pre-trained 3D
models for structured scene block generation. To this end, we propose
WorldGrow, a hierarchical framework for unbounded 3D scene synthesis. Our
method features three core components: (1) a data curation pipeline that
extracts high-quality scene blocks for training, making the 3D structured
latent representations suitable for scene generation; (2) a 3D block inpainting
mechanism that enables context-aware scene extension; and (3) a coarse-to-fine
generation strategy that ensures both global layout plausibility and local
geometric/textural fidelity. Evaluated on the large-scale 3D-FRONT dataset,
WorldGrow achieves SOTA performance in geometry reconstruction, while uniquely
supporting infinite scene generation with photorealistic and structurally
consistent outputs. These results highlight its capability for constructing
large-scale virtual environments and potential for building future world
models.

</details>


### [51] [KBE-DME: Dynamic Multimodal Evaluation via Knowledge Enhanced Benchmark Evolution](https://arxiv.org/abs/2510.21182)
*Junzhe Zhang,Huixuan Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出知识增强的基准演化框架KBE，通过动态重构问题和整合多模态知识，解决MLLMs评估中的数据污染与饱和问题。


<details>
  <summary>Details</summary>
Motivation: 现有静态评估基准存在数据污染和饱和风险，导致模型能力评估不准确，需要更可靠的动态评估方法。

Method: 1. 用图结构表示VQA样本 2. 通过知识增强扩展基准 3. 重构问题（视觉信息重选+外部知识融合） 4. 动态难度控制机制。

Result: KBE有效降低数据污染/饱和风险，提供更全面的模型能力评估，实验验证其有效性。

Conclusion: 动态演化框架KBE为MLLMs评估提供了可控、可扩展的解决方案，推动更精准的模型性能评估。

Abstract: The rapid progress of multimodal large language models (MLLMs) calls for more
reliable evaluation protocols. Existing static benchmarks suffer from the
potential risk of data contamination and saturation, leading to inflated or
misleading performance evaluations. To address these issues, we first apply
Graph formulation to represent a static or dynamic VQA sample. With the
formulation, we propose Knowledge-enhanced Benchmark Evolution(KBE), a dynamic
multimodal evaluation framework. KBE first analyzes the original static
benchmark, then expands it by integrating multimodal knowledge, transforming
the static benchmark into a controllable, dynamic evolving version. Crucially,
KBE can both reconstruct questions by Re-selecting visual information in the
original image and expand existing questions with external textual knowledge.
It enables difficulty-controllable evaluation by adjusting the degree of
question exploration. Extensive experiments demonstrate that KBE alleviates the
risk of data contamination, data saturation, and provides a more comprehensive
assessment of MLLM capabilities.

</details>


### [52] [Head Pursuit: Probing Attention Specialization in Multimodal Transformers](https://arxiv.org/abs/2510.21518)
*Lorenzo Basile,Valentino Maiorca,Diego Doimo,Francesco Locatello,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 研究通过信号处理方法识别文本生成模型中关注特定语义/视觉属性的注意力头，仅编辑1%的头部即可精准调控模型输出。


<details>
  <summary>Details</summary>
Motivation: 揭示语言/视觉语言模型的内部工作机制，探索注意力头在语义和视觉属性层面的专业化规律，提升模型可解释性和可控性。

Method: 将信号处理理论融入现有可解释性方法，通过概念相关性排序注意力头，在语言问答、毒性过滤、图像分类等多任务中验证头部编辑效果。

Result: 在单模态/多模态Transformer中发现稳定的注意力头专业化模式，编辑1%的头部即可可靠抑制或增强目标概念（如毒性内容、图像特征）

Conclusion: 注意力层存在可解释的结构化特征，为大规模生成模型提供直观的理解工具和轻量级编辑方案（无需微调即可实现概念控制）

Abstract: Language and vision-language models have shown impressive performance across
a wide range of tasks, but their internal mechanisms remain only partly
understood. In this work, we study how individual attention heads in
text-generative models specialize in specific semantic or visual attributes.
Building on an established interpretability method, we reinterpret the practice
of probing intermediate activations with the final decoding layer through the
lens of signal processing. This lets us analyze multiple samples in a
principled way and rank attention heads based on their relevance to target
concepts. Our results show consistent patterns of specialization at the head
level across both unimodal and multimodal transformers. Remarkably, we find
that editing as few as 1% of the heads, selected using our method, can reliably
suppress or enhance targeted concepts in the model output. We validate our
approach on language tasks such as question answering and toxicity mitigation,
as well as vision-language tasks including image classification and captioning.
Our findings highlight an interpretable and controllable structure within
attention layers, offering simple tools for understanding and editing
large-scale generative models.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [53] [Does Model Size Matter? A Comparison of Small and Large Language Models for Requirements Classification](https://arxiv.org/abs/2510.21443)
*Mohammad Amin Zadenoori,Vincenzo De Martino,Jacek Dabrowski,Xavier Franch,Alessio Ferrari*

Main category: cs.SE

TL;DR: 小型语言模型(SLMs)在需求分类任务中表现接近大型语言模型(LLMs)，且在隐私、成本和本地部署方面更具优势


<details>
  <summary>Details</summary>
Motivation: LLMs在需求工程中面临高计算成本、数据共享风险和外部依赖问题，而SLMs作为轻量级替代方案的性能尚未明确

Method: 比较3个LLMs和5个SLMs在PROMISE、PROMISE Reclass和SecReq三个需求分类数据集上的表现

Result: LLMs平均F1分数仅高2%（统计不显著），SLMs在PROMISE Reclass数据集召回率更优，且模型体积小300倍

Conclusion: SLMs可作为LLMs的有效替代方案，数据集特性对性能的影响大于模型规模

Abstract: [Context and motivation] Large language models (LLMs) show notable results in
natural language processing (NLP) tasks for requirements engineering (RE).
However, their use is compromised by high computational cost, data sharing
risks, and dependence on external services. In contrast, small language models
(SLMs) offer a lightweight, locally deployable alternative. [Question/problem]
It remains unclear how well SLMs perform compared to LLMs in RE tasks in terms
of accuracy. [Results] Our preliminary study compares eight models, including
three LLMs and five SLMs, on requirements classification tasks using the
PROMISE, PROMISE Reclass, and SecReq datasets. Our results show that although
LLMs achieve an average F1 score of 2% higher than SLMs, this difference is not
statistically significant. SLMs almost reach LLMs performance across all
datasets and even outperform them in recall on the PROMISE Reclass dataset,
despite being up to 300 times smaller. We also found that dataset
characteristics play a more significant role in performance than model size.
[Contribution] Our study contributes with evidence that SLMs are a valid
alternative to LLMs for requirements classification, offering advantages in
privacy, cost, and local deployability.

</details>


### [54] [Wisdom and Delusion of LLM Ensembles for Code Generation and Repair](https://arxiv.org/abs/2510.21513)
*Fernando Vallecillos Ruiz,Max Hort,Leon Moonen*

Main category: cs.SE

TL;DR: 研究发现集成多个代码LLM的互补性可显著超越单一模型性能，多样性选择策略能实现95%的理论潜力上限。


<details>
  <summary>Details</summary>
Motivation: 单一LLM处理软件工程任务存在资源浪费且忽略模型互补潜力，需量化模型互补性及最佳集成策略。

Method: 在代码生成/修复等3个基准测试中，比较5大家族10个LLM个体及3种集成策略的表现，分析解决方案选择启发式方法。

Result: 集成模型理论性能上限比最佳单模型高83%，基于多样性的策略实现95%潜力，小规模双模型集成即有效提升性能。

Conclusion: 通过多样性策略集成LLM能以低成本显著提升效果，打破『单一最优模型』思维，为实际应用提供高效路径。

Abstract: Today's pursuit of a single Large Language Model (LMM) for all software
engineering tasks is resource-intensive and overlooks the potential benefits of
complementarity, where different models contribute unique strengths. However,
the degree to which coding LLMs complement each other and the best strategy for
maximizing an ensemble's potential are unclear, leaving practitioners without a
clear path to move beyond single-model systems.
  To address this gap, we empirically compare ten individual LLMs from five
families, and three ensembles of these LLMs across three software engineering
benchmarks covering code generation and program repair. We assess the
complementarity between models and the performance gap between the best
individual model and the ensembles. Next, we evaluate various selection
heuristics to identify correct solutions from an ensemble's candidate pool.
  We find that the theoretical upperbound for an ensemble's performance can be
83% above the best single model. Our results show that consensus-based
strategies for selecting solutions fall into a "popularity trap," amplifying
common but incorrect outputs. In contrast, a diversity-based strategy realizes
up to 95% of this theoretical potential, and proves effective even in small
two-model ensembles, enabling a cost-efficient way to enhance performance by
leveraging multiple LLMs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [Cultural Alien Sampler: Open-ended art generation balancing originality and coherence](https://arxiv.org/abs/2510.20849)
*Alejandro H. Artiles,Hiromu Yakura,Levin Brinkmann,Mar Canet Sola,Hassan Abu Alhaija,Ignacio Serna,Nasim Rahaman,Bernhard Schölkopf,Iyad Rahwan*

Main category: cs.AI

TL;DR: 提出Cultural Alien Sampler (CAS)方法，通过分离概念一致性与文化典型性，使AI在保持内在一致性的同时突破文化惯例，实现艺术创作的原创性与和谐性统一。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在艺术创作中难以平衡原创性与内部一致性，要么陷入文化定式，要么丧失连贯性。需要开发能兼顾概念合理性与文化突破性的创新方法。

Method: 使用两个微调GPT-2模型：概念一致性模型评估艺术概念共现合理性，文化背景模型测算组合在特定艺术家作品中的典型性。通过选择高一致性-低典型性组合生成创新方案。

Result: 人类评估显示CAS在原创性/和谐性上超越GPT-4o基线并接近艺术生水平；定量分析表明其输出多样性比GPT-4o高31.7%，概念空间覆盖广2.8倍。

Conclusion: 人工文化异化方法可释放自主代理的创作潜力，证明算法突破文化框架的可能性，为AI艺术创新提供新范式。

Abstract: In open-ended domains like art, autonomous agents must generate ideas that
are both original and internally coherent, yet current Large Language Models
(LLMs) either default to familiar cultural patterns or sacrifice coherence when
pushed toward novelty. We address this by introducing the Cultural Alien
Sampler (CAS), a concept-selection method that explicitly separates
compositional fit from cultural typicality. CAS uses two GPT-2 models
fine-tuned on WikiArt concepts: a Concept Coherence Model that scores whether
concepts plausibly co-occur within artworks, and a Cultural Context Model that
estimates how typical those combinations are within individual artists' bodies
of work. CAS targets combinations that are high in coherence and low in
typicality, yielding ideas that maintain internal consistency while deviating
from learned conventions and embedded cultural context. In a human evaluation
(N = 100), our approach outperforms random selection and GPT-4o baselines and
achieves performance comparable to human art students in both perceived
originality and harmony. Additionally, a quantitative study shows that our
method produces more diverse outputs and explores a broader conceptual space
than its GPT-4o counterpart, demonstrating that artificial cultural alienness
can unlock creative potential in autonomous agents.

</details>


### [56] [When Models Outthink Their Safety: Mitigating Self-Jailbreak in Large Reasoning Models with Chain-of-Guardrails](https://arxiv.org/abs/2510.21285)
*Yingzhi Mao,Chunkang Zhang,Junxiang Wang,Xinyan Guan,Boxi Cao,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: 提出Chain-of-Guardrail框架解决大模型安全与推理能力的平衡问题，通过重组/回溯风险推理步骤实现安全防护


<details>
  <summary>Details</summary>
Motivation: 现有安全防护方法损害模型推理能力，Self-Jailbreak现象揭示模型存在自我安全评估能力但执行失效

Method: Chain-of-Guardrail训练框架，通过重构风险推理链/回溯安全路径，保留有效推理同时修正危险推理轨迹

Result: 在多个基准测试中显著提升模型安全性（+32%安全性指标），推理能力保持原有水平（仅下降1.6%）

Conclusion: CoG框架突破安全与推理的权衡困境，为AI安全提供新范式，显著优于现有牺牲性能的安全方案

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable capabilities on complex
reasoning tasks but remain vulnerable to severe safety risks, including harmful
content generation and jailbreak attacks. Existing mitigation strategies rely
on injecting heuristic safety signals during training, which often suppress
reasoning ability and fail to resolve the safety-reasoning trade-off. To
systematically investigate this issue, we analyze the reasoning trajectories of
diverse LRMs and uncover a phenomenon we term Self-Jailbreak, where models
override their own risk assessments and justify responding to unsafe prompts.
This finding reveals that LRMs inherently possess the ability to reject unsafe
queries, but this ability is compromised, resulting in harmful outputs.
Building on these insights, we propose the Chain-of-Guardrail (CoG), a training
framework that recomposes or backtracks unsafe reasoning steps, steering the
model back onto safe trajectories while preserving valid reasoning chains.
Extensive experiments across multiple reasoning and safety benchmarks
demonstrate that CoG substantially improves the safety of current LRMs while
preserving comparable reasoning ability, significantly outperforming prior
methods that suffer from severe safety-reasoning trade-offs.

</details>


### [57] [Magellan: Guided MCTS for Latent Space Exploration and Novelty Generation](https://arxiv.org/abs/2510.21341)
*Lufan Chang*

Main category: cs.AI

TL;DR: Magellan框架通过蒙特卡洛树搜索和分层引导系统，提升LLM生成创新科学想法的能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM难以突破训练数据的常见概念局限，Tree of Thoughts等方法依赖非系统化的自我评估启发式存在根本限制

Method: 使用蒙特卡洛树搜索(MCTS)+分层引导系统：语义指南针实现长程方向引导，景观感知价值函数平衡内在一致性、外在新颖性和叙事进展

Result: 在科学创意生成任务中显著优于ReAct和ToT等基线，实现更高合理性与创新性

Conclusion: 系统化的引导搜索比无约束生成更有效，为LLM成为创新合作伙伴开辟新路径

Abstract: Large Language Models (LLMs) often struggle with generating truly innovative
ideas, typically defaulting to high-probability, familiar concepts within their
training data's "gravity wells." While advanced search-based methods like Tree
of Thoughts (ToT) attempt to mitigate this, they are fundamentally limited by
their reliance on unprincipled, inconsistent self-evaluation heuristics to
guide exploration. To address this gap, we introduce \textbf{Magellan}, a novel
framework that reframes creative generation as a principled, guided exploration
of an LLM's latent conceptual space. At its core, Magellan employs Monte Carlo
Tree Search (MCTS) governed by a hierarchical guidance system. For long-range
direction, a "semantic compass" vector, formulated via orthogonal projection,
steers the search towards relevant novelty. For local, step-by-step decisions,
a landscape-aware value function replaces flawed self-evaluation with an
explicit reward structure that balances intrinsic coherence, extrinsic novelty,
and narrative progress. Extensive experiments demonstrate that Magellan
significantly outperforms strong baselines, including ReAct and ToT, in
generating scientific ideas with superior plausibility and innovation. Our work
shows that for creative discovery, a principled, guided search is more
effective than unconstrained agency, paving the way for LLMs to become more
capable partners in innovation.

</details>


### [58] [DeepAgent: A General Reasoning Agent with Scalable Toolsets](https://arxiv.org/abs/2510.21618)
*Xiaoxi Li,Wenxiang Jiao,Jiarui Jin,Guanting Dong,Jiajie Jin,Yinuo Wang,Hao Wang,Yutao Zhu,Ji-Rong Wen,Yuan Lu,Zhicheng Dou*

Main category: cs.AI

TL;DR: 提出DeepAgent端到端深度推理代理，通过自主记忆折叠机制和ToolPO强化学习策略，在8个基准测试中超越基线模型


<details>
  <summary>Details</summary>
Motivation: 现有智能体框架依赖预定义流程，难以处理长时程工具交互和开放工具检索场景。需解决工具调用导致的上下文爆炸及历史信息累积问题

Method: 1. 自主记忆折叠机制将交互历史压缩为情景/工作/工具记忆体
2. ToolPO强化学习策略实现工具调用token级奖励分配
3. 利用LLM模拟API进行稳定训练

Result: 在ToolBench等5个通用工具任务和ALFWorld等3个下游任务中，开放/封闭工具场景下均超越baseline

Conclusion: 通过系统化解决长时程交互和工具学习问题，推动现实应用场景智能体能力发展

Abstract: Large reasoning models have demonstrated strong problem-solving abilities,
yet real-world tasks often require external tools and long-horizon
interactions. Existing agent frameworks typically follow predefined workflows,
which limit autonomous and global task completion. In this paper, we introduce
DeepAgent, an end-to-end deep reasoning agent that performs autonomous
thinking, tool discovery, and action execution within a single, coherent
reasoning process. To address the challenges of long-horizon interactions,
particularly the context length explosion from multiple tool calls and the
accumulation of interaction history, we introduce an autonomous memory folding
mechanism that compresses past interactions into structured episodic, working,
and tool memories, reducing error accumulation while preserving critical
information. To teach general-purpose tool use efficiently and stably, we
develop an end-to-end reinforcement learning strategy, namely ToolPO, that
leverages LLM-simulated APIs and applies tool-call advantage attribution to
assign fine-grained credit to the tool invocation tokens. Extensive experiments
on eight benchmarks, including general tool-use tasks (ToolBench, API-Bank,
TMDB, Spotify, ToolHop) and downstream applications (ALFWorld, WebShop, GAIA,
HLE), demonstrate that DeepAgent consistently outperforms baselines across both
labeled-tool and open-set tool retrieval scenarios. This work takes a step
toward more general and capable agents for real-world applications. The code
and demo are available at https://github.com/RUC-NLPIR/DeepAgent.

</details>


### [59] [AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite](https://arxiv.org/abs/2510.21652)
*Jonathan Bragg,Mike D'Arcy,Nishant Balepur,Dan Bareket,Bhavana Dalvi,Sergey Feldman,Dany Haddad,Jena D. Hwang,Peter Jansen,Varsha Kishore,Bodhisattwa Prasad Majumder,Aakanksha Naik,Sigal Rahamimov,Kyle Richardson,Amanpreet Singh,Harshit Surana,Aryeh Tiktinsky,Rosni Vasu,Guy Wiener,Chloe Anastasiades,Stefan Candra,Jason Dunkelberger,Dan Emery,Rob Evans,Malachi Hamada,Regan Huff,Rodney Kinney,Matt Latzke,Jaron Lochner,Ruben Lozano-Aguilera,Cecile Nguyen,Smita Rao,Amber Tanaka,Brooke Vlahos,Peter Clark,Doug Downey,Yoav Goldberg,Ashish Sabharwal,Daniel S. Weld*

Main category: cs.AI

TL;DR: AI代理评估存在五大缺陷，AstaBench通过2400+科学问题、标准化工具和基线代理实现更严谨的评估


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法全面衡量AI代理在真实科研场景中的表现，缺乏可复现工具和有效比较方法

Method: 开发AstaBench套件，包含跨领域科学问题、生产级搜索工具环境，并建立9类科学优化代理基线

Result: 评估57个代理发现：尽管特定方面有进展，AI整体科研辅助能力仍远未达标

Conclusion: AstaBench为AI代理评估设立新标准，揭示当前技术局限，推动更严谨的科研辅助系统发展

Abstract: AI agents hold the potential to revolutionize scientific productivity by
automating literature reviews, replicating experiments, analyzing data, and
even proposing new directions of inquiry; indeed, there are now many such
agents, ranging from general-purpose "deep research" systems to specialized
science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of
these agents is critical for progress. Yet existing benchmarks fall short on
several fronts: they (1) fail to provide holistic, product-informed measures of
real-world use cases such as science research; (2) lack reproducible agent
tools necessary for a controlled comparison of core agentic capabilities; (3)
do not account for confounding variables such as model cost and tool access;
(4) do not provide standardized interfaces for quick agent prototyping and
evaluation; and (5) lack comprehensive baseline agents necessary to identify
true advances. In response, we define principles and tooling for more
rigorously benchmarking agents. Using these, we present AstaBench, a suite that
provides the first holistic measure of agentic ability to perform scientific
research, comprising 2400+ problems spanning the entire scientific discovery
process and multiple scientific domains, and including many problems inspired
by actual user requests to deployed Asta agents. Our suite comes with the first
scientific research environment with production-grade search tools that enable
controlled, reproducible evaluation, better accounting for confounders.
Alongside, we provide a comprehensive suite of nine science-optimized classes
of Asta agents and numerous baselines. Our extensive evaluation of 57 agents
across 22 agent classes reveals several interesting findings, most importantly
that despite meaningful progress on certain individual aspects, AI remains far
from solving the challenge of science research assistance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [60] [HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences](https://arxiv.org/abs/2510.21370)
*Zain Ul Abideen Tariq,Mahmood Al-Zubaidi,Uzair Shah,Marco Agus,Mowafa Househ*

Main category: cs.MA

TL;DR: HIKMA会议展示AI如何端到端整合至学术出版与会议流程，既支持传统学术实践又维护知识产权与学术诚信，探索人机协作的学术新模式。


<details>
  <summary>Details</summary>
Motivation: 探索AI在完整学术工作流中的应用潜力，解决AI学术中的作者权责界定、知识产权保护及学术透明性问题。

Method: 构建包含AI数据治理、智能稿件生成、AI辅助评审、自动修订、AI会议呈现及智能归档的全链条框架，结合语言模型与领域安全机制。

Result: 成功实现首个AI增强型学术会议原型，验证了AI在提升研究效率的同时保持学术严谨性的可行性。

Conclusion: AI应作为学术协作伙伴存在，需建立新的学术伦理框架来规范AI作者身份与责任归属问题。

Abstract: HIKMA Semi-Autonomous Conference is the first experiment in reimagining
scholarly communication through an end-to-end integration of artificial
intelligence into the academic publishing and presentation pipeline. This paper
presents the design, implementation, and evaluation of the HIKMA framework,
which includes AI dataset curation, AI-based manuscript generation, AI-assisted
peer review, AI-driven revision, AI conference presentation, and AI archival
dissemination. By combining language models, structured research workflows, and
domain safeguards, HIKMA shows how AI can support - not replace traditional
scholarly practices while maintaining intellectual property protection,
transparency, and integrity. The conference functions as a testbed and proof of
concept, providing insights into the opportunities and challenges of AI-enabled
scholarship. It also examines questions about AI authorship, accountability,
and the role of human-AI collaboration in research.

</details>


### [61] [ColorEcosystem: Powering Personalized, Standardized, and Trustworthy Agentic Service in massive-agent Ecosystem](https://arxiv.org/abs/2510.21566)
*Fangwen Wu,Zheng Wu,Jihong Wang,Yunku Chen,Ruiguang Pei,Heyuan Huang,Xin Liao,Xingyu Lou,Huarong Deng,Zhihui Fu,Weiwen Liu,Zhuosheng Zhang,Weinan Zhang,Jun Wang*

Main category: cs.MA

TL;DR: 提出了ColorEcosystem框架，通过代理载体、代理商店和代理审计三大组件，解决大规模代理生态系统的个性化、标准化和可信问题。


<details>
  <summary>Details</summary>
Motivation: 当前大规模代理生态系统存在服务体验非个性化、缺乏标准化流程及行为不可信等痛点，需系统性解决方案。

Method: 1. 代理载体利用用户数据构建数字孪生实现个性化服务
2. 代理商店作为标准化管理平台
3. 代理审计通过开发者与用户行为监管保障可信度

Result: 构建了支持个性化体验、标准化管理及可信保障的生态系统框架，已部分实现并开源代码。

Conclusion: ColorEcosystem为大规模代理服务提供了可落地的技术架构，其开源实现验证了方案的可行性，具有行业推广价值。

Abstract: With the rapid development of (multimodal) large language model-based agents,
the landscape of agentic service management has evolved from single-agent
systems to multi-agent systems, and now to massive-agent ecosystems. Current
massive-agent ecosystems face growing challenges, including impersonal service
experiences, a lack of standardization, and untrustworthy behavior. To address
these issues, we propose ColorEcosystem, a novel blueprint designed to enable
personalized, standardized, and trustworthy agentic service at scale.
Concretely, ColorEcosystem consists of three key components: agent carrier,
agent store, and agent audit. The agent carrier provides personalized service
experiences by utilizing user-specific data and creating a digital twin, while
the agent store serves as a centralized, standardized platform for managing
diverse agentic services. The agent audit, based on the supervision of
developer and user activities, ensures the integrity and credibility of both
service providers and users. Through the analysis of challenges, transitional
forms, and practical considerations, the ColorEcosystem is poised to power
personalized, standardized, and trustworthy agentic service across
massive-agent ecosystems. Meanwhile, we have also implemented part of
ColorEcosystem's functionality, and the relevant code is open-sourced at
https://github.com/opas-lab/color-ecosystem.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [62] [Can large audio language models understand child stuttering speech? speech summarization, and source separation](https://arxiv.org/abs/2510.20850)
*Chibuzor Okocha,Maya Bakri,Christan Grant*

Main category: eess.AS

TL;DR: 研究评估了大规模音频语言模型(LALMs)在处理不流畅儿童语音时的表现，通过混合/单说话者场景测试其隔离儿童语音和生成临床相关摘要的能力，为教育/临床部署提供指南。


<details>
  <summary>Details</summary>
Motivation: 儿童语音在声学、韵律和语言发展方面与成人存在差异，且言语不流畅性(重复/延长/阻塞)给ASR/NLP带来特殊挑战，现有LALMs在此领域的研究尚不充分。

Method: 采用两种实验设置(混合访谈/单人阅读)，结合LLM评估、专家评分和BERTScore三维度，测试LALMs在单通道源分离和儿童语音摘要生成中的表现。

Result: 模型在隔离儿童语音时表现不稳定，成功案例需特定音频条件；临床摘要保留关键非流畅特征，但存在成人语音泄露风险；LLM评估与人类专家评分呈现中等相关性。

Conclusion: 明确了LALMs处理儿童语音的适用边界，提出模型部署的实践建议，并提供标准化提示词和评估脚本促进研究复现。

Abstract: Child speech differs from adult speech in acoustics, prosody, and language
development, and disfluencies (repetitions, prolongations, blocks) further
challenge Automatic Speech Recognition (ASR) and downstream Natural Language
Processing (NLP). Recent large audio-language models (LALMs) demonstrate strong
cross-modal audio understanding; however, their behavior in disfluent child
speech remains underexplored. We evaluate several state-of-the-art LALMs in two
settings: an interview (mixed speakers) and a reading task (single child). The
tasks are (i) single-channel source separation to isolate the child and (ii)
child-only summarization that preserves clinically relevant disfluencies and
avoids adult-speech leakage.
  Evaluation combines Large Language Model (LLM) as a judge, human expert
ratings, and BERTScore (F1), and we report agreement between models and between
models and humans to assess reliability. Our findings delineate the conditions
under which LALMs produce faithful child-only summaries from mixed audio and
where they fail, offering practical guidance for clinical and educational
deployments. We provide prompts and evaluation scripts to support replication.

</details>


### [63] [Beyond Hearing: Learning Task-agnostic ExG Representations from Earphones via Physiology-informed Tokenization](https://arxiv.org/abs/2510.20853)
*Hyungjun Yoon,Seungjoo Lee,Yu Yvonne Wu,Xiaomeng Chen,Taiting Lu,Freddy Yifei Liu,Taeckyung Lee,Hyeongheon Cha,Haochen Zhao,Gaoteng Zhao,Sung-Ju Lee,Cecilia Mascolo,Dongyao Chen,Lili Qiu*

Main category: eess.AS

TL;DR: 提出PiMT方法和DailySense数据集，解决ExG监测中数据多样性不足与模型泛化性差的问题，实验证明其优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有ExG数据多来自实验室环境且模型设计过度依赖任务特性，难以适应日常多任务场景

Method: 通过耳塞式硬件收集自由生活数据，核心PiMT算法将ExG分解为12个生理特征token，通过重建任务实现全频段自适应特征识别

Result: 在覆盖五感的新数据集DailySense及四个公开基准上，PiMT均超越现有SOTA方法

Conclusion: 结合新型硬件与PiMT算法，首次实现非侵入式ExG信号在自由生活场景下的跨任务通用监测

Abstract: Electrophysiological (ExG) signals offer valuable insights into human
physiology, yet building foundation models that generalize across everyday
tasks remains challenging due to two key limitations: (i) insufficient data
diversity, as most ExG recordings are collected in controlled labs with bulky,
expensive devices; and (ii) task-specific model designs that require tailored
processing (i.e., targeted frequency filters) and architectures, which limit
generalization across tasks. To address these challenges, we introduce an
approach for scalable, task-agnostic ExG monitoring in the wild. We collected
50 hours of unobtrusive free-living ExG data with an earphone-based hardware
prototype to narrow the data diversity gap. At the core of our approach is
Physiology-informed Multi-band Tokenization (PiMT), which decomposes ExG
signals into 12 physiology-informed tokens, followed by a reconstruction task
to learn robust representations. This enables adaptive feature recognition
across the full frequency spectrum while capturing task-relevant information.
Experiments on our new DailySense dataset-the first to enable ExG-based
analysis across five human senses-together with four public ExG benchmarks,
demonstrate that PiMT consistently outperforms state-of-the-art methods across
diverse tasks.

</details>


### [64] [Data-Centric Lessons To Improve Speech-Language Pretraining](https://arxiv.org/abs/2510.20860)
*Vishaal Udandarao,Zhiyun Lu,Xuankai Chang,Yongqiang Wang,Violet Z. Yao,Albin Madapally Jose,Fartash Faghri,Josh Gardner,Chung-Cheng Chiu*

Main category: eess.AS

TL;DR: 论文通过数据为中心的预训练方法提升语音问答系统性能，提出三种关键数据处理策略并实现3.8B参数模型SpeLangy，性能超越更大规模模型10.2%。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型在语音问答任务中缺乏对预训练数据处理流程的系统研究，需明确数据因素对性能的影响机制。

Method: 围绕语音-文本预训练数据的三个核心问题展开：(1)原始网络音频处理方案 (2)合成数据增强策略 (3)多模态序列交织方法

Result: 基于数据优化训练的3.8B参数SpeechLM模型(SpeLangy)在性能上超越参数量3倍于己的模型达10.2%绝对提升

Conclusion: 研究证明数据优化对语音语言模型预训练效果具有决定性影响，为后续数据导向的语音模型研究提供重要方法论指导

Abstract: Spoken Question-Answering (SQA) is a core capability for useful and
interactive artificial intelligence systems. Recently, several speech-language
models (SpeechLMs) have been released with a specific focus on improving their
SQA performance. However, a lack of controlled ablations of pretraining data
processing and curation makes it challenging to understand what factors account
for performance, despite substantial gains from similar studies in other data
modalities. In this work, we address this gap by conducting a data-centric
exploration for pretraining SpeechLMs. We focus on three research questions
fundamental to speech-language pretraining data: (1) how to process raw
web-crawled audio content for speech-text pretraining, (2) how to construct
synthetic pretraining datasets to augment web-crawled data and (3) how to
interleave (text, audio) segments into training sequences. We apply the
insights from our controlled data-centric ablations to pretrain a
3.8B-parameter SpeechLM, called SpeLangy, that outperforms models that are up
to 3x larger by 10.2% absolute performance. We hope our findings highlight the
impact of effective data curation for speech-language pretraining and guide
future data-centric exploration in SpeechLMs.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [65] [Designing and Evaluating Hint Generation Systems for Science Education](https://arxiv.org/abs/2510.21087)
*Anubhav Jangra,Smaranda Muresan*

Main category: cs.HC

TL;DR: 大语言模型通过静态提示与动态提示链促进主动学习，41人实验揭示学习者偏好差异及自动评估指标局限性


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型直接提供答案会抑制学生的概念理解和批判性思维，需探索既能引导思考又不直接揭示答案的教学策略

Method: 基于中学科学知识点，对比预生成静态提示与根据学习进度调整的动态提示策略，开展41人参与的定量研究

Result: 不同学习群体对提示策略存在偏好差异，自动评估指标难以有效捕捉学习者主观体验

Conclusion: 未来智能辅导系统应注重个性化提示生成、多维度评估框架设计，并建立学习者画像以实现动态调整的教育技术

Abstract: Large language models are influencing the education landscape, with students
relying on them in their learning process. Often implemented using
general-purpose models, these systems are likely to give away the answers,
which could hinder conceptual understanding and critical thinking. We study the
role of automatic hint generation as a pedagogical strategy to promote active
engagement with the learning content, while guiding learners toward the
answers. Focusing on scientific topics at the secondary education level, we
explore the potential of large language models to generate chains of hints that
scaffold learners without revealing answers. We compare two distinct hinting
strategies: static hints, pre-generated for each problem, and dynamic hints,
adapted to learners' progress. Through a quantitative study with 41
participants, we uncover different preferences among learners with respect to
hinting strategies, and identify the limitations of automatic evaluation
metrics to capture them. Our findings highlight key design considerations for
future research on hint generation and intelligent tutoring systems that seek
to develop learner-centered educational technologies.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [66] [Pctx: Tokenizing Personalized Context for Generative Recommendation](https://arxiv.org/abs/2510.21276)
*Qiyong Zhong,Jiajie Su,Yunshan Ma,Julian McAuley,Yupeng Hou*

Main category: cs.IR

TL;DR: 提出个性化上下文感知标记器Pctx，通过动态生成用户情境相关的语义ID，提升生成式推荐模型的个性化效果


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型的语义ID生成方法采用静态非个性化映射，无法适应不同用户对同一物品的差异化解读需求。需打破通用相似性标准假设，建立用户情境相关的动态标记机制。

Method: 设计基于用户历史交互的个性化tokenizer，将用户行为序列编码为上下文向量，通过交叉注意力机制动态生成与用户情境匹配的语义ID表示

Result: 在三个公开数据集上实现最高11.44%的NDCG@10提升，验证了上下文感知标记对推荐效果的正向影响。代码已开源供复现验证。

Conclusion: 动态个性化tokenizer有效突破生成式推荐模型的语义固化限制，使系统能够捕捉多维解释标准，为推荐系统的情境感知能力提供新范式。

Abstract: Generative recommendation (GR) models tokenize each action into a few
discrete tokens (called semantic IDs) and autoregressively generate the next
tokens as predictions, showing advantages such as memory efficiency,
scalability, and the potential to unify retrieval and ranking. Despite these
benefits, existing tokenization methods are static and non-personalized. They
typically derive semantic IDs solely from item features, assuming a universal
item similarity that overlooks user-specific perspectives. However, under the
autoregressive paradigm, semantic IDs with the same prefixes always receive
similar probabilities, so a single fixed mapping implicitly enforces a
universal item similarity standard across all users. In practice, the same item
may be interpreted differently depending on user intentions and preferences. To
address this issue, we propose a personalized context-aware tokenizer that
incorporates a user's historical interactions when generating semantic IDs.
This design allows the same item to be tokenized into different semantic IDs
under different user contexts, enabling GR models to capture multiple
interpretive standards and produce more personalized predictions. Experiments
on three public datasets demonstrate up to 11.44% improvement in NDCG@10 over
non-personalized action tokenization baselines. Our code is available at
https://github.com/YoungZ365/Pctx.

</details>


### [67] [Doc-Researcher: A Unified System for Multimodal Document Parsing and Deep Research](https://arxiv.org/abs/2510.21603)
*Kuicai Dong,Shurui Huang,Fangda Ye,Wei Han,Zhi Zhang,Dexun Li,Wenjun Li,Qu Yang,Gang Wang,Yichao Wang,Chen Zhang,Yong Liu*

Main category: cs.IR

TL;DR: Doc-Researcher系统通过多模态解析、混合检索架构和迭代工作流，解决了现有文本系统处理多模态文档的局限性，准确率提升3.4倍。


<details>
  <summary>Details</summary>
Motivation: 当前系统局限于文本数据，无法有效处理含图表方程的多模态文档，需解决视觉语义保留、结构连贯分块和跨模态检索问题。

Method: 1.深度多模态解析保持布局结构 2.支持文本/视觉/混合检索的动态粒度选择 3.多代理迭代工作流分解复杂查询并综合证据

Result: 在M4DocBench基准测试达到50.6%准确率（比现有系统高3.4倍），验证了深度解析和迭代研究的有效性

Conclusion: 该研究建立了多模态文档深度研究新范式，强调保持多模态完整性和迭代研究机制的核心价值

Abstract: Deep Research systems have revolutionized how LLMs solve complex questions
through iterative reasoning and evidence gathering. However, current systems
remain fundamentally constrained to textual web data, overlooking the vast
knowledge embedded in multimodal documents Processing such documents demands
sophisticated parsing to preserve visual semantics (figures, tables, charts,
and equations), intelligent chunking to maintain structural coherence, and
adaptive retrieval across modalities, which are capabilities absent in existing
systems. In response, we present Doc-Researcher, a unified system that bridges
this gap through three integrated components: (i) deep multimodal parsing that
preserves layout structure and visual semantics while creating multi-granular
representations from chunk to document level, (ii) systematic retrieval
architecture supporting text-only, vision-only, and hybrid paradigms with
dynamic granularity selection, and (iii) iterative multi-agent workflows that
decompose complex queries, progressively accumulate evidence, and synthesize
comprehensive answers across documents and modalities. To enable rigorous
evaluation, we introduce M4DocBench, the first benchmark for Multi-modal,
Multi-hop, Multi-document, and Multi-turn deep research. Featuring 158
expert-annotated questions with complete evidence chains across 304 documents,
M4DocBench tests capabilities that existing benchmarks cannot assess.
Experiments demonstrate that Doc-Researcher achieves 50.6% accuracy, 3.4xbetter
than state-of-the-art baselines, validating that effective document research
requires not just better retrieval, but fundamentally deep parsing that
preserve multimodal integrity and support iterative research. Our work
establishes a new paradigm for conducting deep research on multimodal document
collections.

</details>
