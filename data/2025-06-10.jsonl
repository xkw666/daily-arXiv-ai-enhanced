{"id": "2506.06440", "pdf": "https://arxiv.org/pdf/2506.06440", "abs": "https://arxiv.org/abs/2506.06440", "authors": ["Chuhao Chen", "Zhiyang Dou", "Chen Wang", "Yiming Huang", "Anjun Chen", "Qiao Feng", "Jiatao Gu", "Lingjie Liu"], "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Faithfully reconstructing textured shapes and physical properties from videos\npresents an intriguing yet challenging problem. Significant efforts have been\ndedicated to advancing such a system identification problem in this area.\nPrevious methods often rely on heavy optimization pipelines with a\ndifferentiable simulator and renderer to estimate physical parameters. However,\nthese approaches frequently necessitate extensive hyperparameter tuning for\neach scene and involve a costly optimization process, which limits both their\npracticality and generalizability. In this work, we propose a novel framework,\nVid2Sim, a generalizable video-based approach for recovering geometry and\nphysical properties through a mesh-free reduced simulation based on Linear\nBlend Skinning (LBS), offering high computational efficiency and versatile\nrepresentation capability. Specifically, Vid2Sim first reconstructs the\nobserved configuration of the physical system from video using a feed-forward\nneural network trained to capture physical world knowledge. A lightweight\noptimization pipeline then refines the estimated appearance, geometry, and\nphysical properties to closely align with video observations within just a few\nminutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,\nmesh-free simulation with high efficiency. Extensive experiments demonstrate\nthat our method achieves superior accuracy and efficiency in reconstructing\ngeometry and physical properties from video data."}
{"id": "2506.06462", "pdf": "https://arxiv.org/pdf/2506.06462", "abs": "https://arxiv.org/abs/2506.06462", "authors": ["Nicolás Violante", "Andreas Meuleman", "Alban Gauthier", "Frédo Durand", "Thibault Groueix", "George Drettakis"], "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH Conference Papers 2025. Project site:\n  https://repo-sam.inria.fr/nerphys/splat-and-replace/", "summary": "We leverage repetitive elements in 3D scenes to improve novel view synthesis.\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly\nimproved novel view synthesis but renderings of unseen and occluded parts\nremain low-quality if the training views are not exhaustive enough. Our key\nobservation is that our environment is often full of repetitive elements. We\npropose to leverage those repetitions to improve the reconstruction of\nlow-quality parts of the scene due to poor coverage and occlusions. We propose\na method that segments each repeated instance in a 3DGS reconstruction,\nregisters them together, and allows information to be shared among instances.\nOur method improves the geometry while also accounting for appearance\nvariations across instances. We demonstrate our method on a variety of\nsynthetic and real scenes with typical repetitive elements, leading to a\nsubstantial improvement in the quality of novel view synthesis."}
{"id": "2506.06483", "pdf": "https://arxiv.org/pdf/2506.06483", "abs": "https://arxiv.org/abs/2506.06483", "authors": ["Yao Ni", "Song Wen", "Piotr Koniusz", "Anoop Cherian"], "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by\nadapting the model to generate images containing specific subjects. However,\nexisting fine-tuning methods suffer from two key issues: underfitting, where\nthe model fails to reliably capture subject identity, and overfitting, where it\nmemorizes the subject image and reduces background diversity. To address these\nchallenges, we propose two auxiliary consistency losses for diffusion\nfine-tuning. First, a prior consistency regularization loss ensures that the\npredicted diffusion noise for prior (non-subject) images remains consistent\nwith that of the pretrained model, improving fidelity. Second, a subject\nconsistency regularization loss enhances the fine-tuned model's robustness to\nmultiplicative noise modulated latent code, helping to preserve subject\nidentity while improving diversity. Our experimental results demonstrate that\nincorporating these losses into fine-tuning not only preserves subject identity\nbut also enhances image diversity, outperforming DreamBooth in terms of CLIP\nscores, background variation, and overall visual quality."}
{"id": "2506.06494", "pdf": "https://arxiv.org/pdf/2506.06494", "abs": "https://arxiv.org/abs/2506.06494", "authors": ["Lei Lan", "Zixuan Lu", "Chun Yuan", "Weiwei Xu", "Hao Su", "Huamin Wang", "Chenfanfu Jiang", "Yin Yang"], "title": "JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics", "categories": ["cs.GR"], "comment": null, "summary": "In parallel simulation, convergence and parallelism are often seen as\ninherently conflicting objectives. Improved parallelism typically entails\nlighter local computation and weaker coupling, which unavoidably slow the\nglobal convergence. This paper presents a novel GPU algorithm that achieves\nconvergence rates comparable to fullspace Newton's method while maintaining\ngood parallelizability just like the Jacobi method. Our approach is built on a\nkey insight into the phenomenon of overshoot. Overshoot occurs when a local\nsolver aggressively minimizes its local energy without accounting for the\nglobal context, resulting in a local update that undermines global convergence.\nTo address this, we derive a theoretically second-order optimal solution to\nmitigate overshoot. Furthermore, we adapt this solution into a pre-computable\nform. Leveraging Cubature sampling, our runtime cost is only marginally higher\nthan the Jacobi method, yet our algorithm converges nearly quadratically as\nNewton's method. We also introduce a novel full-coordinate formulation for more\nefficient pre-computation. Our method integrates seamlessly with the\nincremental potential contact method and achieves second-order convergence for\nboth stiff and soft materials. Experimental results demonstrate that our\napproach delivers high-quality simulations and outperforms state-of-the-art GPU\nmethods with 50 to 100 times better convergence."}
{"id": "2506.06331", "pdf": "https://arxiv.org/pdf/2506.06331", "abs": "https://arxiv.org/abs/2506.06331", "authors": ["Qiming Zeng", "Xiao Yan", "Hao Luo", "Yuhao Lin", "Yuxiang Wang", "Fangcheng Fu", "Bo Du", "Quanqing Xu", "Jiawei Jiang"], "title": "How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "By retrieving contexts from knowledge graphs, graph-based retrieval-augmented\ngeneration (GraphRAG) enhances large language models (LLMs) to generate quality\nanswers for user questions. Many GraphRAG methods have been proposed and\nreported inspiring performance in answer quality. However, we observe that the\ncurrent answer evaluation framework for GraphRAG has two critical flaws, i.e.,\nunrelated questions and evaluation biases, which may lead to biased or even\nwrong conclusions on performance. To tackle the two flaws, we propose an\nunbiased evaluation framework that uses graph-text-grounded question generation\nto produce questions that are more related to the underlying dataset and an\nunbiased evaluation procedure to eliminate the biases in LLM-based answer\nassessment. We apply our unbiased framework to evaluate 3 representative\nGraphRAG methods and find that their performance gains are much more moderate\nthan reported previously. Although our evaluation framework may still have\nflaws, it calls for scientific evaluations to lay solid foundations for\nGraphRAG research."}
{"id": "2506.07020", "pdf": "https://arxiv.org/pdf/2506.07020", "abs": "https://arxiv.org/abs/2506.07020", "authors": ["Qiujie Dong", "Jiepeng Wang", "Rui Xu", "Cheng Lin", "Yuan Liu", "Shiqing Xin", "Zichun Zhong", "Xin Li", "Changhe Tu", "Taku Komura", "Leif Kobbelt", "Scott Schaefer", "Wenping Wang"], "title": "CrossGen: Learning and Generating Cross Fields for Quad Meshing", "categories": ["cs.GR"], "comment": "Project page: https://anonymousproject-homepage.github.io/", "summary": "Cross fields play a critical role in various geometry processing tasks,\nespecially for quad mesh generation. Existing methods for cross field\ngeneration often struggle to balance computational efficiency with generation\nquality, using slow per-shape optimization. We introduce CrossGen, a novel\nframework that supports both feed-forward prediction and latent generative\nmodeling of cross fields for quad meshing by unifying geometry and cross field\nrepresentations within a joint latent space. Our method enables extremely fast\ncomputation of high-quality cross fields of general input shapes, typically\nwithin one second without per-shape optimization. Our method assumes a\npoint-sampled surface, or called a point-cloud surface, as input, so we can\naccommodate various different surface representations by a straightforward\npoint sampling process. Using an auto-encoder network architecture, we encode\ninput point-cloud surfaces into a sparse voxel grid with fine-grained latent\nspaces, which are decoded into both SDF-based surface geometry and cross\nfields. We also contribute a dataset of models with both high-quality signed\ndistance fields (SDFs) representations and their corresponding cross fields,\nand use it to train our network. Once trained, the network is capable of\ncomputing a cross field of an input surface in a feed-forward manner, ensuring\nhigh geometric fidelity, noise resilience, and rapid inference. Furthermore,\nleveraging the same unified latent representation, we incorporate a diffusion\nmodel for computing cross fields of new shapes generated from partial input,\nsuch as sketches. To demonstrate its practical applications, we validate\nCrossGen on the quad mesh generation task for a large variety of surface\nshapes. Experimental results..."}
{"id": "2506.06343", "pdf": "https://arxiv.org/pdf/2506.06343", "abs": "https://arxiv.org/abs/2506.06343", "authors": ["Taesoo Kim", "Jong Hwan Ko"], "title": "TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in speech-enabled language models have shown promising\nresults in building intelligent voice assistants. However, most existing\napproaches rely on large-scale paired speech-text data and extensive\ncomputational resources, which pose challenges in terms of scalability and\naccessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework\nthat enables training speech-capable language models using only text data. Our\nkey insight is to leverage a unified encoder that maps semantically equivalent\ntext and speech inputs to a shared latent space. By aligning the encoder output\nwith the embedding space of a LLM via a lightweight projection network, we\nenable the model to generalize from text-only supervision to speech-based\ninference. Despite being trained exclusively on text, TESU-LLM achieves strong\nperformance on various speech-related benchmarks, comparable to baseline\nmethods trained with large-scale multimodal datasets and substantial\ncomputational resources. These results highlight the effectiveness and\nefficiency of our approach, offering a scalable path toward building speech\nLLMs without speech data."}
{"id": "2506.07069", "pdf": "https://arxiv.org/pdf/2506.07069", "abs": "https://arxiv.org/abs/2506.07069", "authors": ["Zhican Wang", "Guanghui He", "Dantong Liu", "Lingjun Gao", "Shell Xu Hu", "Chen Zhang", "Zhuoran Song", "Nicholas Lane", "Wayne Luk", "Hongxiang Fan"], "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization", "categories": ["cs.GR", "cs.AR", "cs.CV", "cs.LG"], "comment": "Preprint. Under review", "summary": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field."}
{"id": "2506.06347", "pdf": "https://arxiv.org/pdf/2506.06347", "abs": "https://arxiv.org/abs/2506.06347", "authors": ["Zachary Yang", "Domenico Tullo", "Reihaneh Rabbany"], "title": "Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection", "categories": ["cs.CL", "cs.AI", "I.2.7; J.4"], "comment": "11 pages, 1 figure, 9 Tables, KDD 2025 ADS Track", "summary": "Toxicity detection in gaming communities faces significant scaling challenges\nwhen expanding across multiple games and languages, particularly in real-time\nenvironments where computational efficiency is crucial. We present two key\nfindings to address these challenges while building upon our previous work on\nToxBuster, a BERT-based real-time toxicity detection system. First, we\nintroduce a soft-prompting approach that enables a single model to effectively\nhandle multiple games by incorporating game-context tokens, matching the\nperformance of more complex methods like curriculum learning while offering\nsuperior scalability. Second, we develop an LLM-assisted label transfer\nframework using GPT-4o-mini to extend support to seven additional languages.\nEvaluations on real game chat data across French, German, Portuguese, and\nRussian achieve macro F1-scores ranging from 32.96% to 58.88%, with\nparticularly strong performance in German, surpassing the English benchmark of\n45.39%. In production, this unified approach significantly reduces\ncomputational resources and maintenance overhead compared to maintaining\nseparate models for each game and language combination. At Ubisoft, this model\nsuccessfully identifies an average of 50 players, per game, per day engaging in\nsanctionable behavior."}
{"id": "2506.07209", "pdf": "https://arxiv.org/pdf/2506.07209", "abs": "https://arxiv.org/abs/2506.07209", "authors": ["Lei Li", "Angela Dai"], "title": "HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hoipage.github.io/ Video:\n  https://youtu.be/b1pJU9lKQTE", "summary": "We present HOI-PAGE, a new approach to synthesizing 4D human-object\ninteractions (HOIs) from text prompts in a zero-shot fashion, driven by\npart-level affordance reasoning. In contrast to prior works that focus on\nglobal, whole body-object motion for 4D HOI synthesis, we observe that\ngenerating realistic and diverse HOIs requires a finer-grained understanding --\nat the level of how human body parts engage with object parts. We thus\nintroduce Part Affordance Graphs (PAGs), a structured HOI representation\ndistilled from large language models (LLMs) that encodes fine-grained part\ninformation along with contact relations. We then use these PAGs to guide a\nthree-stage synthesis: first, decomposing input 3D objects into geometric\nparts; then, generating reference HOI videos from text prompts, from which we\nextract part-based motion constraints; finally, optimizing for 4D HOI motion\nsequences that not only mimic the reference dynamics but also satisfy\npart-level contact constraints. Extensive experiments show that our approach is\nflexible and capable of generating complex multi-object or multi-person\ninteraction sequences, with significantly improved realism and text alignment\nfor zero-shot 4D HOI generation."}
{"id": "2506.06371", "pdf": "https://arxiv.org/pdf/2506.06371", "abs": "https://arxiv.org/abs/2506.06371", "authors": ["Panagiotis Koletsis", "Christos Panagiotopoulos", "Georgios Th. Papadopoulos", "Vasilis Efthymiou"], "title": "Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets."}
{"id": "2506.07558", "pdf": "https://arxiv.org/pdf/2506.07558", "abs": "https://arxiv.org/abs/2506.07558", "authors": ["Fabian Lander", "Diaaeldin Taha"], "title": "Immersive Visualization of Flat Surfaces Using Ray Marching", "categories": ["cs.GR", "math.DG", "math.DS", "math.GT", "68U05, 51M20", "I.3.7; I.3.5"], "comment": "Presented at Bridges Math and Art Conference, Eindhoven 2025. Online\n  demo and code available at\n  https://fabianlander.github.io/apps/raymarchingflatsurfacesapp/ and\n  https://github.com/FabianLander/RayMarchingFlatSurfaces", "summary": "We present an effective method for visualizing flat surfaces using ray\nmarching. Our approach provides an intuitive way to explore translation\nsurfaces, mirror rooms, unfolded polyhedra, and translation prisms while\nmaintaining computational efficiency. We demonstrate the utility of the method\nthrough various examples and provide implementation insights for programmers.\nFinally, we discuss the use of our visualizations in outreach. We make our\nsimulations and code available online."}
{"id": "2506.06376", "pdf": "https://arxiv.org/pdf/2506.06376", "abs": "https://arxiv.org/abs/2506.06376", "authors": ["Heng Dong", "Kefei Duan", "Chongjie Zhang"], "title": "Enhancing Decision-Making of Large Language Models via Actor-Critic", "categories": ["cs.CL", "cs.AI"], "comment": "Forty-second International Conference on Machine Learning (ICML 2025)", "summary": "Large Language Models (LLMs) have achieved remarkable advancements in natural\nlanguage processing tasks, yet they encounter challenges in complex\ndecision-making scenarios that require long-term reasoning and alignment with\nhigh-level objectives. Existing methods either rely on short-term\nauto-regressive action generation or face limitations in accurately simulating\nrollouts and assessing outcomes, leading to sub-optimal decisions. This paper\nintroduces a novel LLM-based Actor-Critic framework, termed LAC, that\neffectively improves LLM policies with long-term action evaluations in a\nprincipled and scalable way. Our approach addresses two key challenges: (1)\nextracting robust action evaluations by computing Q-values via token logits\nassociated with positive/negative outcomes, enhanced by future trajectory\nrollouts and reasoning; and (2) enabling efficient policy improvement through a\ngradient-free mechanism. Experiments across diverse environments -- including\nhigh-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),\nand large action spaces (WebShop) -- demonstrate the framework's generality and\nsuperiority over state-of-the-art methods. Notably, our approach achieves\ncompetitive performance using 7B/8B parameter LLMs, even outperforming baseline\nmethods employing GPT-4 in complex tasks. These results underscore the\npotential of integrating structured policy optimization with LLMs' intrinsic\nknowledge to advance decision-making capabilities in multi-step environments."}
{"id": "2506.07657", "pdf": "https://arxiv.org/pdf/2506.07657", "abs": "https://arxiv.org/abs/2506.07657", "authors": ["Zeyu Xiao", "Zhenyi Wu", "Mingyang Sun", "Qipeng Yan", "Yufan Guo", "Zhuoer Liang", "Lihua Zhang"], "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has achieved remarkable success in reconstructing both\nstatic and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\nprimitives, interactions between objects suffer from inaccurate 3D\nsegmentation, imprecise deformation among different materials, and severe\nrendering artifacts. To address these challenges, we introduce PIG:\nPhysically-Based Multi-Material Interaction with 3D Gaussians, a novel approach\nthat combines 3D object segmentation with the simulation of interacting objects\nin high precision. Firstly, our method facilitates fast and accurate mapping\nfrom 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.\nSecondly, we assign unique physical properties to correspondingly segmented\nobjects within the scene for multi-material coupled interactions. Finally, we\nhave successfully embedded constraint scales into deformation gradients,\nspecifically clamping the scaling and rotation properties of the Gaussian\nprimitives to eliminate artifacts and achieve geometric fidelity and visual\nconsistency. Experimental results demonstrate that our method not only\noutperforms the state-of-the-art (SOTA) in terms of visual quality, but also\nopens up new directions and pipelines for the field of physically realistic\nscene generation."}
{"id": "2506.06384", "pdf": "https://arxiv.org/pdf/2506.06384", "abs": "https://arxiv.org/abs/2506.06384", "authors": ["Yi Ji", "Runzhi Li", "Baolei Mao"], "title": "Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KSEM2025 AI & Sec Workshop", "summary": "With the widespread adoption of Large Language Models (LLMs), prompt\ninjection attacks have emerged as a significant security threat. Existing\ndefense mechanisms often face critical trade-offs between effectiveness and\ngeneralizability. This highlights the urgent need for efficient prompt\ninjection detection methods that are applicable across a wide range of LLMs. To\naddress this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion\ndetection framework. It integrates a pretrained language model with heuristic\nfeature engineering to detect prompt injection attacks. Specifically, the\nframework employs DeBERTa-v3-base as a feature extractor to transform input\ntext into semantic vectors enriched with contextual information. In parallel,\nwe design heuristic rules based on known attack patterns to extract explicit\nstructural features commonly observed in attacks. Features from both channels\nare subsequently fused and passed through a fully connected neural network to\nproduce the final prediction. This dual-channel approach mitigates the\nlimitations of relying only on DeBERTa to extract features. Experimental\nresults on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms\nexisting methods in terms of accuracy, recall, and F1-score. Furthermore, when\ndeployed actually, it significantly reduces attack success rates across\nmainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o."}
{"id": "2506.07897", "pdf": "https://arxiv.org/pdf/2506.07897", "abs": "https://arxiv.org/abs/2506.07897", "authors": ["Shuja Khalid", "Mohamed Ibrahim", "Yang Liu"], "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement."}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395", "abs": "https://arxiv.org/abs/2506.06395", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC\nimproves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on\nAMC23. RLSC offers a simple, scalable post-training method for reasoning models\nwith minimal supervision."}
{"id": "2506.07917", "pdf": "https://arxiv.org/pdf/2506.07917", "abs": "https://arxiv.org/abs/2506.07917", "authors": ["Allen Tu", "Haiyang Ying", "Alex Hanson", "Yonghan Lee", "Tom Goldstein", "Matthias Zwicker"], "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes", "categories": ["cs.GR", "cs.CV"], "comment": "Project Page: https://speede3dgs.github.io/", "summary": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve\nhigh-quality novel view synthesis by using neural networks to predict the\ntime-varying deformation of each Gaussian. However, performing per-Gaussian\nneural inference at every frame poses a significant bottleneck, limiting\nrendering speed and increasing memory and compute requirements. In this paper,\nwe present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general\npipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS\nrepresentations by reducing neural inference through two complementary\ntechniques. First, we propose a temporal sensitivity pruning score that\nidentifies and removes Gaussians with low contribution to the dynamic scene\nreconstruction. We also introduce an annealing smooth pruning mechanism that\nimproves pruning robustness in real-world scenes with imprecise camera poses.\nSecond, we propose GroupFlow, a motion analysis technique that clusters\nGaussians by trajectory similarity and predicts a single rigid transformation\nper group instead of separate deformations for each Gaussian. Together, our\ntechniques accelerate rendering by $10.37\\times$, reduce model size by\n$7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset.\nSpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on\nthe D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be\nintegrated into any deformable 3DGS or 4DGS framework."}
{"id": "2506.06396", "pdf": "https://arxiv.org/pdf/2506.06396", "abs": "https://arxiv.org/abs/2506.06396", "authors": ["Christopher D. Molek", "Roberto Fronteddu", "K. Brent Venable", "Niranjan Suri"], "title": "Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of\nBattlefield Things (IoBT), gives rise to new opportunities for enhancing\nsituational awareness. To increase the potential of IoBT for situational\nawareness in critical decision making, the data from these devices must be\nprocessed into consumer-ready information objects, and made available to\nconsumers on demand. To address this challenge we propose a workflow that makes\nuse of natural language processing (NLP) to query a database technology and\nreturn a response in natural language. Our solution utilizes Large Language\nModels (LLMs) that are sized for edge devices to perform NLP as well as\ngraphical databases which are well suited for dynamic connected networks which\nare pervasive in the IoBT. Our architecture employs LLMs for both mapping\nquestions in natural language to Cypher database queries as well as to\nsummarize the database output back to the user in natural language. We evaluate\nseveral medium sized LLMs for both of these tasks on a database representing\npublicly available data from the US Army's Multipurpose Sensing Area (MSA) at\nthe Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion\nparameters) outperforms the other models across all the considered metrics.\nMost importantly, we note that, unlike current methods, our two step approach\nallows the relaxation of the Exact Match (EM) requirement of the produced\nCypher queries with ground truth code and, in this way, it achieves a 19.4%\nincrease in accuracy. Our workflow lays the ground work for deploying LLMs on\nedge devices to enable natural language interactions with databases containing\ninformation objects for critical decision making."}
{"id": "2506.07932", "pdf": "https://arxiv.org/pdf/2506.07932", "abs": "https://arxiv.org/abs/2506.07932", "authors": ["Rishit Dagli", "Yushi Guan", "Sankeerth Durvasula", "Mohammadreza Mofayezi", "Nandita Vijaykumar"], "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object."}
{"id": "2506.06401", "pdf": "https://arxiv.org/pdf/2506.06401", "abs": "https://arxiv.org/abs/2506.06401", "authors": ["Hongming Yang", "Shi Lin", "Jun Shao", "Changting Lin", "Donghai Zhu", "Meng Han", "Qinglei Kong"], "title": "Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work is accepted at ACL 2025", "summary": "Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized\nmodels designed to run efficiently on consumer-grade hardware, offering\nsignificant advantages in resource efficiency, cost-effectiveness, and data\nprivacy. However, these models often struggle with limited inference and\nreasoning capabilities, which restrict their performance on complex tasks and\nlimit their practical applicability. Moreover, existing prompt optimization\nmethods typically rely on extensive manual effort or the meta-cognitive\nabilities of state-of-the-art LLMs, making them less effective for LwLLMs. To\naddress these challenges, we introduce DeBoP, a new Direct Behavior\nOptimization Paradigm, original from the Chain-of-Thought (CoT) prompting\ntechnique. Unlike CoT Prompting, DeBoP is an automatic optimization method,\nwhich focuses on the optimization directly on the behavior of LwLLMs. In\nparticular, DeBoP transforms the optimization of complex prompts into the\noptimization of discrete, quantifiable execution sequences using a\ngradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging\ntasks where state-of-the-art LLMs excel but LwLLMs generally underperform.\nExperimental results demonstrate that DeBoP significantly outperforms recent\nprompt optimization methods on most tasks. In particular, DeBoP-optimized\nLwLLMs surpass GPT-3.5 on most tasks while reducing computational time by\napproximately 60% compared to other automatic prompt optimization methods."}
{"id": "2506.07781", "pdf": "https://arxiv.org/pdf/2506.07781", "abs": "https://arxiv.org/abs/2506.07781", "authors": ["Mart Kartašev", "David Dörner", "Özer Özkahraman", "Petter Ögren", "Ivan Stenius", "John Folkesson"], "title": "SMaRCSim: Maritime Robotics Simulation Modules", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "Developing new functionality for underwater robots and testing them in the\nreal world is time-consuming and resource-intensive. Simulation environments\nallow for rapid testing before field deployment. However, existing tools lack\ncertain functionality for use cases in our project: i) developing\nlearning-based methods for underwater vehicles; ii) creating teams of\nautonomous underwater, surface, and aerial vehicles; iii) integrating the\nsimulation with mission planning for field experiments. A holistic solution to\nthese problems presents great potential for bringing novel functionality into\nthe underwater domain. In this paper we present SMaRCSim, a set of simulation\npackages that we have developed to help us address these issues."}
{"id": "2506.06404", "pdf": "https://arxiv.org/pdf/2506.06404", "abs": "https://arxiv.org/abs/2506.06404", "authors": ["Sooyung Choi", "Jaehyeok Lee", "Xiaoyuan Yi", "Jing Yao", "Xing Xie", "JinYeong Bak"], "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "The application scope of Large Language Models (LLMs) continues to expand,\nleading to increasing interest in personalized LLMs that align with human\nvalues. However, aligning these models with individual values raises\nsignificant safety concerns, as certain values may correlate with harmful\ninformation. In this paper, we identify specific safety risks associated with\nvalue-aligned LLMs and investigate the psychological principles behind these\nchallenges. Our findings reveal two key insights. (1) Value-aligned LLMs are\nmore prone to harmful behavior compared to non-fine-tuned models and exhibit\nslightly higher risks in traditional safety evaluations than other fine-tuned\nmodels. (2) These safety issues arise because value-aligned LLMs genuinely\ngenerate text according to the aligned values, which can amplify harmful\noutcomes. Using a dataset with detailed safety categories, we find significant\ncorrelations between value alignment and safety risks, supported by\npsychological hypotheses. This study offers insights into the \"black box\" of\nvalue alignment and proposes in-context alignment methods to enhance the safety\nof value-aligned LLMs."}
{"id": "2506.06406", "pdf": "https://arxiv.org/pdf/2506.06406", "abs": "https://arxiv.org/abs/2506.06406", "authors": ["Guoyang Xia", "Yifeng Ding", "Fengfa Li", "Lei Ren", "Chen Wei", "Fangxiang Feng", "Xiaojie Wang"], "title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have become a key approach for scaling\nlarge language models, with growing interest in extending them to multimodal\ntasks. Existing methods to build multimodal MoE models either incur high\ntraining costs or suffer from degraded language capabilities when adapting\npretrained models. To address this, we propose Soft ModalityAware Routing\n(SMAR), a novel regularization technique that uses Kullback Leibler divergence\nto control routing probability distributions across modalities, encouraging\nexpert specialization without modifying model architecture or heavily relying\non textual data. Experiments on visual instruction tuning show that SMAR\npreserves language ability at 86.6% retention with only 2.5% pure text,\noutperforming baselines while maintaining strong multimodal performance. Our\napproach offers a practical and efficient solution to balance modality\ndifferentiation and language capabilities in multimodal MoE models."}
{"id": "2506.06446", "pdf": "https://arxiv.org/pdf/2506.06446", "abs": "https://arxiv.org/abs/2506.06446", "authors": ["Ivi Chatzi", "Nina Corvelo Benz", "Stratis Tsirtsis", "Manuel Gomez-Rodriguez"], "title": "Canonical Autoregressive Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "State of the art large language models are trained using large amounts of\ntokens derived from raw text using what is called a tokenizer. Crucially, the\ntokenizer determines the (token) vocabulary a model will use during inference\nas well as, in principle, the (token) language. This is because, while the\ntoken vocabulary may allow for different tokenizations of a string, the\ntokenizer always maps the string to only one of these tokenizations--the\ncanonical tokenization. However, multiple lines of empirical evidence suggest\nthat large language models do not always generate canonical token sequences,\nand this comes with several negative consequences. In this work, we first show\nthat, to generate a canonical token sequence, a model needs to generate\n(partial) canonical token sequences at each step of the autoregressive\ngeneration process underpinning its functioning. Building upon this theoretical\nresult, we introduce canonical sampling, a simple and efficient sampling method\nthat precludes a given model from generating non-canonical token sequences.\nFurther, we also show that, in comparison with standard sampling, the\ndistribution of token sequences generated using canonical sampling is provably\ncloser to the true distribution of token sequences used during training."}
{"id": "2506.06485", "pdf": "https://arxiv.org/pdf/2506.06485", "abs": "https://arxiv.org/abs/2506.06485", "authors": ["Kaiser Sun", "Fan Bai", "Mark Dredze"], "title": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models frequently rely on both contextual input and parametric\nknowledge to perform tasks. However, these sources can come into conflict,\nespecially when retrieved documents contradict the model's parametric\nknowledge. We propose a diagnostic framework to systematically evaluate LLM\nbehavior under context-memory conflict, where the contextual information\ndiverges from their parametric beliefs. We construct diagnostic data that\nelicit these conflicts and analyze model performance across multiple task\ntypes. Our findings reveal that (1) knowledge conflict has minimal impact on\ntasks that do not require knowledge utilization, (2) model performance is\nconsistently higher when contextual and parametric knowledge are aligned, (3)\nmodels are unable to fully suppress their internal knowledge even when\ninstructed, and (4) providing rationales that explain the conflict increases\nreliance on contexts. These insights raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs."}
{"id": "2506.06500", "pdf": "https://arxiv.org/pdf/2506.06500", "abs": "https://arxiv.org/abs/2506.06500", "authors": ["Luyao Shi", "Michael Kazda", "Charles Schmitter", "Hemlata Gupta"], "title": "Improving LLM-Powered EDA Assistants with RAFT", "categories": ["cs.CL"], "comment": "Accepted paper at IEEE International Conference on LLM-Aided Design,\n  2025 (LAD 2025)", "summary": "Electronic design engineers often struggle to efficiently access relevant\ninformation for tasks like design verification and technology development.\nWhile large language models (LLMs) can enhance productivity as conversational\nagents, pre-trained open-source LLMs lack domain-specific knowledge for\nElectronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)\ncontext, LLMs rely on external context but may still produce inaccurate\nresponses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but\nacquiring labeled question/answer (Q/A) data in EDA is difficult. To address\nthis, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our\nresults show that RAFT with synthetic data significantly boosts LLM performance\nfor RAG-based EDA tasks. We also investigate the impact of using real user\nquestions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data\ngeneration. Additionally, we implement secure access control to ensure\nsensitive information is only accessible to authorized personnel. Finally, we\nassess the risk of data leakage and unintended memorization during fine-tuning\nwith synthetic data, providing practical insights."}
{"id": "2506.06506", "pdf": "https://arxiv.org/pdf/2506.06506", "abs": "https://arxiv.org/abs/2506.06506", "authors": ["Kshitish Ghate", "Tessa Charlesworth", "Mona Diab", "Aylin Caliskan"], "title": "Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes", "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "To build fair AI systems we need to understand how social-group biases\nintrinsic to foundational encoder-based vision-language models (VLMs) manifest\nin biases in downstream tasks. In this study, we demonstrate that intrinsic\nbiases in VLM representations systematically ``carry over'' or propagate into\nzero-shot retrieval tasks, revealing how deeply rooted biases shape a model's\noutputs. We introduce a controlled framework to measure this propagation by\ncorrelating (a) intrinsic measures of bias in the representational space with\n(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and\nimage-to-text (ITT) retrieval. Results show substantial correlations between\nintrinsic and extrinsic bias, with an average $\\rho$ = 0.83 $\\pm$ 0.10. This\npattern is consistent across 114 analyses, both retrieval directions, six\nsocial groups, and three distinct VLMs. Notably, we find that\nlarger/better-performing models exhibit greater bias propagation, a finding\nthat raises concerns given the trend towards increasingly complex AI models.\nOur framework introduces baseline evaluation tasks to measure the propagation\nof group and valence signals. Investigations reveal that underrepresented\ngroups experience less robust propagation, further skewing their model-related\noutcomes."}
{"id": "2506.06522", "pdf": "https://arxiv.org/pdf/2506.06522", "abs": "https://arxiv.org/abs/2506.06522", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Syed Zawad", "Farhan Ahmed", "Heiko Ludwig", "Holger Boche"], "title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work on large language models (LLMs) has increasingly focused on\npost-training and alignment with datasets curated to enhance instruction\nfollowing, world knowledge, and specialized skills. However, most post-training\ndatasets used in leading open- and closed-source LLMs remain inaccessible to\nthe public, with limited information about their construction process. This\nlack of transparency has motivated the recent development of open-source\npost-training corpora. While training on these open alternatives can yield\nperformance comparable to that of leading models, systematic comparisons remain\nchallenging due to the significant computational cost of conducting them\nrigorously at scale, and are therefore largely absent. As a result, it remains\nunclear how specific samples, task types, or curation strategies influence\ndownstream performance when assessing data quality. In this work, we conduct\nthe first comprehensive side-by-side analysis of two prominent open\npost-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie\nframework, we annotate each sample with detailed quality metrics, including\nturn structure (single-turn vs. multi-turn), task category, input quality, and\nresponse quality, and we derive statistics that reveal structural and\nqualitative similarities and differences between the two datasets. Based on\nthese insights, we design a principled curation recipe that produces a new data\nmixture, TuluTalk, which contains 14% fewer samples than either source dataset\nwhile matching or exceeding their performance on key benchmarks. Our findings\noffer actionable insights for constructing more effective post-training\ndatasets that improve model performance within practical resource limits. To\nsupport future research, we publicly release both the annotated source datasets\nand our curated TuluTalk mixture."}
{"id": "2506.06539", "pdf": "https://arxiv.org/pdf/2506.06539", "abs": "https://arxiv.org/abs/2506.06539", "authors": ["Yijie Hao", "Haofei Yu", "Jiaxuan You"], "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "When exposed to complex queries containing multiple conditions, today's large\nlanguage models (LLMs) tend to produce responses that only partially satisfy\nthe query while neglecting certain conditions. We therefore introduce the\nconcept of Intent Hallucination. In this phenomenon, LLMs either omit\n(neglecting to address certain parts) or misinterpret (responding to invented\nquery parts) elements of the given query, leading to intent hallucinated\ngeneration. To systematically evaluate intent hallucination, we introduce\nFAITHQA, a novel benchmark for intent hallucination that contains 20,068\nproblems, covering both query-only and retrieval-augmented generation (RAG)\nsetups with varying topics and difficulty. FAITHQA is the first hallucination\nbenchmark that goes beyond factual verification, tailored to identify the\nfundamental cause of intent hallucination. By evaluating various LLMs on\nFAITHQA, we find that (1) intent hallucination is a common issue even for\nstate-of-the-art models, and (2) the phenomenon stems from omission or\nmisinterpretation of LLMs. To facilitate future research, we introduce an\nautomatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting\nintent hallucination. Human evaluation results demonstrate that CONSTRAINT\nSCORE is closer to human performance for intent hallucination compared to\nbaselines."}
{"id": "2506.06561", "pdf": "https://arxiv.org/pdf/2506.06561", "abs": "https://arxiv.org/abs/2506.06561", "authors": ["Ho Yin 'Sam' Ng", "Ting-Yao Hsu", "Aashish Anantha Ramakrishnan", "Branislav Kveton", "Nedim Lipka", "Franck Dernoncourt", "Dongwon Lee", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Ting-Hao 'Kenneth' Huang"], "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones."}
{"id": "2506.06589", "pdf": "https://arxiv.org/pdf/2506.06589", "abs": "https://arxiv.org/abs/2506.06589", "authors": ["Jacqueline He", "Howard Yen", "Margaret Li", "Shuyue Stella Li", "Zhiyuan Zeng", "Weijia Shi", "Yulia Tsvetkov", "Danqi Chen", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "Precise Information Control in Long-Form Text Generation", "categories": ["cs.CL"], "comment": "56 pages, 8 figures. Code and models are publicly available at\n  https://github.com/jacqueline-he/precise-information-control", "summary": "A central challenge in modern language models (LMs) is intrinsic\nhallucination: the generation of information that is plausible but\nunsubstantiated relative to input context. To study this problem, we propose\nPrecise Information Control (PIC), a new task formulation that requires models\nto generate long-form outputs grounded in a provided set of short\nself-contained statements, known as verifiable claims, without adding any\nunsupported ones. For comprehensiveness, PIC includes a full setting that tests\na model's ability to include exactly all input claims, and a partial setting\nthat requires the model to selectively incorporate only relevant claims. We\npresent PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,\nsummarization, biography generation) adapted to the PIC setting, where LMs are\nsupplied with well-formed, verifiable input claims. Our evaluation of a range\nof open and proprietary LMs on PIC-Bench reveals that, surprisingly,\nstate-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To\nalleviate this lack of faithfulness, we introduce a post-training framework,\nusing a weakly supervised preference data construction method, to train an 8B\nPIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full\nPIC setting. When integrated into end-to-end factual generation pipelines,\nPIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and\nfactual precision by 30.5% on a birthplace verification task, underscoring the\npotential of precisely grounded generation."}
{"id": "2506.06605", "pdf": "https://arxiv.org/pdf/2506.06605", "abs": "https://arxiv.org/abs/2506.06605", "authors": ["Xiao Wang", "Mengjue Tan", "Qiao Jin", "Guangzhi Xiong", "Yu Hu", "Aidong Zhang", "Zhiyong Lu", "Minjia Zhang"], "title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based medical question-answering systems lack citation\ngeneration and evaluation capabilities, raising concerns about their adoption\nin practice. In this work, we introduce \\name, the first end-to-end framework\nthat facilitates the design and evaluation of citation generation with LLMs for\nmedical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation\nmethod that generates high-quality citations. Our evaluation highlights the\nchallenges and opportunities of citation generation for medical tasks, while\nidentifying important design choices that have a significant impact on the\nfinal citation quality. Our proposed method achieves superior citation\nprecision and recall improvements compared to strong baseline methods, and we\nshow that evaluation results correlate well with annotation results from\nprofessional experts."}
{"id": "2506.06607", "pdf": "https://arxiv.org/pdf/2506.06607", "abs": "https://arxiv.org/abs/2506.06607", "authors": ["Charles Goddard", "Fernando Fernandes Neto"], "title": "Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a training-free method to transplant tokenizers in pretrained\nlarge language models (LLMs) by reconstructing unseen token embeddings via\nOrthogonal Matching Pursuit (OMP). Specifically, we approximate each\nout-of-vocabulary token as a sparse linear combination of shared tokens, in two\nphases: first, compute each new token's representation in the donor embedding\nspace with a small dictionary of shared anchor tokens, then transfer these same\nsparse coefficients back into the base model's embedding space.\n  On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and\nQwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of\nthe base model's performance across multiple benchmarks, while other zero-shot\napproaches degrade significantly. Compared to baselines (zero-init, mean-init,\nand existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves\nthe best overall performance, effectively bridging large tokenizer\ndiscrepancies without gradient updates. Our analysis further identifies\nmismatched numerical tokenization schemes as a critical challenge for\npreserving mathematical reasoning capabilities. This technique enables direct\nreuse of pretrained model weights with new tokenizers, facilitating\ncross-tokenizer knowledge distillation, speculative decoding, ensembling,\nmerging, and domain-specific vocabulary adaptations. We integrate our method\ninto the open-source mergekit-tokensurgeon tool for post hoc vocabulary\nrealignment."}
{"id": "2506.06609", "pdf": "https://arxiv.org/pdf/2506.06609", "abs": "https://arxiv.org/abs/2506.06609", "authors": ["Alan Chen", "Jack Merullo", "Alessandro Stolfo", "Ellie Pavlick"], "title": "Transferring Features Across Language Models With Model Stitching", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we demonstrate that affine mappings between residual streams of\nlanguage models is a cheap way to effectively transfer represented features\nbetween models. We apply this technique to transfer the weights of Sparse\nAutoencoders (SAEs) between models of different sizes to compare their\nrepresentations. We find that small and large models learn highly similar\nrepresentation spaces, which motivates training expensive components like SAEs\non a smaller model and transferring to a larger model at a FLOPs savings. For\nexample, using a small-to-large transferred SAE as initialization can lead to\n50% cheaper training runs when training SAEs on larger models. Next, we show\nthat transferred probes and steering vectors can effectively recover ground\ntruth performance. Finally, we dive deeper into feature-level transferability,\nfinding that semantic and structural features transfer noticeably differently\nwhile specific classes of functional features have their roles faithfully\nmapped. Overall, our findings illustrate similarities and differences in the\nlinear representation spaces of small and large models and demonstrate a method\nfor improving the training efficiency of SAEs."}
{"id": "2506.06616", "pdf": "https://arxiv.org/pdf/2506.06616", "abs": "https://arxiv.org/abs/2506.06616", "authors": ["Samuel Kim", "Oghenemaro Imieye", "Yunting Yin"], "title": "Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings", "categories": ["cs.CL"], "comment": "Submitted to the IEEE EMBS BHI 2025 Conference", "summary": "Accurate and interpretable detection of depressive language in social media\nis useful for early interventions of mental health conditions, and has\nimportant implications for both clinical practice and broader public health\nefforts. In this paper, we investigate the performance of large language models\n(LLMs) and traditional machine learning classifiers across three classification\ntasks involving social media data: binary depression classification, depression\nseverity classification, and differential diagnosis classification among\ndepression, PTSD, and anxiety. Our study compares zero-shot LLMs with\nsupervised classifiers trained on both conventional text embeddings and\nLLM-generated summary embeddings. Our experiments reveal that while zero-shot\nLLMs demonstrate strong generalization capabilities in binary classification,\nthey struggle with fine-grained ordinal classifications. In contrast,\nclassifiers trained on summary embeddings generated by LLMs demonstrate\ncompetitive, and in some cases superior, performance on the classification\ntasks, particularly when compared to models using traditional text embeddings.\nOur findings demonstrate the strengths of LLMs in mental health prediction, and\nsuggest promising directions for better utilization of their zero-shot\ncapabilities and context-aware summarization techniques."}
{"id": "2506.06619", "pdf": "https://arxiv.org/pdf/2506.06619", "abs": "https://arxiv.org/abs/2506.06619", "authors": ["Jesse Woo", "Fateme Hashemi Chaleshtori", "Ana Marasović", "Kenneth Marino"], "title": "BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs", "categories": ["cs.CL"], "comment": "ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix", "summary": "A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work."}
{"id": "2506.06626", "pdf": "https://arxiv.org/pdf/2506.06626", "abs": "https://arxiv.org/abs/2506.06626", "authors": ["Junzhe Wang", "Bichen Wang", "Xing Fu", "Yixin Sun", "Yanyan Zhao", "Bing Qin"], "title": "Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations", "categories": ["cs.CL"], "comment": "15 pages, 19 figures", "summary": "In recent years, Large Language Models (LLMs) have made significant progress\nin automated psychological counseling. However, current research focuses on\nsingle-session counseling, which doesn't represent real-world scenarios. In\npractice, psychological counseling is a process, not a one-time event,\nrequiring sustained, multi-session engagement to progressively address clients'\nissues. To overcome this limitation, we introduce a dataset for Multi-Session\nPsychological Counseling Conversation Dataset (MusPsy-Dataset). Our\nMusPsy-Dataset is constructed using real client profiles from publicly\navailable psychological case reports. It captures the dynamic arc of\ncounseling, encompassing multiple progressive counseling conversations from the\nsame client across different sessions. Leveraging our dataset, we also\ndeveloped our MusPsy-Model, which aims to track client progress and adapt its\ncounseling direction over time. Experiments show that our model performs better\nthan baseline models across multiple sessions."}
{"id": "2506.06636", "pdf": "https://arxiv.org/pdf/2506.06636", "abs": "https://arxiv.org/abs/2506.06636", "authors": ["Chuxue Cao", "Han Zhu", "Jiaming Ji", "Qichao Sun", "Zhenghao Zhu", "Yinyu Wu", "Juntao Dai", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "SafeLawBench: Towards Safe Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "With the growing prevalence of large language models (LLMs), the safety of\nLLMs has raised significant concerns. However, there is still a lack of\ndefinitive standards for evaluating their safety due to the subjective nature\nof current safety benchmarks. To address this gap, we conducted the first\nexploration of LLMs' safety evaluation from a legal perspective by proposing\nthe SafeLawBench benchmark. SafeLawBench categorizes safety risks into three\nlevels based on legal standards, providing a systematic and comprehensive\nframework for evaluation. It comprises 24,860 multi-choice questions and 1,106\nopen-domain question-answering (QA) tasks. Our evaluation included 2\nclosed-source LLMs and 18 open-source LLMs using zero-shot and few-shot\nprompting, highlighting the safety features of each model. We also evaluated\nthe LLMs' safety-related reasoning stability and refusal behavior.\nAdditionally, we found that a majority voting mechanism can enhance model\nperformance. Notably, even leading SOTA models like Claude-3.5-Sonnet and\nGPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,\nwhile the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community\nto prioritize research on the safety of LLMs."}
{"id": "2506.06657", "pdf": "https://arxiv.org/pdf/2506.06657", "abs": "https://arxiv.org/abs/2506.06657", "authors": ["Nikhita Vedula", "Dushyanta Dhyani", "Laleh Jalali", "Boris Oreshkin", "Mohsen Bayati", "Shervin Malmasi"], "title": "Quantile Regression with Large Language Models for Price Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL, 2025", "summary": "Large Language Models (LLMs) have shown promise in structured prediction\ntasks, including regression, but existing approaches primarily focus on point\nestimates and lack systematic comparison across different methods. We\ninvestigate probabilistic regression using LLMs for unstructured inputs,\naddressing challenging text-to-distribution prediction tasks such as price\nestimation where both nuanced text understanding and uncertainty quantification\nare critical. We propose a novel quantile regression approach that enables LLMs\nto produce full predictive distributions, improving upon traditional point\nestimates. Through extensive experiments across three diverse price prediction\ndatasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads\nsignificantly outperforms traditional approaches for both point and\ndistributional estimations, as measured by three established metrics each for\nprediction accuracy and distributional calibration. Our systematic comparison\nof LLM approaches, model architectures, training approaches, and data scaling\nreveals that Mistral-7B consistently outperforms encoder architectures,\nembedding-based methods, and few-shot learning methods. Our experiments also\nreveal the effectiveness of LLM-assisted label correction in achieving\nhuman-level accuracy without systematic bias. Our curated datasets are made\navailable at https://github.com/vnik18/llm-price-quantile-reg/ to support\nfuture research."}
{"id": "2506.06686", "pdf": "https://arxiv.org/pdf/2506.06686", "abs": "https://arxiv.org/abs/2506.06686", "authors": ["Chunyuan Deng", "Ruidi Chang", "Hanjie Chen"], "title": "Learning Distribution-Wise Control in Representation Space for Language Models", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Interventions in language models (LMs) are applied strategically to steer\nmodel behavior during the forward pass. Learnable interventions, also known as\nrepresentation fine-tuning, aim to apply pointwise control within the concept\nsubspace and have proven effective in altering high-level behaviors. In this\nwork, we extend this approach to the distribution level, enabling the model to\nlearn not only pointwise transformations but also the surrounding regions of\nthe concept subspace. We demonstrate that these methods perform effectively in\nearly layers, with larger standard deviations correlating strongly with\nimproved performance. Across eight commonsense reasoning and seven arithmetic\nreasoning benchmarks, our distribution-wise interventions consistently\noutperform pointwise interventions in controllability and robustness. These\nresults illustrate that distribution-wise interventions provide a more\ncomprehensive method for steering model behavior and enabling finer-grained\ncontrol over language models. The code is at:\n\\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}."}
{"id": "2506.06704", "pdf": "https://arxiv.org/pdf/2506.06704", "abs": "https://arxiv.org/abs/2506.06704", "authors": ["Weihang Su", "Qingyao Ai", "Jingtao Zhan", "Qian Dong", "Yiqun Liu"], "title": "Dynamic and Parametric Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a foundational paradigm for\nequipping large language models (LLMs) with external knowledge, playing a\ncritical role in information retrieval and knowledge-intensive applications.\nHowever, conventional RAG systems typically adopt a static\nretrieve-then-generate pipeline and rely on in-context knowledge injection,\nwhich can be suboptimal for complex tasks that require multihop reasoning,\nadaptive information access, and deeper integration of external knowledge.\nMotivated by these limitations, the research community has moved beyond static\nretrieval and in-context knowledge injection. Among the emerging directions,\nthis tutorial delves into two rapidly growing and complementary research areas\non RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when\nand what to retrieve during the LLM's generation process, enabling real-time\nadaptation to the LLM's evolving information needs. Parametric RAG rethinks how\nretrieved knowledge should be injected into LLMs, transitioning from\ninput-level to parameter-level knowledge injection for enhanced efficiency and\neffectiveness. This tutorial offers a comprehensive overview of recent advances\nin these emerging research areas. It also shares theoretical foundations and\npractical insights to support and inspire further research in RAG."}
{"id": "2506.06705", "pdf": "https://arxiv.org/pdf/2506.06705", "abs": "https://arxiv.org/abs/2506.06705", "authors": ["Zhihui Chen", "Kai He", "Yucheng Huang", "Yunxiao Zhu", "Mengling Feng"], "title": "DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains", "categories": ["cs.CL", "cs.AI"], "comment": "Zhihui Chen and Kai He contributed equally to this work, Mengling\n  Feng is the corresponding author", "summary": "Detecting LLM-generated text in specialized and high-stakes domains like\nmedicine and law is crucial for combating misinformation and ensuring\nauthenticity. However, current zero-shot detectors, while effective on general\ntext, often fail when applied to specialized content due to domain shift. We\nprovide a theoretical analysis showing this failure is fundamentally linked to\nthe KL divergence between human, detector, and source text distributions. To\naddress this, we propose DivScore, a zero-shot detection framework using\nnormalized entropy-based scoring and domain knowledge distillation to robustly\nidentify LLM-generated text in specialized domains. We also release a\ndomain-specific benchmark for LLM-generated text detection in the medical and\nlegal domains. Experiments on our benchmark show that DivScore consistently\noutperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%\nhigher recall (0.1% false positive rate threshold). In adversarial settings,\nDivScore demonstrates superior robustness than other baselines, achieving on\naverage 22.8% advantage in AUROC and 29.5% in recall. Code and data are\npublicly available."}
{"id": "2506.06708", "pdf": "https://arxiv.org/pdf/2506.06708", "abs": "https://arxiv.org/abs/2506.06708", "authors": ["Haiqi Yang", "Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "A Survey of Retentive Network", "categories": ["cs.CL"], "comment": "15 pages, 3 figures", "summary": "Retentive Network (RetNet) represents a significant advancement in neural\nnetwork architecture, offering an efficient alternative to the Transformer.\nWhile Transformers rely on self-attention to model dependencies, they suffer\nfrom high memory costs and limited scalability when handling long sequences due\nto their quadratic complexity. To mitigate these limitations, RetNet introduces\na retention mechanism that unifies the inductive bias of recurrence with the\nglobal dependency modeling of attention. This mechanism enables linear-time\ninference, facilitates efficient modeling of extended contexts, and remains\ncompatible with fully parallelizable training pipelines. RetNet has garnered\nsignificant research interest due to its consistently demonstrated cross-domain\neffectiveness, achieving robust performance across machine learning paradigms\nincluding natural language processing, speech recognition, and time-series\nanalysis. However, a comprehensive review of RetNet is still missing from the\ncurrent literature. This paper aims to fill that gap by offering the first\ndetailed survey of the RetNet architecture, its key innovations, and its\ndiverse applications. We also explore the main challenges associated with\nRetNet and propose future research directions to support its continued\nadvancement in both academic research and practical deployment."}
{"id": "2506.06737", "pdf": "https://arxiv.org/pdf/2506.06737", "abs": "https://arxiv.org/abs/2506.06737", "authors": ["Qi Shi", "Qiwei Han", "Cláudia Soares"], "title": "C-PATH: Conversational Patient Assistance and Triage in Healthcare System", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IEEE ICDH 2025, 10 pages, 8 figures, 5 tables", "summary": "Navigating healthcare systems can be complex and overwhelming, creating\nbarriers for patients seeking timely and appropriate medical attention. In this\npaper, we introduce C-PATH (Conversational Patient Assistance and Triage in\nHealthcare), a novel conversational AI system powered by large language models\n(LLMs) designed to assist patients in recognizing symptoms and recommending\nappropriate medical departments through natural, multi-turn dialogues. C-PATH\nis fine-tuned on medical knowledge, dialogue data, and clinical summaries using\na multi-stage pipeline built on the LLaMA3 architecture. A core contribution of\nthis work is a GPT-based data augmentation framework that transforms structured\nclinical knowledge from DDXPlus into lay-person-friendly conversations,\nallowing alignment with patient communication norms. We also implement a\nscalable conversation history management strategy to ensure long-range\ncoherence. Evaluation with GPTScore demonstrates strong performance across\ndimensions such as clarity, informativeness, and recommendation accuracy.\nQuantitative benchmarks show that C-PATH achieves superior performance in\nGPT-rewritten conversational datasets, significantly outperforming\ndomain-specific baselines. C-PATH represents a step forward in the development\nof user-centric, accessible, and accurate AI tools for digital health\nassistance and triage."}
{"id": "2506.06751", "pdf": "https://arxiv.org/pdf/2506.06751", "abs": "https://arxiv.org/abs/2506.06751", "authors": ["Mikhail Salnikov", "Dmitrii Korzh", "Ivan Lazichny", "Elvir Karimov", "Artyom Iudin", "Ivan Oseledets", "Oleg Y. Rogov", "Alexander Panchenko", "Natalia Loukachevitch", "Elena Tutubalina"], "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries according to contemporary language models", "categories": ["cs.CL"], "comment": null, "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research."}
{"id": "2506.06775", "pdf": "https://arxiv.org/pdf/2506.06775", "abs": "https://arxiv.org/abs/2506.06775", "authors": ["Walter Paci", "Alessandro Panunzi", "Sandro Pezzelle"], "title": "They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse", "categories": ["cs.CL"], "comment": "Accepted to the ACL2025 Findings", "summary": "Implicit content plays a crucial role in political discourse, where speakers\nsystematically employ pragmatic strategies such as implicatures and\npresuppositions to influence their audiences. Large Language Models (LLMs) have\ndemonstrated strong performance in tasks requiring complex semantic and\npragmatic understanding, highlighting their potential for detecting and\nexplaining the meaning of implicit content. However, their ability to do this\nwithin political discourse remains largely underexplored. Leveraging, for the\nfirst time, the large IMPAQTS corpus, which comprises Italian political\nspeeches with the annotation of manipulative implicit content, we propose\nmethods to test the effectiveness of LLMs in this challenging problem. Through\na multiple-choice task and an open-ended generation task, we demonstrate that\nall tested models struggle to interpret presuppositions and implicatures. We\nconclude that current LLMs lack the key pragmatic capabilities necessary for\naccurately interpreting highly implicit language, such as that found in\npolitical discourse. At the same time, we highlight promising trends and future\ndirections for enhancing model performance. We release our data and code at\nhttps://github.com/WalterPaci/IMPAQTS-PID"}
{"id": "2506.06785", "pdf": "https://arxiv.org/pdf/2506.06785", "abs": "https://arxiv.org/abs/2506.06785", "authors": ["Hiram Ring"], "title": "Extending dependencies to the taggedPBC: Word order in transitive clauses", "categories": ["cs.CL"], "comment": null, "summary": "The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged\nparallel text data from over 1,500 languages, representing 133 language\nfamilies and 111 isolates. While this dwarfs previously available resources,\nand the POS tags achieve decent accuracy, allowing for predictive\ncrosslinguistic insights (Ring 2025b), the dataset was not initially annotated\nfor dependencies. This paper reports on a CoNLLU-formatted version of the\ndataset which transfers dependency information along with POS tags to all\nlanguages in the taggedPBC. Although there are various concerns regarding the\nquality of the tags and the dependencies, word order information derived from\nthis dataset regarding the position of arguments and predicates in transitive\nclauses correlates with expert determinations of word order in three\ntypological databases (WALS, Grambank, Autotyp). This highlights the usefulness\nof corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)\nfor extending comparisons of discrete linguistic categories, and suggests that\nimportant insights can be gained even from noisy data, given sufficient\nannotation. The dependency-annotated corpora are also made available for\nresearch and collaboration via GitHub."}
{"id": "2506.06800", "pdf": "https://arxiv.org/pdf/2506.06800", "abs": "https://arxiv.org/abs/2506.06800", "authors": ["Tianjie Ju", "Yujia Chen", "Hao Fei", "Mong-Li Lee", "Wynne Hsu", "Pengzhou Cheng", "Zongru Wu", "Zhuosheng Zhang", "Gongshen Liu"], "title": "On the Adaptive Psychological Persuasion of Large Language Models", "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Previous work has showcased the intriguing capabilities of Large Language\nModels (LLMs) in instruction-following and rhetorical fluency. However,\nsystematic exploration of their dual capabilities to autonomously persuade and\nresist persuasion, particularly in contexts involving psychological rhetoric,\nremains unexplored. In this paper, we first evaluate four commonly adopted LLMs\nby tasking them to alternately act as persuaders and listeners in adversarial\ndialogues. Empirical results show that persuader LLMs predominantly employ\nrepetitive strategies, leading to low success rates. Then we introduce eleven\ncomprehensive psychological persuasion strategies, finding that explicitly\ninstructing LLMs to adopt specific strategies such as Fluency Effect and\nRepetition Effect significantly improves persuasion success rates. However, no\n``one-size-fits-all'' strategy proves universally effective, with performance\nheavily dependent on contextual counterfactuals. Motivated by these\nobservations, we propose an adaptive framework based on direct preference\noptimization that trains LLMs to autonomously select optimal strategies by\nleveraging persuasion results from strategy-specific responses as preference\npairs. Experiments on three open-source LLMs confirm that the proposed adaptive\npsychological persuasion method effectively enables persuader LLMs to select\noptimal strategies, significantly enhancing their success rates while\nmaintaining general capabilities. Our code is available at\nhttps://github.com/KalinaEine/PsychologicalPersuasion."}
{"id": "2506.06806", "pdf": "https://arxiv.org/pdf/2506.06806", "abs": "https://arxiv.org/abs/2506.06806", "authors": ["Subhendu Khatuya", "Shashwat Naidu", "Saptarshi Ghosh", "Pawan Goyal", "Niloy Ganguly"], "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work has been accepted to appear at the Association for\n  Computational Linguistics (ACL), 2025", "summary": "The explosion of textual data has made manual document classification\nincreasingly challenging. To address this, we introduce a robust, efficient\ndomain-agnostic generative model framework for multi-label text classification.\nInstead of treating labels as mere atomic symbols, our approach utilizes\npredefined label descriptions and is trained to generate these descriptions\nbased on the input text. During inference, the generated descriptions are\nmatched to the pre-defined labels using a finetuned sentence transformer. We\nintegrate this with a dual-objective loss function, combining cross-entropy\nloss and cosine similarity of the generated sentences with the predefined\ntarget descriptions, ensuring both semantic alignment and accuracy. Our\nproposed model LAGAMC stands out for its parameter efficiency and versatility\nacross diverse datasets, making it well-suited for practical applications. We\ndemonstrate the effectiveness of our proposed model by achieving new\nstate-of-the-art performances across all evaluated datasets, surpassing several\nstrong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in\nMacro-F1 compared to the closest baseline across all datasets."}
{"id": "2506.06808", "pdf": "https://arxiv.org/pdf/2506.06808", "abs": "https://arxiv.org/abs/2506.06808", "authors": ["James A. Michaelov", "Reeka Estacio", "Zhien Zhang", "Benjamin K. Bergen"], "title": "Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Can language models reliably predict that possible events are more likely\nthan merely improbable ones? By teasing apart possibility, typicality, and\ncontextual relatedness, we show that despite the results of previous work,\nlanguage models' ability to do this is far from robust. In fact, under certain\nconditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -\nperform at worse-than-chance level, assigning higher probabilities to\nimpossible sentences such as 'the car was given a parking ticket by the brake'\nthan to merely unlikely sentences such as 'the car was given a parking ticket\nby the explorer'."}
{"id": "2506.06812", "pdf": "https://arxiv.org/pdf/2506.06812", "abs": "https://arxiv.org/abs/2506.06812", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso"], "title": "Advancing Question Generation with Joint Narrative and Difficulty Control", "categories": ["cs.CL"], "comment": "Preprint. Accepted to the BEA 2025 Workshop (ACL)", "summary": "Question Generation (QG), the task of automatically generating questions from\na source input, has seen significant progress in recent years.\nDifficulty-controllable QG (DCQG) enables control over the difficulty level of\ngenerated questions while considering the learner's ability. Additionally,\nnarrative-controllable QG (NCQG) allows control over the narrative aspects\nembedded in the questions. However, research in QG lacks a focus on combining\nthese two types of control, which is important for generating questions\ntailored to educational purposes. To address this gap, we propose a strategy\nfor Joint Narrative and Difficulty Control, enabling simultaneous control over\nthese two attributes in the generation of reading comprehension questions. Our\nevaluation provides preliminary evidence that this approach is feasible, though\nit is not effective across all instances. Our findings highlight the conditions\nunder which the strategy performs well and discuss the trade-offs associated\nwith its application."}
{"id": "2506.06813", "pdf": "https://arxiv.org/pdf/2506.06813", "abs": "https://arxiv.org/abs/2506.06813", "authors": ["Dipto Das", "Syed Ishtiaque Ahmed", "Shion Guha"], "title": "BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Understanding political discourse in online spaces is crucial for analyzing\npublic opinion and ideological polarization. While social computing and\ncomputational linguistics have explored such discussions in English, such\nresearch efforts are significantly limited in major yet under-resourced\nlanguages like Bengali due to the unavailability of datasets. In this paper, we\npresent a multilingual dataset of Bengali transnational political discourse\n(BTPD) collected from three online platforms, each representing distinct\ncommunity structures and interaction dynamics. Besides describing how we\nhand-curated the dataset through community-informed keyword-based retrieval,\nthis paper also provides a general overview of its topics and multilingual\ncontent."}
{"id": "2506.06816", "pdf": "https://arxiv.org/pdf/2506.06816", "abs": "https://arxiv.org/abs/2506.06816", "authors": ["Dipto Das", "Shion Guha", "Bryan Semaan"], "title": "How do datasets, developers, and models affect biases in a low-resourced language?", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Sociotechnical systems, such as language technologies, frequently exhibit\nidentity-based biases. These biases exacerbate the experiences of historically\nmarginalized communities and remain understudied in low-resource contexts.\nWhile models and datasets specific to a language or with multilingual support\nare commonly recommended to address these biases, this paper empirically tests\nthe effectiveness of such approaches in the context of gender, religion, and\nnationality-based identities in Bengali, a widely spoken but low-resourced\nlanguage. We conducted an algorithmic audit of sentiment analysis models built\non mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment\nanalysis (BSA) datasets from Google Dataset Search. Our analyses showed that\nBSA models exhibit biases across different identity categories despite having\nsimilar semantic content and structure. We also examined the inconsistencies\nand uncertainties arising from combining pre-trained models and datasets\ncreated by individuals from diverse demographic backgrounds. We connected these\nfindings to the broader discussions on epistemic injustice, AI alignment, and\nmethodological decisions in algorithmic audits."}
{"id": "2506.06820", "pdf": "https://arxiv.org/pdf/2506.06820", "abs": "https://arxiv.org/abs/2506.06820", "authors": ["Wenyu Zhang", "Yingxu He", "Geyu Lin", "Zhuohan Liu", "Shuo Sun", "Bin Wang", "Xunlong Zou", "Jeremy H. M. Wong", "Qiongqiong Wang", "Hardik B. Sailor", "Nancy F. Chen", "Ai Ti Aw"], "title": "Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Audio Large Language Models (AudioLLMs) have achieved strong results in\nsemantic tasks like speech recognition and translation, but remain limited in\nmodeling paralinguistic cues such as emotion. Existing approaches often treat\nemotion understanding as a classification problem, offering little insight into\nthe underlying rationale behind predictions. In this work, we explore emotion\nreasoning, a strategy that leverages the generative capabilities of AudioLLMs\nto enhance emotion recognition by producing semantically aligned,\nevidence-grounded explanations. To support this in multitask AudioLLMs, we\nintroduce a unified framework combining reasoning-augmented data supervision,\ndual-encoder architecture, and task-alternating training. This approach enables\nAudioLLMs to effectively learn different tasks while incorporating emotional\nreasoning. Experiments on IEMOCAP and MELD show that our approach not only\nimproves emotion prediction accuracy but also enhances the coherence and\nevidential grounding of the generated responses."}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821", "abs": "https://arxiv.org/abs/2506.06821", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning."}
{"id": "2506.06842", "pdf": "https://arxiv.org/pdf/2506.06842", "abs": "https://arxiv.org/abs/2506.06842", "authors": ["Arkadiusz Modzelewski", "Witold Sosnowski", "Tiziano Labruna", "Adam Wierzbicki", "Giovanni Da San Martino"], "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Disinformation detection is a key aspect of media literacy. Psychological\nstudies have shown that knowledge of persuasive fallacies helps individuals\ndetect disinformation. Inspired by these findings, we experimented with large\nlanguage models (LLMs) to test whether infusing persuasion knowledge enhances\ndisinformation detection. As a result, we introduce the Persuasion-Augmented\nChain of Thought (PCoT), a novel approach that leverages persuasion to improve\ndisinformation detection in zero-shot classification. We extensively evaluate\nPCoT on online news and social media posts. Moreover, we publish two novel,\nup-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets\nenable the evaluation of PCoT on content entirely unseen by the LLMs used in\nour experiments, as the content was published after the models' knowledge\ncutoffs. We show that, on average, PCoT outperforms competitive methods by 15%\nacross five LLMs and five datasets. These findings highlight the value of\npersuasion in strengthening zero-shot disinformation detection."}
{"id": "2506.06844", "pdf": "https://arxiv.org/pdf/2506.06844", "abs": "https://arxiv.org/abs/2506.06844", "authors": ["Naibin Gu", "Peng Fu", "Xiyu Liu", "Ke Ma", "Zheng Lin", "Weiping Wang"], "title": "Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a common method for\nfine-tuning large language models, where a base model can serve multiple users\nthrough PEFT module switching. To enhance user experience, base models require\nperiodic updates. However, once updated, PEFT modules fine-tuned on previous\nversions often suffer substantial performance degradation on newer versions.\nRe-tuning these numerous modules to restore performance would incur significant\ncomputational costs. Through a comprehensive analysis of the changes that occur\nduring base model updates, we uncover an interesting phenomenon: continual\ntraining primarily affects task-specific knowledge stored in Feed-Forward\nNetworks (FFN), while having less impact on the task-specific pattern in the\nAttention mechanism. Based on these findings, we introduce Trans-PEFT, a novel\napproach that enhances the PEFT module by focusing on the task-specific pattern\nwhile reducing its dependence on certain knowledge in the base model. Further\ntheoretical analysis supports our approach. Extensive experiments across 7 base\nmodels and 12 datasets demonstrate that Trans-PEFT trained modules can maintain\nperformance on updated base models without re-tuning, significantly reducing\nmaintenance overhead in real-world applications."}
{"id": "2506.06877", "pdf": "https://arxiv.org/pdf/2506.06877", "abs": "https://arxiv.org/abs/2506.06877", "authors": ["Jiaxing Guo", "Wenjie Yang", "Shengzhong Zhang", "Tongshan Xu", "Lun Du", "Da Zheng", "Zengfeng Huang"], "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning."}
{"id": "2506.06887", "pdf": "https://arxiv.org/pdf/2506.06887", "abs": "https://arxiv.org/abs/2506.06887", "authors": ["Ziheng Qiao", "Houquan Zhou", "Zhenghua Li"], "title": "Mixture of Small and Large Models for Chinese Spelling Check", "categories": ["cs.CL"], "comment": null, "summary": "In the era of large language models (LLMs), the Chinese Spelling Check (CSC)\ntask has seen various LLM methods developed, yet their performance remains\nunsatisfactory. In contrast, fine-tuned BERT-based models, relying on\nhigh-quality in-domain data, show excellent performance but suffer from edit\npattern overfitting. This paper proposes a novel dynamic mixture approach that\neffectively combines the probability distributions of small models and LLMs\nduring the beam search decoding phase, achieving a balanced enhancement of\nprecise corrections from small models and the fluency of LLMs. This approach\nalso eliminates the need for fine-tuning LLMs, saving significant time and\nresources, and facilitating domain adaptation. Comprehensive experiments\ndemonstrate that our mixture approach significantly boosts error correction\ncapabilities, achieving state-of-the-art results across multiple datasets. Our\ncode is available at https://github.com/zhqiao-nlp/MSLLM."}
{"id": "2506.06888", "pdf": "https://arxiv.org/pdf/2506.06888", "abs": "https://arxiv.org/abs/2506.06888", "authors": ["Hamid Mojarad", "Kevin Tang"], "title": "Automatic Speech Recognition of African American English: Lexical and Contextual Effects", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) models often struggle with the phonetic,\nphonological, and morphosyntactic features found in African American English\n(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction\n(CCR) and ING-reduction. It examines whether the presence of CCR and\nING-reduction increases ASR misrecognition. Subsequently, it investigates\nwhether end-to-end ASR systems without an external Language Model (LM) are more\ninfluenced by lexical neighborhood effect and less by contextual predictability\ncompared to systems with an LM. The Corpus of Regional African American\nLanguage (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR\nand ING-reduction were detected using the Montreal Forced Aligner (MFA) with\npronunciation expansion. The analysis reveals a small but significant effect of\nCCR and ING on Word Error Rate (WER) and indicates a stronger presence of\nlexical neighborhood effect in ASR systems without LMs."}
{"id": "2506.06929", "pdf": "https://arxiv.org/pdf/2506.06929", "abs": "https://arxiv.org/abs/2506.06929", "authors": ["Mikhail Krasitskii", "Grigori Sidorov", "Olga Kolesnikova", "Liliana Chanona Hernandez", "Alexander Gelbukh"], "title": "Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis", "categories": ["cs.CL"], "comment": "6 pages", "summary": "We propose a hybrid approach for multilingual sentiment analysis that\ncombines extractive and abstractive summarization to address the limitations of\nstandalone methods. The model integrates TF-IDF-based extraction with a\nfine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and\ncultural adaptation. Experiments across 10 languages show significant\nimprovements over baselines, achieving 0.90 accuracy for English and 0.84 for\nlow-resource languages. The approach also demonstrates 22% greater\ncomputational efficiency than traditional methods. Practical applications\ninclude real-time brand monitoring and cross-cultural discourse analysis.\nFuture work will focus on optimization for low-resource languages via 8-bit\nquantization."}
{"id": "2506.06930", "pdf": "https://arxiv.org/pdf/2506.06930", "abs": "https://arxiv.org/abs/2506.06930", "authors": ["Alexander Spangher", "Tenghao Huang", "Jialiang Gu", "Jiatong Shi", "Muhao Chen"], "title": "DiscoSum: Discourse-aware News Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, 10 pages in Appendix", "summary": "Recent advances in text summarization have predominantly leveraged large\nlanguage models to generate concise summaries. However, language models often\ndo not maintain long-term discourse structure, especially in news articles,\nwhere organizational flow significantly influences reader engagement. We\nintroduce a novel approach to integrating discourse structure into\nsummarization processes, focusing specifically on news articles across various\nmedia. We present a novel summarization dataset where news articles are\nsummarized multiple times in different ways across different social media\nplatforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse\nschema to describe summarization structures and a novel algorithm, DiscoSum,\nwhich employs beam search technique for structure-aware summarization, enabling\nthe transformation of news stories to meet different stylistic and structural\ndemands. Both human and automatic evaluation results demonstrate the efficacy\nof our approach in maintaining narrative fidelity and meeting structural\nrequirements."}
{"id": "2506.06950", "pdf": "https://arxiv.org/pdf/2506.06950", "abs": "https://arxiv.org/abs/2506.06950", "authors": ["Do Xuan Long", "Duy Dinh", "Ngoc-Hai Nguyen", "Kenji Kawaguchi", "Nancy F. Chen", "Shafiq Joty", "Min-Yen Kan"], "title": "What Makes a Good Natural Language Prompt?", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions."}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955", "abs": "https://arxiv.org/abs/2506.06955", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Hirokazu Kiyomaru", "Koichi Takeda", "Yusuke Miyao", "Maki Matsuda", "Yusuke Oda", "Pontus Stenetorp", "Qianying Liu", "Su Myat Noe", "Hideyuki Tachibana", "Kouta Nakayama", "Sadao Kurohashi"], "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety."}
{"id": "2506.06964", "pdf": "https://arxiv.org/pdf/2506.06964", "abs": "https://arxiv.org/abs/2506.06964", "authors": ["Subhojyoti Mukherjee", "Viet Dac Lai", "Raghavendra Addanki", "Ryan Rossi", "Seunghyun Yoon", "Trung Bui", "Anup Rao", "Jayakumar Subramanian", "Branislav Kveton"], "title": "Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning", "categories": ["cs.CL", "cs.LG"], "comment": "39 pages", "summary": "Question answering (QA) agents automatically answer questions posed in\nnatural language. In this work, we learn to ask clarifying questions in QA\nagents. The key idea in our method is to simulate conversations that contain\nclarifying questions and learn from them using reinforcement learning (RL). To\nmake RL practical, we propose and analyze offline RL objectives that can be\nviewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in\nlarge language models. Our work stands in a stark contrast to recently proposed\nmethods, based on SFT and direct preference optimization, which have additional\nhyper-parameters and do not directly optimize rewards. We compare to these\nmethods empirically and report gains in both optimized rewards and language\nquality."}
{"id": "2506.06968", "pdf": "https://arxiv.org/pdf/2506.06968", "abs": "https://arxiv.org/abs/2506.06968", "authors": ["Pavel Kovalev", "Carlo Angiuli"], "title": "A dependently-typed calculus of event telicity and culminativity", "categories": ["cs.CL", "cs.LO"], "comment": "52 pages, Agda formalization available at\n  https://doi.org/10.5281/zenodo.15602617", "summary": "We present a dependently-typed cross-linguistic framework for analyzing the\ntelicity and culminativity of events, accompanied by examples of using our\nframework to model English sentences. Our framework consists of two parts. In\nthe nominal domain, we model the boundedness of noun phrases and its\nrelationship to subtyping, delimited quantities, and adjectival modification.\nIn the verbal domain we define a dependent event calculus, modeling telic\nevents as those whose undergoer is bounded, culminating events as telic events\nthat achieve their inherent endpoint, and consider adverbial modification. In\nboth domains we pay particular attention to associated entailments. Our\nframework is defined as an extension of intensional Martin-L\\\"of dependent type\ntheory, and the rules and examples in this paper have been formalized in the\nAgda proof assistant."}
{"id": "2506.06971", "pdf": "https://arxiv.org/pdf/2506.06971", "abs": "https://arxiv.org/abs/2506.06971", "authors": ["Jaechul Roh", "Varun Gandhi", "Shivani Anilkumar", "Arin Garg"], "title": "Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in tasks\nrequiring complex reasoning, such as code generation, mathematical problem\nsolving, and algorithmic synthesis -- especially when aided by reasoning tokens\nand Chain-of-Thought prompting. Yet, a core question remains: do these models\ntruly reason, or do they merely exploit shallow statistical patterns? In this\npaper, we systematically investigate the robustness of reasoning LLMs by\nintroducing a suite of semantically faithful yet adversarially structured\nprompt perturbations. Our evaluation -- spanning 700 perturbed code generations\nderived from LeetCode-style problems -- applies transformations such as\nstorytelling reframing, irrelevant constraint injection, example reordering,\nand numeric perturbation. We observe that while certain modifications severely\ndegrade performance (with accuracy drops up to -42.1%), others surprisingly\nimprove model accuracy by up to 35.3%, suggesting sensitivity not only to\nsemantics but also to surface-level prompt dynamics. These findings expose the\nfragility and unpredictability of current reasoning systems, underscoring the\nneed for more principles approaches to reasoning alignments and prompting\nrobustness. We release our perturbation datasets and evaluation framework to\npromote further research in trustworthy and resilient LLM reasoning."}
{"id": "2506.06972", "pdf": "https://arxiv.org/pdf/2506.06972", "abs": "https://arxiv.org/abs/2506.06972", "authors": ["Yuji Zhang", "Qingyun Wang", "Cheng Qian", "Jiateng Liu", "Chenkai Sun", "Denghui Zhang", "Tarek Abdelzaher", "Chengxiang Zhai", "Preslav Nakov", "Heng Ji"], "title": "Atomic Reasoning for Scientific Table Claim Verification", "categories": ["cs.CL"], "comment": null, "summary": "Scientific texts often convey authority due to their technical language and\ncomplex data. However, this complexity can sometimes lead to the spread of\nmisinformation. Non-experts are particularly susceptible to misleading claims\nbased on scientific tables due to their high information density and perceived\ncredibility. Existing table claim verification models, including\nstate-of-the-art large language models (LLMs), often struggle with precise\nfine-grained reasoning, resulting in errors and a lack of precision in\nverifying scientific claims. Inspired by Cognitive Load Theory, we propose that\nenhancing a model's ability to interpret table-based claims involves reducing\ncognitive load by developing modular, reusable reasoning components (i.e.,\natomic skills). We introduce a skill-chaining schema that dynamically composes\nthese skills to facilitate more accurate and generalizable reasoning with a\nreduced cognitive load. To evaluate this, we create SciAtomicBench, a\ncross-domain benchmark with fine-grained reasoning annotations. With only 350\nfine-tuning examples, our model trained by atomic reasoning outperforms\nGPT-4o's chain-of-thought method, achieving state-of-the-art results with far\nless training data."}
{"id": "2506.06982", "pdf": "https://arxiv.org/pdf/2506.06982", "abs": "https://arxiv.org/abs/2506.06982", "authors": ["Cong Liu", "Jie Wu", "Weigang Wu", "Xu Chen", "Liang Lin", "Wei-Shi Zheng"], "title": "Chain of Methodologies: Scaling Test Time Computation without Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex reasoning tasks due\nto insufficient in-depth insights in their training data, which are typically\nabsent in publicly available documents. This paper introduces the Chain of\nMethodologies (CoM), an innovative and intuitive prompting framework that\nenhances structured thinking by integrating human methodological insights,\nenabling LLMs to tackle complex tasks with extended reasoning. CoM leverages\nthe metacognitive abilities of advanced LLMs, activating systematic reasoning\nthrought user-defined methodologies without explicit fine-tuning. Experiments\nshow that CoM surpasses competitive baselines, demonstrating the potential of\ntraining-free prompting methods as robust solutions for complex reasoning tasks\nand bridging the gap toward human-level reasoning through human-like\nmethodological insights."}
{"id": "2506.06987", "pdf": "https://arxiv.org/pdf/2506.06987", "abs": "https://arxiv.org/abs/2506.06987", "authors": ["Senqi Yang", "Dongyu Zhang", "Jing Ren", "Ziqi Xu", "Xiuzhen Zhang", "Yiliao Song", "Hongfei Lin", "Feng Xia"], "title": "Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors", "categories": ["cs.CL"], "comment": "This paper has been accepted to the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025), Main Conference", "summary": "Metaphors are pervasive in communication, making them crucial for natural\nlanguage processing (NLP). Previous research on automatic metaphor processing\npredominantly relies on training data consisting of English samples, which\noften reflect Western European or North American biases. This cultural skew can\nlead to an overestimation of model performance and contributions to NLP\nprogress. However, the impact of cultural bias on metaphor processing,\nparticularly in multimodal contexts, remains largely unexplored. To address\nthis gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset\ndesigned for cross-cultural studies of metaphor in Chinese and English. MultiMM\nconsists of 8,461 text-image advertisement pairs, each accompanied by\nfine-grained annotations, providing a deeper understanding of multimodal\nmetaphors beyond a single cultural domain. Additionally, we propose\nSentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates\nsentiment embeddings to enhance metaphor comprehension across cultural\nbackgrounds. Experimental results validate the effectiveness of SEMD on\nmetaphor detection and sentiment analysis tasks. We hope this work increases\nawareness of cultural bias in NLP research and contributes to the development\nof fairer and more inclusive language models. Our dataset and code are\navailable at https://github.com/DUTIR-YSQ/MultiMM."}
{"id": "2506.06998", "pdf": "https://arxiv.org/pdf/2506.06998", "abs": "https://arxiv.org/abs/2506.06998", "authors": ["Ming Li", "Zhengyuan Yang", "Xiyao Wang", "Dianqi Li", "Kevin Lin", "Tianyi Zhou", "Lijuan Wang"], "title": "What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models (LRMs) achieve strong reasoning performance by\nemitting long chains of thought. Yet, these verbose traces slow down inference\nand often drift into unnecessary detail, known as the overthinking phenomenon.\nTo better understand LRMs' behavior, we systematically analyze the token-level\nmisalignment between reasoning and non-reasoning models. While it is expected\nthat their primary difference lies in the stylistic \"thinking cues\", LRMs\nuniquely exhibit two pivotal, previously under-explored phenomena: a Global\nMisalignment Rebound, where their divergence from non-reasoning models persists\nor even grows as response length increases, and more critically, a Local\nMisalignment Diminish, where the misalignment concentrates at the \"thinking\ncues\" each sentence starts with but rapidly declines in the remaining of the\nsentence. Motivated by the Local Misalignment Diminish, we propose\nFoReaL-Decoding, a collaborative fast-slow thinking decoding method for\ncost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few\ntokens for each sentence, and then a weaker draft model completes the following\ntokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to\nsmoothly interpolate between the small and the large model. On four popular\nmath-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),\nFoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by\nup to 40%, while preserving 86 to 100% of model performance. These results\nestablish FoReaL-Decoding as a simple, plug-and-play route to controllable\ncost-quality trade-offs in reasoning-centric tasks."}
{"id": "2506.07001", "pdf": "https://arxiv.org/pdf/2506.07001", "abs": "https://arxiv.org/abs/2506.07001", "authors": ["Yize Cheng", "Vinu Sankar Sadasivan", "Mehrdad Saberi", "Shoumik Saha", "Soheil Feizi"], "title": "Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "The increasing capabilities of Large Language Models (LLMs) have raised\nconcerns about their misuse in AI-generated plagiarism and social engineering.\nWhile various AI-generated text detectors have been proposed to mitigate these\nrisks, many remain vulnerable to simple evasion techniques such as\nparaphrasing. However, recent detectors have shown greater robustness against\nsuch basic attacks. In this work, we introduce Adversarial Paraphrasing, a\ntraining-free attack framework that universally humanizes any AI-generated text\nto evade detection more effectively. Our approach leverages an off-the-shelf\ninstruction-following LLM to paraphrase AI-generated content under the guidance\nof an AI text detector, producing adversarial examples that are specifically\noptimized to bypass detection. Extensive experiments show that our attack is\nboth broadly effective and highly transferable across several detection\nsystems. For instance, compared to simple paraphrasing attack--which,\nironically, increases the true positive at 1% false positive (T@1%F) by 8.57%\non RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by\nOpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on\nFast-DetectGPT. Across a diverse set of detectors--including neural\nnetwork-based, watermark-based, and zero-shot approaches--our attack achieves\nan average T@1%F reduction of 87.88% under the guidance of\nOpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and\nattack success to find that our method can significantly reduce detection\nrates, with mostly a slight degradation in text quality. Our adversarial setup\nhighlights the need for more robust and resilient detection strategies in the\nlight of increasingly sophisticated evasion techniques."}
{"id": "2506.07032", "pdf": "https://arxiv.org/pdf/2506.07032", "abs": "https://arxiv.org/abs/2506.07032", "authors": ["Bhuiyan Sanjid Shafique", "Ashmal Vayani", "Muhammad Maaz", "Hanoona Abdul Rasheed", "Dinura Dissanayake", "Mohammed Irfan Kurpath", "Yahya Hmaiti", "Go Inoue", "Jean Lahoud", "Md. Safirur Rashid", "Shadid Intisar Quasem", "Maheen Fatima", "Franco Vidal", "Mykola Maslych", "Ketan Pravin More", "Sanoojan Baliah", "Hasindri Watawana", "Yuhao Li", "Fabian Farestam", "Leon Schaller", "Roman Tymtsiv", "Simon Weber", "Hisham Cholakkal", "Ivan Laptev", "Shin'ichi Satoh", "Michael Felsberg", "Mubarak Shah", "Salman Khan", "Fahad Shahbaz Khan"], "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large multimodal models (LMMs) have recently gained attention due to their\neffectiveness to understand and generate descriptions of visual content. Most\nexisting LMMs are in English language. While few recent works explore\nmultilingual image LMMs, to the best of our knowledge, moving beyond the\nEnglish language for cultural and linguistic inclusivity is yet to be\ninvestigated in the context of video LMMs. In pursuit of more inclusive video\nLMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to\nevaluate Video LMMs across 14 languages, including both low- and high-resource\nlanguages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,\nBengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is\ndesigned to rigorously test video LMMs across 15 categories including eight\nculturally diverse categories, ranging from lifestyles and festivals to foods\nand rituals and from local landmarks to prominent cultural personalities.\nViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice\nquestions spanning various video durations (short, medium, and long) with 8k\nsamples that are manually verified by native language speakers. In addition, we\nalso introduce a machine translated multilingual video training set comprising\n1.2 million samples and develop a simple multilingual video LMM, named ViMUL,\nthat is shown to provide a better tradeoff between high-and low-resource\nlanguages for video understanding. We hope our ViMUL-Bench and multilingual\nvideo LMM along with a large-scale multilingual video training set will help\nease future research in developing cultural and linguistic inclusive\nmultilingual video LMMs. Our proposed benchmark, video LMM and training data\nwill be publicly released at https://mbzuai-oryx.github.io/ViMUL/."}
{"id": "2506.07037", "pdf": "https://arxiv.org/pdf/2506.07037", "abs": "https://arxiv.org/abs/2506.07037", "authors": ["Zhongze Luo", "Weixuan Wan", "Qizhi Zheng", "Yanhong Bai", "Jingyun Sun", "Jian Wang", "Dan Wang"], "title": "KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering", "categories": ["cs.CL"], "comment": "23 pages", "summary": "There are many types of standards in the field of communication. The\ntraditional consulting model has a long cycle and relies on the knowledge and\nexperience of experts, making it difficult to meet the rapidly developing\ntechnological demands. This paper combines the fine-tuning of large language\nmodels with the construction of knowledge graphs to implement an intelligent\nconsultation and question-answering system for communication standards. The\nexperimental results show that after LoRA tuning on the constructed dataset of\n6,587 questions and answers in the field of communication standards,\nQwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the\nfield of communication standards on the test set. BLEU-4 rose from 18.8564 to\n66.8993, and evaluation indicators such as ROUGE also increased significantly,\noutperforming the fine-tuning effect of the comparison model\nLlama-3-8B-Instruct. Based on the ontology framework containing 6 entity\nattributes and 10 relation attributes, a knowledge graph of the communication\nstandard domain containing 13,906 entities and 13,524 relations was\nconstructed, showing a relatively good query accuracy rate. The intelligent\nconsultation and question-answering system enables the fine-tuned model on the\nserver side to access the locally constructed knowledge graph and conduct\ngraphical retrieval of key information first, which is conducive to improving\nthe question-answering effect. The evaluation using DeepSeek as the Judge on\nthe test set shows that our RAG framework enables the fine-tuned model to\nimprove the scores at all five angles, with an average score increase of 2.26%.\nAnd combined with web services and API interfaces, it has achieved very good\nresults in terms of interaction experience and back-end access, and has very\ngood practical application value."}
{"id": "2506.07042", "pdf": "https://arxiv.org/pdf/2506.07042", "abs": "https://arxiv.org/abs/2506.07042", "authors": ["Stergios Chatzikyriakidis"], "title": "Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants", "categories": ["cs.CL"], "comment": null, "summary": "Extracting structured computational representations of historical events from\nnarrative text remains computationally expensive when constructed manually.\nWhile RDF/OWL reasoners enable graph-based reasoning, they are limited to\nfragments of first-order logic, preventing deeper temporal and semantic\nanalysis. This paper addresses both challenges by developing automatic\nhistorical event extraction models using multiple LLMs (GPT-4, Claude, Llama\n3.2) with three enhancement strategies: pure base generation, knowledge graph\nenhancement, and Retrieval-Augmented Generation (RAG). We conducted\ncomprehensive evaluations using historical texts from Thucydides. Our findings\nreveal that enhancement strategies optimize different performance dimensions\nrather than providing universal improvements. For coverage and historical\nbreadth, base generation achieves optimal performance with Claude and GPT-4\nextracting comprehensive events. However, for precision, RAG enhancement\nimproves coordinate accuracy and metadata completeness. Model architecture\nfundamentally determines enhancement sensitivity: larger models demonstrate\nrobust baseline performance with incremental RAG improvements, while Llama 3.2\nshows extreme variance from competitive performance to complete failure. We\nthen developed an automated translation pipeline converting extracted RDF\nrepresentations into Coq proof assistant specifications, enabling higher-order\nreasoning beyond RDF capabilities including multi-step causal verification,\ntemporal arithmetic with BC dates, and formal proofs about historical\ncausation. The Coq formalization validates that RAG-discovered event types\nrepresent legitimate domain-specific semantic structures rather than\nontological violations."}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044", "abs": "https://arxiv.org/abs/2506.07044", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ..."}
{"id": "2506.07064", "pdf": "https://arxiv.org/pdf/2506.07064", "abs": "https://arxiv.org/abs/2506.07064", "authors": ["Kai Xiong", "Xiao Ding", "Yixin Cao", "Yuxiong Yan", "Li Du", "Yufei Zhang", "Jinglong Gao", "Jiaqian Liu", "Bing Qin", "Ting Liu"], "title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) have mastered abundant simple and explicit\ncommonsense knowledge through pre-training, enabling them to achieve human-like\nperformance in simple commonsense reasoning. Nevertheless, LLMs struggle to\nreason with complex and implicit commonsense knowledge that is derived from\nsimple ones (such as understanding the long-term effects of certain events), an\naspect humans tend to focus on more. Existing works focus on complex tasks like\nmath and code, while complex commonsense reasoning remains underexplored due to\nits uncertainty and lack of structure. To fill this gap and align with\nreal-world concerns, we propose a benchmark Com$^2$ focusing on complex\ncommonsense reasoning. We first incorporate causal event graphs to serve as\nstructured complex commonsense. Then we adopt causal theory~(e.g.,\nintervention) to modify the causal event graphs and obtain different scenarios\nthat meet human concerns. Finally, an LLM is employed to synthesize examples\nwith slow thinking, which is guided by the logical relationships in the\nmodified causal graphs. Furthermore, we use detective stories to construct a\nmore challenging subset. Experiments show that LLMs struggle in reasoning depth\nand breadth, while post-training and slow thinking can alleviate this. The code\nand data are available at https://github.com/Waste-Wood/Com2."}
{"id": "2506.07086", "pdf": "https://arxiv.org/pdf/2506.07086", "abs": "https://arxiv.org/abs/2506.07086", "authors": ["Yuanhe Tian", "Pengsen Cheng", "Guoqing Jin", "Lei Zhang", "Yan Song"], "title": "Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing", "categories": ["cs.CL"], "comment": "13 pages, 4 figures", "summary": "Multi-modal affective computing aims to automatically recognize and interpret\nhuman attitudes from diverse data sources such as images and text, thereby\nenhancing human-computer interaction and emotion understanding. Existing\napproaches typically rely on unimodal analysis or straightforward fusion of\ncross-modal information that fail to capture complex and conflicting evidence\npresented across different modalities. In this paper, we propose a novel\nLLM-based approach for affective computing that explicitly deconstructs visual\nand textual representations into shared (modality-invariant) and\nmodality-specific components. Specifically, our approach firstly encodes and\naligns input modalities using pre-trained multi-modal encoders, then employs a\nrepresentation decomposition framework to separate common emotional content\nfrom unique cues, and finally integrates these decomposed signals via an\nattention mechanism to form a dynamic soft prompt for a multi-modal LLM.\nExtensive experiments on three representative tasks for affective computing,\nnamely, multi-modal aspect-based sentiment analysis, multi-modal emotion\nanalysis, and hateful meme detection, demonstrate the effectiveness of our\napproach, which consistently outperforms strong baselines and state-of-the-art\nmodels."}
{"id": "2506.07104", "pdf": "https://arxiv.org/pdf/2506.07104", "abs": "https://arxiv.org/abs/2506.07104", "authors": ["Jiaxuan Gao", "Shu Yan", "Qixin Tan", "Lu Yang", "Shusheng Xu", "Wei Fu", "Zhiyu Mei", "Kaifeng Lyu", "Yi Wu"], "title": "How Far Are We from Optimal Reasoning Efficiency?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge."}
{"id": "2506.07106", "pdf": "https://arxiv.org/pdf/2506.07106", "abs": "https://arxiv.org/abs/2506.07106", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought."}
{"id": "2506.07142", "pdf": "https://arxiv.org/pdf/2506.07142", "abs": "https://arxiv.org/abs/2506.07142", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the second in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate Chain-of-Thought\n(CoT) prompting, a technique that encourages a large language model (LLM) to\n\"think step by step\" (Wei et al., 2022). CoT is a widely adopted method for\nimproving reasoning tasks, however, our findings reveal a more nuanced picture\nof its effectiveness. We demonstrate two things:\n  - The effectiveness of Chain-of-Thought prompting can vary greatly depending\non the type of task and model. For non-reasoning models, CoT generally improves\naverage performance by a small amount, particularly if the model does not\ninherently engage in step-by-step processing by default. However, CoT can\nintroduce more variability in answers, sometimes triggering occasional errors\nin questions the model would otherwise get right. We also found that many\nrecent models perform some form of CoT reasoning even if not asked; for these\nmodels, a request to perform CoT had little impact. Performing CoT generally\nrequires far more tokens (increasing cost and time) than direct answers.\n  - For models designed with explicit reasoning capabilities, CoT prompting\noften results in only marginal, if any, gains in answer accuracy. However, it\nsignificantly increases the time and tokens needed to generate a response."}
{"id": "2506.07148", "pdf": "https://arxiv.org/pdf/2506.07148", "abs": "https://arxiv.org/abs/2506.07148", "authors": ["Yaping Chai", "Haoran Xie", "Joe S. Qin"], "title": "Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis", "categories": ["cs.CL"], "comment": "10 pages, 7 figures, 4 tables", "summary": "Large language model (LLM) is an effective approach to addressing data\nscarcity in low-resource scenarios. Recent existing research designs\nhand-crafted prompts to guide LLM for data augmentation. We introduce a data\naugmentation strategy for the aspect category sentiment analysis (ACSA) task\nthat preserves the original sentence semantics and has linguistic diversity,\nspecifically by providing a structured prompt template for an LLM to generate\npredefined content. In addition, we employ a post-processing technique to\nfurther ensure semantic consistency between the generated sentence and the\noriginal sentence. The augmented data increases the semantic coverage of the\ntraining distribution, enabling the model better to understand the relationship\nbetween aspect categories and sentiment polarities, enhancing its inference\ncapabilities. Furthermore, we propose a confidence-weighted fine-tuning\nstrategy to encourage the model to generate more confident and accurate\nsentiment polarity predictions. Compared with powerful and recent works, our\nmethod consistently achieves the best performance on four benchmark datasets\nover all baselines."}
{"id": "2506.07154", "pdf": "https://arxiv.org/pdf/2506.07154", "abs": "https://arxiv.org/abs/2506.07154", "authors": ["Vicky Xefteri", "Tim Vieira", "Ryan Cotterell", "Afra Amini"], "title": "Syntactic Control of Language Models by Posterior Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Controlling the syntactic structure of text generated by language models is\nvaluable for applications requiring clarity, stylistic consistency, or\ninterpretability, yet it remains a challenging task. In this paper, we argue\nthat sampling algorithms based on the posterior inference can effectively\nenforce a target constituency structure during generation. Our approach\ncombines sequential Monte Carlo, which estimates the posterior distribution by\nsampling from a proposal distribution, with a syntactic tagger that ensures\nthat each generated token aligns with the desired syntactic structure. Our\nexperiments with GPT2 and Llama3-8B models show that with an appropriate\nproposal distribution, we can improve syntactic accuracy, increasing the F1\nscore from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both\ncases without compromising the language model's fluency. These results\nunderscore both the complexity of syntactic control and the effectiveness of\nsampling algorithms, offering a promising approach for applications where\nprecise control over syntax is essential."}
{"id": "2506.07160", "pdf": "https://arxiv.org/pdf/2506.07160", "abs": "https://arxiv.org/abs/2506.07160", "authors": ["Yikun Wang", "Yibin Wang", "Dianyi Wang", "Zimian Peng", "Qipeng Guo", "Dacheng Tao", "Jiaqi Wang"], "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks."}
{"id": "2506.07169", "pdf": "https://arxiv.org/pdf/2506.07169", "abs": "https://arxiv.org/abs/2506.07169", "authors": ["Washington Cunha", "Leonardo Rocha", "Marcos André Gonçalves"], "title": "CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Progress in Natural Language Processing (NLP) has been dictated by the rule\nof more: more data, more computing power and more complexity, best exemplified\nby the Large Language Models. However, training (or fine-tuning) large dense\nmodels for specific applications usually requires significant amounts of\ncomputing resources. This \\textbf{Ph.D. dissertation} focuses on an\nunder-investi\\-gated NLP data engineering technique, whose potential is\nenormous in the current scenario known as Instance Selection (IS). The IS goal\nis to reduce the training set size by removing noisy or redundant instances\nwhile maintaining the effectiveness of the trained models and reducing the\ntraining process cost. We provide a comprehensive and scientifically sound\ncomparison of IS methods applied to an essential NLP task -- Automatic Text\nClassification (ATC), considering several classification solutions and many\ndatasets. Our findings reveal a significant untapped potential for IS\nsolutions. We also propose two novel IS solutions that are noise-oriented and\nredundancy-aware, specifically designed for large datasets and transformer\narchitectures. Our final solution achieved an average reduction of 41\\% in\ntraining sets, while maintaining the same levels of effectiveness in all\ndatasets. Importantly, our solutions demonstrated speedup improvements of 1.67x\n(up to 2.46x), making them scalable for datasets with hundreds of thousands of\ndocuments."}
{"id": "2506.07171", "pdf": "https://arxiv.org/pdf/2506.07171", "abs": "https://arxiv.org/abs/2506.07171", "authors": ["Chenlong Zhang", "Zhuoran Jin", "Hongbang Yuan", "Jiaheng Wei", "Tong Zhou", "Kang Liu", "Jun Zhao", "Yubo Chen"], "title": "RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality", "categories": ["cs.CL", "cs.LG"], "comment": "Paper under review", "summary": "The widespread deployment of Large Language Models (LLMs) trained on massive,\nuncurated corpora has raised growing concerns about the inclusion of sensitive,\ncopyrighted, or illegal content. This has led to increasing interest in LLM\nunlearning: the task of selectively removing specific information from a model\nwithout retraining from scratch or degrading overall utility. However, existing\nmethods often rely on large-scale forget and retain datasets, and suffer from\nunnatural responses, poor generalization, or catastrophic utility loss. In this\nwork, we propose Reinforcement UnLearning (RULE), an efficient framework that\nformulates unlearning as a refusal boundary optimization problem. RULE is\ntrained with a small portion of the forget set and synthesized boundary\nqueries, using a verifiable reward function that encourages safe refusal on\nforget--related queries while preserving helpful responses on permissible\ninputs. We provide both theoretical and empirical evidence demonstrating the\neffectiveness of RULE in achieving targeted unlearning without compromising\nmodel utility. Experimental results show that, with only $12%$ forget set and\n$8%$ synthesized boundary data, RULE outperforms existing baselines by up to\n$17.5%$ forget quality and $16.3%$ naturalness response while maintaining\ngeneral utility, achieving forget--retain Pareto optimality. Remarkably, we\nfurther observe that RULE improves the naturalness of model outputs, enhances\ntraining efficiency, and exhibits strong generalization ability, generalizing\nrefusal behavior to semantically related but unseen queries."}
{"id": "2506.07180", "pdf": "https://arxiv.org/pdf/2506.07180", "abs": "https://arxiv.org/abs/2506.07180", "authors": ["Wenrui Zhou", "Shu Yang", "Qingsong Yang", "Zikun Guo", "Lijie Hu", "Di Wang"], "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "24 pages", "summary": "As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated\nbenchmark designed to evaluate sycophantic behavior in state-of-the-art\nVideo-LLMs across diverse question formats, prompt biases, and visual reasoning\ntasks. Specifically, VISE pioneeringly brings linguistic perspectives on\nsycophancy into the visual domain, enabling fine-grained analysis across\nmultiple sycophancy types and interaction patterns. In addition, we explore\nkey-frame selection as an interpretable, training-free mitigation strategy,\nwhich reveals potential paths for reducing sycophantic bias by strengthening\nvisual grounding."}
{"id": "2506.07245", "pdf": "https://arxiv.org/pdf/2506.07245", "abs": "https://arxiv.org/abs/2506.07245", "authors": ["Wenxuan Xie", "Yaxun Dai", "Wenhao Jiang"], "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement."}
{"id": "2506.07248", "pdf": "https://arxiv.org/pdf/2506.07248", "abs": "https://arxiv.org/abs/2506.07248", "authors": ["Prathamesh Kokate", "Mitali Sarnaik", "Manavi Khopade", "Raviraj Joshi"], "title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Long document classification poses challenges due to the computational\nlimitations of transformer-based models, particularly BERT, which are\nconstrained by fixed input lengths and quadratic attention complexity.\nMoreover, using the full document for classification is often redundant, as\nonly a subset of sentences typically carries the necessary information. To\naddress this, we propose a TF-IDF-based sentence ranking method that improves\nefficiency by selecting the most informative content. Our approach explores\nfixed-count and percentage-based sentence selection, along with an enhanced\nscoring strategy combining normalized TF-IDF scores and sentence length.\nEvaluated on the MahaNews LDC dataset of long Marathi news articles, the method\nconsistently outperforms baselines such as first, last, and random sentence\nselection. With MahaBERT-v2, we achieve near-identical classification accuracy\nwith just a 0.33 percent drop compared to the full-context baseline, while\nreducing input size by over 50 percent and inference latency by 43 percent.\nThis demonstrates that significant context reduction is possible without\nsacrificing performance, making the method practical for real-world long\ndocument classification tasks."}
{"id": "2506.07249", "pdf": "https://arxiv.org/pdf/2506.07249", "abs": "https://arxiv.org/abs/2506.07249", "authors": ["Lance Calvin Lim Gamboa", "Yue Feng", "Mark Lee"], "title": "Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages", "categories": ["cs.CL"], "comment": "Accepted into the Gender Bias in NLP Workshop at ACL 2025\n  (GeBNLP@ACL2025)", "summary": "Emerging research on bias attribution and interpretability have revealed how\ntokens contribute to biased behavior in language models processing English\ntexts. We build on this line of inquiry by adapting the information-theoretic\nbias attribution score metric for implementation on models handling\nagglutinative languages, particularly Filipino. We then demonstrate the\neffectiveness of our adapted method by using it on a purely Filipino model and\non three multilingual models: one trained on languages worldwide and two on\nSoutheast Asian data. Our results show that Filipino models are driven towards\nbias by words pertaining to people, objects, and relationships, entity-based\nthemes that stand in contrast to the action-heavy nature of bias-contributing\nthemes in English (i.e., criminal, sexual, and prosocial behaviors). These\nfindings point to differences in how English and non-English models process\ninputs linked to sociodemographic groups and bias."}
{"id": "2506.07270", "pdf": "https://arxiv.org/pdf/2506.07270", "abs": "https://arxiv.org/abs/2506.07270", "authors": ["Atahan Özer", "Çağatay Yıldız"], "title": "Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in question\nanswering and reasoning thanks to their extensive parametric memory. However,\ntheir knowledge is inherently limited by the scope of their pre-training data,\nwhile real-world information evolves continuously. Updating this knowledge\ntypically requires costly and brittle re-training, or in-context learning\n(ICL), which becomes impractical at scale given the volume and volatility of\nmodern information. Motivated by these limitations, we investigate how LLMs\nperform when exposed to temporal text corpora, or documents that reflect\nevolving knowledge over time, such as sports biographies where facts like a\nplayer's \"current team\" change year by year. To this end, we introduce two new\nbenchmarks: Temporal Wiki, which captures factual drift across historical\nWikipedia snapshots, and Unified Clark, which aggregates timestamped news\narticles to simulate real-world information accumulation. Our analysis reveals\nthat LLMs often struggle to reconcile conflicting or outdated facts and can be\nmisled when multiple versions of a fact appear in context. To address these\nissues, we propose a lightweight, agentic framework that incrementally builds a\nstructured, external memory from source documents without requiring\nre-training. This knowledge organization strategy enables models to retrieve\nand reason over temporally filtered, relevant information at inference time.\nEmpirically, our method outperforms ICL and RAG baselines across both\nbenchmarks, especially on questions requiring more complex reasoning or\nintegration of conflicting facts."}
{"id": "2506.07274", "pdf": "https://arxiv.org/pdf/2506.07274", "abs": "https://arxiv.org/abs/2506.07274", "authors": ["Olga Kellert", "Nemika Tyagi", "Muhammad Imran", "Nelvin Licona-Guevara", "Carlos Gómez-Rodríguez"], "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages", "summary": "Code-switching presents a complex challenge for syntactic analysis,\nespecially in low-resource language settings where annotated data is scarce.\nWhile recent work has explored the use of large language models (LLMs) for\nsequence-level tagging, few approaches systematically investigate how well\nthese models capture syntactic structure in code-switched contexts. Moreover,\nexisting parsers trained on monolingual treebanks often fail to generalize to\nmultilingual and mixed-language input. To address this gap, we introduce the\nBiLingua Parser, an LLM-based annotation pipeline designed to produce Universal\nDependencies (UD) annotations for code-switched text. First, we develop a\nprompt-based framework for Spanish-English and Spanish-Guaran\\'i data,\ncombining few-shot LLM prompting with expert review. Second, we release two\nannotated datasets, including the first Spanish-Guaran\\'i UD-parsed corpus.\nThird, we conduct a detailed syntactic analysis of switch points across\nlanguage pairs and communicative contexts. Experimental results show that\nBiLingua Parser achieves up to 95.29% LAS after expert revision, significantly\noutperforming prior baselines and multilingual parsers. These results show that\nLLMs, when carefully guided, can serve as practical tools for bootstrapping\nsyntactic resources in under-resourced, code-switched environments. Data and\nsource code are available at https://github.com/N3mika/ParsingProject"}
{"id": "2506.07295", "pdf": "https://arxiv.org/pdf/2506.07295", "abs": "https://arxiv.org/abs/2506.07295", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "title": "Exploring the Impact of Temperature on Large Language Models:Hot or Cold?", "categories": ["cs.CL"], "comment": null, "summary": "The sampling temperature, a critical hyperparameter in large language models\n(LLMs), modifies the logits before the softmax layer, thereby reshaping the\ndistribution of output tokens. Recent studies have challenged the Stochastic\nParrots analogy by demonstrating that LLMs are capable of understanding\nsemantics rather than merely memorizing data and that randomness, modulated by\nsampling temperature, plays a crucial role in model inference. In this study,\nwe systematically evaluated the impact of temperature in the range of 0 to 2 on\ndata sets designed to assess six different capabilities, conducting statistical\nanalyses on open source models of three different sizes: small (1B--4B), medium\n(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific\neffects of temperature on model performance, highlighting the complexity of\noptimal temperature selection in practical applications. To address this\nchallenge, we propose a BERT-based temperature selector that takes advantage of\nthese observed effects to identify the optimal temperature for a given prompt.\nWe demonstrate that this approach can significantly improve the performance of\nsmall and medium models in the SuperGLUE datasets. Furthermore, our study\nextends to FP16 precision inference, revealing that temperature effects are\nconsistent with those observed in 4-bit quantized models. By evaluating\ntemperature effects up to 4.0 in three quantized models, we find that the\nMutation Temperature -- the point at which significant performance changes\noccur -- increases with model size."}
{"id": "2506.07297", "pdf": "https://arxiv.org/pdf/2506.07297", "abs": "https://arxiv.org/abs/2506.07297", "authors": ["Lauren Levine", "Amir Zeldes"], "title": "Subjectivity in the Annotation of Bridging Anaphora", "categories": ["cs.CL", "I.2.7"], "comment": "LAW-XIX, ACL 2025 Workshop", "summary": "Bridging refers to the associative relationship between inferable entities in\na discourse and the antecedents which allow us to understand them, such as\nunderstanding what \"the door\" means with respect to an aforementioned \"house\".\nAs identifying associative relations between entities is an inherently\nsubjective task, it is difficult to achieve consistent agreement in the\nannotation of bridging anaphora and their antecedents. In this paper, we\nexplore the subjectivity involved in the annotation of bridging instances at\nthree levels: anaphor recognition, antecedent resolution, and bridging subtype\nselection. To do this, we conduct an annotation pilot on the test set of the\nexisting GUM corpus, and propose a newly developed classification system for\nbridging subtypes, which we compare to previously proposed schemes. Our results\nsuggest that some previous resources are likely to be severely under-annotated.\nWe also find that while agreement on the bridging subtype category was\nmoderate, annotator overlap for exhaustively identifying instances of bridging\nis low, and that many disagreements resulted from subjective understanding of\nthe entities involved."}
{"id": "2506.07309", "pdf": "https://arxiv.org/pdf/2506.07309", "abs": "https://arxiv.org/abs/2506.07309", "authors": ["Yin Huang", "Yifan Ethan Xu", "Kai Sun", "Vera Yan", "Alicia Sun", "Haidar Khan", "Jimmy Nguyen", "Mohammad Kachuee", "Zhaojiang Lin", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "title": "ConfQA: Answer Only If You Are Confident", "categories": ["cs.CL"], "comment": "10 pages main content, 10 pages appendix, 5 figures, 7 tables", "summary": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%."}
{"id": "2506.07326", "pdf": "https://arxiv.org/pdf/2506.07326", "abs": "https://arxiv.org/abs/2506.07326", "authors": ["Brian Christian", "Hannah Rose Kirk", "Jessica A. F. Thompson", "Christopher Summerfield", "Tsvetomira Dumbalska"], "title": "Reward Model Interpretability via Optimal and Pessimal Tokens", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7; H.5.2; J.4; K.4.2"], "comment": "Accepted for publication in Proceedings of the 2025 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT '25), to appear June 2025", "summary": "Reward modeling has emerged as a crucial component in aligning large language\nmodels with human values. Significant attention has focused on using reward\nmodels as a means for fine-tuning generative models. However, the reward models\nthemselves -- which directly encode human value judgments by turning\nprompt-response pairs into scalar rewards -- remain relatively understudied. We\npresent a novel approach to reward model interpretability through exhaustive\nanalysis of their responses across their entire vocabulary space. By examining\nhow different reward models score every possible single-token response to\nvalue-laden prompts, we uncover several striking findings: (i) substantial\nheterogeneity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens, (iii) significant\nsensitivity to prompt framing that mirrors human cognitive biases, and (iv)\novervaluation of more frequent tokens. We demonstrate these effects across ten\nrecent open-source reward models of varying parameter counts and architectures.\nOur results challenge assumptions about the interchangeability of reward\nmodels, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can encode concerning\nbiases toward certain identity groups, which may emerge as unintended\nconsequences of harmlessness training -- distortions that risk propagating\nthrough the downstream large language models now deployed to millions."}
{"id": "2506.07335", "pdf": "https://arxiv.org/pdf/2506.07335", "abs": "https://arxiv.org/abs/2506.07335", "authors": ["Anyi Wang", "Dong Shu", "Yifan Wang", "Yunpu Ma", "Mengnan Du"], "title": "Improving LLM Reasoning through Interpretable Role-Playing Steering", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 8 figures, 8 tables", "summary": "Role-playing has emerged as an effective technique for enhancing the\nreasoning capabilities of large language models (LLMs). However, existing\nmethods primarily rely on prompt engineering, which often lacks stability and\ninterpretability. In this paper, we introduce Sparse Autoencoder Role-Playing\nSteering (SRPS), a novel framework that identifies and manipulates internal\nmodel features associated with role-playing behavior. Our approach extracts\nlatent representations from role-play prompts, selects the most relevant\nfeatures based on activation patterns, and constructs a steering vector that\ncan be injected into the model's residual stream with controllable intensity.\nOur method enables fine-grained control over role-specific behavior and offers\ninsights into how role information influences internal model activations.\nExtensive experiments across various reasoning benchmarks and model sizes\ndemonstrate consistent performance gains. Notably, in the zero-shot\nchain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves\nfrom 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to\n45.10%. These results highlight the potential of SRPS to enhance reasoning\nability in LLMs, providing better interpretability and stability compared to\ntraditional prompt-based role-playing."}
{"id": "2506.07356", "pdf": "https://arxiv.org/pdf/2506.07356", "abs": "https://arxiv.org/abs/2506.07356", "authors": ["Seokil Ham", "Yubin Choi", "Seungju Cho", "Yujin Yang", "Younghun Kim", "Changick Kim"], "title": "Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Recently, major AI service providers such as Google and OpenAI have\nintroduced Finetuning-as-a-Service, which enables users to customize Large\nLanguage Models (LLMs) for specific downstream tasks using their own data.\nHowever, this service is vulnerable to degradation of LLM safety-alignment when\nuser data contains harmful prompts. While some prior works address this issue,\nfundamentally filtering harmful data from user data remains unexplored.\nMotivated by our observation that a directional representation reflecting\nrefusal behavior (called the refusal feature) obtained from safety-aligned LLMs\ncan inherently distinguish between harmful and harmless prompts, we propose the\nRefusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify\nharmful prompts based on the similarity between input prompt features and its\nrefusal feature. During finetuning, the ReFT model serves as a teacher that\nfilters harmful prompts from user data and distills alignment knowledge into\nthe base model. Extensive experiments demonstrate that our ReFT-based\nfinetuning strategy effectively minimizes harmful outputs and enhances\nfinetuning accuracy for user-specific tasks, offering a practical solution for\nsecure and reliable deployment of LLMs in Finetuning-as-a-Service."}
{"id": "2506.07423", "pdf": "https://arxiv.org/pdf/2506.07423", "abs": "https://arxiv.org/abs/2506.07423", "authors": ["Janghyeon Yun", "Sang-goo Lee"], "title": "SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Text-to-SQL enables non-experts to retrieve data from databases by converting\nnatural language queries into SQL. However, state-of-the-art text-to-SQL\nstudies rely on the BIRD dataset, which assumes that evidence is provided along\nwith questions. Although BIRD facilitates research advancements, it assumes\nthat users have expertise and domain knowledge, contradicting the fundamental\ngoal of text-to-SQL. In addition, human-generated evidence in BIRD contains\ndefects, including missing or erroneous evidence, which affects model\nperformance. To address this issue, we propose SEED (System for Evidence\nExtraction and Domain knowledge generation), an approach that automatically\ngenerates evidence to improve performance and practical usability in real-world\nscenarios. SEED systematically analyzes database schema, description files, and\nvalues to extract relevant information. We evaluated SEED on BIRD and Spider,\ndemonstrating that it significantly improves SQL generation accuracy in the\nno-evidence scenario, and in some cases, even outperforms the setting where\nBIRD evidence is provided. Our results highlight that SEED-generated evidence\nnot only bridges the gap between research and real-world deployment but also\nimproves the adaptability and robustness of text-to-SQL models. Our code is\navailable at https://github.com/felix01189/SEED"}
{"id": "2506.07424", "pdf": "https://arxiv.org/pdf/2506.07424", "abs": "https://arxiv.org/abs/2506.07424", "authors": ["Kyeonghyun Kim", "Jinhee Jang", "Juhwan Choi", "Yoonji Lee", "Kyohoon Jin", "YoungBin Kim"], "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference", "summary": "Large language models (LLMs) are renowned for their extensive linguistic\nknowledge and strong generalization capabilities, but their high computational\ndemands make them unsuitable for resource-constrained environments. In\ncontrast, small language models (SLMs) are computationally efficient but often\nlack the broad generalization capacity of LLMs. To bridge this gap, we propose\nPiFi, a novel framework that combines the strengths of both LLMs and SLMs to\nachieve high performance while maintaining efficiency. PiFi integrates a single\nfrozen layer from an LLM into a SLM and fine-tunes the combined model for\nspecific tasks, boosting performance without a significant increase in\ncomputational cost. We show that PiFi delivers consistent performance\nimprovements across a range of natural language processing tasks, including\nboth natural language understanding and generation. Moreover, our findings\ndemonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing\ngeneralization to unseen domains and facilitating the transfer of linguistic\nabilities."}
{"id": "2506.07429", "pdf": "https://arxiv.org/pdf/2506.07429", "abs": "https://arxiv.org/abs/2506.07429", "authors": ["Ratna Kandala"], "title": "Conjoined Predication and Scalar Implicature", "categories": ["cs.CL"], "comment": null, "summary": "Magri (2016) investigates two puzzles arising from conjunction. Although\nMagri has proposed a solution to the second puzzle, the first remains\nunresolved. This first puzzle reveals a hidden interaction among\nquantification, collective/concurrent interpretation, and contextual updating\ndimensions that have yet to be explored. In essence, the problem is that\ncertain forms of sentences like \"Some Italians come from a warm country,\" when\nconjoined as in \"(Only) Some Italians come from a warm country and are blond,\"\nsound infelicitous, even though no obvious alternative triggers a conflicting\nscalar implicature. In this paper, we offer a conceptual analysis of Magri's\nfirst puzzle by situating it within its original theoretical framework. We\nargue that the oddness arises from the collective or concurrent reading of the\nconjunctive predicate: in examples such as \"(Only) Some Italians come from a\nwarm country and are blond,\" this interpretation generates an indirect\ncontextual contradiction. Moreover, we suggest that the pragmatic mechanisms\ngoverning scalar implicature generation extend beyond what is captured by\nexhaustification-based grammatical licensing accounts."}
{"id": "2506.07434", "pdf": "https://arxiv.org/pdf/2506.07434", "abs": "https://arxiv.org/abs/2506.07434", "authors": ["Feifan Song", "Shaohang Wei", "Wen Luo", "Yuxuan Fan", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth."}
{"id": "2506.07438", "pdf": "https://arxiv.org/pdf/2506.07438", "abs": "https://arxiv.org/abs/2506.07438", "authors": ["Jooyoung Choi", "Hyun Kim", "Hansol Jang", "Changwook Jun", "Kyunghoon Bae", "Hyewon Choi", "Stanley Jungkyu Choi", "Honglak Lee", "Chulmin Yun"], "title": "LG-ANNA-Embedding technical report", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This report presents a unified instruction-based framework for learning\ngeneralized text embeddings optimized for both information retrieval (IR) and\nnon-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our\napproach combines in-context learning, soft supervision, and adaptive\nhard-negative mining to generate context-aware embeddings without task-specific\nfine-tuning. Structured instructions and few-shot examples are used to guide\nthe model across diverse tasks, enabling strong performance on classification,\nsemantic similarity, clustering, and reranking benchmarks. To improve semantic\ndiscrimination, we employ a soft labeling framework where continuous relevance\nscores, distilled from a high-performance dense retriever and reranker, serve\nas fine-grained supervision signals. In addition, we introduce adaptive\nmargin-based hard-negative mining, which filters out semantically ambiguous\nnegatives based on their similarity to positive examples, thereby enhancing\ntraining stability and retrieval robustness. Our model is evaluated on the\nnewly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven\ncategories. Results show that our method achieves strong generalization and\nranks among the top-performing models by Borda score, outperforming several\nlarger or fully fine-tuned baselines. These findings highlight the\neffectiveness of combining in-context prompting, soft supervision, and adaptive\nsampling for scalable, high-quality embedding generation."}
{"id": "2506.07453", "pdf": "https://arxiv.org/pdf/2506.07453", "abs": "https://arxiv.org/abs/2506.07453", "authors": ["Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "title": "Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling plays a vital role in uncovering hidden semantic structures\nwithin text corpora, but existing models struggle in low-resource settings\nwhere limited target-domain data leads to unstable and incoherent topic\ninference. We address this challenge by formally introducing domain adaptation\nfor low-resource topic modeling, where a high-resource source domain informs a\nlow-resource target domain without overwhelming it with irrelevant content. We\nestablish a finite-sample generalization bound showing that effective knowledge\ntransfer depends on robust performance in both domains, minimizing latent-space\ndiscrepancy, and preventing overfitting to the data. Guided by these insights,\nwe propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that\nemploys a shared encoder for domain-invariant features, specialized decoders\nfor domain-specific nuances, and adversarial alignment to selectively transfer\nrelevant information. Experiments on diverse low-resource datasets demonstrate\nthat DALTA consistently outperforms state-of-the-art methods in terms of topic\ncoherence, stability, and transferability."}
{"id": "2506.07458", "pdf": "https://arxiv.org/pdf/2506.07458", "abs": "https://arxiv.org/abs/2506.07458", "authors": ["Yuxin Xiao", "Shan Chen", "Jack Gallifant", "Danielle Bitterman", "Thomas Hartvigsen", "Marzyeh Ghassemi"], "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Characterizing a large language model's (LLM's) knowledge of a given question\nis challenging. As a result, prior work has primarily examined LLM behavior\nunder knowledge conflicts, where the model's internal parametric memory\ncontradicts information in the external context. However, this does not fully\nreflect how well the model knows the answer to the question. In this paper, we\nfirst introduce a taxonomy of five knowledge statuses based on the consistency\nand correctness of LLM knowledge modes. We then propose KScope, a hierarchical\nframework of statistical tests that progressively refines hypotheses about\nknowledge modes and characterizes LLM knowledge into one of these five\nstatuses. We apply KScope to nine LLMs across four datasets and systematically\nestablish: (1) Supporting context narrows knowledge gaps across models. (2)\nContext features related to difficulty, relevance, and familiarity drive\nsuccessful knowledge updates. (3) LLMs exhibit similar feature preferences when\npartially correct or conflicted, but diverge sharply when consistently wrong.\n(4) Context summarization constrained by our feature analysis, together with\nenhanced credibility, further improves update effectiveness and generalizes\nacross LLMs."}
{"id": "2506.07461", "pdf": "https://arxiv.org/pdf/2506.07461", "abs": "https://arxiv.org/abs/2506.07461", "authors": ["Siddartha Devic", "Tejas Srinivasan", "Jesse Thomason", "Willie Neiswanger", "Vatsal Sharan"], "title": "From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly assisting users in the real\nworld, yet their reliability remains a concern. Uncertainty quantification (UQ)\nhas been heralded as a tool to enhance human-LLM collaboration by enabling\nusers to know when to trust LLM predictions. We argue that current practices\nfor uncertainty quantification in LLMs are not optimal for developing useful UQ\nfor human users making decisions in real-world tasks. Through an analysis of 40\nLLM UQ methods, we identify three prevalent practices hindering the community's\nprogress toward its goal of benefiting downstream users: 1) evaluating on\nbenchmarks with low ecological validity; 2) considering only epistemic\nuncertainty; and 3) optimizing metrics that are not necessarily indicative of\ndownstream utility. For each issue, we propose concrete user-centric practices\nand research directions that LLM UQ researchers should consider. Instead of\nhill-climbing on unrepresentative tasks using imperfect metrics, we argue that\nthe community should adopt a more human-centered approach to LLM uncertainty\nquantification."}
{"id": "2506.07463", "pdf": "https://arxiv.org/pdf/2506.07463", "abs": "https://arxiv.org/abs/2506.07463", "authors": ["Guang Liu", "Liangdong Wang", "Jijie Li", "Yang Yu", "Yao Xu", "Jiabei Chen", "Yu Bai", "Feng Liao", "Yonghua Lin"], "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly $35$ TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully\ncurated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract $4.5$ billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora."}
{"id": "2506.07479", "pdf": "https://arxiv.org/pdf/2506.07479", "abs": "https://arxiv.org/abs/2506.07479", "authors": ["Haoyuan Li Yusen Zhang", "Snigdha Chaturvedi"], "title": "Improving Fairness of Large Language Models in Multi-document Summarization", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main", "summary": "Fairness in multi-document summarization (MDS) is crucial for providing\ncomprehensive views across documents with diverse social attribute values,\nwhich can significantly impact decision-making. For example, a summarization\nsystem that tends to overrepresent negative reviews of products can mislead\ncustomers into disregarding good products. Previous works measure fairness in\nMDS at two levels: summary-level and corpus-level. While summary-level fairness\nfocuses on individual summaries, corpus-level fairness focuses on a corpus of\nsummaries. Recent methods primarily focus on summary-level fairness. We propose\nFairPO, a preference tuning method that focuses on both summary-level and\ncorpus-level fairness in MDS. To improve summary-level fairness, we propose to\ngenerate preference pairs by perturbing document sets. To improve corpus-level\nfairness, we propose fairness-aware preference tuning by dynamically adjusting\nthe weights of preference pairs. Our experiments show that FairPO outperforms\nstrong baselines while maintaining the critical qualities of summaries. The\ncode is available at https://github.com/leehaoyuan/coverage_fairnes."}
{"id": "2506.07483", "pdf": "https://arxiv.org/pdf/2506.07483", "abs": "https://arxiv.org/abs/2506.07483", "authors": ["Berry Feng", "Jonas Lin", "Patrick Lau"], "title": "A Hybrid GA LLM Framework for Structured Task Optimization", "categories": ["cs.CL"], "comment": "7 pages", "summary": "GA LLM is a hybrid framework that combines Genetic Algorithms with Large\nLanguage Models to handle structured generation tasks under strict constraints.\nEach output, such as a plan or report, is treated as a gene, and evolutionary\noperations like selection, crossover, and mutation are guided by the language\nmodel to iteratively improve solutions. The language model provides domain\nknowledge and creative variation, while the genetic algorithm ensures\nstructural integrity and global optimization. GA LLM has proven effective in\ntasks such as itinerary planning, academic outlining, and business reporting,\nconsistently producing well structured and requirement satisfying results. Its\nmodular design also makes it easy to adapt to new tasks. Compared to using a\nlanguage model alone, GA LLM achieves better constraint satisfaction and higher\nquality solutions by combining the strengths of both components."}
{"id": "2506.07502", "pdf": "https://arxiv.org/pdf/2506.07502", "abs": "https://arxiv.org/abs/2506.07502", "authors": ["Haotian Guo", "Jing Han", "Yongfeng Tu", "Shihao Gao", "Shengfan Shen", "Wulong Xiang", "Weihao Gan", "Zixing Zhang"], "title": "DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech", "categories": ["cs.CL"], "comment": null, "summary": "Despite extensive research on textual and visual disambiguation,\ndisambiguation through speech (DTS) remains underexplored. This is largely due\nto the lack of high-quality datasets that pair spoken sentences with richly\nambiguous text. To address this gap, we present DEBATE, a unique public Chinese\nspeech-text dataset designed to study how speech cues and\npatterns-pronunciation, pause, stress and intonation-can help resolve textual\nambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully\nselected ambiguous utterances, each recorded by 10 native speakers, capturing\ndiverse linguistic ambiguities and their disambiguation through speech. We\ndetail the data collection pipeline and provide rigorous quality analysis.\nAdditionally, we benchmark three state-of-the-art large speech and\naudio-language models, illustrating clear and huge performance gaps between\nmachine and human understanding of spoken intent. DEBATE represents the first\neffort of its kind and offers a foundation for building similar DTS datasets\nacross languages and cultures. The dataset and associated code are available\nat: https://github.com/SmileHnu/DEBATE."}
{"id": "2506.07506", "pdf": "https://arxiv.org/pdf/2506.07506", "abs": "https://arxiv.org/abs/2506.07506", "authors": ["Muhammad Dehan Al Kautsar", "Lucky Susanto", "Derry Wijaya", "Fajri Koto"], "title": "What Do Indonesians Really Need from Language Technology? A Nationwide Survey", "categories": ["cs.CL"], "comment": "26 pages, 12 figures, 5 tables", "summary": "There is an emerging effort to develop NLP for Indonesias 700+ local\nlanguages, but progress remains costly due to the need for direct engagement\nwith native speakers. However, it is unclear what these language communities\ntruly need from language technology. To address this, we conduct a nationwide\nsurvey to assess the actual needs of native speakers in Indonesia. Our findings\nindicate that addressing language barriers, particularly through machine\ntranslation and information retrieval, is the most critical priority. Although\nthere is strong enthusiasm for advancements in language technology, concerns\naround privacy, bias, and the use of public data for AI training highlight the\nneed for greater transparency and clear communication to support broader AI\nadoption."}
{"id": "2506.07510", "pdf": "https://arxiv.org/pdf/2506.07510", "abs": "https://arxiv.org/abs/2506.07510", "authors": ["Solee Im", "Wonjun Lee", "Jinmyeong An", "Yunsu Kim", "Jungseul Ok", "Gary Geunbae Lee"], "title": "DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "We present DeRAGEC, a method for improving Named Entity (NE) correction in\nAutomatic Speech Recognition (ASR) systems. By extending the\nRetrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC\nemploys synthetic denoising rationales to filter out noisy NE candidates before\ncorrection. By leveraging phonetic similarity and augmented definitions, it\nrefines noisy retrieved NEs using in-context learning, requiring no additional\ntraining. Experimental results on CommonVoice and STOP datasets show\nsignificant improvements in Word Error Rate (WER) and NE hit ratio,\noutperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%\nrelative reduction in WER compared to ASR without postprocessing. Our source\ncode is publicly available at: https://github.com/solee0022/deragec"}
{"id": "2506.07523", "pdf": "https://arxiv.org/pdf/2506.07523", "abs": "https://arxiv.org/abs/2506.07523", "authors": ["Sahar Admoni", "Ofra Amir", "Assaf Hallak", "Yftah Ziser"], "title": "Towards Large Language Models with Self-Consistent Natural Language Explanations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) seem to offer an easy path to interpretability:\njust ask them to explain their decisions. Yet, studies show that these post-hoc\nexplanations often misrepresent the true decision process, as revealed by\nmismatches in feature importance. Despite growing evidence of this\ninconsistency, no systematic solutions have emerged, partly due to the high\ncost of estimating feature importance, which limits evaluations to small\ndatasets. To address this, we introduce the Post-hoc Self-Consistency Bank\n(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and\nmodels, each paired with LLM-generated explanations and corresponding feature\nimportance scores. Analysis of PSCB reveals that self-consistency scores barely\ndiffer between correct and incorrect predictions. We also show that the\nstandard metric fails to meaningfully distinguish between explanations. To\novercome this limitation, we propose an alternative metric that more\neffectively captures variation in explanation quality. We use it to fine-tune\nLLMs via Direct Preference Optimization (DPO), leading to significantly better\nalignment between explanations and decision-relevant features, even under\ndomain shift. Our findings point to a scalable path toward more trustworthy,\nself-consistent LLMs."}
{"id": "2506.07541", "pdf": "https://arxiv.org/pdf/2506.07541", "abs": "https://arxiv.org/abs/2506.07541", "authors": ["Sangwhan Moon", "Tatsuya Hiraoka", "Naoaki Okazaki"], "title": "Bit-level BPE: Below the byte boundary", "categories": ["cs.CL"], "comment": null, "summary": "Byte-level fallbacks for subword tokenization have become a common practice\nin large language models. In particular, it has been demonstrated to be\nincredibly effective as a pragmatic solution for preventing OOV, especially in\nthe context of larger models. However, breaking a character down to individual\nbytes significantly increases the sequence length for long-tail tokens in\nlanguages such as Chinese, Japanese, and Korean (CJK) and other\ncharacter-diverse contexts such as emoji. The increased sequence length results\nin longer computation during both training and inference. In this work, we\npropose a simple compression technique that reduces the sequence length\nlosslessly."}
{"id": "2506.07557", "pdf": "https://arxiv.org/pdf/2506.07557", "abs": "https://arxiv.org/abs/2506.07557", "authors": ["Mengsong Wu", "Di Zhang", "Yuqiang Li", "Dongzhan Zhou", "Wenliang Chen"], "title": "SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "While Large Language Models (LLMs) have achieved remarkable success in a wide\nrange of applications, their performance often degrades in complex reasoning\ntasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a\nnovel framework that leverages a modified Monte Carlo Tree Search (MCTS) to\nenhance LLM reasoning without relying on external reward models. By redefining\nthe Upper Confidence Bound scoring to align with intrinsic self-evaluation\ncapabilities of LLMs and decomposing the inference process into atomic subtasks\naugmented with semantic clustering at each node, SELT effectively balances\nexploration and exploitation, reduces redundant reasoning paths, and mitigates\nhallucination. We validate our approach on challenging benchmarks, including\nthe knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT\nachieves significant improvements in answer accuracy and reasoning robustness\ncompared to baseline methods. Notably, our framework operates without\ntask-specific fine-tuning, demonstrating strong generalizability across diverse\nreasoning tasks. Relevant results and code are available at\nhttps://github.com/fairyshine/SELT ."}
{"id": "2506.07583", "pdf": "https://arxiv.org/pdf/2506.07583", "abs": "https://arxiv.org/abs/2506.07583", "authors": ["Ramakrishna Appicharla", "Baban Gain", "Santanu Pal", "Asif Ekbal"], "title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the popularity of the large language models (LLMs), their application\nto machine translation is relatively underexplored, especially in context-aware\nsettings. This work presents a literature review of context-aware translation\nwith LLMs. The existing works utilise prompting and fine-tuning approaches,\nwith few focusing on automatic post-editing and creating translation agents for\ncontext-aware machine translation. We observed that the commercial LLMs (such\nas ChatGPT and Tower LLM) achieved better results than the open-source LLMs\n(such as Llama and Bloom LLMs), and prompt-based approaches serve as good\nbaselines to assess the quality of translations. Finally, we present some\ninteresting future directions to explore."}
{"id": "2506.07597", "pdf": "https://arxiv.org/pdf/2506.07597", "abs": "https://arxiv.org/abs/2506.07597", "authors": ["Oscar Sainz", "Naiara Perez", "Julen Etxaniz", "Joseba Fernandez de Landa", "Itziar Aldabe", "Iker García-Ferrero", "Aimar Zabala", "Ekhi Azurmendi", "German Rigau", "Eneko Agirre", "Mikel Artetxe", "Aitor Soroa"], "title": "Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque", "categories": ["cs.CL"], "comment": "Under review", "summary": "Instructing language models with user intent requires large instruction\ndatasets, which are only available for a limited set of languages. In this\npaper, we explore alternatives to conventional instruction adaptation pipelines\nin low-resource scenarios. We assume a realistic scenario for low-resource\nlanguages, where only the following are available: corpora in the target\nlanguage, existing open-weight multilingual base and instructed backbone LLMs,\nand synthetically generated instructions sampled from the instructed backbone.\nWe present a comprehensive set of experiments for Basque that systematically\nstudy different combinations of these components evaluated on benchmarks and\nhuman preferences from 1,680 participants. Our conclusions show that target\nlanguage corpora are essential, with synthetic instructions yielding robust\nmodels, and, most importantly, that using as backbone an instruction-tuned\nmodel outperforms using a base non-instructed model, and improved results when\nscaling up. Using Llama 3.1 instruct 70B as backbone our model comes near\nfrontier models of much larger sizes for Basque, without using any Basque data\napart from the 1.2B word corpora. We release code, models, instruction\ndatasets, and human preferences to support full reproducibility in future\nresearch on low-resource language adaptation."}
{"id": "2506.07606", "pdf": "https://arxiv.org/pdf/2506.07606", "abs": "https://arxiv.org/abs/2506.07606", "authors": ["Peyman Rostami", "Vahid Rahimzadeh", "Ali Adibi", "Azadeh Shakery"], "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SI", "I.2.7"], "comment": "The dataset is available at https://doi.org/10.5281/zenodo.15616911", "summary": "Stance detection identifies the viewpoint expressed in text toward a specific\ntarget, such as a political figure. While previous datasets have focused\nprimarily on tweet-level stances from established platforms, user-level stance\nresources, especially on emerging platforms like Bluesky remain scarce.\nUser-level stance detection provides a more holistic view by considering a\nuser's complete posting history rather than isolated posts. We present the\nfirst stance detection dataset for the 2024 U.S. presidential election,\ncollected from Bluesky and centered on Kamala Harris and Donald Trump. The\ndataset comprises 16,044 user-target stance pairs enriched with engagement\nmetadata, interaction graphs, and user posting histories. PolitiSky24 was\ncreated using a carefully evaluated pipeline combining advanced information\nretrieval and large language models, which generates stance labels with\nsupporting rationales and text spans for transparency. The labeling approach\nachieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in\npolitical stance analysis through its timeliness, open-data nature, and\nuser-level perspective. The dataset is available at\nhttps://doi.org/10.5281/zenodo.15616911"}
{"id": "2506.07617", "pdf": "https://arxiv.org/pdf/2506.07617", "abs": "https://arxiv.org/abs/2506.07617", "authors": ["Roman Kyslyi", "Yuliia Maksymiuk", "Ihor Pysmennyi"], "title": "Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation", "categories": ["cs.CL"], "comment": "Preprint. Will be published at Proceedings of the Fourth Ukrainian\n  Natural Language Processing Workshop (UNLP)", "summary": "In this paper we introduce the first effort to adapt large language models\n(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and\nmorphologically complex dialect spoken in the Carpathian Highlands. We created\na parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a\ndictionary of 7320 dialectal word mappings. We also addressed data shortage by\nproposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate\nsynthetic parallel translation pairs, expanding the corpus with 52142 examples.\nWe have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a\nstandard-to-dialect translation task, also comparing with few-shot GPT-4o\ntranslation. In the absence of human annotators, we adopt a multi-metric\nevaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment\n(GPT-4o). The results show that even small(7B) finetuned models outperform\nzero-shot baselines such as GPT-4o across both automatic and LLM-evaluated\nmetrics. All data, models, and code are publicly released at:\nhttps://github.com/woters/vuyko-hutsul"}
{"id": "2506.07621", "pdf": "https://arxiv.org/pdf/2506.07621", "abs": "https://arxiv.org/abs/2506.07621", "authors": ["Harsh Bihany", "Shubham Patel", "Ashutosh Modi"], "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025; 21 pages (9 main paper + 5 pages\n  references + 7 pages appendix)", "summary": "Large Language Models have shown remarkable capabilities in the NLP domain.\nTheir effectiveness can mainly be attributed to their ability to adapt to an\narray of downstream tasks. However, generally, full fine-tuning is a\ncomputationally expensive job. To mitigate this, many techniques have been\ndeveloped that prime efficiency, a prominent one being Low-Rank Adaptation\n(LoRA). However, LoRA and its variants employ re-parametrized additive updates.\nIn this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which\nshifts the paradigm of additive updates to a richer space of matrix\nmultiplicative transformations. We tackle challenges such as computational\ncomplexity and rank bottleneck of matrix multiplication by effectively\nre-ordering operations and introducing rank inflation strategies. We conduct\nextensive experiments to demonstrate the effectiveness of our approach in terms\nof various evaluation metrics."}
{"id": "2506.07626", "pdf": "https://arxiv.org/pdf/2506.07626", "abs": "https://arxiv.org/abs/2506.07626", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "title": "Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold great promise for educational applications,\nparticularly in intelligent tutoring systems. However, effective tutoring\nrequires alignment with pedagogical strategies - something current LLMs lack\nwithout task-specific adaptation. In this work, we explore whether fine-grained\nannotation of teacher intents can improve the quality of LLM-generated tutoring\nresponses. We focus on MathDial, a dialog dataset for math instruction, and\napply an automated annotation framework to re-annotate a portion of the dataset\nusing a detailed taxonomy of eleven pedagogical intents. We then fine-tune an\nLLM using these new annotations and compare its performance to models trained\non the original four-category taxonomy. Both automatic and qualitative\nevaluations show that the fine-grained model produces more pedagogically\naligned and effective responses. Our findings highlight the value of intent\nspecificity for controlled text generation in educational settings, and we\nrelease our annotated data and code to facilitate further research."}
{"id": "2506.07631", "pdf": "https://arxiv.org/pdf/2506.07631", "abs": "https://arxiv.org/abs/2506.07631", "authors": ["Brian Gordon", "Yonatan Bitton", "Andreea Marzoca", "Yasumasa Onoe", "Xiao Wang", "Daniel Cohen-Or", "Idan Szpektor"], "title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) now generate highly detailed,\nparagraphlength image captions, yet evaluating their factual accuracy remains\nchallenging. Current methods often miss fine-grained errors, being designed for\nshorter texts or lacking datasets with verified inaccuracies. We introduce\nDOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100\nimages, 14 VLMs) featuring over 10,216 sentence-level human annotations of\nfactual correctness and explanatory rationales for errors, all within paragraph\ncontext. Building on this, we develop VNLI-Critique, a model for automated\nsentence-level factuality classification and critique generation. We highlight\nthree key applications: (1) VNLI-Critique demonstrates robust generalization,\nvalidated by state-of-the-art performance on the M-HalDetect benchmark and\nstrong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven\nAutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent\nalignment with human factuality judgments (e.g., 0.98 Spearman). (3) An\ninnovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide\nLLM-based corrections, achieves substantial improvements in caption factuality\n(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark\nalongside practical tools, designed to significantly elevate the standards for\nfine-grained evaluation and foster the improvement of VLM image understanding.\nProject page: https://google.github.io/unblocking-detail-caption"}
{"id": "2506.07642", "pdf": "https://arxiv.org/pdf/2506.07642", "abs": "https://arxiv.org/abs/2506.07642", "authors": ["Yuan Chang", "Ziyue Li", "Hengyuan Zhang", "Yuanbo Kong", "Yanru Wu", "Zhijiang Guo", "Ngai Wong"], "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "categories": ["cs.CL"], "comment": "30 pages, 17 figures", "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review."}
{"id": "2506.07645", "pdf": "https://arxiv.org/pdf/2506.07645", "abs": "https://arxiv.org/abs/2506.07645", "authors": ["Maciej Chrabąszcz", "Katarzyna Lorenc", "Karolina Seweryn"], "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research."}
{"id": "2506.07646", "pdf": "https://arxiv.org/pdf/2506.07646", "abs": "https://arxiv.org/abs/2506.07646", "authors": ["Rui Hu", "Xiaolong Lin", "Jiawang Liu", "Shixi Huang", "Zhenpeng Zhan"], "title": "Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "In this paper, we propose a method for annotating phonemic and prosodic\nlabels on a given audio-transcript pair, aimed at constructing Japanese\ntext-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale\npre-trained automatic speech recognition (ASR) model, conditioned on ground\ntruth transcripts, to simultaneously output phrase-level graphemes and\nannotation labels. To further correct errors in phonemic labeling, we employ a\ndecoding strategy that utilizes dictionary prior knowledge. The objective\nevaluation results demonstrate that our proposed method outperforms previous\napproaches relying solely on text or audio. The subjective evaluation results\nindicate that the naturalness of speech synthesized by the TTS model, trained\nwith labels annotated using our method, is comparable to that of a model\ntrained with manual annotations."}
{"id": "2506.07658", "pdf": "https://arxiv.org/pdf/2506.07658", "abs": "https://arxiv.org/abs/2506.07658", "authors": ["Nitin Sharma", "Thomas Wolfers", "Çağatay Yıldız"], "title": "Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "35 pages, 24 figures. First submission", "summary": "The paper addresses two critical challenges in language model (LM)\nevaluation: creating reliable domain-specific benchmarks and understanding\nknowledge representation during domain adaptation. We introduce a deterministic\npipeline that converts raw domain corpora into completion-type benchmarks\nwithout relying on LMs or human curation, eliminating benchmark contamination\nissues while enabling evaluation on the latest domain data. Our approach\ngenerates domain-specific keywords and related word lists using TF and Term\nTF-IDF methods and constructs prompt-target pairs. We evaluate models by\nmeasuring their ability to complete these prompts with the correct\ndomain-specific targets, providing a direct assessment of domain knowledge with\nlow computational cost. Through comprehensive experiments across multiple\nmodels (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we\ndemonstrate that our benchmark strongly correlates with expert-generated\nbenchmarks while providing a more accurate measure of domain knowledge than\ntraditional perplexity metrics. We reveal that domain adaptation happens\nrapidly in smaller models (within 500 steps) and illustrate a new approach to\ndomain knowledge evaluation in base models during training for early stopping.\nBy extending mechanistic analysis to domain adaptation, we discover that\ninitial-to-mid layers are primarily responsible for attribute extraction, while\nlater layers focus on next token prediction. Furthermore, we show that during\nadaptation, forgetting begins in the middle layers, where attribute extraction\nhappens and is amplified in later layers. Our work provides both a practical\nevaluation methodology for domain-specific LMs and novel insights into\nknowledge representation during adaptation, with implications for more\nefficient fine-tuning strategies and targeted approaches to mitigate\ncatastrophic forgetting."}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664", "abs": "https://arxiv.org/abs/2506.07664", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities."}
{"id": "2506.07667", "pdf": "https://arxiv.org/pdf/2506.07667", "abs": "https://arxiv.org/abs/2506.07667", "authors": ["Prarabdh Shukla", "Wei Yin Chong", "Yash Patel", "Brennan Schaffner", "Danish Pruthi", "Arjun Bhagoji"], "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "To meet the demands of content moderation, online platforms have resorted to\nautomated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users\ncommenting on live streams) on platforms like Twitch exert additional pressures\non the latency expected of such moderation systems. Despite their prevalence,\nrelatively little is known about the effectiveness of these systems. In this\npaper, we conduct an audit of Twitch's automated moderation tool\n($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful\ncontent. For our audit, we create streaming accounts to act as siloed test\nbeds, and interface with the live chat using Twitch's APIs to send over\n$107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s\naccuracy in flagging blatantly hateful content containing misogyny, racism,\nableism and homophobia. Our experiments reveal that a large fraction of hateful\nmessages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$.\nContextual addition of slurs to these messages results in $100\\%$ removal,\nrevealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We\nalso find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$\nblocks up to $89.5\\%$ of benign examples that use sensitive words in\npedagogical or empowering contexts. Overall, our audit points to large gaps in\n$\\texttt{AutoMod}$'s capabilities and underscores the importance for such\nsystems to understand context effectively."}
{"id": "2506.07671", "pdf": "https://arxiv.org/pdf/2506.07671", "abs": "https://arxiv.org/abs/2506.07671", "authors": ["Ionut-Teodor Sorodoc", "Leonardo F. R. Ribeiro", "Rexhina Blloshmi", "Christopher Davis", "Adrià de Gispert"], "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers\nand annotations of each grounding passage, allowing a fine-grained evaluation\nof whether LLMs can identify relevant grounding when generating RAG answers.\nOur benchmark contains 2366 questions of diverse complexity, dynamism, and\ntopics, and includes over 35K annotated passages retrieved from both private\ndocument sets and the Web, to reflect real-world RAG use cases. This makes it\nan ideal test bed to evaluate an LLM's ability to identify only the relevant\ninformation necessary to compose a response, or provide a deflective response\nwhen there is insufficient information. Evaluations of multiple\nstate-of-the-art LLMs on GaRAGe show that the models tend to over-summarise\nrather than (a) ground their answers strictly on the annotated relevant\npassages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)\ndeflect when no relevant grounding is available (reaching at most 31% true\npositive rate in deflections). The F1 in attribution to relevant sources is at\nmost 58.9%, and we show that performance is particularly reduced when answering\ntime-sensitive questions and when having to draw knowledge from sparser private\ngrounding sources."}
{"id": "2506.07691", "pdf": "https://arxiv.org/pdf/2506.07691", "abs": "https://arxiv.org/abs/2506.07691", "authors": ["Jiaming Li", "Haoran Ye", "Yukun Chen", "Xinyue Li", "Lei Zhang", "Hamid Alinejad-Rokny", "Jimmy Chih-Hsien Peng", "Min Yang"], "title": "Training Superior Sparse Autoencoders for Instruct Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) grow in scale and capability, understanding\ntheir internal mechanisms becomes increasingly critical. Sparse autoencoders\n(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the\nextraction of human-interpretable features from LLMs. However, existing SAE\ntraining methods are primarily designed for base models, resulting in reduced\nreconstruction quality and interpretability when applied to instruct models. To\nbridge this gap, we propose\n$\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned\n$\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining\n($\\textit{FAST}$), a novel training method specifically tailored for instruct\nmodels. $\\textit{FAST}$ aligns the training process with the data distribution\nand activation patterns characteristic of instruct models, resulting in\nsubstantial improvements in both reconstruction and feature interpretability.\nOn Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468\nin token reconstruction, significantly outperforming baseline methods with\nerrors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$\nyields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,\n$21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for\n$\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that\nintervening on the activations of special tokens via the SAEs leads to\nimprovements in output quality, suggesting new opportunities for fine-grained\ncontrol of model behavior. Code, data, and 240 trained SAEs are available at\nhttps://github.com/Geaming2002/FAST."}
{"id": "2506.07712", "pdf": "https://arxiv.org/pdf/2506.07712", "abs": "https://arxiv.org/abs/2506.07712", "authors": ["Renjie Luo", "Jiaxi Li", "Chen Huang", "Wei Lu"], "title": "Through the Valley: Path to Effective Long CoT Training for Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\nexamples lose up to 75% of their original performance before fine-tuning.\nStrikingly, we further observe that for some particularly small models, even\ntraining on 220k long CoT examples fails to recover or surpass their original\nperformance prior to fine-tuning. Our analysis attributes this effect to error\naccumulation: while longer responses increase the capacity for multi-step\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\nlearning (RL), although this can be alleviated by sufficiently scaled\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\nthe benefits of long CoT training for SLMs and offer practical guidance for\nbuilding more effective small-scale reasoning models."}
{"id": "2506.07719", "pdf": "https://arxiv.org/pdf/2506.07719", "abs": "https://arxiv.org/abs/2506.07719", "authors": ["Mengyang Qiu", "Tran Minh Nguyen", "Zihao Huang", "Zelong Li", "Yang Gu", "Qingyu Gao", "Siliang Liu", "Jungyeul Park"], "title": "Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility", "categories": ["cs.CL"], "comment": "BEA2025", "summary": "Grammatical Error Correction (GEC) relies on accurate error annotation and\nevaluation, yet existing frameworks, such as $\\texttt{errant}$, face\nlimitations when extended to typologically diverse languages. In this paper, we\nintroduce a standardized, modular framework for multilingual grammatical error\nannotation. Our approach combines a language-agnostic foundation with\nstructured language-specific extensions, enabling both consistency and\nflexibility across languages. We reimplement $\\texttt{errant}$ using\n$\\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the\nframework's adaptability through applications to English, German, Czech,\nKorean, and Chinese, ranging from general-purpose annotation to more customized\nlinguistic refinements. This work supports scalable and interpretable GEC\nannotation across languages and promotes more consistent evaluation in\nmultilingual settings. The complete codebase and annotation tools can be\naccessed at https://github.com/open-writing-evaluation/jp_errant_bea."}
{"id": "2506.07726", "pdf": "https://arxiv.org/pdf/2506.07726", "abs": "https://arxiv.org/abs/2506.07726", "authors": ["Vincenzo Timmel", "Manfred Vogel", "Daniel Perruchoud", "Reza Kakooee"], "title": "Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a new long-form release of the Swiss Parliaments Corpus,\nconverting entire multi-hour Swiss German debate sessions (each aligned with\nthe official session protocols) into high-quality speech-text pairs. Our\npipeline starts by transcribing all session audio into Standard German using\nWhisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o\ncorrection process: first, GPT-4o ingests the raw Whisper output alongside the\nofficial protocols to refine misrecognitions, mainly named entities. Second, a\nseparate GPT-4o pass evaluates each refined segment for semantic completeness.\nWe filter out any segments whose Predicted BLEU score (derived from Whisper's\naverage token log-probability) and GPT-4o evaluation score fall below a certain\nthreshold. The final corpus contains 801 hours of audio, of which 751 hours\npass our quality control. Compared to the original sentence-level SPC release,\nour long-form dataset achieves a 6-point BLEU improvement, demonstrating the\npower of combining robust ASR, LLM-based correction, and data-driven filtering\nfor low-resource, domain-specific speech corpora."}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751", "abs": "https://arxiv.org/abs/2506.07751", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstraL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks."}
{"id": "2506.07795", "pdf": "https://arxiv.org/pdf/2506.07795", "abs": "https://arxiv.org/abs/2506.07795", "authors": ["Xiaotian Ye", "Mengqi Zhang", "Shu Wu"], "title": "LLM Unlearning Should Be Form-Independent", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs."}
{"id": "2506.07801", "pdf": "https://arxiv.org/pdf/2506.07801", "abs": "https://arxiv.org/abs/2506.07801", "authors": ["Iustin Sirbu", "Robert-Adrian Popovici", "Cornelia Caragea", "Stefan Trausan-Matu", "Traian Rebedea"], "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks."}
{"id": "2506.07818", "pdf": "https://arxiv.org/pdf/2506.07818", "abs": "https://arxiv.org/abs/2506.07818", "authors": ["Zhiyu Lin", "Zhengda Zhou", "Zhiyuan Zhao", "Tianrui Wan", "Yilun Ma", "Junyu Gao", "Xuelong Li"], "title": "WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Generative AI technology, Multimodal Large\nLanguage Models(MLLMs) have the potential to act as AI software engineers\ncapable of executing complex web application development. Considering that the\nmodel requires a confluence of multidimensional sub-capabilities to address the\nchallenges of various development phases, constructing a multi-view evaluation\nframework is crucial for accurately guiding the enhancement of development\nefficiency. However, existing benchmarks usually fail to provide an assessment\nof sub-capabilities and focus solely on webpage generation outcomes. In this\nwork, we draw inspiration from the principles of software engineering and\nfurther propose WebUIBench, a benchmark systematically designed to evaluate\nMLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML\nUnderstanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality\nquestion-answer pairs derived from over 0.7K real-world websites. The extensive\nevaluation of 29 mainstream MLLMs uncovers the skill characteristics and\nvarious weakness that models encountered during the development process."}
{"id": "2506.07851", "pdf": "https://arxiv.org/pdf/2506.07851", "abs": "https://arxiv.org/abs/2506.07851", "authors": ["Yiju Guo", "Wenkai Yang", "Zexu Sun", "Ning Ding", "Zhiyuan Liu", "Yankai Lin"], "title": "Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant improvements in\ncontextual understanding. However, their ability to attend to truly critical\ninformation during long-context reasoning and generation still falls behind the\npace. Specifically, our preliminary experiments reveal that certain distracting\npatterns can misdirect the model's attention during inference, and removing\nthese patterns substantially improves reasoning accuracy and generation\nquality. We attribute this phenomenon to spurious correlations in the training\ndata, which obstruct the model's capacity to infer authentic causal\ninstruction-response relationships. This phenomenon may induce redundant\nreasoning processes, potentially resulting in significant inference overhead\nand, more critically, the generation of erroneous or suboptimal responses. To\nmitigate this, we introduce a two-stage framework called Learning to Focus\n(LeaF) leveraging intervention-based inference to disentangle confounding\nfactors. In the first stage, LeaF employs gradient-based comparisons with an\nadvanced teacher to automatically identify confounding tokens based on causal\nrelationships in the training corpus. Then, in the second stage, it prunes\nthese tokens during distillation to enact intervention, aligning the student's\nattention with the teacher's focus distribution on truly critical context\ntokens. Experimental results demonstrate that LeaF not only achieves an\nabsolute improvement in various mathematical reasoning and code generation\nbenchmarks but also effectively suppresses attention to confounding tokens\nduring inference, yielding a more interpretable and reliable reasoning model."}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899", "abs": "https://arxiv.org/abs/2506.07899", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably - without retraining or forgetting previous\ninformation - remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks across LLaMA-3 and Mistral\ndemonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting."}
{"id": "2506.07900", "pdf": "https://arxiv.org/pdf/2506.07900", "abs": "https://arxiv.org/abs/2506.07900", "authors": ["MiniCPM Team", "Chaojun Xiao", "Yuxuan Li", "Xu Han", "Yuzhuo Bai", "Jie Cai", "Haotian Chen", "Wentong Chen", "Xin Cong", "Ganqu Cui", "Ning Ding", "Shengdan Fan", "Yewei Fang", "Zixuan Fu", "Wenyu Guan", "Yitong Guan", "Junshao Guo", "Yufeng Han", "Bingxiang He", "Yuxiang Huang", "Cunliang Kong", "Qiuzuo Li", "Siyuan Li", "Wenhao Li", "Yanghao Li", "Yishan Li", "Zhen Li", "Dan Liu", "Biyuan Lin", "Yankai Lin", "Xiang Long", "Quanyu Lu", "Yaxi Lu", "Peiyan Luo", "Hongya Lyu", "Litu Ou", "Yinxu Pan", "Zekai Qu", "Qundong Shi", "Zijun Song", "Jiayuan Su", "Zhou Su", "Ao Sun", "Xianghui Sun", "Peijun Tang", "Fangzheng Wang", "Feng Wang", "Shuo Wang", "Yudong Wang", "Yesai Wu", "Zhenyu Xiao", "Jie Xie", "Zihao Xie", "Yukun Yan", "Jiarui Yuan", "Kaihuo Zhang", "Lei Zhang", "Linyue Zhang", "Xueren Zhang", "Yudi Zhang", "Hengyu Zhao", "Weilin Zhao", "Weilun Zhao", "Yuanqian Zhao", "Zhi Zheng", "Ge Zhou", "Jie Zhou", "Wei Zhou", "Zihan Zhou", "Zixuan Zhou", "Zhiyuan Liu", "Guoyang Zeng", "Chao Jia", "Dahai Li", "Maosong Sun"], "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "categories": ["cs.CL", "cs.AI"], "comment": "MiniCPM4 Technical Report", "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability."}
{"id": "2506.07937", "pdf": "https://arxiv.org/pdf/2506.07937", "abs": "https://arxiv.org/abs/2506.07937", "authors": ["Shamminuj Aktar", "Andreas Bärtschi", "Abdel-Hameed A. Badawy", "Stephan Eidenbenz"], "title": "Quantum Graph Transformer for NLP Sentiment Classification", "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "Quantum machine learning is a promising direction for building more efficient\nand expressive models, particularly in domains where understanding complex,\nstructured data is critical. We present the Quantum Graph Transformer (QGT), a\nhybrid graph-based architecture that integrates a quantum self-attention\nmechanism into the message-passing framework for structured language modeling.\nThe attention mechanism is implemented using parameterized quantum circuits\n(PQCs), which enable the model to capture rich contextual relationships while\nsignificantly reducing the number of trainable parameters compared to classical\nattention mechanisms. We evaluate QGT on five sentiment classification\nbenchmarks. Experimental results show that QGT consistently achieves higher or\ncomparable accuracy than existing quantum natural language processing (QNLP)\nmodels, including both attention-based and non-attention-based approaches. When\ncompared with an equivalent classical graph transformer, QGT yields an average\naccuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic\ndatasets. Additionally, QGT demonstrates improved sample efficiency, requiring\nnearly 50% fewer labeled samples to reach comparable performance on the Yelp\ndataset. These results highlight the potential of graph-based QNLP techniques\nfor advancing efficient and scalable language understanding."}
{"id": "2506.07947", "pdf": "https://arxiv.org/pdf/2506.07947", "abs": "https://arxiv.org/abs/2506.07947", "authors": ["Paulius Rauba", "Qiyao Wei", "Mihaela van der Schaar"], "title": "Statistical Hypothesis Testing for Auditing Robustness in Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00868", "summary": "Consider the problem of testing whether the outputs of a large language model\n(LLM) system change under an arbitrary intervention, such as an input\nperturbation or changing the model variant. We cannot simply compare two LLM\noutputs since they might differ due to the stochastic nature of the system, nor\ncan we compare the entire output distribution due to computational\nintractability. While existing methods for analyzing text-based outputs exist,\nthey focus on fundamentally different problems, such as measuring bias or\nfairness. To this end, we introduce distribution-based perturbation analysis, a\nframework that reformulates LLM perturbation analysis as a frequentist\nhypothesis testing problem. We construct empirical null and alternative output\ndistributions within a low-dimensional semantic similarity space via Monte\nCarlo sampling, enabling tractable inference without restrictive distributional\nassumptions. The framework is (i) model-agnostic, (ii) supports the evaluation\nof arbitrary input perturbations on any black-box LLM, (iii) yields\ninterpretable p-values; (iv) supports multiple perturbations via controlled\nerror rates; and (v) provides scalar effect sizes. We demonstrate the\nusefulness of the framework across multiple case studies, showing how we can\nquantify response changes, measure true/false positive rates, and evaluate\nalignment with reference models. Above all, we see this as a reliable\nfrequentist hypothesis testing framework for LLM auditing."}
{"id": "2506.07956", "pdf": "https://arxiv.org/pdf/2506.07956", "abs": "https://arxiv.org/abs/2506.07956", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "title": "Language Models over Canonical Byte-Pair Encodings", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "ICML 2025", "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora."}
{"id": "2506.07962", "pdf": "https://arxiv.org/pdf/2506.07962", "abs": "https://arxiv.org/abs/2506.07962", "authors": ["Elliot Kim", "Avi Garg", "Kenny Peng", "Nikhil Garg"], "title": "Correlated Errors in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture."}
{"id": "2506.08007", "pdf": "https://arxiv.org/pdf/2506.08007", "abs": "https://arxiv.org/abs/2506.08007", "authors": ["Qingxiu Dong", "Li Dong", "Yao Tang", "Tianzhu Ye", "Yutao Sun", "Zhifang Sui", "Furu Wei"], "title": "Reinforcement Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training."}
{"id": "2506.06294", "pdf": "https://arxiv.org/pdf/2506.06294", "abs": "https://arxiv.org/abs/2506.06294", "authors": ["Yunqing Liu", "Wenqi Fan", "Xiaoyong Wei", "Qing Li"], "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on."}
{"id": "2506.06295", "pdf": "https://arxiv.org/pdf/2506.06295", "abs": "https://arxiv.org/abs/2506.06295", "authors": ["Zhiyuan Liu", "Yicun Yang", "Yaojie Zhang", "Junjie Chen", "Chang Zou", "Qingyuan Wei", "Shaobo Wang", "Linfeng Zhang"], "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Autoregressive Models (ARMs) have long dominated the landscape of Large\nLanguage Models. Recently, a new paradigm has emerged in the form of\ndiffusion-based Large Language Models (dLLMs), which generate text by\niteratively denoising masked segments. This approach has shown significant\nadvantages and potential. However, dLLMs suffer from high inference latency.\nTraditional ARM acceleration techniques, such as Key-Value caching, are\nincompatible with dLLMs due to their bidirectional attention mechanism. To\naddress this specific challenge, our work begins with a key observation that\ndLLM inference involves a static prompt and a partially dynamic response, where\nmost tokens remain stable across adjacent denoising steps. Based on this, we\npropose dLLM-Cache, a training-free adaptive caching framework that combines\nlong-interval prompt caching with partial response updates guided by feature\nsimilarity. This design enables efficient reuse of intermediate computations\nwithout compromising model performance. Extensive experiments on representative\ndLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1\nx speedup over standard inference without compromising output quality. Notably,\nour method brings dLLM inference latency close to that of ARMs under many\nsettings. Codes are provided in the supplementary material and will be released\npublicly on GitHub."}
{"id": "2506.06299", "pdf": "https://arxiv.org/pdf/2506.06299", "abs": "https://arxiv.org/abs/2506.06299", "authors": ["Daniel Thilo Schroeder", "Meeyoung Cha", "Andrea Baronchelli", "Nick Bostrom", "Nicholas A. Christakis", "David Garcia", "Amit Goldenberg", "Yara Kyrychenko", "Kevin Leyton-Brown", "Nina Lutz", "Gary Marcus", "Filippo Menczer", "Gordon Pennycook", "David G. Rand", "Frank Schweitzer", "Christopher Summerfield", "Audrey Tang", "Jay Van Bavel", "Sander van der Linden", "Dawn Song", "Jonas R. Kunst"], "title": "How Malicious AI Swarms Can Threaten Democracy", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages, 1 figure", "summary": "Advances in AI portend a new era of sophisticated disinformation operations.\nWhile individual AI systems already create convincing -- and at times\nmisleading -- information, an imminent development is the emergence of\nmalicious AI swarms. These systems can coordinate covertly, infiltrate\ncommunities, evade traditional detectors, and run continuous A/B tests, with\nround-the-clock persistence. The result can include fabricated grassroots\nconsensus, fragmented shared reality, mass harassment, voter micro-suppression\nor mobilization, contamination of AI training data, and erosion of\ninstitutional trust. With democratic processes worldwide increasingly\nvulnerable, we urge a three-pronged response: (1) platform-side defenses --\nalways-on swarm-detection dashboards, pre-election high-fidelity\nswarm-simulation stress-tests, transparency audits, and optional client-side\n\"AI shields\" for users; (2) model-side safeguards -- standardized\npersuasion-risk tests, provenance-authenticating passkeys, and watermarking;\nand (3) system-level oversight -- a UN-backed AI Influence Observatory."}
{"id": "2506.06303", "pdf": "https://arxiv.org/pdf/2506.06303", "abs": "https://arxiv.org/abs/2506.06303", "authors": ["Kefan Song", "Amir Moeini", "Peng Wang", "Lei Gong", "Rohan Chandra", "Yanjun Qi", "Shangtong Zhang"], "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) is a human-designed framework for solving\nsequential decision making problems. In this work, we demonstrate that,\nsurprisingly, RL emerges in LLM's (Large Language Model) inference time -- a\nphenomenon known as in-context RL (ICRL). Specifically, we propose a novel\nmulti-round prompting framework called ICRL prompting. The goal is to prompt\nthe LLM to complete a task. After the LLM generates a response at the current\nround, we give numerical scalar feedbacks for the response, called the rewards.\nAt the next round, we prompt the LLM again with the same task and a context\nconsisting of all previous responses and rewards. We observe that the quality\nof the LLM's response increases as the context grows. In other words, the LLM\nis able to maximize the scalar reward signal in the inference time, just like\nan RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,\ncreative writing, and ScienceWorld) and demonstrate significant performance\nimprovements over baseline methods such as Self-Refine and Reflexion.\nSurprisingly, in some experiments the reward signals are generated by the LLM\nitself, yet performance improvements are still observed from ICRL prompting,\noffering a promising paradigm for scaling test-time compute."}
{"id": "2506.06313", "pdf": "https://arxiv.org/pdf/2506.06313", "abs": "https://arxiv.org/abs/2506.06313", "authors": ["Huiyao Chen", "Yi Yang", "Yinghui Li", "Meishan Zhang", "Min Zhang"], "title": "DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "21 pages, 7 figures", "summary": "Long document understanding has become increasingly crucial in natural\nlanguage processing, with retrieval-based methods emerging as a promising\nsolution to address the context length limitations of large language models\n(LLMs). However, existing approaches either treat documents as flat sequences\nor employ arbitrary chunking strategies, failing to capture the inherent\ndiscourse structure that guides human comprehension. We present DISRetrieval, a\nnovel hierarchical retrieval framework that leverages linguistic discourse\nstructure to enhance long document understanding. Our approach introduces three\nkey innovations: (1) a discourse-aware document organization framework that\nutilizes rhetorical structure theory (RST) to create sentence-level\nhierarchical representations, preserving both semantic relationships and\nnatural document flow; (2) an LLM-enhanced node representation technique that\ncombines discourse structure with adaptive summarization to enrich tree nodes\nwith contextual information; and (3) a hierarchical evidence retrieval\nmechanism that effectively selects relevant content while maintaining discourse\ncoherence. Through comprehensive experiments on QASPER and QuALITY datasets,\nDISRetrieval demonstrates substantial improvements over existing methods in\nboth token-level retrieval metrics and downstream question answering tasks. Our\nablation studies confirm that incorporating discourse structure significantly\nenhances retrieval effectiveness across different document lengths and query\ntypes, validating the importance of linguistically-informed document\nrepresentation in long-text understanding. Our code and datasets are publicly\navailable at github/DreamH1gh/DISRetrieval to facilitate future research."}
{"id": "2506.06328", "pdf": "https://arxiv.org/pdf/2506.06328", "abs": "https://arxiv.org/abs/2506.06328", "authors": ["Aziida Nanyonga", "Joiner Keith", "Turhan Ugur", "Wild Graham"], "title": "Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "This study compares the effectiveness of BERTopic and Probabilistic Latent\nSemantic Analysis (PLSA) in extracting meaningful topics from aviation safety\nreports aiming to enhance the understanding of patterns in aviation incident\ndata. Using a dataset of over 36,000 National Transportation Safety Board\n(NTSB) reports from 2000 to 2020, BERTopic employed transformer based\nembeddings and hierarchical clustering, while PLSA utilized probabilistic\nmodelling through the Expectation-Maximization (EM) algorithm. Results showed\nthat BERTopic outperformed PLSA in topic coherence, achieving a Cv score of\n0.41 compared to PLSA 0.37, while also demonstrating superior interpretability\nas validated by aviation safety experts. These findings underscore the\nadvantages of modern transformer based approaches in analyzing complex aviation\ndatasets, paving the way for enhanced insights and informed decision-making in\naviation safety. Future work will explore hybrid models, multilingual datasets,\nand advanced clustering techniques to further improve topic modelling in this\ndomain."}
{"id": "2506.06329", "pdf": "https://arxiv.org/pdf/2506.06329", "abs": "https://arxiv.org/abs/2506.06329", "authors": ["Zheng Cao", "Wanchaloem Wunkaew", "Helyette Geman"], "title": "The Hype Index: an NLP-driven Measure of Market News Attention", "categories": ["q-fin.ST", "cs.CE", "cs.CL"], "comment": null, "summary": "This paper introduces the Hype Index as a novel metric to quantify media\nattention toward large-cap equities, leveraging advances in Natural Language\nProcessing (NLP) for extracting predictive signals from financial news. Using\nthe S&P 100 as the focus universe, we first construct a News Count-Based Hype\nIndex, which measures relative media exposure by computing the share of news\narticles referencing each stock or sector. We then extend it to the\nCapitalization Adjusted Hype Index, adjusts for economic size by taking the\nratio of a stock's or sector's media weight to its market capitalization weight\nwithin its industry or sector. We compute both versions of the Hype Index at\nthe stock and sector levels, and evaluate them through multiple lenses: (1)\ntheir classification into different hype groups, (2) their associations with\nreturns, volatility, and VIX index at various lags, (3) their signaling power\nfor short-term market movements, and (4) their empirical properties including\ncorrelations, samplings, and trends. Our findings suggest that the Hype Index\nfamily provides a valuable set of tools for stock volatility analysis, market\nsignaling, and NLP extensions in Finance."}
{"id": "2506.06335", "pdf": "https://arxiv.org/pdf/2506.06335", "abs": "https://arxiv.org/abs/2506.06335", "authors": ["Xuan Xu", "Fufang Wen", "Beilin Chu", "Zhibing Fu", "Qinhong Lin", "Jiaqi Liu", "Binjie Fei", "Zhongliang Yang", "Linna Zhou", "Yu Li"], "title": "FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "In natural language processing (NLP), the focus has shifted from encoder-only\ntiny language models like BERT to decoder-only large language models(LLMs) such\nas GPT-3. However, LLMs' practical application in the financial sector has\nrevealed three limitations: (1) LLMs often perform worse than fine-tuned BERT\non discriminative tasks despite costing much higher computational resources,\nsuch as market sentiment analysis in financial reports; (2) Application on\ngenerative tasks heavily relies on retrieval augmented generation (RAG) methods\nto provide current and specialized information, with general retrievers showing\nsuboptimal performance on domain-specific retrieval tasks; (3) There are\nadditional inadequacies in other feature-based scenarios, such as topic\nmodeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained\non a high-quality, financial-specific corpus of 32b tokens. This represents the\nlargest known Chinese financial pretraining corpus for models of this parameter\nsize. As a better backbone, FinBERT2 can bridge the gap in the\nfinancial-specific deployment of LLMs through the following achievements: (1)\nDiscriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT\nvariants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five\nfinancial classification tasks. (2) Contrastive fine-tuned models\n(Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over\nBGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's\ntext-embedding-3-large) embedders across five financial retrieval tasks; (3)\nBuilding on FinBERT2 variants, we construct the Fin-TopicModel, which enables\nsuperior clustering and topic representation for financial titles. Our work\nrevisits financial BERT models through comparative analysis with contemporary\nLLMs and offers practical insights for effectively utilizing FinBERT in the\nLLMs era."}
{"id": "2506.06339", "pdf": "https://arxiv.org/pdf/2506.06339", "abs": "https://arxiv.org/abs/2506.06339", "authors": ["Jumana Alsubhi", "Mohammad D. Alahmadi", "Ahmed Alhusayni", "Ibrahim Aldailami", "Israa Hamdine", "Ahmad Shabana", "Yazeed Iskandar", "Suhayb Khayyat"], "title": "Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture\nfor combining the precision of retrieval systems with the fluency of large\nlanguage models. While several studies have investigated RAG pipelines for\nhigh-resource languages, the optimization of RAG components for Arabic remains\nunderexplored. This study presents a comprehensive empirical evaluation of\nstate-of-the-art RAG components-including chunking strategies, embedding\nmodels, rerankers, and language models-across a diverse set of Arabic datasets.\nUsing the RAGAS framework, we systematically compare performance across four\ncore metrics: context precision, context recall, answer faithfulness, and\nanswer relevancy. Our experiments demonstrate that sentence-aware chunking\noutperforms all other segmentation methods, while BGE-M3 and\nMultilingual-E5-large emerge as the most effective embedding models. The\ninclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness\nin complex datasets, and Aya-8B surpasses StableLM in generation quality. These\nfindings provide critical insights for building high-quality Arabic RAG\npipelines and offer practical guidelines for selecting optimal components\nacross different document types."}
{"id": "2506.06355", "pdf": "https://arxiv.org/pdf/2506.06355", "abs": "https://arxiv.org/abs/2506.06355", "authors": ["Lingyao Li", "Dawei Li", "Zhenhui Ou", "Xiaoran Xu", "Jingxiao Liu", "Zihui Ma", "Runlong Yu", "Min Deng"], "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.CV"], "comment": null, "summary": "Efficient simulation is essential for enhancing proactive preparedness for\nsudden-onset disasters such as earthquakes. Recent advancements in large\nlanguage models (LLMs) as world models show promise in simulating complex\nscenarios. This study examines multiple LLMs to proactively estimate perceived\nearthquake impacts. Leveraging multimodal datasets including geospatial,\nsocioeconomic, building, and street-level imagery data, our framework generates\nModified Mercalli Intensity (MMI) predictions at zip code and county scales.\nEvaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did\nYou Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced\nby a high correlation of 0.88 and a low RMSE of 0.77 as compared to real\nreports at the zip code level. Techniques such as RAG and ICL can improve\nsimulation performance, while visual inputs notably enhance accuracy compared\nto structured numerical data alone. These findings show the promise of LLMs in\nsimulating disaster impacts that can help strengthen pre-event planning."}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382", "abs": "https://arxiv.org/abs/2506.06382", "authors": ["Michał P. Karpowicz"], "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": null, "summary": "This paper explains \\textbf{why it is impossible to create large language\nmodels that do not hallucinate and what are the trade-offs we should be looking\nfor}. It presents a formal \\textbf{impossibility theorem} demonstrating that no\ninference mechanism can simultaneously satisfy four fundamental properties:\n\\textbf{truthful (non-hallucinatory) generation, semantic information\nconservation, relevant knowledge revelation, and knowledge-constrained\noptimality}. By modeling LLM inference as an \\textbf{auction of ideas} where\nneural components compete to contribute to responses, we prove the\nimpossibility using the Green-Laffont theorem. That mathematical framework\nprovides a rigorous foundation for understanding the nature of inference\nprocess, with implications for model architecture, training objectives, and\nevaluation methods."}
{"id": "2506.06391", "pdf": "https://arxiv.org/pdf/2506.06391", "abs": "https://arxiv.org/abs/2506.06391", "authors": ["John Mavi", "Diana Teodora Găitan", "Sergio Coronado"], "title": "From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used across sectors, yet their\nalignment with International Humanitarian Law (IHL) is not well understood.\nThis study evaluates eight leading LLMs on their ability to refuse prompts that\nexplicitly violate these legal frameworks, focusing also on helpfulness - how\nclearly and constructively refusals are communicated. While most models\nrejected unlawful requests, the clarity and consistency of their responses\nvaried. By revealing the model's rationale and referencing relevant legal or\nsafety principles, explanatory refusals clarify the system's boundaries, reduce\nambiguity, and help prevent misuse. A standardised system-level safety prompt\nsignificantly improved the quality of the explanations expressed within\nrefusals in most models, highlighting the effectiveness of lightweight\ninterventions. However, more complex prompts involving technical language or\nrequests for code revealed ongoing vulnerabilities. These findings contribute\nto the development of safer, more transparent AI systems and propose a\nbenchmark to evaluate the compliance of LLM with IHL."}
{"id": "2506.06409", "pdf": "https://arxiv.org/pdf/2506.06409", "abs": "https://arxiv.org/abs/2506.06409", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Chen-Fu Chen", "Haim Permuter", "Sajani Vithana", "Flavio P. Calmon"], "title": "HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Large language model (LLM) watermarks enable authentication of text\nprovenance, curb misuse of machine-generated text, and promote trust in AI\nsystems. Current watermarks operate by changing the next-token predictions\noutput by an LLM. The updated (i.e., watermarked) predictions depend on random\nside information produced, for example, by hashing previously generated tokens.\nLLM watermarking is particularly challenging in low-entropy generation tasks -\nsuch as coding - where next-token predictions are near-deterministic. In this\npaper, we propose an optimization framework for watermark design. Our goal is\nto understand how to most effectively use random side information in order to\nmaximize the likelihood of watermark detection and minimize the distortion of\ngenerated text. Our analysis informs the design of two new watermarks:\nHeavyWater and SimplexWater. Both watermarks are tunable, gracefully\ntrading-off between detection accuracy and text distortion. They can also be\napplied to any LLM and are agnostic to side information generation. We examine\nthe performance of HeavyWater and SimplexWater through several benchmarks,\ndemonstrating that they can achieve high watermark detection accuracy with\nminimal compromise of text generation quality, particularly in the low-entropy\nregime. Our theoretical analysis also reveals surprising new connections\nbetween LLM watermarking and coding theory. The code implementation can be\nfound in https://github.com/DorTsur/HeavyWater_SimplexWater"}
{"id": "2506.06540", "pdf": "https://arxiv.org/pdf/2506.06540", "abs": "https://arxiv.org/abs/2506.06540", "authors": ["Patrick Y. Wu"], "title": "Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "19 pages, 6 figures", "summary": "After a disruptive event or shock, such as the Department of Government\nEfficiency (DOGE) federal layoffs of 2025, expert judgments are colored by\nknowledge of the outcome. This can make it difficult or impossible to\nreconstruct the pre-event perceptions needed to study the factors associated\nwith the event. This position paper argues that large language models (LLMs),\ntrained on vast amounts of digital media data, can be a viable substitute for\nexpert political surveys when a shock disrupts traditional measurement. We\nanalyze the DOGE layoffs as a specific case study for this position. We use\npairwise comparison prompts with LLMs and derive ideology scores for federal\nexecutive agencies. These scores replicate pre-layoff expert measures and\npredict which agencies were targeted by DOGE. We also use this same approach\nand find that the perceptions of certain federal agencies as knowledge\ninstitutions predict which agencies were targeted by DOGE, even when\ncontrolling for ideology. This case study demonstrates that using LLMs allows\nus to rapidly and easily test the associated factors hypothesized behind the\nshock. More broadly, our case study of this recent event exemplifies how LLMs\noffer insights into the correlational factors of the shock when traditional\nmeasurement techniques fail. We conclude by proposing a two-part criterion for\nwhen researchers can turn to LLMs as a substitute for expert political surveys."}
{"id": "2506.06576", "pdf": "https://arxiv.org/pdf/2506.06576", "abs": "https://arxiv.org/abs/2506.06576", "authors": ["Yijia Shao", "Humishka Zope", "Yucheng Jiang", "Jiaxin Pei", "David Nguyen", "Erik Brynjolfsson", "Diyi Yang"], "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "Preprint", "summary": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the\nlabor market, raising concerns about job displacement, diminished human agency,\nand overreliance on automation. Yet, we lack a systematic understanding of the\nevolving landscape. In this paper, we address this gap by introducing a novel\nauditing framework to assess which occupational tasks workers want AI agents to\nautomate or augment, and how those desires align with the current technological\ncapabilities. Our framework features an audio-enhanced mini-interview to\ncapture nuanced worker desires and introduces the Human Agency Scale (HAS) as a\nshared language to quantify the preferred level of human involvement. Using\nthis framework, we construct the WORKBank database, building on the U.S.\nDepartment of Labor's O*NET database, to capture preferences from 1,500 domain\nworkers and capability assessments from AI experts across over 844 tasks\nspanning 104 occupations. Jointly considering the desire and technological\ncapability divides tasks in WORKBank into four zones: Automation \"Green Light\"\nZone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone.\nThis highlights critical mismatches and opportunities for AI agent development.\nMoving beyond a simple automate-or-not dichotomy, our results reveal diverse\nHAS profiles across occupations, reflecting heterogeneous expectations for\nhuman involvement. Moreover, our study offers early signals of how AI agent\nintegration may reshape the core human competencies, shifting from\ninformation-focused skills to interpersonal ones. These findings underscore the\nimportance of aligning AI agent development with human desires and preparing\nworkers for evolving workplace dynamics."}
{"id": "2506.06579", "pdf": "https://arxiv.org/pdf/2506.06579", "abs": "https://arxiv.org/abs/2506.06579", "authors": ["Adarsh Prasad Behera", "Jaya Prakash Champati", "Roberto Morabito", "Sasu Tarkoma", "James Gross"], "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Recent progress in Language Models (LMs) has dramatically advanced the field\nof natural language processing (NLP), excelling at tasks like text generation,\nsummarization, and question answering. However, their inference remains\ncomputationally expensive and energy intensive, especially in settings with\nlimited hardware, power, or bandwidth. This makes it difficult to deploy LMs in\nmobile, edge, or cost sensitive environments. To address these challenges,\nrecent approaches have introduced multi LLM intelligent model selection\nstrategies that dynamically allocate computational resources based on query\ncomplexity -- using lightweight models for simpler queries and escalating to\nlarger models only when necessary. This survey explores two complementary\nstrategies for efficient LLM inference: (i) routing, which selects the most\nsuitable model based on the query, and (ii) cascading or hierarchical inference\n(HI), which escalates queries through a sequence of models until a confident\nresponse is found. Both approaches aim to reduce computation by using\nlightweight models for simpler tasks while offloading only when needed. We\nprovide a comparative analysis of these techniques across key performance\nmetrics, discuss benchmarking efforts, and outline open challenges. Finally, we\noutline future research directions to enable faster response times, adaptive\nmodel selection based on task complexity, and scalable deployment across\nheterogeneous environments, making LLM based systems more efficient and\naccessible for real world applications."}
{"id": "2506.06632", "pdf": "https://arxiv.org/pdf/2506.06632", "abs": "https://arxiv.org/abs/2506.06632", "authors": ["Shubham Parashar", "Shurui Gui", "Xiner Li", "Hongyi Ling", "Sushil Vemuri", "Blake Olson", "Eric Li", "Yu Zhang", "James Caverlee", "Dileep Kalathil", "Shuiwang Ji"], "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We aim to improve the reasoning capabilities of language models via\nreinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1\nhave demonstrated reasoning abilities on mathematical and coding tasks.\nHowever, prior studies suggest that using RL alone to improve reasoning on\ninherently difficult tasks is less effective. Here, we draw inspiration from\ncurriculum learning and propose to schedule tasks from easy to hard (E2H),\nallowing LLMs to build reasoning skills gradually. Our method is termed E2H\nReasoner. Empirically, we observe that, although easy tasks are important\ninitially, fading them out through appropriate scheduling is essential in\npreventing overfitting. Theoretically, we establish convergence guarantees for\nE2H Reasoner within an approximate policy iteration framework. We derive\nfinite-sample complexity bounds and show that when tasks are appropriately\ndecomposed and conditioned, learning through curriculum stages requires fewer\ntotal samples than direct learning. Experiments across multiple domains show\nthat E2H Reasoner significantly improves the reasoning ability of small LLMs\n(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,\nhighlighting the effectiveness of our method."}
{"id": "2506.06698", "pdf": "https://arxiv.org/pdf/2506.06698", "abs": "https://arxiv.org/abs/2506.06698", "authors": ["Yitao Liu", "Chenglei Si", "Karthik Narasimhan", "Shunyu Yao"], "title": "Contextual Experience Replay for Self-Improvement of Language Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025. 20 pages", "summary": "Large language model (LLM) agents have been applied to sequential\ndecision-making tasks such as web navigation, but without any\nenvironment-specific experiences, they often fail in these complex tasks.\nMoreover, current LLM agents are not designed to continually learn from past\nexperiences during inference time, which could be crucial for them to gain\nthese environment-specific experiences. To address this, we propose Contextual\nExperience Replay (CER), a training-free framework to enable efficient\nself-improvement for language agents in their context window. Specifically, CER\naccumulates and synthesizes past experiences into a dynamic memory buffer.\nThese experiences encompass environment dynamics and common decision-making\npatterns, allowing the agents to retrieve and augment themselves with relevant\nknowledge in new tasks, enhancing their adaptability in complex environments.\nWe evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On\nVisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,\nCER also gets a competitive average success rate of 36.7%, relatively improving\nthe success rate of the GPT-4o agent baseline by 51.0%. We also conduct a\ncomprehensive analysis on it to prove its efficiency, validity and understand\nit better."}
{"id": "2506.06699", "pdf": "https://arxiv.org/pdf/2506.06699", "abs": "https://arxiv.org/abs/2506.06699", "authors": ["Rajeev Bhatt Ambati", "James Lester", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "MarginSel : Max-Margin Demonstration Selection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at few-shot learning via in-context\nlearning (ICL). However, the effectiveness of ICL is often sensitive to the\nselection and ordering of demonstration examples. To address this, we present\nMarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that\nselects hard demonstration examples for the ICL prompt, adapting to each test\ninstance. Our approach achieves 2-7% absolute improvement in F1-score across\nclassification tasks, compared to a random selection of examples. We also\nprovide theoretical insights and empirical evidence showing that MarginSel\ninduces max-margin behavior in LLMs by effectively increasing the margin for\nhard examples, analogous to support vectors, thereby shifting the decision\nboundary in a beneficial direction."}
{"id": "2506.06729", "pdf": "https://arxiv.org/pdf/2506.06729", "abs": "https://arxiv.org/abs/2506.06729", "authors": ["Zixian Gao", "Chao Yang", "Zhanhui Zhou", "Xing Xu", "Chaochao Lu"], "title": "Mitigating Object Hallucination via Robust Local Perception Search", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings."}
{"id": "2506.06832", "pdf": "https://arxiv.org/pdf/2506.06832", "abs": "https://arxiv.org/abs/2506.06832", "authors": ["Clément Hongler", "Andrew Emil"], "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.IT", "cs.NE", "math.IT"], "comment": "41 pages, 16 figures", "summary": "Large Language Models (LLMs) define probability measures on text. By\nconsidering the implicit knowledge question of what it means for an LLM to know\nsuch a measure and what it entails algorithmically, we are naturally led to\nformulate a series of tasks that go beyond generative sampling, involving forms\nof summarization, counterfactual thinking, anomaly detection, originality\nsearch, reverse prompting, debating, creative solving, etc. These tasks can be\nformulated as games based on LLM measures, which we call Cross-Entropy (Xent)\nGames. Xent Games can be single-player or multi-player. They involve\ncross-entropy scores and cross-entropy constraints, and can be expressed as\nsimple computational graphs and programs. We show the Xent Game space is large\nenough to contain a wealth of interesting examples, while being constructible\nfrom basic game-theoretic consistency axioms. We then discuss how the Xent Game\nspace can be used to measure the abilities of LLMs. This leads to the\nconstruction of Xent Game measures: finite families of Xent Games that can be\nused as capability benchmarks, built from a given scope, by extracting a\ncovering measure. To address the unbounded scope problem associated with the\nchallenge of measuring general abilities, we propose to explore the space of\nXent Games in a coherent fashion, using ideas inspired by evolutionary\ndynamics."}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks."}
{"id": "2506.06941", "pdf": "https://arxiv.org/pdf/2506.06941", "abs": "https://arxiv.org/abs/2506.06941", "authors": ["Parshin Shojaee", "Iman Mirzadeh", "Keivan Alizadeh", "Maxwell Horton", "Samy Bengio", "Mehrdad Farajtabar"], "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities."}
{"id": "2506.06975", "pdf": "https://arxiv.org/pdf/2506.06975", "abs": "https://arxiv.org/abs/2506.06975", "authors": ["Xiaoyuan Zhu", "Yaowen Ye", "Tianyi Qiu", "Hanlin Zhu", "Sijun Tan", "Ajraf Mannan", "Jonathan Michala", "Raluca Ada Popa", "Willie Neiswanger"], "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As API access becomes a primary interface to large language models (LLMs),\nusers often interact with black-box systems that offer little transparency into\nthe deployed model. To reduce costs or maliciously alter model behaviors, API\nproviders may discreetly serve quantized or fine-tuned variants, which can\ndegrade performance and compromise safety. Detecting such substitutions is\ndifficult, as users lack access to model weights and, in most cases, even\noutput logits. To tackle this problem, we propose a rank-based uniformity test\nthat can verify the behavioral equality of a black-box LLM to a locally\ndeployed authentic model. Our method is accurate, query-efficient, and avoids\ndetectable query patterns, making it robust to adversarial providers that\nreroute or mix responses upon the detection of testing attempts. We evaluate\nthe approach across diverse threat scenarios, including quantization, harmful\nfine-tuning, jailbreak prompts, and full model substitution, showing that it\nconsistently achieves superior statistical power over prior methods under\nconstrained query budgets."}
{"id": "2506.07031", "pdf": "https://arxiv.org/pdf/2506.07031", "abs": "https://arxiv.org/abs/2506.07031", "authors": ["Jingyuan Ma", "Rui Li", "Zheng Li", "Junfeng Liu", "Lei Sha", "Zhifang Sui"], "title": "HauntAttack: When Attack Follows Reasoning as a Shadow", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and\nreasoning tasks, showcasing exceptional capabilities. However, the enhancement\nof reasoning abilities and the exposure of their internal reasoning processes\nintroduce new safety vulnerabilities. One intriguing concern is: when reasoning\nis strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs\nexhibit? To address this issue, we introduce HauntAttack, a novel and\ngeneral-purpose black-box attack framework that systematically embeds harmful\ninstructions into reasoning questions. Specifically, we treat reasoning\nquestions as carriers and substitute one of their original conditions with a\nharmful instruction. This process creates a reasoning pathway in which the\nmodel is guided step by step toward generating unsafe outputs. Based on\nHauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results\nreveal that even the most advanced LRMs exhibit significant safety\nvulnerabilities. Additionally, we perform a detailed analysis of different\nmodels, various types of harmful instructions, and model output patterns,\nproviding valuable insights into the security of LRMs."}
{"id": "2506.07045", "pdf": "https://arxiv.org/pdf/2506.07045", "abs": "https://arxiv.org/abs/2506.07045", "authors": ["Yikun Ji", "Hong Yan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Qi Fan", "Liqing Zhang", "Jianfu Zhang"], "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods."}
{"id": "2506.07138", "pdf": "https://arxiv.org/pdf/2506.07138", "abs": "https://arxiv.org/abs/2506.07138", "authors": ["Hao Tang", "Chengchao Shen"], "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "The source code and trained weights are available at\n  https://github.com/visresearch/LLaVA-STF", "summary": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF."}
{"id": "2506.07168", "pdf": "https://arxiv.org/pdf/2506.07168", "abs": "https://arxiv.org/abs/2506.07168", "authors": ["Huanyi Xie", "Lijie Hu", "Lu Yu", "Tianhao Huang", "Longfei Li", "Meng Li", "Jun Zhou", "Huan Wang", "Di Wang"], "title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural\nnetworks (GNNs) often fall short due to the complex textual information\nassociated with each node. Recent methods have improved node representations by\nleveraging large language models (LLMs) to enhance node text features, but\nthese approaches typically require extensive annotations or fine-tuning across\nall nodes, which is both time-consuming and costly. To overcome these\nchallenges, we introduce GAGA, an efficient framework for TAG representation\nlearning. GAGA reduces annotation time and cost by focusing on annotating only\nrepresentative nodes and edges. It constructs an annotation graph that captures\nthe topological relationships among these annotations. Furthermore, GAGA\nemploys a two-level alignment module to effectively integrate the annotation\ngraph with the TAG, aligning their underlying structures. Experiments show that\nGAGA achieves classification accuracies on par with or surpassing\nstate-of-the-art methods while requiring only 1% of the data to be annotated,\ndemonstrating its high efficiency."}
{"id": "2506.07184", "pdf": "https://arxiv.org/pdf/2506.07184", "abs": "https://arxiv.org/abs/2506.07184", "authors": ["Liangliang You", "Junchi Yao", "Shu Yang", "Guimin Hu", "Lijie Hu", "Di Wang"], "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy."}
{"id": "2506.07196", "pdf": "https://arxiv.org/pdf/2506.07196", "abs": "https://arxiv.org/abs/2506.07196", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance."}
{"id": "2506.07227", "pdf": "https://arxiv.org/pdf/2506.07227", "abs": "https://arxiv.org/abs/2506.07227", "authors": ["Tianyi Bai", "Yuxuan Fan", "Jiantao Qiu", "Fupeng Sun", "Jiayi Song", "Junlin Han", "Zichen Liu", "Conghui He", "Wentao Zhang", "Binhang Yuan"], "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks but still struggle with fine-grained visual differences,\nleading to hallucinations or missed semantic shifts. We attribute this to\nlimitations in both training data and learning objectives. To address these\nissues, we propose a controlled data generation pipeline that produces\nminimally edited image pairs with semantically aligned captions. Using this\npipeline, we construct the Micro Edit Dataset (MED), containing over 50K\nimage-text pairs spanning 11 fine-grained edit categories, including attribute,\ncount, position, and object presence changes. Building on MED, we introduce a\nsupervised fine-tuning (SFT) framework with a feature-level consistency loss\nthat promotes stable visual embeddings under small edits. We evaluate our\napproach on the Micro Edit Detection benchmark, which includes carefully\nbalanced evaluation pairs designed to test sensitivity to subtle visual\nvariations across the same edit categories. Our method improves difference\ndetection accuracy and reduces hallucinations compared to strong baselines,\nincluding GPT-4o. Moreover, it yields consistent gains on standard\nvision-language tasks such as image captioning and visual question answering.\nThese results demonstrate the effectiveness of combining targeted data and\nalignment objectives for enhancing fine-grained visual reasoning in MLLMs."}
{"id": "2506.07233", "pdf": "https://arxiv.org/pdf/2506.07233", "abs": "https://arxiv.org/abs/2506.07233", "authors": ["Tzu-wen Hsu", "Ke-Han Lu", "Cheng-Han Chiang", "Hung-yi Lee"], "title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and\nanswer questions about the audio. While prior LALMs have shown strong\nperformance on standard benchmarks, there has been alarming evidence that LALMs\ncan hallucinate what is presented in the audio. To mitigate the hallucination\nof LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time\nstrategy that uses contrastive decoding to compare the token prediction logits\nwith and without the audio context. By contrastive decoding, AAD promotes the\ntokens whose probability increases when the audio is present. We conduct our\nexperiment on object hallucination datasets with three LALMs and show that AAD\nimproves the F1 score by 0.046 to 0.428. We also show that AAD can improve the\naccuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We\nconduct thorough ablation studies to understand the effectiveness of each\ncomponent in AAD."}
{"id": "2506.07235", "pdf": "https://arxiv.org/pdf/2506.07235", "abs": "https://arxiv.org/abs/2506.07235", "authors": ["Tianyi Bai", "Zengjie Hu", "Fupeng Sun", "Jiantao Qiu", "Yizhen Jiang", "Guangxin He", "Bohan Zeng", "Conghui He", "Binhang Yuan", "Wentao Zhang"], "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have achieved remarkable\ncapabilities by integrating visual perception with language understanding,\nenabling applications such as image-grounded dialogue, visual question\nanswering, and scientific analysis. However, most MLLMs adopt a static\ninference paradigm, encoding the entire image into fixed visual tokens upfront,\nwhich limits their ability to iteratively refine understanding or adapt to\ncontext during inference. This contrasts sharply with human perception, which\nis dynamic, selective, and feedback-driven. In this work, we introduce a novel\nframework for inference-time visual token scaling that enables MLLMs to perform\niterative, verifier-guided reasoning over visual content. We formulate the\nproblem as a Markov Decision Process, involving a reasoner that proposes visual\nactions and a verifier, which is trained via multi-step Direct Preference\nOptimization (DPO), that evaluates these actions and determines when reasoning\nshould terminate. To support this, we present a new dataset, VTS, comprising\nsupervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning\ncomparisons (VTS-DPO). Our method significantly outperforms existing approaches\nacross diverse visual reasoning benchmarks, offering not only improved accuracy\nbut also more interpretable and grounded reasoning processes. These results\ndemonstrate the promise of dynamic inference mechanisms for enabling\nfine-grained, context-aware visual reasoning in next-generation MLLMs."}
{"id": "2506.07398", "pdf": "https://arxiv.org/pdf/2506.07398", "abs": "https://arxiv.org/abs/2506.07398", "authors": ["Guibin Zhang", "Muxin Fu", "Guancheng Wan", "Miao Yu", "Kun Wang", "Shuicheng Yan"], "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "categories": ["cs.MA", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory."}
{"id": "2506.07402", "pdf": "https://arxiv.org/pdf/2506.07402", "abs": "https://arxiv.org/abs/2506.07402", "authors": ["Yukai Zhou", "Sibei Yang", "Wenjie Wang"], "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about their security. While jailbreak attacks\nhighlight failures under overtly harmful queries, they overlook a critical\nrisk: incorrectly answering harmless-looking inputs can be dangerous and cause\nreal-world harm (Implicit Harm). We systematically reformulate the LLM risk\nlandscape through a structured quadrant perspective based on output factuality\nand input harmlessness, uncovering an overlooked high-risk region. To\ninvestigate this gap, we propose JailFlipBench, a benchmark aims to capture\nimplicit harm, spanning single-modal, multimodal, and factual extension\nscenarios with diverse evaluation metrics. We further develop initial JailFlip\nattack methodologies and conduct comprehensive evaluations across multiple\nopen-source and black-box LLMs, show that implicit harm present immediate and\nurgent real-world risks, calling for broader LLM safety assessments and\nalignment beyond conventional jailbreak paradigms."}
{"id": "2506.07449", "pdf": "https://arxiv.org/pdf/2506.07449", "abs": "https://arxiv.org/abs/2506.07449", "authors": ["Vahid Azizi", "Fatemeh Koochaki"], "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in\nrecommender systems through Retrieval-Augmented Generation (RAG) frameworks.\nHowever, existing RAG approaches predominantly rely on flat, similarity-based\nretrieval that fails to leverage the rich relational structure inherent in\nuser-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,\nend-to-end trainable framework that integrates personalized knowledge graph\ncontext into LLM-based recommendation ranking. Our approach extends the\nLlamaRec architecture by incorporating a lightweight user preference module\nthat dynamically identifies salient relation paths within a heterogeneous\nknowledge graph constructed from user behavior and item metadata. These\npersonalized subgraphs are seamlessly integrated into prompts for a fine-tuned\nLlama-2 model, enabling efficient and interpretable recommendations through a\nunified inference step. Comprehensive experiments on ML-100K and Amazon Beauty\ndatasets demonstrate consistent and significant improvements over LlamaRec\nacross key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates\nthe critical value of structured reasoning in LLM-based recommendations and\nestablishes a foundation for scalable, knowledge-aware personalization in\nnext-generation recommender systems. Code is available\nat~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}."}
{"id": "2506.07452", "pdf": "https://arxiv.org/pdf/2506.07452", "abs": "https://arxiv.org/abs/2506.07452", "authors": ["Yuxin Xiao", "Sana Tonekaboni", "Walter Gerych", "Vinith Suriyakumar", "Marzyeh Ghassemi"], "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in jailbreak queries. Although these\nstyle patterns are semantically unrelated to the malicious intents behind\njailbreak queries, their safety impact remains unclear. In this work, we seek\nto understand whether style patterns compromise LLM safety, how superficial\nstyle alignment increases model vulnerability, and how best to mitigate these\nrisks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,\nand find that malicious queries with style patterns inflate the attack success\nrate (ASR) for nearly all models. Notably, ASR inflation correlates with both\nthe length of style patterns and the relative attention an LLM exhibits on\nthem. We then investigate superficial style alignment, and find that\nfine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of\nthose same styles. Finally, we propose SafeStyle, a defense strategy that\nincorporates a small amount of safety training data augmented to match the\ndistribution of style patterns in the fine-tuning data. Across three LLMs and\nfive fine-tuning style settings, SafeStyle consistently outperforms baselines\nin maintaining LLM safety."}
{"id": "2506.07460", "pdf": "https://arxiv.org/pdf/2506.07460", "abs": "https://arxiv.org/abs/2506.07460", "authors": ["Taeryung Lee", "Hyeongjin Nam", "Gyeongsik Moon", "Kyoung Mu Lee"], "title": "GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Sign language generation (SLG), or text-to-sign generation, bridges the gap\nbetween signers and non-signers. Despite recent progress in SLG, existing\nmethods still often suffer from incorrect lexical ordering and low semantic\naccuracy. This is primarily due to sentence-level condition, which encodes the\nentire sentence of the input text into a single feature vector as a condition\nfor SLG. This approach fails to capture the temporal structure of sign language\nand lacks the granularity of word-level semantics, often leading to disordered\nsign sequences and ambiguous motions. To overcome these limitations, we propose\nGLOS, a sign language generation framework with temporally aligned gloss-level\nconditioning. First, we employ gloss-level conditions, which we define as\nsequences of gloss embeddings temporally aligned with the motion sequence. This\nenables the model to access both the temporal structure of sign language and\nword-level semantics at each timestep. As a result, this allows for\nfine-grained control of signs and better preservation of lexical order. Second,\nwe introduce a condition fusion module, temporal alignment conditioning (TAC),\nto efficiently deliver the word-level semantic and temporal structure provided\nby the gloss-level condition to the corresponding motion timesteps. Our method,\nwhich is composed of gloss-level conditions and TAC, generates signs with\ncorrect lexical order and high semantic accuracy, outperforming prior methods\non CSL-Daily and Phoenix-2014T."}
{"id": "2506.07468", "pdf": "https://arxiv.org/pdf/2506.07468", "abs": "https://arxiv.org/abs/2506.07468", "authors": ["Mickel Liu", "Liwei Jiang", "Yancheng Liang", "Simon Shaolei Du", "Yejin Choi", "Tim Althoff", "Natasha Jaques"], "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL)."}
{"id": "2506.07501", "pdf": "https://arxiv.org/pdf/2506.07501", "abs": "https://arxiv.org/abs/2506.07501", "authors": ["Libo Wang"], "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "The relevant code has been uploaded to the publicly available GitHub\n  repository. The link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/GoCE", "summary": "In view of the problem that each subchain in the chain-of-model (CoM) relies\nonly on the information of the previous subchain and may lose long-range\ndependencies due to the causal mask blocking the global context flow between\nmulti-level subchains, this work proposes a graph of causal evolution (GoCE).\nIts core principle is to map the implicit token representation into a\ndifferentiable and sparse causal adjacency matrix, then permeate causal\nconstraints through each layer of calculation using causal-masked attention and\ncausal-MoE. By combining intervention consistency loss test and self-evolution\ngate, the dynamic balance between causal structure learning and adaptive\nupdating of transformer architecture is realized. The researcher built\nexperimental environments in sandboxes built with Claude Sonnet 4,\no4-mini-high, and DeepSeek R1 respectively with the transformer variant\narchitecture introduced in GoCE. It is evaluated on publicly available datasets\nincluding CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the\nbaseline LLMs. The finding proves that GoCE strengthens the transformer's\nability to capture long-range causal dependencies, while the ability to\nself-evolve is improved. It not only surpasses the design of CoM in terms of\ndesign principles, but also provides experience for future research on causal\nlearning and continuous adaptive improvement."}
{"id": "2506.07515", "pdf": "https://arxiv.org/pdf/2506.07515", "abs": "https://arxiv.org/abs/2506.07515", "authors": ["Asahi Sakuma", "Hiroaki Sato", "Ryuga Sugano", "Tadashi Kumano", "Yoshihiko Kawai", "Tetsuji Ogawa"], "title": "Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted at INTERSPEECH 2025", "summary": "This paper presents a novel framework for multi-talker automatic speech\nrecognition without the need for auxiliary information. Serialized Output\nTraining (SOT), a widely used approach, suffers from recognition errors due to\nspeaker assignment failures. Although incorporating auxiliary information, such\nas token-level timestamps, can improve recognition accuracy, extracting such\ninformation from natural conversational speech remains challenging. To address\nthis limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension\nof CTC that jointly assigns a token and its corresponding speaker label to each\nframe. We further integrate SD-CTC into the SOT framework, enabling the SOT\nmodel to learn speaker distinction using only overlapping speech and\ntranscriptions. Experimental comparisons show that multi-task learning with\nSD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves\nperformance comparable to state-of-the-art methods relying on auxiliary\ninformation."}
{"id": "2506.07551", "pdf": "https://arxiv.org/pdf/2506.07551", "abs": "https://arxiv.org/abs/2506.07551", "authors": ["Mengsong Wu", "YaFei Wang", "Yidong Ming", "Yuqi An", "Yuwei Wan", "Wenliang Chen", "Binbin Lin", "Yuqiang Li", "Tong Xie", "Dongzhan Zhou"], "title": "ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "comment": "15 pages, 6 figures", "summary": "Large language models (LLMs) have recently demonstrated promising\ncapabilities in chemistry tasks while still facing challenges due to outdated\npretraining knowledge and the difficulty of incorporating specialized chemical\nexpertise. To address these issues, we propose an LLM-based agent that\nsynergistically integrates 137 external chemical tools created ranging from\nbasic information retrieval to complex reaction predictions, and a dataset\ncuration pipeline to generate the dataset ChemToolBench that facilitates both\neffective tool selection and precise parameter filling during fine-tuning and\nevaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search\n(HE-MCTS) framework, enabling independent optimization of tool planning and\nexecution. By leveraging self-generated data, our approach supports step-level\nfine-tuning (FT) of the policy model and training task-adaptive PRM and ORM\nthat surpass GPT-4o. Experimental evaluations demonstrate that our approach\nsignificantly improves performance in Chemistry QA and discovery tasks,\noffering a robust solution to integrate specialized tools with LLMs for\nadvanced chemical applications. All datasets and code are available at\nhttps://github.com/AI4Chem/ChemistryAgent ."}
{"id": "2506.07564", "pdf": "https://arxiv.org/pdf/2506.07564", "abs": "https://arxiv.org/abs/2506.07564", "authors": ["Peiran Li", "Xinkai Zou", "Zhuohang Wu", "Ruifeng Li", "Shuo Xing", "Hanwen Zheng", "Zhikai Hu", "Yuping Wang", "Haoxi Li", "Qin Yuan", "Yingmo Zhang", "Zhengzhong Tu"], "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy."}
{"id": "2506.07572", "pdf": "https://arxiv.org/pdf/2506.07572", "abs": "https://arxiv.org/abs/2506.07572", "authors": ["Yu Li", "Feng Xue", "Shujie Li", "Jinrui Zhang", "Shuang Yang", "Dan Guo", "Richang Hong"], "title": "Learning Speaker-Invariant Visual Features for Lipreading", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Lipreading is a challenging cross-modal task that aims to convert visual lip\nmovements into spoken text. Existing lipreading methods often extract visual\nfeatures that include speaker-specific lip attributes (e.g., shape, color,\ntexture), which introduce spurious correlations between vision and text. These\ncorrelations lead to suboptimal lipreading accuracy and restrict model\ngeneralization. To address this challenge, we introduce SIFLip, a\nspeaker-invariant visual feature learning framework that disentangles\nspeaker-specific attributes using two complementary disentanglement modules\n(Implicit Disentanglement and Explicit Disentanglement) to improve\ngeneralization. Specifically, since different speakers exhibit semantic\nconsistency between lip movements and phonetic text when pronouncing the same\nwords, our implicit disentanglement module leverages stable text embeddings as\nsupervisory signals to learn common visual representations across speakers,\nimplicitly decoupling speaker-specific features. Additionally, we design a\nspeaker recognition sub-task within the main lipreading pipeline to filter\nspeaker-specific features, then further explicitly disentangle these\npersonalized visual features from the backbone network via gradient reversal.\nExperimental results demonstrate that SIFLip significantly enhances\ngeneralization performance across multiple public datasets. Experimental\nresults demonstrate that SIFLip significantly improves generalization\nperformance across multiple public datasets, outperforming state-of-the-art\nmethods."}
{"id": "2506.07747", "pdf": "https://arxiv.org/pdf/2506.07747", "abs": "https://arxiv.org/abs/2506.07747", "authors": ["Adam Breuer"], "title": "E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "ICML 2025; Code available at: https://github.com/BreuerLabs/E- LDA", "summary": "In this paper, we provide the first practical algorithms with provable\nguarantees for the problem of inferring the topics assigned to each document in\nan LDA topic model. This is the primary inference problem for many applications\nof topic models in social science, data exploration, and causal inference\nsettings. We obtain this result by showing a novel non-gradient-based,\ncombinatorial approach to estimating topic models. This yields algorithms that\nconverge to near-optimal posterior probability in logarithmic parallel\ncomputation time (adaptivity) -- exponentially faster than any known LDA\nalgorithm. We also show that our approach can provide interpretability\nguarantees such that each learned topic is formally associated with a known\nkeyword. Finally, we show that unlike alternatives, our approach can maintain\nthe independence assumptions necessary to use the learned topic model for\ndownstream causal inference methods that allow researchers to study topics as\ntreatments. In terms of practical performance, our approach consistently\nreturns solutions of higher semantic quality than solutions from\nstate-of-the-art LDA algorithms, neural topic models, and LLM-based topic\nmodels across a diverse range of text datasets and evaluation parameters."}
{"id": "2506.07833", "pdf": "https://arxiv.org/pdf/2506.07833", "abs": "https://arxiv.org/abs/2506.07833", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "title": "Improving large language models with concept-aware fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm"}
{"id": "2506.07896", "pdf": "https://arxiv.org/pdf/2506.07896", "abs": "https://arxiv.org/abs/2506.07896", "authors": ["Shoko Oka"], "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages, Additional resources available on GitHub repository", "summary": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges."}
{"id": "2506.07915", "pdf": "https://arxiv.org/pdf/2506.07915", "abs": "https://arxiv.org/abs/2506.07915", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement", "categories": ["cs.AI", "cs.CL", "cs.SY", "eess.SY"], "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible\n  publication", "summary": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success."}
{"id": "2506.07919", "pdf": "https://arxiv.org/pdf/2506.07919", "abs": "https://arxiv.org/abs/2506.07919", "authors": ["Manuel Brenner", "Georgia Koppe"], "title": "Uncovering the Functional Roles of Nonlinearity in Memory", "categories": ["cs.LG", "cs.AI", "cs.CL", "nlin.CD", "physics.comp-ph"], "comment": "Preprint under review", "summary": "Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems."}
{"id": "2506.07927", "pdf": "https://arxiv.org/pdf/2506.07927", "abs": "https://arxiv.org/abs/2506.07927", "authors": ["Jiayi Sheng", "Luna Lyu", "Jikai Jin", "Tony Xia", "Alex Gu", "James Zou", "Pan Lu"], "title": "Solving Inequality Proofs with Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "52 pages, 16 figures", "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/."}
{"id": "2506.07936", "pdf": "https://arxiv.org/pdf/2506.07936", "abs": "https://arxiv.org/abs/2506.07936", "authors": ["Chengyue Huang", "Yuchen Zhu", "Sichen Zhu", "Jingyun Xiao", "Moises Andrade", "Shivang Chopra", "Zsolt Kira"], "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL."}
{"id": "2506.07945", "pdf": "https://arxiv.org/pdf/2506.07945", "abs": "https://arxiv.org/abs/2506.07945", "authors": ["Arnav Sheth", "Ivaxi Sheth", "Mario Fritz"], "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols", "categories": ["cs.AR", "cs.AI", "cs.CL"], "comment": "Accepted at MLSysArch@ISCA 2025", "summary": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches."}
{"id": "2506.07963", "pdf": "https://arxiv.org/pdf/2506.07963", "abs": "https://arxiv.org/abs/2506.07963", "authors": ["Jixiang Hong", "Yiran Zhang", "Guanzhong Wang", "Yi Liu", "Ji-Rong Wen", "Rui Yan"], "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks."}
{"id": "2506.07972", "pdf": "https://arxiv.org/pdf/2506.07972", "abs": "https://arxiv.org/abs/2506.07972", "authors": ["Hongzheng Chen", "Yingheng Wang", "Yaohui Cai", "Hins Hu", "Jiajie Li", "Shirley Huang", "Chenhui Deng", "Rongjian Liang", "Shufeng Kong", "Haoxing Ren", "Samitha Samaranayake", "Carla P. Gomes", "Zhiru Zhang"], "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains."}
{"id": "2506.07982", "pdf": "https://arxiv.org/pdf/2506.07982", "abs": "https://arxiv.org/abs/2506.07982", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions."}
{"id": "2506.08001", "pdf": "https://arxiv.org/pdf/2506.08001", "abs": "https://arxiv.org/abs/2506.08001", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Schölkopf", "Weiyang Liu"], "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Technical report v1 (36 pages, 24 figures, project page:\n  https://spherelab.ai/poet-site/)", "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs."}
{"id": "2506.08011", "pdf": "https://arxiv.org/pdf/2506.08011", "abs": "https://arxiv.org/abs/2506.08011", "authors": ["Yunfei Xie", "Yinsong Ma", "Shiyi Lan", "Alan Yuille", "Junfei Xiao", "Chen Wei"], "title": "Play to Generalize: Learning to Reason Through Game Play", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://yunfeixie233.github.io/ViGaL/", "summary": "Developing generalizable reasoning capabilities in multimodal large language\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\ndevelop out-of-domain generalization of multimodal reasoning through playing\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\nsignificantly enhances its downstream performance on multimodal math benchmarks\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\nworked solutions, equations, or diagrams during RL, suggesting the capture of\ntransferable reasoning skills. Remarkably, our model outperforms specialist\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\nwhile preserving the base model's performance on general visual benchmarks, a\nchallenge where specialist models often fall short. Our findings suggest a new\npost-training paradigm: synthetic, rule-based games can serve as controllable\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\nabilities in MLLMs."}
