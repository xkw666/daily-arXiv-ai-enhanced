<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 128]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.CR](#cs.CR) [Total: 4]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
*Juvenal Domingos Júnior,Augusto Faria,E. Seiti de Oliveira,Erick de Brito,Matheus Teotonio,Andre Assumpção,Diedre Carmo,Roberto Lotufo,Jayr Pereira*

Main category: cs.CL

TL;DR: 研究者开发了BR-TaxQA-R数据集用于巴西税法问答，基于RAG的系统在响应相关性优于商业模型，但商业模型在事实正确性更佳，强调法律领域仍需人工验证。


<details>
  <summary>Details</summary>
Motivation: 解决高风险税务领域AI答案的法律有效性需求，比较不同技术方案优劣，推动法律AI应用发展。

Method: 构建715问的巴西税法数据集，采用OpenAI嵌入+RAG管道，对比文本分割策略，使用RAGAS指标评估ChatGPT等商业工具。

Result: 定制RAG响应相关性最优，商业模型事实正确性得分更高，揭示法律严谨性与语言流畅性的技术取舍。

Conclusion: 税务等高危领域需保持人类专家审核机制，公开数据集促进法律AI发展，AI与人工协同是未来方向。

Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.

</details>


### [2] [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
*Aliakbar Nafar,Kristen Brent Venable,Zijun Cui,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 利用大语言模型的概率知识构建贝叶斯网络参数，通过结合小数据优化概率分布并建立评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型作为事实知识库的能力，但其在生成现实事件概率知识方面的潜力尚未充分挖掘，特别是在贝叶斯网络参数化中的应用价值。

Method: 在80个公开贝叶斯网络上测试LLMs生成条件概率的能力，对比基线方法(随机/均匀分布、next-token概率)，并探索LLM分布作为专家先验优化小数据分布的方法。

Result: LLM生成的条件概率显著优于基线，可减少76%系统偏差。同时确立了包含多种提示策略的LLM概率知识评估基准体系。

Conclusion: 提出了LLM概率知识与实际数据结合的贝叶斯网络自动构建范式，首次系统建立了大模型概率知识提取能力的评估标准。

Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.

</details>


### [3] [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
*Dong Won Lee,Hae Won Park,Cynthia Breazeal,Louis-Philippe Morency*

Main category: cs.CL

TL;DR: 提出基于大语言模型的奖励分解框架，通过分解全局反馈为局部奖励优化对话生成，无需人工设计奖励或细粒度反馈。


<details>
  <summary>Details</summary>
Motivation: 现有对话代理对齐方法依赖人工设计奖励函数或细粒度反馈标注，本研究旨在利用大语言模型的推理能力自动分解全局反馈信号。

Method: 开发纯文本和多模态两种变体：1) 仅用对话文本提示LLM分解奖励 2) 整合音调/注视/面部情感等多模态行为线索。通过蒸馏局部奖励构建轻量级模型，驱动强化学习微调。

Result: 在对话质量的人类评估中超越现有奖励分解方法，证明大语言模型具有优异的奖励分解能力。

Conclusion: 大语言模型作为奖励分解器有效避免了人工奖励设计，其多模态信息整合能力为对话代理对齐提供了新范式。

Abstract: We propose a large language model based reward decomposition framework for
aligning dialogue agents using only a single session-level feedback signal. We
leverage the reasoning capabilities of a frozen, pretrained large language
model (LLM) to infer fine-grained local implicit rewards by decomposing global,
session-level feedback. Our first text-only variant prompts the LLM to perform
reward decomposition using only the dialogue transcript. The second multimodal
variant incorporates additional behavioral cues, such as pitch, gaze, and
facial affect, expressed as natural language descriptions. These inferred
turn-level rewards are distilled into a lightweight reward model, which we
utilize for RL-based fine-tuning for dialogue generation. We evaluate both
text-only and multimodal variants against state-of-the-art reward decomposition
methods and demonstrate notable improvements in human evaluations of
conversation quality, suggesting that LLMs are strong reward decomposers that
obviate the need for manual reward shaping and granular human feedback.

</details>


### [4] [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
*Parth Sarin,Juan Pablo Alperin*

Main category: cs.CL

TL;DR: 开发开放权重语言模型工具用于引文标注，提升全球引文网络追踪能力


<details>
  <summary>Details</summary>
Motivation: 解决全球南方知识共享网络信息不足导致的学术边缘化问题，打破殖民化知识格局

Method: 构建预印本与论文的引文数据集，评估多款开放模型在引文标注任务中的表现

Result: 现有语言模型（特别是Qwen3-0.6B）在多次迭代中展现高精度标注能力，超越现有方法

Conclusion: 该工具显著提升引文网络准确性，推动研究索引服务和元科学发展

Abstract: A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.

</details>


### [5] [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
*Ryo Kamoi,Yusen Zhang,Nan Zhang,Sarkar Snigdha Sarathi Das,Rui Zhang*

Main category: cs.CL

TL;DR: 提出FoVer方法，利用形式验证工具自动标注步骤级错误标签训练PRMs，实现跨任务泛化


<details>
  <summary>Details</summary>
Motivation: 传统PRMs依赖人工标注步骤级错误标签且局限于数学推理任务，需解决自动标注与任务泛化问题

Method: 使用Z3/Isabelle等工具对形式逻辑和定理证明任务自动生成错误标签，基于此训练LLM-based PRMs

Result: FoVer训练的PRMs在ProcessBench步骤验证和12个推理基准测试中显著优于基线模型

Conclusion: 形式验证自动标注有效替代人工标注，且PRMs具备跨任务泛化能力，开源数据集/模型推动相关研究

Abstract: Process Reward Models (PRMs), which provide step-by-step feedback on the
reasoning generated by Large Language Models (LLMs), are receiving increasing
attention. However, two key research gaps remain: collecting accurate
step-level error labels for training typically requires costly human
annotation, and existing PRMs are limited to math reasoning problems. In
response to these gaps, this paper aims to address the challenges of automatic
dataset creation and the generalization of PRMs to diverse reasoning tasks. To
achieve this goal, we propose FoVer, an approach for training PRMs on
step-level error labels automatically annotated by formal verification tools,
such as Z3 for formal logic and Isabelle for theorem proof, which provide
automatic and accurate verification for symbolic tasks. Using this approach, we
synthesize a training dataset with error labels on LLM responses for formal
logic and theorem proof tasks without human annotation. Although this data
synthesis is feasible only for tasks compatible with formal verification, we
observe that LLM-based PRMs trained on our dataset exhibit cross-task
generalization, improving verification across diverse reasoning tasks.
Specifically, PRMs trained with FoVer significantly outperform baseline PRMs
based on the original LLMs and achieve competitive or superior results compared
to state-of-the-art PRMs trained on labels annotated by humans or stronger
models, as measured by step-level verification on ProcessBench and Best-of-K
performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,
and BBH. The datasets, models, and code are provided at
https://github.com/psunlpgroup/FoVer.

</details>


### [6] [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
*Linxi Zhao,Sofian Zalouk,Christian K. Belardi,Justin Lovelace,Jin Peng Zhou,Kilian Q. Weinberger,Yoav Artzi,Jennifer J. Sun*

Main category: cs.CL

TL;DR: 提出新型大记忆语言模型（LMLM），通过结合内部参数与外部数据库存储事实知识，减少模型对权重记忆的依赖，实现知识显式可编辑验证。


<details>
  <summary>Details</summary>
Motivation: 解决传统语言模型将知识与语言模式黑箱化存储的问题，旨在提升事实知识的可检查性、可更新性和可验证性。

Method: 预训练时对检索到的外部事实进行目标性掩码，强制模型通过外部查询而非权重记忆获取知识。

Result: LMLM在标准基准测试中与更大规模的知识密集型模型表现相当，同时支持知识库的显式编辑与验证。

Conclusion: 该研究从根本上改变了语言模型管理事实知识的方式，推动模型向透明化、可解释性方向发展。

Abstract: Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.

</details>


### [7] [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
*Anirudh Maiya,Razan Alghamdi,Maria Leonor Pacheco,Ashutosh Trivedi,Fabio Somenzi*

Main category: cs.CL

TL;DR: LLMs在解决数独任务中展示出有限解题能力，但均无法提供符合人类战略思维的解释


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在人类-AI协作决策中提供可信、渐进、定制化解释的能力，以数独为典型场景

Method: 评估5个LLM模型在66个标准数独谜题上的解题与解释能力

Result: 1个模型展现有限解题能力，所有模型均无法呈现战略推理或直觉式解题过程的解释

Conclusion: LLMs要成为有效决策伙伴，需在解释生成与战略推理能力方面取得重大突破

Abstract: The success of Large Language Models (LLMs) in human-AI collaborative
decision-making hinges on their ability to provide trustworthy, gradual, and
tailored explanations. Solving complex puzzles, such as Sudoku, offers a
canonical example of this collaboration, where clear and customized
explanations often hold greater importance than the final solution. In this
study, we evaluate the performance of five LLMs in solving and explaining
\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving
puzzles, none can explain the solution process in a manner that reflects
strategic reasoning or intuitive problem-solving. These findings underscore
significant challenges that must be addressed before LLMs can become effective
partners in human-AI collaborative decision-making.

</details>


### [8] [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
*Mehrdad ghassabi,Pedram Rostami,Hamidreza Baradaran Kashani,Amirhossein Poursina,Zahra Kazemi,Milad Tavakoli*

Main category: cs.CL

TL;DR: 通过爬取波斯语医疗杂志和真实医患问答数据构建首个波斯语医疗数据集，并基于此微调语言模型，显著提升了小模型在波斯语医疗问答中的准确性。


<details>
  <summary>Details</summary>
Motivation: 波斯语医疗领域缺乏高质量数据集，导致小语言模型在低资源语言环境中难以应对专业医疗问答需求，需通过开放网络数据解决这一瓶颈。

Method: 1. 爬取医疗杂志构建语料库 2. 收集真实医患QA对 3. 用上述数据微调基线模型

Result: 微调后模型在医疗问答基准测试中准确率提升，生成的回答质量优于原始基线模型

Conclusion: 验证了利用开放网络数据增强小语言模型在医疗领域的可行性，为资源有限的波斯语医疗AI应用提供了创新解决方案

Abstract: The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.

</details>


### [9] [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
*Sasha Boguraev,Christopher Potts,Kyle Mahowald*

Main category: cs.CL

TL;DR: 大型语言模型通过因果可解释性方法揭示句法抽象机制，推动语言学理论发展


<details>
  <summary>Details</summary>
Motivation: 通过分析LLMs内部机制来验证/改进现有语言学理论关于填充-缺口结构的共性假设

Method: 使用分布式交换干预技术分析LLMs对英语疑问句、关系从句等结构的处理机制

Result: 模型收敛出相似的结构分析范式，发现频率/填充类型/语境等被忽视的影响因素

Conclusion: LLMs的机制性分析能为语言学理论提供新的实证依据和改进方向

Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.

</details>


### [10] [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
*Roland Daynauth,Christopher Clarke,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.CL

TL;DR: 提出SLMEval熵最大化校准方法，显著提升开放任务评估效果并降低成本


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估校准方法在真实开放任务中表现不佳（如与人类判断呈负相关），需开发更通用的校准方案

Method: 基于少量人类偏好数据，通过估计潜在质量分布并重新加权评估分数实现高效校准

Result: 在两个真实生产用例中达到0.57 Spearman相关性，成本比GPT-4评估器降低5-30倍

Conclusion: SLMEval有效解决现有校准方法在开放任务中的局限性，实现更高相关性且显著提升成本效益

Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.

</details>


### [11] [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
*Wenrui Yu,Yiyi Chen,Johannes Bjerva,Sokol Kosta,Qiongxiu Li*

Main category: cs.CL

TL;DR: 提出LAGO图优化方法，通过语言相似性增强跨语言嵌入反演攻击的迁移性，仅需每语言10样本即可实现高效隐私攻击


<details>
  <summary>Details</summary>
Motivation: 现有跨语言嵌入反演攻击方法独立处理各语言，未充分利用语言间相似性。本文发现语言相似性可显著提升攻击迁移性，需开发语言感知的隐私保护方法

Method: 构建图约束分布式优化框架，将句法和词汇相似性作为边约束，整合Frobenius正则化与线性不等式/全变分约束，实现跨语言参数协同学习

Result: 在多种语言和嵌入模型上实现Rouge-L指标10-20%提升，验证语言相似性约束对攻击迁移性的关键作用，特别在低资源场景（每语言仅10样本）表现突出

Conclusion: 揭示语言相似性在隐私攻击中的放大器效应，为多语言NLP系统的隐私保护设计提供新方向，强调需开发语言感知的隐私嵌入方法

Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.

</details>


### [12] [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
*Yash Saxena,Anpur Padia,Mandar S Chaudhary,Kalpa Gunaratna,Srinivasan Parthasarathy,Manas Gaur*

Main category: cs.CL

TL;DR: 提出METEORA框架替代传统RAG的相似性重排序方法，通过基于理由的段落选择机制提升生成精度与抗对抗攻击能力


<details>
  <summary>Details</summary>
Motivation: 传统RAG依赖相似性检索和基于top-k的启发式重排序，存在可解释性差、对抗内容防御薄弱等问题

Method: 两阶段框架：1) 使用DPO微调LLM生成查询相关的rationales 2) 三阶段段落选择（局部相关性配对→肘部检测全局筛选→上下文扩展）+ 验证器进行一致性检查

Result: 在六大多领域数据集上实现33.34%的准确率提升，使用段落量减少50%；对抗场景F1从0.10提升至0.44

Conclusion: METEORA为RAG系统提供了可解释、高效且鲁棒的新范式，通过rationale驱动机制实现安全可靠的生成

Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md

</details>


### [13] [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
*Wei Liu,Siya Qi,Xinyu Wang,Chen Qian,Yali Du,Yulan He*

Main category: cs.CL

TL;DR: 提出NOVER框架——无需外部验证器的强化学习方法，仅需监督微调数据即可实现跨领域激励训练，性能超越同尺寸蒸馏模型7.7%


<details>
  <summary>Details</summary>
Motivation: 现有激励训练方法依赖外部验证器，限制了在数学/编程以外领域的应用。使用奖励模型需要昂贵标注数据

Method: 通过标准监督微调数据构建强化学习信号，支持逆向激励训练等新范式

Result: 在文本任务中超越同规模模型（如DeepSeek R1 671B蒸馏模型）7.7个百分点

Conclusion: NOVER框架突破验证器依赖，为LLM优化开辟新路径，其灵活性支持逆向激励等创新训练方式

Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.

</details>


### [14] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
*Sheshera Mysore,Debarati Das,Hancheng Cao,Bahareh Sarrafzadeh*

Main category: cs.CL

TL;DR: 研究用户与大型语言模型协作写作时识别出核心交互模式(PATHs)，揭示写作意图与协作行为的动态关联


<details>
  <summary>Details</summary>
Motivation: 探索用户在复杂写作场景中如何主动调整LLM输出，超越传统任务分类的静态分析，理解动态协作本质

Method: 通过大规模真实用户数据分析(Bing Copilot/WildChat)，采用行为模式聚类和统计相关性研究

Result: 识别5种核心PATHs(意图修订/文本探索/提问/风格调整/内容注入)，发现特定写作意图与PATHs显著相关

Conclusion: LLM对齐需支持动态协作框架，适应不同写作意图的演化路径，提升人机共创效率

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [15] [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
*Burak Erinç Çetin,Yıldırım Özen,Elif Naz Demiryılmaz,Kaan Engür,Cagri Toraman*

Main category: cs.CL

TL;DR: 研究对29个开源大语言模型进行跨语言伦理评估，发现模型在安全性与公平性表现较好但可靠性待提升，Gemma和Qwen模型综合表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究存在伦理评估维度狭窄、语言覆盖单一、模型样本不足等问题，需建立更全面的评估框架指导安全开发。

Method: 采用LLM-as-a-Judge方法，在英语/土耳其语环境下从鲁棒性、可靠性、安全性、公平性四个维度评估29个开源模型。

Result: 多数模型优化侧重安全与公平，大参数模型伦理表现更优（Gemma/Qwen最佳），可靠性成为普遍短板。

Conclusion: 伦理评估可脱离语言依赖实现，模型规模与伦理表现正相关，为跨语言AI伦理评估提供新范式。

Abstract: Generative large language models present significant potential but also raise
critical ethical concerns. Most studies focus on narrow ethical dimensions, and
also limited diversity of languages and models. To address these gaps, we
conduct a broad ethical evaluation of 29 recent open-source large language
models using a novel data collection including four ethical aspects:
Robustness, reliability, safety, and fairness. We analyze model behavior in
both a commonly used language, English, and a low-resource language, Turkish.
Our aim is to provide a comprehensive ethical assessment and guide safer model
development by filling existing gaps in evaluation breadth, language coverage,
and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal
that optimization efforts for many open-source models appear to have
prioritized safety and fairness, and demonstrated good robustness while
reliability remains a concern. We demonstrate that ethical evaluation can be
effectively conducted independently of the language used. In addition, models
with larger parameter counts tend to exhibit better ethical performance, with
Gemma and Qwen models demonstrating the most ethical behavior among those
evaluated.

</details>


### [16] [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
*Yu Zhang*

Main category: cs.CL

TL;DR: 1979-2024年顶级NLP会议研究表明：语言模型内外影响力最大，伦理类话题在政策领域受关注但学术引用较少


<details>
  <summary>Details</summary>
Motivation: 量化不同NLP主题在学术圈内外的传播差异，揭示研究影响力的分布特征

Method: 通过分析ACL/EMNLP/NAACL论文的学术引用及专利/媒体/政策文献的外部引用

Result: 语言模型影响力最广，语言学基础类研究影响较低；伦理公平类研究政策关注度高但学术引用少

Conclusion: 学术与外部影响力总体趋同，但不同领域有侧重：专利关注应用，媒体/政策关注社会影响

Abstract: We investigate the impacts of NLP research published in top-tier conferences
(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from
research articles and external sources such as patents, media, and policy
documents, we examine how different NLP topics are consumed both within the
academic community and by the broader public. Our findings reveal that language
modeling has the widest internal and external influence, while linguistic
foundations have lower impacts. We also observe that internal and external
impacts generally align, but topics like ethics, bias, and fairness show
significant attention in policy documents with much fewer academic citations.
Additionally, external domains exhibit distinct preferences, with patents
focusing on practical NLP applications and media and policy documents engaging
more with the societal implications of NLP models.

</details>


### [17] [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 探讨小型语言模型在工业级文本分类任务中的应用潜力及效率优势


<details>
  <summary>Details</summary>
Motivation: 现有大型解码器模型存在推理效率低、依赖提示质量、资源消耗大等痛点，工业场景亟需更轻量的解决方案

Method: 通过提示工程和监督微调方法，在邮件分类、法律文档分类、长学术文本分类等实际场景进行系统评估

Result: 验证了小模型在保持分类性能的同时显著提升VRAM利用率，实现性能与资源效率的平衡

Conclusion: 经过适当优化的小型模型能够有效支持工业级文本分类任务的本地化部署，为实际应用提供新思路

Abstract: With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.

</details>


### [18] [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
*KMA Solaiman*

Main category: cs.CL

TL;DR: BiasLab是包含300篇政治新闻标注的数据集，支持可解释的意识形态偏见建模。通过双重标注体系、GPT-4o模拟标注对比，提供丰富的理论依据标注，助力开发透明NLP系统。


<details>
  <summary>Details</summary>
Motivation: 解决现有政治偏见检测缺乏可解释性的问题，通过细粒度标注体系建立可操作的偏见解释模型，推动社会意识NLP系统发展。

Method: 1. 从900篇候选文档筛选政治新闻
2. 众包标注（民主党/共和党情感双标尺+理论指标）
3. 标注质量控制（定向资格认证+试点优化）
4. GPT-4o模式化标注对比实验
5. 定义感知漂移预测/理论分类两大任务

Result: 1. 标注者一致性Kappa>0.7
2. 13%标注与媒体源偏见错位
3. GPT-4o在右倾内容误判率达29%
4. 基线模型F1仅0.62，显示解释性建模难度

Conclusion: BiasLab通过理论标注推动可解释偏见分析，公开数据集促进人机交互可解释性研究，为NLP系统透明化提供新基准。

Abstract: We present BiasLab, a dataset of 300 political news articles annotated for
perceived ideological bias. These articles were selected from a curated
900-document pool covering diverse political events and source biases. Each
article is labeled by crowdworkers along two independent scales, assessing
sentiment toward the Democratic and Republican parties, and enriched with
rationale indicators. The annotation pipeline incorporates targeted worker
qualification and was refined through pilot-phase analysis. We quantify
inter-annotator agreement, analyze misalignment with source-level outlet bias,
and organize the resulting labels into interpretable subsets. Additionally, we
simulate annotation using schema-constrained GPT-4o, enabling direct comparison
to human labels and revealing mirrored asymmetries, especially in
misclassifying subtly right-leaning content. We define two modeling tasks:
perception drift prediction and rationale type classification, and report
baseline performance to illustrate the challenge of explainable bias detection.
BiasLab's rich rationale annotations provide actionable interpretations that
facilitate explainable modeling of political bias, supporting the development
of transparent, socially aware NLP systems. We release the dataset, annotation
schema, and modeling code to encourage research on human-in-the-loop
interpretability and the evaluation of explanation effectiveness in real-world
settings.

</details>


### [19] [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
*Gagan Bhatia,Maxime Peyrard,Wei Zhao*

Main category: cs.CL

TL;DR: 研究发现BPE分词器的日期碎片化问题会损害时间推理性能，提出评估指标DateAugBench测试集，并揭示LLMs通过分层抽象机制重组日期片段的现象。


<details>
  <summary>Details</summary>
Motivation: 传统BPE分词器将连续日期切分为无意义片段（如20250312→202/503/12），导致时间推理结构模糊并降低模型性能，需系统性评估和机制分析。

Method: 1) 提出可解释的日期碎片化比率指标；2) 构建含6500样本的DateAugBench测试集；3) 通过层次化探测和因果注意力分析揭示LLM的日期抽象机制。

Result: 过度碎片化使历史/未来日期准确率下降10%，模型越大日期抽象速度越快，LLMs重组路径为年→月→日（与人类认知相反）。

Conclusion: 日期碎片化显著影响推理性能，大模型通过分层注意力快速修复碎片，但重组逻辑与人类认知存在系统性差异。

Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future
regimes; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day).

</details>


### [20] [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
*Yash Kumar Atri,Thomas H Shin,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 提出自适应检索增强生成模型bRAGgen，整合实时医学证据提升减重手术患者信息支持，配套发布首个大样本专业数据集bRAGq。


<details>
  <summary>Details</summary>
Motivation: 减肥手术患者需多学科长期随访，但存在医疗资源获取障碍和时效性信息短缺问题，传统方案易导致信息过时风险。

Method: 开发动态阈值驱动的RAG架构bRAGgen，当置信度不足时自动检索最新证据；构建经外科专家验证的1302问bRAGq专业数据集。

Result: 在语言模型指标和外科专家双盲评估中，bRAGgen在临床准确性和相关性上显著优于现有先进模型。

Conclusion: 该自更新系统有效解决减重手术信息时效痛点，为患者护理提供持续可靠的决策支持，降低错误信息风险。

Abstract: While bariatric and metabolic surgery (MBS) is considered the gold standard
treatment for severe and morbid obesity, its therapeutic efficacy hinges upon
active and longitudinal engagement with multidisciplinary providers, including
surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This
engagement spans the entire patient journey, from preoperative preparation to
long-term postoperative management. However, this process is often hindered by
numerous healthcare disparities, such as logistical and access barriers, which
impair easy patient access to timely, evidence-based, clinician-endorsed
information. To address these gaps, we introduce bRAGgen, a novel adaptive
retrieval-augmented generation (RAG)-based model that autonomously integrates
real-time medical evidence when response confidence dips below dynamic
thresholds. This self-updating architecture ensures that responses remain
current and accurate, reducing the risk of misinformation. Additionally, we
present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,
validated by an expert bariatric surgeon. bRAGq constitutes the first
large-scale, domain-specific benchmark for comprehensive MBS care. In a
two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models
using both large language model (LLM)--based metrics and expert surgeon review.
Across all evaluation dimensions, bRAGgen demonstrates substantially superior
performance in generating clinically accurate and relevant responses.

</details>


### [21] [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
*Yue Li,Xin Yi,Dongsheng Shi,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: 提出分层安全校准方法HSR，通过逐级恢复关键注意力头和神经元，有效解决大型视觉-语言模型剪枝后安全性下降问题


<details>
  <summary>Details</summary>
Motivation: 模型剪枝技术虽能压缩模型规模，但会显著降低模型安全性能，阻碍其在资源受限场景的安全部署

Method: 分层次校准安全性能：1. 量化注意力头安全贡献值筛选关键注意力头 2. 在选定注意力头内恢复对安全性能起决定作用的神经元

Result: 在多个模型架构和剪枝方案中验证有效性，显著提升剪枝模型的安全性能，首次实现剪枝后LVLMs的安全性能恢复

Conclusion: 分层安全校准机制成功解决了模型压缩与安全性能的平衡问题，为资源受限场景下的安全模型部署提供了新思路

Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.

</details>


### [22] [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
*Bo Li,Gexiang Fang,Wei Ye,Zhenghua Xu,Jinglei Zhang,Hao Cheng,Shikun Zhang*

Main category: cs.CL

TL;DR: 提出MPL框架，通过整合多种编程语言和虚拟运行的function-prompt，显著提升信息抽取任务的结构化输出效果


<details>
  <summary>Details</summary>
Motivation: 现有研究局限于Python语言的应用，未能充分利用C++/Java等其他编程语言在监督微调阶段的潜力，制约了结构化输出的优化空间

Method: 在监督微调阶段集成多种编程语言，创新性引入function-prompt机制配合虚拟运行环境，实现更高效的代码风格输入模拟

Result: 跨数据集实验验证了框架有效性，尤其在复杂结构化输出场景下展现显著优势，并通过消融实验证实多语言协同效应

Conclusion: 多编程语言协同策略为信息抽取开辟新路径，虚拟运行机制大幅提升计算效率，开源代码库推动领域技术发展

Abstract: Recent research in information extraction (IE) focuses on utilizing
code-style inputs to enhance structured output generation. The intuition behind
this is that the programming languages (PLs) inherently exhibit greater
structural organization than natural languages (NLs). This structural advantage
makes PLs particularly suited for IE tasks. Nevertheless, existing research
primarily focuses on Python for code-style simulation, overlooking the
potential of other widely-used PLs (e.g., C++ and Java) during the supervised
fine-tuning (SFT) phase. In this research, we propose \textbf{M}ultiple
\textbf{P}rogramming \textbf{L}anguages with large language models for
information extraction (abbreviated as \textbf{MPL}), a novel framework that
explores the potential of incorporating different PLs in the SFT phase.
Additionally, we introduce \texttt{function-prompt} with virtual running to
simulate code-style inputs more effectively and efficiently. Experimental
results on a wide range of datasets demonstrate the effectiveness of MPL.
Furthermore, we conduct extensive experiments to provide a comprehensive
analysis. We have released our code for future research.

</details>


### [23] [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
*Haotian Lan,Yao Gao,Yujun Cheng,Wei Yuan,Kun Wang*

Main category: cs.CL

TL;DR: 提出结合无监督内容提取与有监督微调的双重LLM框架，量化社交媒体旅游期待，发现休闲社交期待比自然情感因素更具影响力


<details>
  <summary>Details</summary>
Motivation: 用户生成内容(UGC)对旅游决策至关重要，但传统分析方法缺乏扩展性，需开发可扩展的旅游期待量化方法

Method: 无监督LLM提取UGC期待特征 + 基于调查数据的有监督微调组合框架

Result: 休闲/社交期待比基础性自然/情感要素更能驱动用户参与，验证框架在体验个性化和社会化旅行推广中的有效性

Conclusion: 通过确立LLM的旅游期待量化能力，框架可扩展至消费行为研究，展示计算社会科学在营销优化中的变革潜力

Abstract: Social media's rise establishes user-generated content (UGC) as pivotal for
travel decisions, yet analytical methods lack scalability. This study
introduces a dual-method LLM framework: unsupervised expectation extraction
from UGC paired with survey-informed supervised fine-tuning. Findings reveal
leisure/social expectations drive engagement more than foundational
natural/emotional factors. By establishing LLMs as precision tools for
expectation quantification, we advance tourism analytics methodology and
propose targeted strategies for experience personalization and social travel
promotion. The framework's adaptability extends to consumer behavior research,
demonstrating computational social science's transformative potential in
marketing optimization.

</details>


### [24] [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
*Hyopil Shin,Sangah Lee,Dongjun Jang,Wooseok Song,Jaeyoon Kim,Chaeyoung Oh,Hyemi Jo,Youngchae Ahn,Sihyun Oh,Hyohyeong Chang,Sunkyoung Kim,Jinsik Lee*

Main category: cs.CL

TL;DR: 提出KoBALT基准测试，包含700个多选问题覆盖韩语五大语言领域，用于评估韩语大模型的语言理解能力，揭示现有模型在音系学和形态学领域的薄弱表现。


<details>
  <summary>Details</summary>
Motivation: 现有韩语基准测试缺乏语言学深度和类型学基础，难以准确评估形态复杂的韩语模型真实语言理解能力。KoBALT旨在通过语言学驱动的问题设计，减少数据污染风险。

Method: 构建24种语言现象的专家标注数据集，采用n-gram低重叠设计。评估20个主流大模型，并通过95名标注者进行人类偏好验证。

Result: 最佳模型总体准确率61%，语义领域66%最高，音系学(31%)和形态学(36%)最弱。人类评估结果与KoBALT评分高度相关(r=0.89)。

Conclusion: KoBALT填补了韩语模型语言学评估的空白，为评估真实语言能力提供可靠框架，推动类型学多样语言的评估研究。

Abstract: We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a
comprehensive linguistically-motivated benchmark comprising 700 multiple-choice
questions spanning 24 phenomena across five linguistic domains: syntax,
semantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed
to advance the evaluation of large language models (LLMs) in Korean, a
morphologically rich language, by addressing the limitations of conventional
benchmarks that often lack linguistic depth and typological grounding. It
introduces a suite of expert-curated, linguistically motivated questions with
minimal n-gram overlap with standard Korean corpora, substantially mitigating
the risk of data contamination and allowing a more robust assessment of true
language understanding. Our evaluation of 20 contemporary LLMs reveals
significant performance disparities, with the highest-performing model
achieving 61\% general accuracy but showing substantial variation across
linguistic domains - from stronger performance in semantics (66\%) to
considerable weaknesses in phonology (31\%) and morphology (36\%). Through
human preference evaluation with 95 annotators, we demonstrate a strong
correlation between KoBALT scores and human judgments, validating our
benchmark's effectiveness as a discriminative measure of Korean language
understanding. KoBALT addresses critical gaps in linguistic evaluation for
typologically diverse languages and provides a robust framework for assessing
genuine linguistic competence in Korean language models.

</details>


### [25] [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
*Yue Zhou,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: 研究发现LLMs在数学/编程领域对非裔美国人群体存在归因偏见（正确率低18%），在写作评估中对亚裔作者存在系统性偏见，且模型可视化时会自动分配种族刻板颜色


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLMs的表面刻板印象，而本研究揭示其在解决方案真实性评估中存在的深层人口统计偏见，这对教育评估等应用场景构成潜在风险

Method: 通过5个与人类价值观对齐的LLMs（数学/编程/常识/写作四类问题），采用解决方案归因实验（1200次）和双盲评估实验，结合可视化代码生成分析模型内部偏见

Result: 1. 所有模型在数学编码领域对非洲裔正确解决方案归因率低18% 2. 写作评估中亚洲作者得分最低（-22%）3. 83%可视化代码自动分配种族刻板颜色（如非洲裔→红色）

Conclusion: 人口统计偏见已渗透到LLMs的推理过程中，远超出表面刻板印象，需要针对模型内部表征进行去偏处理，特别是在教育评估等高风险场景需建立双重校验机制

Abstract: Despite LLMs' explicit alignment against demographic stereotypes, they have
been shown to exhibit biases under various social contexts. In this work, we
find that LLMs exhibit concerning biases in how they associate solution
veracity with demographics. Through experiments across five human value-aligned
LLMs on mathematics, coding, commonsense, and writing problems, we reveal two
forms of such veracity biases: Attribution Bias, where models
disproportionately attribute correct solutions to certain demographic groups,
and Evaluation Bias, where models' assessment of identical solutions varies
based on perceived demographic authorship. Our results show pervasive biases:
LLMs consistently attribute fewer correct solutions and more incorrect ones to
African-American groups in math and coding, while Asian authorships are least
preferred in writing evaluation. In additional studies, we show LLMs
automatically assign racially stereotypical colors to demographic groups in
visualization code, suggesting these biases are deeply embedded in models'
reasoning processes. Our findings indicate that demographic bias extends beyond
surface-level stereotypes and social context provocations, raising concerns
about LLMs' deployment in educational and evaluation settings.

</details>


### [26] [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
*Hyang Cui*

Main category: cs.CL

TL;DR: 提出基于LLM生成参考+语义评估的混合机器翻译质量评估方法，在8种语言对中超越传统直接评分法


<details>
  <summary>Details</summary>
Motivation: 现有LLM直接评分法在段落级与人工判断相关性低，需更可靠的质量评估范式

Method: 1. 使用仅解码器LLM生成高质量参考译文 2. 通过句子嵌入计算语义相似度评分

Result: 覆盖8个LLM和8种语言对的实验显示，本方法优于LLM直接评分基线及非LLM指标（MTME）

Conclusion: 生成式评估范式验证了结合流畅生成与语义评估的混合方法有效性，推动MTQE评估范式革新

Abstract: Recent studies have applied large language models (LLMs) to machine
translation quality estimation (MTQE) by prompting models to assign numeric
scores. Nonetheless, these direct scoring methods tend to show low
segment-level correlation with human judgments. In this paper, we propose a
generation-based evaluation paradigm that leverages decoder-only LLMs to
produce high-quality references, followed by semantic similarity scoring using
sentence embeddings. We conduct the most extensive evaluation to date in MTQE,
covering 8 LLMs and 8 language pairs. Empirical results show that our method
outperforms both intra-LLM direct scoring baselines and external non-LLM
reference-free metrics from MTME. These findings demonstrate the strength of
generation-based evaluation and support a shift toward hybrid approaches that
combine fluent generation with accurate semantic assessment.

</details>


### [27] [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
*Menschikov Mikhail,Alexander Kharitonov,Maiia Kotyga,Vadim Porvatov,Anna Zhukovskaya,David Kagramanyan,Egor Shvetsov,Evgeny Burnaev*

Main category: cs.CL

TL;DR: 研究发现大语言模型存在模型驱动的位置偏见，不同语言表现差异显著（如Qwen2.5-7B偏好尾部信息），显式位置提示反而降低准确率，且熵值与模型准确性无直接关联。


<details>
  <summary>Details</summary>
Motivation: 探索位置偏见与语言多样性的交互关系，填补现有研究对多语言场景下位置偏见动态机制的认知空白。

Method: 跨语言实验设计：使用英语/俄语/德语/印地语/越南语五种类型学差异语言，测试模型位置敏感度、显式位置引导效果及句法-位置交互。

Result: 1. 位置偏好呈现模型特异性（Qwen2.5-7B尾部偏见）
2. 显式位置提示使多语言准确率下降20-35%
3. 符合位置偏见的输入使熵值增加15%
4. 在印地语中强制施加优势语序

Conclusion: 提示工程需考虑跨语言位置敏感性，最小熵原则不能预测模型表现，自由语序语言处理存在隐性语法约束。

Abstract: Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.

</details>


### [28] [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
*Shicheng Xu,Liang Pang,Yunchang Zhu,Jia Gu,Zihao Wei,Jingcheng Deng,Feiyang Pan,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出强化学习蒸馏框架RLKD，通过生成式结构奖励模型(GSRM)实现教师模型多分支推理结构的有效迁移，突破传统监督微调(SFT)的扁平化限制。


<details>
  <summary>Details</summary>
Motivation: 传统SFT蒸馏仅捕捉教师模型的表面推理路径，无法传递其隐含的元推理-子问题解决的多分支结构，认知神经科学指出真实推理需多步骤交织。

Method: 设计GSRM将推理路径拆解为元推理与子问题解决步骤，通过结构对齐奖励驱动强化学习训练，使学生模型内化教师的结构化推理模式。

Result: 在0.1%极小数据量下，RLKD仍超越标准SFT-RL训练流程，显著释放学生模型的潜在推理能力。

Conclusion: RLKD通过结构奖励引导的强化学习，实现知识蒸馏从路径模仿到结构内化的范式转变，为小模型推理能力提升提供新路径。

Abstract: Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.

</details>


### [29] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
*Bin Xu,Yu Bai,Huashan Sun,Yiguan Lin,Siming Liu,Xinyue Liang,Yaolin Li,Yang Gao,Heyan Huang*

Main category: cs.CL

TL;DR: 构建首个教育场景多维度评测基准EduBench，通过合成数据覆盖9大场景并成功训练出与顶级大模型相当的小型专用模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育场景的应用缺乏针对性优化和系统化评估标准，难以满足师生多维需求。

Method: 1) 创建含4000+教育情境的合成数据集 2) 设计覆盖12个关键维度的评估体系 3) 人工标注确保评估质量 4) 训练专用小型语言模型

Result: 训练的小型模型在测试集上达到Deepseek V3等顶尖大模型水平，资源效率显著提升。

Conclusion: 为教育导向的语言模型开发提供了系统性评估框架和实践基础，开源数据代码推动领域发展。

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [30] [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
*Mingbo Song,Heming Xia,Jun Zhang,Chak Tou Leong,Qiancheng Xu,Wenjie Li,Sujian Li*

Main category: cs.CL

TL;DR: 提出KNN-SSD算法，通过K近邻搜索匹配不同领域输入与跳层策略，提升自推测解码的领域泛化能力，实现LLM推理1.3x-1.6x加速


<details>
  <summary>Details</summary>
Motivation: 现有自推测解码的跳层方案对领域迁移敏感，加速性能大幅下降。需增强该范式的领域适应能力

Method: 引入KNN搜索机制，动态匹配不同领域输入与最优跳层组合，构建更鲁棒的推测解码框架

Result: 在多模型多任务测试中，KNN-SSD实现1.3x-1.6x的LLM推理加速效果

Conclusion: KNN-SSD有效解决领域敏感问题，在保持无参免训练优势的同时，显著提升推测解码的加速性能和泛化能力

Abstract: Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in acceleration performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.

</details>


### [31] [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
*Mengyang Qiu,Zoe Brisebois,Siena Sun*

Main category: cs.CL

TL;DR: 大型语言模型在音位流畅性任务中无法有效模拟人类个体差异，输出多样性不足且结构僵化


<details>
  <summary>Details</summary>
Motivation: 验证LLMs能否再现人类在认知任务中的行为变异性，特别是音位流畅性任务中的个体差异

Method: 评估34种模型配置（含不同提示方式/采样温度/模型类型），与106名人类参与者的反应进行对比分析

Result: Claude 3.7 Sonnet接近人类平均表现，但所有模型均未能复现人类变异性，LLM输出多样性低且结构固化

Conclusion: LLMs在模拟人类认知行为方面存在根本性限制，不能有效替代人类被试进行个体差异研究

Abstract: Large language models (LLMs) are increasingly explored as substitutes for
human participants in cognitive tasks, but their ability to simulate human
behavioral variability remains unclear. This study examines whether LLMs can
approximate individual differences in the phonemic fluency task, where
participants generate words beginning with a target letter. We evaluated 34
model configurations, varying prompt specificity, sampling temperature, and
model type, and compared outputs to responses from 106 human participants.
While some configurations, especially Claude 3.7 Sonnet, matched human averages
and lexical preferences, none reproduced the scope of human variability. LLM
outputs were consistently less diverse and structurally rigid, and LLM
ensembles failed to increase diversity. Network analyses further revealed
fundamental differences in retrieval structure between humans and models. These
results highlight key limitations in using LLMs to simulate human cognition and
behavior.

</details>


### [32] [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
*Yuqing Yang,Robin Jia*

Main category: cs.CL

TL;DR: 研究发现大型语言模型虽具备撤回错误答案的能力，但实际撤回频率较低，其撤回行为与内部信念密切相关。监督微调可显著提升模型自我修正能力。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在自知错误时是否承认（撤回行为）的机制，揭示模型内部信念对自我修正行为的影响机制。

Method: 1. 构建模型特定数据集评估撤回行为
2. 分析内部信念指标与撤回关联
3. 通过注意力干预实验验证因果关系
4. 实施监督微调改进性能

Result: 模型仅偶尔撤回错误答案；内部信念决定撤回行为；监督微调使准确率提升31.7%（GPT-4）

Conclusion: 语言模型的自我修正能力受其内部信念驱动，通过参数调整可优化模型的自我验证机制，为构建更可靠的AI系统提供新方向。

Abstract: Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as "retraction" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they "believe" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.

</details>


### [33] [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
*Abhay Kumara Sri Krishna Nandiraju,Gondy Leroy,David Kauchak,Arif Ahmed*

Main category: cs.CL

TL;DR: 研究通过生成式AI简化健康信息时发现关键信息缺失问题，提出五种修复方法并验证添加所有缺失实体能有效提升文本质量。


<details>
  <summary>Details</summary>
Motivation: 生成式AI简化健康信息时可能遗漏关键内容，需系统评估缺失信息对理解的影响及修复方案的有效性。

Method: 收集50篇健康文本用GPT-4简化，设计五种方法（全实体/全词语/TOP3实体/随机实体对照）补全缺失，使用余弦相似度和ROUGE评估语义与内容一致性。

Result: 添加所有缺失实体显著改善文本再生效果（优于TOP3实体和随机对照），现有工具可识别实体但无法有效排序重要性。

Conclusion: 实体补全策略能有效提升简化文本完整性，但需开发更优的实体重要性评估算法以完善生成式AI的医疗信息处理能力。

Abstract: Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.

</details>


### [34] [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
*Ying Zhang,Benjamin Heinzerling,Dongyuan Li,Ryoma Ishigaki,Yuta Hitomi,Kentaro Inui*

Main category: cs.CL

TL;DR: 混合训练通过促进共享参数提升语言模型事实回忆的泛化能力


<details>
  <summary>Details</summary>
Motivation: 探索不同训练策略（两阶段训练 vs 混合训练）对语言模型事实回忆能力的影响机制，特别是参数动态变化的作用

Method: 使用跨任务梯度追踪技术分析共享参数分布，在Llama-3.2B和Pythia-2.8B模型上进行合成事实回忆数据集实验

Result: 混合训练形成更大规模且集中化的共享参数网络，促进跨任务知识泛化

Conclusion: 共享参数的出现是语言模型实现跨任务知识泛化的关键机制

Abstract: Fact recall, the ability of language models (LMs) to retrieve specific
factual knowledge, remains a challenging task despite their impressive general
capabilities. Common training strategies often struggle to promote robust
recall behavior with two-stage training, which first trains a model with
fact-storing examples (e.g., factual statements) and then with fact-recalling
examples (question-answer pairs), tending to encourage rote memorization rather
than generalizable fact retrieval. In contrast, mixed training, which jointly
uses both types of examples, has been empirically shown to improve the ability
to recall facts, but the underlying mechanisms are still poorly understood. In
this work, we investigate how these training strategies affect how model
parameters are shaped during training and how these differences relate to their
ability to recall facts. We introduce cross-task gradient trace to identify
shared parameters, those strongly influenced by both fact-storing and
fact-recalling examples. Our analysis on synthetic fact recall datasets with
the Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a
larger and more centralized set of shared parameters. These findings suggest
that the emergence of parameters may play a key role in enabling LMs to
generalize factual knowledge across task formulations.

</details>


### [35] [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
*Zirui He,Mingyu Jin,Bo Shen,Ali Payani,Yongfeng Zhang,Mengnan Du*

Main category: cs.CL

TL;DR: 提出基于稀疏表示空间的监督引导方法，通过稀疏自编码器和子空间约束实现更精准可控的LLM行为干预


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放域生成场景中难以可靠控制大语言模型行为，需要更可解释且高效的干预方式

Method: 1. 使用稀疏自编码器提取解耦语义的稀疏潜在表征 2. 训练线性分类器定位任务相关子空间 3. 在子空间内学习目标对齐的监督引导向量

Result: 在情感/真实性/政治倾向控制任务中，相比基线方法成功率提升且生成质量下降更小，仅需极小表征子空间即可有效干预

Conclusion: 稀疏表征子空间约束使模型控制更具靶向性和可解释性，为LLM行为干预提供了新范式

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
sparse, interpretable representation spaces. We employ sparse autoencoders
(SAEs)to obtain sparse latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.

</details>


### [36] [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
*Sophie Wu,Jan Philip Wahle,Saif M. Mohammad*

Main category: cs.CL

TL;DR: 首次通过大规模自然语言数据分析情绪、具身化与日常语言的关系，发现身体部位提及（BPMs）与情绪强度及健康结果显著相关


<details>
  <summary>Details</summary>
Motivation: 探索身体语言在情感表达中的具身化特征及其与人类福祉的潜在联系，填补NLP与情感科学的交叉研究空白

Method: 构建包含身体部位提及的英语在线文本语料库（博客和推文），采用词-情绪关联词典和人工标注分析情感特征

Result: 1. BPMs出现频率5-10%且时空分布差异显著；2. BPMs文本情绪强度更高（非显性身体反应场景亦然）；3. 身体语言与健康负面指标强相关

Conclusion: 身体部位词汇研究为NLP、情感科学和人类福祉的跨学科研究提供了新方向，具有重要方法论和应用价值

Abstract: This paper is the first investigation of the connection between emotion,
embodiment, and everyday language in a large sample of natural language data.
We created corpora of body part mentions (BPMs) in online English text (blog
posts and tweets). This includes a subset featuring human annotations for the
emotions of the person whose body part is mentioned in the text. We show that
BPMs are common in personal narratives and tweets (~5% to 10% of posts include
BPMs) and that their usage patterns vary markedly by time and %geographic
location. Using word-emotion association lexicons and our annotated data, we
show that text containing BPMs tends to be more emotionally charged, even when
the BPM is not explicitly used to describe a physical reaction to the emotion
in the text. Finally, we discover a strong and statistically significant
correlation between body-related language and a variety of poorer health
outcomes. In sum, we argue that investigating the role of body-part related
words in language can open up valuable avenues of future research at the
intersection of NLP, the affective sciences, and the study of human wellbeing.

</details>


### [37] [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 通过优化上下文学习(ICL)的演示配置策略，显著提升多模态大语言模型在情感分析任务中的表现，证明其具备与监督模型相当的情感感知能力


<details>
  <summary>Details</summary>
Motivation: 传统零样本范式在多模态情感分析(MSA)任务中表现不佳，质疑多模态大语言模型的情感理解能力，需探索模型潜力

Method: 将零样本扩展为上下文学习范式，系统优化演示样本的检索机制、呈现形式和分布策略，并设计偏差抵消机制

Result: 在六个MSA数据集上实现平均准确率提升：相比零样本提高15.9%，相比随机ICL基线提高11.2%

Conclusion: 通过合理的ICL配置策略，多模态大语言模型展现出强大的情感分析能力，为实际应用提供了新范式

Abstract: The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.

</details>


### [38] [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
*Anfeng Xu,Tiantian Feng,So Hyun Kim,Somer Bishop,Catherine Lord,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 探索大语言模型在纠正儿童对话语音识别错误中的应用效果，发现对零样本和CTC模型有效，但对上下文整合和微调自回归模型效果有限


<details>
  <summary>Details</summary>
Motivation: 儿童语音识别存在显著挑战，大语言模型在对话场景中的应用潜力尚未充分挖掘

Method: 使用两个儿童对话数据集，测试大语言模型在零样本和微调ASR（CTC与Whisper）输出中的纠错能力

Result: LLMs能有效提升零样本ASR和CTC模型的性能，但整合上下文信息或处理微调自回归模型时改进有限

Conclusion: 证实LLMs在特定场景下的纠错价值，同时揭示了处理优化ASR输出和上下文整合方面的技术挑战

Abstract: Automatic Speech Recognition (ASR) has recently shown remarkable progress,
but accurately transcribing children's speech remains a significant challenge.
Recent developments in Large Language Models (LLMs) have shown promise in
improving ASR transcriptions. However, their applications in child speech
including conversational scenarios are underexplored. In this study, we explore
the use of LLMs in correcting ASR errors for conversational child speech. We
demonstrate the promises and challenges of LLMs through experiments on two
children's conversational speech datasets with both zero-shot and fine-tuned
ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR
outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs
to improve ASR performance when incorporating contextual information or when
using fine-tuned autoregressive ASR (e.g., Whisper) outputs.

</details>


### [39] [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
*Jisu Kim,Youngwoo Shin,Uiji Hwang,Jihun Choi,Richeng Xuan,Taeuk Kim*

Main category: cs.CL

TL;DR: LLMs处理习语时采用记忆+上下文推理的混合机制，组合性习语尤其依赖推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs多语言环境下习语处理机制的深入分析，需揭示其底层认知逻辑。

Method: 构建六语言习语数据集MIDAS，系统评估模型表现并解析关键影响因素。

Result: 模型通过知识检索与推理推断的协同实现习语理解，组合型习语展现更强推理依赖性。

Conclusion: LLMs的习语能力源于记忆与推理的动态交互，为优化多语言语义理解提供新视角。

Abstract: Idioms have long posed a challenge due to their unique linguistic properties,
which set them apart from other common expressions. While recent studies have
leveraged large language models (LLMs) to handle idioms across various tasks,
e.g., idiom-containing sentence generation and idiomatic machine translation,
little is known about the underlying mechanisms of idiom processing in LLMs,
particularly in multilingual settings. To this end, we introduce MIDAS, a new
large-scale dataset of idioms in six languages, each paired with its
corresponding meaning. Leveraging this resource, we conduct a comprehensive
evaluation of LLMs' idiom processing ability, identifying key factors that
influence their performance. Our findings suggest that LLMs rely not only on
memorization, but also adopt a hybrid approach that integrates contextual cues
and reasoning, especially when processing compositional idioms. This implies
that idiom understanding in LLMs emerges from an interplay between internal
knowledge retrieval and reasoning-based inference.

</details>


### [40] [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
*Jiwon Moon,Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: LLM评估者在代码评估中存在对语义等效代码的表面变化敏感问题，六种偏见类型影响评估公平性


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为代码评估者的普及，需验证其对语义等效但形式不同代码的评估可靠性

Method: 通过定义六种代码评估偏见类型，在5种编程语言和多个LLM上进行系统性实证研究

Result: 所有测试的LLM均存在正负偏见，生成测试用例后仍无法消除评分偏差

Conclusion: 当前LLM代码评估方法存在系统性缺陷，需要开发更鲁棒的评估框架

Abstract: With the growing use of large language models(LLMs) as evaluators, their
application has expanded to code evaluation tasks, where they assess the
correctness of generated code without relying on reference implementations.
While this offers scalability and flexibility, it also raises a critical,
unresolved question: Can LLM judges fairly and robustly evaluate semantically
equivalent code with superficial variations? Functionally correct code often
exhibits variations-such as differences in variable names, comments, or
formatting-that should not influence its correctness. Yet, whether LLM judges
can reliably handle these variations remains unclear. We present the first
comprehensive study of this issue, defining six types of potential bias in code
evaluation and revealing their systematic impact on LLM judges. Across five
programming languages and multiple LLMs, we empirically demonstrate that all
tested LLM judges are susceptible to both positive and negative biases,
resulting in inflated or unfairly low scores. Moreover, we observe that LLM
judges remain vulnerable to these biases even when prompted to generate test
cases before scoring, highlighting the need for more robust code evaluation
methods.

</details>


### [41] [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
*Bohao Wu,Qingyun Wang,Yue Guo*

Main category: cs.CL

TL;DR: 提出了两种高效、低资源的个性化术语检测方法(LoRA微调与个性化提示)，在仅使用10%标注数据的情况下性能超越GPT-4和基线模型


<details>
  <summary>Details</summary>
Motivation: 现有个性化术语检测方法需要大量用户特定标注数据和计算资源，难以实际部署应用

Method: 结合LoRA轻量微调与推理时个性化提示策略，并探索有限标注数据与无监督用户信号的混合方法

Result: 个性化LoRA模型F1分数超越GPT-4达21.4%，超过最佳基线8.3%，仅需10%标注数据即达到可比性能

Conclusion: 首次系统探索基于开源模型的高效个性化术语检测，为可扩展的用户自适应NLP系统提供实用解决方案

Abstract: Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.

</details>


### [42] [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
*Ali Sarosh Bangash,Krish Veera,Ishfat Abrar Islam,Raiyan Abdul Baten*

Main category: cs.CL

TL;DR: 自动化频率原创性评分框架MuseRAG，结合LLM与RAG技术，有效替代人工标注实现大规模创意评估


<details>
  <summary>Details</summary>
Motivation: 现有基于人工分类的创意原创性评估方法存在效率低、易出错、难以规模化等问题，亟需自动化解决方案

Method: 开发MuseRAG系统：通过外部编排的RAG框架，利用LLM进行语义检索与零样本分类，实现创意自动分桶与原创性计算

Result: 在5个数据集（16294个创意）中达到与人类标注者0.89的评分相关性，聚类相似性指标AMI达0.59，展现强效度

Conclusion: 该框架支持大规模、意图敏感的原创性评估，为创造力研究提供可靠的技术支持

Abstract: An objective, face-valid way to assess the originality of creative ideas is
to measure how rare each idea is within a population -- an approach long used
in creativity research but difficult to automate at scale. Tabulating response
frequencies via manual bucketing of idea rephrasings is labor-intensive,
error-prone, and brittle under large corpora. We introduce a fully automated,
psychometrically validated pipeline for frequency-based originality scoring.
Our method, MuseRAG, combines large language models (LLMs) with an externally
orchestrated retrieval-augmented generation (RAG) framework. Given a new idea,
the system retrieves semantically similar prior idea buckets and zero-shot
prompts the LLM to judge whether the new idea belongs to an existing bucket or
forms a new one. The resulting buckets enable computation of frequency-based
originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG
matches human annotators in idea clustering structure and resolution (AMI =
0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong
convergent and external validity. Our work enables intent-sensitive,
human-aligned originality scoring at scale to aid creativity research.

</details>


### [43] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
*Wei Zhang,Zhenhong Zhou,Junfeng Fang,Rongwu Xu,Kun Wang,Yuanhe Zhang,Rui Wang,Ge Zhang,Xinfeng Li,Li Sun,Lingjuan Lyu,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 大语言模型在遵循长文本生成的长度指令方面存在显著不足，LIFEBench基准测试揭示了模型实际输出长度与厂商声称能力的差距，推理型模型表现优于专用长文本生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注生成质量而忽视长度约束，研究发现LLMs在简单长度指令遵循任务中表现不佳（如无法生成万字数小说），突显该领域研究空白。

Method: 构建跨4类任务、中英双语、覆盖16-8192词范围的10,800样本基准LIFEBench，评估26个主流LLMs的长度指令遵循能力。

Result: 1. 多数模型在短长度表现合格但超过阈值后急剧恶化 2. 所有模型实际输出远低于厂商宣称最大长度 3. 长上下文模型未能提升长度控制能力 4. 推理型LLMs达到SOTA长度遵循表现

Conclusion: 研究揭示当前LLMs长度控制的基础性缺陷，为未来模型优化提供方向，强调需重新评估模型实际能力与宣传指标的匹配度。

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [44] [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
*Derong Xu,Pengyue Jia,Xiaopeng Li,Yingyi Zhang,Maolin Wang,Qidong Liu,Xiangyu Zhao,Yichao Wang,Huifeng Guo,Ruiming Tang,Enhong Chen,Tong Xu*

Main category: cs.CL

TL;DR: 提出Align-GRAG框架解决图RAG系统存在的冗余节点和表征差异问题，通过双对齐机制提升大模型推理效果


<details>
  <summary>Details</summary>
Motivation: 传统图RAG系统存在检索冗余节点导致输入冗长，且图结构与语言表征不匹配限制知识利用效率

Method: 设计双对齐框架：1) 检索子图后联合优化图编码器与LLM推理总结；2) 使用KL散度和对比损失实现节点剪枝与语义空间统一

Result: 在GraphQA基准的常识推理、场景图理解、知识图谱推理任务中验证有效性

Conclusion: 通过节点/表征双对齐机制实现高效知识筛选与语义融合，为图增强语言模型提供新思路

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.

</details>


### [45] [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
*Viet-Anh Nguyen,Shiqian Zhao,Gia Dao,Runyi Hu,Yi Xie,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 提出SEAL自适应加密攻击框架，通过堆叠加密和动态策略显著提升大型推理模型的越狱成功率


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法难以平衡攻击效果与对抗自适应安全机制的鲁棒性，大型推理模型更强的推理能力可能带来更严重的安全漏洞

Method: 1. 堆叠式多密码组合加密干扰模型推理 2. 随机+自适应动态策略调整密码长度/顺序 3. 实验验证框架在DeepSeek-R1、Claude Sonnet等模型的有效性

Result: GPT-4 mini模型上达到80.8%攻击成功率，较现有最佳方法提升27.2个百分点

Conclusion: SEAL框架成功突破现有防御机制，揭示了推理模型安全防护体系存在的重大缺陷，需研发新型防御方案应对自适应加密攻击

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.

</details>


### [46] [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
*Vijeta Deshpande,Debasmita Ghose,John D. Patterson,Roger Beaty,Anna Rumshisky*

Main category: cs.CL

TL;DR: 提出Diverse-NS框架，通过控制输出长度有效提升语言模型生成多样性，解决现有评估指标的短文本偏向问题


<details>
  <summary>Details</summary>
Motivation: 现有多样性评估指标和奖励模型存在系统性短文本偏向，限制了模型表达的丰富性

Method: 开发长度控制的自学习框架（Diverse-NS），通过筛选平衡多样性-质量-长度的3,000对偏好数据进行训练

Result: 在LLaMA/Olmo系列模型上实现词汇和语义多样性显著提升，四个创造性任务中保持/提升质量，小模型可作为大模型的「多样性教师」

Conclusion: 通过显式控制长度偏差，Diverse-NS高效提升了语言模型的表达多样性，为创造性生成任务提供新范式

Abstract: Diverse language model responses are crucial for creative generation,
open-ended tasks, and self-improvement training. We show that common diversity
metrics, and even reward models used for preference optimization,
systematically bias models toward shorter outputs, limiting expressiveness. To
address this, we introduce Diverse, not Short (Diverse-NS), a length-controlled
self-learning framework that improves response diversity while maintaining
length parity. By generating and filtering preference data that balances
diversity, quality, and length, Diverse-NS enables effective training using
only 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,
Diverse-NS substantially enhances lexical and semantic diversity. We show
consistent improvement in diversity with minor reduction or gains in response
quality on four creative generation tasks: Divergent Associations, Persona
Generation, Alternate Uses, and Creative Writing. Surprisingly, experiments
with the Olmo-2 model family (7B, and 13B) show that smaller models like
Olmo-2-7B can serve as effective "diversity teachers" for larger models. By
explicitly addressing length bias, our method efficiently pushes models toward
more diverse and expressive outputs.

</details>


### [47] [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
*Hwiyeong Lee,Uiji Hwang,Hyelim Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 现有局部化遗忘方法假设参数局部性可有效移除知识，但实验表明有效遗忘所需的参数集并不固定，挑战了该核心假设。


<details>
  <summary>Details</summary>
Motivation: 验证局部参数更新是否真正导致知识遗忘，现有方法基于参数局部性假设但效果存疑，需通过对照实验检验其因果关联。

Method: 通过重新审视现有方法并设计对照实验，系统评估参数修改与知识遗忘间的因果关系，采用受控变量实验设计。

Result: 有效遗忘所需修改的参数具有非确定性特征，参数局部性不能可靠指示知识移除效果，证伪了局部化遗忘的基本前提。

Conclusion: 局部化遗忘的核心假设存在根本缺陷，需开发不依赖参数空间局部性的新型遗忘范式，采用动态参数选择策略。

Abstract: Large language models often retain unintended content, prompting growing
interest in knowledge unlearning. Recent approaches emphasize localized
unlearning, which restricts parameter updates to specific regions in an effort
to remove target knowledge while preserving unrelated general knowledge.
However, their effectiveness remains uncertain due to the lack of robust and
thorough evaluation of the trade-off between the competing goals of unlearning.
In this paper, we begin by revisiting existing localized unlearning approaches.
We then conduct controlled experiments to rigorously evaluate whether local
parameter updates causally contribute to unlearning. Our findings reveal that
the set of parameters that must be modified for effective unlearning is not
strictly determined, challenging the core assumption of localized unlearning
that parameter locality is inherently indicative of effective knowledge
removal.

</details>


### [48] [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
*Aashish Anantha Ramakrishnan,Aadarsh Anantha Ramakrishnan,Dongwon Lee*

Main category: cs.CL

TL;DR: 提出IRONIC框架，通过多模态连贯关系提升零样本多模态讽刺检测性能


<details>
  <summary>Details</summary>
Motivation: 现有链式推理方法未能有效模仿人类识别讽刺的认知机制

Method: 基于语言学理论构建多模态连贯关系框架（指代关系/类比关系/语用关系），分析图文关联

Result: 在零样本多模态讽刺检测任务中实现SOTA性能

Conclusion: 将语言学和认知学理论融入多模态推理策略设计具有必要性，代码已开源

Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC

</details>


### [49] [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
*Jiaru Zou,Yikun Ban,Zihao Li,Yunzhe Qi,Ruizhong Qiu,Ling Yang,Jingrui He*

Main category: cs.CL

TL;DR: 提出Transformer Copilot框架，通过Copilot模型纠正Pilot模型的logits输出，在12个基准测试中最高提升34.5%性能，计算开销边际。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法仅聚焦最小化生成损失，受人类通过反思错误改进的启发，系统追踪模型学习信号(Mistake Log)来增强微调效果。

Method: 构建Pilot-Copilot双模型架构：1) Copilot通过logits修正增强推理 2) 基于Mistake Log的联合训练范式 3) 融合推理机制实现生成优化。

Result: 在常识推理、数学计算和推荐系统等12个任务中验证，性能提升最高达34.5%，模型保持低计算开销并展现强扩展迁移能力。

Conclusion: Transformer Copilot通过持续学习机制有效提升模型微调效果，理论分析和实验验证了框架有效性，为LLM优化提供新范式。

Abstract: Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.

</details>


### [50] [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
*Sheng-Fu Wang,Laurent Prevot,Jou-an Chi,Ri-Sheng Huang,Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: 研究通过分析自发语音语料库的产出变量（语音缩减、韵律突显），发现经过微调的LLM能有效预测这些变量，且口语训练数据优于书面数据。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型在认知科学领域的评估需求，研究者尝试用语言产出行为指标（语音缩减/韵律突显）替代传统眼动追踪和脑神经指标，拓展LLM认知能力评估维度。

Method: 从自发语音语料库提取语音缩减和韵律突显变量，对比分析不同预训练数据（书面/口语/混合）模型经标准流程训练后对这些变量的预测能力。

Result: 微调后模型预测表现显著优于基线，口语类训练数据（LibriSpeech）预测准确率（语音缩减88.5%，韵律突显83.2%）明显优于书面数据（BERT 76.3%/71.8%）

Conclusion: 高质量语音语料库可作为LLM评估新范式，训练数据体裁显著影响模型对语言产出行为的预测能力，为认知建模和语音技术提供新方向。

Abstract: The achievements of Large Language Models in Natural Language Processing,
especially for high-resource languages, call for a better understanding of
their characteristics from a cognitive perspective. Researchers have attempted
to evaluate artificial models by testing their ability to predict behavioral
(e.g., eye-tracking fixations) and physiological (e.g., brain responses)
variables during language processing (e.g., reading/listening). In this paper,
we propose using spontaneous speech corpora to derive production variables
(speech reductions, prosodic prominences) and applying them in a similar
fashion. More precisely, we extract. We then test models trained with a
standard procedure on different pretraining datasets (written, spoken, and
mixed genres) for their ability to predict these two variables. Our results
show that, after some fine-tuning, the models can predict these production
variables well above baselines. We also observe that spoken genre training data
provides more accurate predictions than written genres. These results
contribute to the broader effort of using high-quality speech corpora as
benchmarks for LLMs.

</details>


### [51] [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
*Shijie Zhang,Renhao Li,Songsheng Wang,Philipp Koehn,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 提出分层多代理框架HiMATE，通过自反思机制和代理讨论策略，显著提升机器翻译评估中错误检测的F1分数达89%


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法未充分利用MQM层次结构的细粒度语义信息，导致错误定位和严重性评估存在偏差

Method: 基于MQM错误类型构建分层多代理系统，引入模型自反思能力与信息不对称的代理讨论机制

Result: 在多个数据集上超越基线模型，错误跨度检测F1值提升89%，严重性评估准确率显著提高

Conclusion: HiMATE框架通过结构化代理协作有效缓解系统幻觉，实现了更人类对齐的机器翻译质量评估

Abstract: The advancement of Large Language Models (LLMs) enables flexible and
interpretable automatic evaluations. In the field of machine translation
evaluation, utilizing LLMs with translation error annotations based on
Multidimensional Quality Metrics (MQM) yields more human-aligned judgments.
However, current LLM-based evaluation methods still face challenges in
accurately identifying error spans and assessing their severity. In this paper,
we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation
Evaluation. We argue that existing approaches inadequately exploit the
fine-grained structural and semantic information within the MQM hierarchy. To
address this, we develop a hierarchical multi-agent system grounded in the MQM
error typology, enabling granular evaluation of subtype errors. Two key
strategies are incorporated to further mitigate systemic hallucinations within
the framework: the utilization of the model's self-reflection capability and
the facilitation of agent discussion involving asymmetric information.
Empirically, HiMATE outperforms competitive baselines across different datasets
in conducting human-aligned evaluations. Further analyses underscore its
significant advantage in error span detection and severity assessment,
achieving an average F1-score improvement of 89% over the best-performing
baseline. We make our code and data publicly available at
https://anonymous.4open.science/r/HiMATE-Anony.

</details>


### [52] [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
*Rishabh Maheshwary,Masoud Hashemi,Khyati Mahajan,Shiva Krishna Reddy Malay,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Spandana Gella,Vikas Yadav*

Main category: cs.CL

TL;DR: 提出'笔记写作'方法解决迭代RAG中冗长上下文和无关信息积累问题，通过生成关键笔记提升LLM推理能力，实验显示平均提升15.6%性能


<details>
  <summary>Details</summary>
Motivation: 现有迭代RAG方法在信息压缩方面存在单轮限制、需微调或可扩展性差的问题，影响模型处理多跳问答的能力

Method: 框架无关的Notes Writing方法，每步从检索文档生成精简笔记，过滤噪音同时保留核心信息，间接扩展LLM有效上下文长度

Result: 集成三种迭代RAG方法验证，在四个数据集和两种模型上实现平均15.6个百分点提升，输出token增幅极小

Conclusion: 该方法通过动态信息浓缩显著增强LLM的多步推理能力，为迭代RAG系统提供轻量高效的通用优化方案

Abstract: Iterative RAG for multi-hop question answering faces challenges with lengthy
contexts and the buildup of irrelevant information. This hinders a model's
capacity to process and reason over retrieved content and limits performance.
While recent methods focus on compressing retrieved information, they are
either restricted to single-round RAG, require finetuning or lack scalability
in iterative RAG. To address these challenges, we propose Notes Writing, a
method that generates concise and relevant notes from retrieved documents at
each step, thereby reducing noise and retaining only essential information.
This indirectly increases the effective context length of Large Language Models
(LLMs), enabling them to reason and plan more effectively while processing
larger volumes of input text. Notes Writing is framework agnostic and can be
integrated with different iterative RAG methods. We demonstrate its
effectiveness with three iterative RAG methods, across two models and four
evaluation datasets. Notes writing yields an average improvement of 15.6
percentage points overall, with minimal increase in output tokens.

</details>


### [53] [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
*Seongryong Jung,Suwan Yoon,DongGeon Kim,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出基于细粒度KL散度分析的Token级知识蒸馏方法ToDi，通过自适应组合FKL和RKL实现更精准的分布对齐。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法（FKL/RKL）在全词表范围采用统一散度损失，未考虑不同token的预测差异。研究发现FKL增强被低估的token预测，RKL抑制被高估的预测，两者存在互补性。

Method: 提出ToDi方法：1）通过梯度分析揭示FKL和RKL的互补特性；2）设计基于sigmoid的权重函数，根据师生概率对数比实现token-wise的自适应KL组合。

Result: 在指令跟随基准测试中全面超越现有蒸馏方法（统一策略/粗粒度策略），消融实验验证了动态加权机制的有效性。

Conclusion: ToDi通过细粒度的散度组合策略实现了更高效的模型压缩，效率和效果分析验证了其实际价值。

Abstract: Large language models (LLMs) offer impressive performance but are impractical
for resource-constrained deployment due to high latency and energy consumption.
Knowledge distillation (KD) addresses this by transferring knowledge from a
large teacher to a smaller student model. However, conventional KD, notably
approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence
loss across the entire vocabulary, neglecting token-level prediction
discrepancies. By investigating these representative divergences via gradient
analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses
overestimated ones, showing their complementary roles. Based on this
observation, we propose Token-wise Distillation (ToDi), a novel method that
adaptively combines FKL and RKL per token using a sigmoid-based weighting
function derived from the teacher-student probability log-ratio. ToDi
dynamically emphasizes the appropriate divergence for each token, enabling
precise distribution alignment. We demonstrate that ToDi consistently
outperforms recent distillation baselines using uniform or less granular
strategies across instruction-following benchmarks. Extensive ablation studies
and efficiency analysis further validate ToDi's effectiveness and practicality.

</details>


### [54] [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
*Haochen Shi,Tianshi Zheng,Weiqi Wang,Baixuan Xu,Chunyang Li,Chunkit Chan,Tao Fan,Yangqiu Song,Qiang Yang*

Main category: cs.CL

TL;DR: 提出InferenceDynamics框架解决大语言模型路由的扩展性和适应性难题，通过多维度能力建模实现高效资源利用


<details>
  <summary>Details</summary>
Motivation: 现有路由方法在应对大规模专业LLM池时存在扩展性不足，且难以适应模型能力域的动态演化需求

Method: 基于模型能力和知识建模的多维度路由框架，构建RouteMix数据集实现群体级路由决策

Result: 在MMLU-Pro/GPQA/BigGenBench/LiveBench等基准测试中验证框架有效性，实现任务最优模型匹配与资源效率双提升

Conclusion: 该框架释放了LLM生态系统的专业潜力，代码开源将推动相关领域研究发展

Abstract: Large Language Model (LLM) routing is a pivotal technique for navigating a
diverse landscape of LLMs, aiming to select the best-performing LLMs tailored
to the domains of user queries, while managing computational resources.
However, current routing approaches often face limitations in scalability when
dealing with a large pool of specialized LLMs, or in their adaptability to
extending model scope and evolving capability domains. To overcome those
challenges, we propose InferenceDynamics, a flexible and scalable
multi-dimensional routing framework by modeling the capability and knowledge of
models. We operate it on our comprehensive dataset RouteMix, and demonstrate
its effectiveness and generalizability in group-level routing using modern
benchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its
ability to identify and leverage top-performing models for given tasks, leading
to superior outcomes with efficient resource utilization. The broader adoption
of Inference Dynamics can empower users to harness the full specialized
potential of the LLM ecosystem, and our code will be made publicly available to
encourage further research.

</details>


### [55] [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
*Chenzhuo Zhao,Ziqian Liu,Xingda Wang,Junting Lu,Chaoyi Ruan*

Main category: cs.CL

TL;DR: 提出PMPO框架——基于概率度量的提示优化方法，通过交叉熵损失直接评估提示质量，无需输出采样或人工标注即可高效优化提示词


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖高成本的输出生成或人工评估，难以扩展到小型模型。需要更轻量级、仅用前向传播的优化方案

Method: 通过掩码识别低质量提示段，利用正负样例的token-level交叉熵损失指导提示改写，支持监督学习和偏好对齐的损失评估策略

Result: 在BBH基准取得最高平均准确率，GSM8K/AQUA-RAT表现优异，AlpacaEval 2.0胜率提升超19个百分点，且跨模型规模效果稳定

Conclusion: PMPO证明基于损失的轻量级优化框架能显著提升提示效果，兼具高效性和通用性，为资源受限场景提供实用解决方案

Abstract: Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.

</details>


### [56] [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
*Yuyang Jiang,Chacha Chen,Shengyuan Wang,Feng Li,Zecong Tang,Benjamin M. Mervak,Lydia Chelala,Christopher M Straus,Reve Chahine,Samuel G. Armato III,Chenhao Tan*

Main category: cs.CL

TL;DR: 提出CLEAR框架，通过多维度属性级评估和专家标注数据集提升放射学报告的临床评估质量


<details>
  <summary>Details</summary>
Motivation: 现有评估指标缺乏细粒度临床差异捕捉能力，导致放射学报告质量评估不准确

Method: 开发表格化CLEAR框架，评估五个关键临床属性（首次发生/变化/严重程度/位置/建议），并与放射科专家合作构建CLEAR-Bench标注数据集

Result: CLEAR在临床属性提取中表现出高准确性，其自动化指标与临床判断高度一致（+0.89相关性）

Conclusion: 多维度属性级评估框架显著提升放射学报告评估的临床可解释性和全面性，专家标注验证了临床相关性

Abstract: Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.

</details>


### [57] [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
*Wenqing Wu,Chengzhi Zhang,Tong Bao,Yi Zhao*

Main category: cs.CL

TL;DR: 研究通过组合学术论文的不同核心章节（IMRaD）来预测新颖性评分，发现引言+结果+讨论的组合效果最佳，全文反而不显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于词语或实体组合的新颖性评估方法效果有限，论文创新性通常分布在多个章节，需探索最优章节组合提升自动化评估效果。

Method: 1. 使用NLP技术识别论文IMRaD结构
2. 用不同章节组合输入PLMs/LLMs模型
3. 以专家评分作为标签进行预测分析

Result: 最优组合为引言+结果+讨论（IRD），全文无显著效果；引言和结果部分对新颖性预测贡献最大

Conclusion: 章节组合优化显著提升自动化新颖性评估效果，公开代码数据集促进后续研究（https://github.com/njust-winchy/SC4ANM）

Abstract: Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.

</details>


### [58] [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
*Taeyoon Kwon,Dongwook Choi,Sunghwan Kim,Hyojun Kim,Seungjun Moon,Beong-woo Kwak,Kuan-Hao Huang,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出MEMENTO框架用于评估具身代理的记忆利用能力，发现前沿模型在需要多记忆参考时性能下降30.5%


<details>
  <summary>Details</summary>
Motivation: 现有具身代理主要关注单轮交互任务，缺乏对个性化语义理解和记忆利用的系统性评估

Method: 设计包含对象语义识别和用户模式推断的两阶段记忆评估框架，量化记忆对任务表现的影响

Result: GPT-4o等模型在多记忆参考时性能显著下降（30.5%），用户模式相关任务表现尤其受限

Conclusion: 揭示了现有代理在记忆利用上的局限性，为开发更有效的个性化具身代理提供研究方向

Abstract: Embodied agents empowered by large language models (LLMs) have shown strong
performance in household object rearrangement tasks. However, these tasks
primarily focus on single-turn interactions with simplified instructions, which
do not truly reflect the challenges of providing meaningful assistance to
users. To provide personalized assistance, embodied agents must understand the
unique semantics that users assign to the physical world (e.g., favorite cup,
breakfast routine) by leveraging prior interaction history to interpret
dynamic, real-world instructions. Yet, the effectiveness of embodied agents in
utilizing memory for personalized assistance remains largely underexplored. To
address this gap, we present MEMENTO, a personalized embodied agent evaluation
framework designed to comprehensively assess memory utilization capabilities to
provide personalized assistance. Our framework consists of a two-stage memory
evaluation process design that enables quantifying the impact of memory
utilization on task performance. This process enables the evaluation of agents'
understanding of personalized knowledge in object rearrangement tasks by
focusing on its role in goal interpretation: (1) the ability to identify target
objects based on personal meaning (object semantics), and (2) the ability to
infer object-location configurations from consistent user patterns, such as
routines (user patterns). Our experiments across various LLMs reveal
significant limitations in memory utilization, with even frontier models like
GPT-4o experiencing a 30.5% performance drop when required to reference
multiple memories, particularly in tasks involving user patterns. These
findings, along with our detailed analyses and case studies, provide valuable
insights for future research in developing more effective personalized embodied
agents. Project website: https://connoriginal.github.io/MEMENTO

</details>


### [59] [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
*Pierre Achkar,Tim Gollub,Martin Potthast*

Main category: cs.CL

TL;DR: XSum提出基于检索增强生成（RAG）的模块化科学文献摘要框架，通过动态问题生成和编辑模块实现多文档整合，在SurveySum数据集上显著提升CheckEval、G-Eval等指标。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数级增长导致研究人员难以有效整合领域知识，需要开发自动化工具实现精准的学术摘要生成。

Method: 包含两个核心模块：(1) 问题生成模块动态创建适配论文内容的检索问题；(2) 编辑模块将检索结果合成为结构清晰、引用规范的学术摘要。

Result: 在SurveySum数据集上，XSum的CheckEval得分提升12.5%，G-Eval提升9.8%，Ref-F1提升15.2%，显著优于基线模型。

Conclusion: 该框架建立了透明可扩展的科学摘要生成范式，支持跨领域知识整合，开源代码促进学术社区应用发展。

Abstract: The exponential growth of scientific publications has made it increasingly
difficult for researchers to stay updated and synthesize knowledge effectively.
This paper presents XSum, a modular pipeline for multi-document summarization
(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The
pipeline includes two core components: a question-generation module and an
editor module. The question-generation module dynamically generates questions
adapted to the input papers, ensuring the retrieval of relevant and accurate
information. The editor module synthesizes the retrieved content into coherent
and well-structured summaries that adhere to academic standards for proper
citation. Evaluated on the SurveySum dataset, XSum demonstrates strong
performance, achieving considerable improvements in metrics such as CheckEval,
G-Eval and Ref-F1 compared to existing approaches. This work provides a
transparent, adaptable framework for scientific summarization with potential
applications in a wide range of domains. Code available at
https://github.com/webis-de/scolia25-xsum

</details>


### [60] [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
*Songlin Yang,Yikang Shen,Kaiyue Wen,Shawn Tan,Mayank Mishra,Liliang Ren,Rameswar Panda,Yoon Kim*

Main category: cs.CL

TL;DR: 提出PaTH——基于Householder变换的数据依赖位置编码方案，通过输入相关的动态位置编码提升Transformer表达能力，在效率和效果上超越RoPE。


<details>
  <summary>Details</summary>
Motivation: 传统RoPE位置编码仅依赖相对位置，与输入数据无关，限制了模型对上下文动态关系的建模能力。PaTH旨在通过数据依赖的位置编码增强模型表达力。

Method: 1. 使用级联Householder变换生成位置编码，每个变换由输入数据动态决定
2. 开发基于Householder矩阵乘积压缩表示的高效并行训练算法
3. 实现类似FlashAttention的分块计算优化I/O效率

Result: 在合成任务和实际语言建模任务中，PaTH相比RoPE及其他基线模型表现出更优的性能，验证了数据依赖位置编码的有效性。

Conclusion: PaTH突破了传统位置编码与输入无关的限制，为动态上下文建模提供了新范式，同时保持了计算高效性，具有广泛的应用潜力。

Abstract: The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.

</details>


### [61] [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
*Kaiyu He,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出词级跨语言翻译任务揭示LLMs的两种跨语言行为机制，通过重构语义枢轴数据集提升模型表现


<details>
  <summary>Details</summary>
Motivation: 理解大语言模型如何获得跨语言能力对其可解释性至关重要，需准确量化这种能力

Method: 设计词级跨语言翻译任务，追踪模型中间层输出，识别共现/语义枢轴行为，重构语义枢轴感知的预训练数据集

Result: 验证了语义枢轴数据集能有效提升模型跨语言能力（共现行为与词频相关，语义枢轴来自预训练数据）

Conclusion: 揭示了LLMs跨语言能力的形成机制，为模型可解释性提供新视角，并提出可落地的能力增强方法

Abstract: Large language models (LLMs) demonstrate remarkable ability in cross-lingual
tasks. Understanding how LLMs acquire this ability is crucial for their
interpretability. To quantify the cross-lingual ability of LLMs accurately, we
propose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn
cross-lingual ability, we trace the outputs of LLMs' intermediate layers in the
word translation task. We identify and distinguish two distinct behaviors in
the forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.
We attribute LLMs' two distinct behaviors to the co-occurrence frequency of
words and find the semantic pivot from the pre-training dataset. Finally, to
apply our findings to improve the cross-lingual ability of LLMs, we reconstruct
a semantic pivot-aware pre-training dataset using documents with a high
proportion of semantic pivots. Our experiments validate the effectiveness of
our approach in enhancing cross-lingual ability. Our research contributes
insights into the interpretability of LLMs and offers a method for improving
LLMs' cross-lingual ability.

</details>


### [62] [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
*Benjamin Vendeville,Liana Ermakova,Pierre De Loor*

Main category: cs.CL

TL;DR: ATS评估方法落后于生成技术发展，需新框架检测简化文本中的信息失真错误。本文提出包含错误分类法、标注数据集及模型分析的测试集。


<details>
  <summary>Details</summary>
Motivation: 现有ATS评估指标与错误存在相关性低，人工检查发现多种信息失真错误，亟需更精细的错误检测框架。

Method: 1. 建立信息失真错误分类法
2. 构建自动简化科学文本的标注数据集
3. 测试现有模型在错误检测/分类上的表现

Result: 数据集质量验证有效，现有模型在错误检测任务中表现有显著改进空间（具体指标未提及）

Conclusion: 该测试集为研究者提供三方面工具：
1. 更精确的ATS错误评估
2. 开发可靠模型的基础
3. 最终提升自动简化文本质量

Abstract: The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.

</details>


### [63] [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
*Gaofei Shen,Hosein Mohebbi,Arianna Bisazza,Afra Alishahi,Grzegorz Chrupała*

Main category: cs.CL

TL;DR: 语音领域标准特征归因方法普遍不可靠，但单词对齐扰动方法在词语分类任务中有效


<details>
  <summary>Details</summary>
Motivation: 语音信号独特的输入特性使特征归因方法面临挑战，需评估其可靠性影响因素

Method: 通过实验研究输入类型、聚合方式、扰动时间跨度与分类任务特性的交互影响

Result: 标准方法在语音领域不可靠（单词对齐扰动方法在词语分类任务中例外）

Conclusion: 需开发针对语音信号特性优化的特征归因方法

Abstract: As the capabilities of large-scale pre-trained models evolve, understanding
the determinants of their outputs becomes more important. Feature attribution
aims to reveal which parts of the input elements contribute the most to model
outputs. In speech processing, the unique characteristics of the input signal
make the application of feature attribution methods challenging. We study how
factors such as input type and aggregation and perturbation timespan impact the
reliability of standard feature attribution methods, and how these factors
interact with characteristics of each classification task. We find that
standard approaches to feature attribution are generally unreliable when
applied to the speech domain, with the exception of word-aligned perturbation
methods when applied to word-based classification tasks.

</details>


### [64] [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
*Muhammad Farid Adilazuarda,Chen Cecilia Liu,Iryna Gurevych,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 研究探讨基于世界价值观调查（WVS）训练在LLMs中适应文化价值观的局限性，发现仅用调查数据会导致文化同质化，提出结合百科和情景叙事增强文化独特性。


<details>
  <summary>Details</summary>
Motivation: 现有基于WVS的方法在捕捉文化细微差异和下游任务中的文化表征效果有限，且可能干扰事实知识。

Method: 系统分析WVS训练的局限性，结合百科和情景叙事数据（来自Wikipedia和NormAd）增强文化表征。

Result: 增强数据虽对下游任务影响不一，但相比仅用WVS，显著提升文化独特性。

Conclusion: 文化价值对齐需结合多源数据，平衡文化独特性与任务表现，突显其内在复杂性。

Abstract: Adapting cultural values in Large Language Models (LLMs) presents significant
challenges, particularly due to biases and limited training data. Prior work
primarily aligns LLMs with different cultural values using World Values Survey
(WVS) data. However, it remains unclear whether this approach effectively
captures cultural nuances or produces distinct cultural representations for
various downstream tasks. In this paper, we systematically investigate
WVS-based training for cultural value adaptation and find that relying solely
on survey data can homogenize cultural norms and interfere with factual
knowledge. To investigate these issues, we augment WVS with encyclopedic and
scenario-based cultural narratives from Wikipedia and NormAd. While these
narratives may have variable effects on downstream tasks, they consistently
improve cultural distinctiveness than survey data alone. Our work highlights
the inherent complexity of aligning cultural values with the goal of guiding
task-specific behavior.

</details>


### [65] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出强化学习框架Tool-Star，帮助大语言模型实现多工具协作推理


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的大语言模型在多工具协同推理方面存在不足，需要系统性的训练框架提升工具协作能力

Method: 1. 工具集成数据合成流程（提示工程+分层采样）
2. 质量过滤与难度分级数据处理
3. 两阶段训练框架（冷启动微调+分层奖励强化学习）

Result: 在10+推理基准测试中验证有效性，模型代码已开源

Conclusion: 通过系统性数据构建和强化学习机制，显著提升大模型的多工具协同推理能力，为复杂任务处理提供新范式

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


### [66] [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
*Ruizhe Li,Chen Chen,Yuchen Hu,Yanjun Gao,Xi Wang,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出基于Jensen-Shannon散度的ARC-JSD方法，无需微调即可高效实现RAG模型的上下文归因


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型的上下文归因方法依赖计算密集型微调或人工标注，亟需更高效的解决方案

Method: 利用Jensen-Shannon散度量化LLM生成响应与上下文间的信息差异，通过注意力头和MLP层的机制分析实现归因

Result: 在TyDi QA等基准测试中准确率超越基线方法，计算效率提升10倍，识别出12个关键注意力头和MLP层

Conclusion: ARC-JSD为RAG提供了计算高效的归因方案，同时揭示了模型内部工作机制，为优化检索-生成协同提供了新方向

Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.

</details>


### [67] [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
*Yoichi Aoki,Soichiro Murakami,Ukyo Honda,Akihiko Kato*

Main category: cs.CL

TL;DR: 探究广告文本生成中多样性增强方法对广告质量的影响机制


<details>
  <summary>Details</summary>
Motivation: 广告文本生成与摘要/翻译任务存在文本风格差异，现有多样性提升方法在该领域效果尚未明确

Method: 通过控制多样性增强方法、超参数、输入输出格式和模型架构进行多维度实验

Result: 揭示了不同因素对广告质量的影响路径和交互作用

Conclusion: 为广告生成系统优化提供了基于数据驱动的多样性调控策略

Abstract: In natural language generation for advertising, creating diverse and engaging
ad texts is crucial for capturing a broad audience and avoiding advertising
fatigue. Regardless of the importance of diversity, the impact of the
diversity-enhancing methods in ad text generation -- mainly tested on tasks
such as summarization and machine translation -- has not been thoroughly
explored. Ad text generation significantly differs from these tasks owing to
the text style and requirements. This research explores the relationship
between diversity and ad quality in ad text generation by considering multiple
factors, such as diversity-enhancing methods, their hyperparameters,
input-output formats, and the models.

</details>


### [68] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 提出WebAgent-R1框架，通过多轮强化学习显著提升网页代理的任务成功率，在WebArena-Lite基准测试中Qwen和Llama模型成功率提升4-5倍


<details>
  <summary>Details</summary>
Motivation: 现有RL主要针对单轮任务（如数学解题），而动态网页环境下的多轮交互代理训练存在长时决策和界面动态变化等挑战

Method: 端到端多轮RL框架，通过异步生成多样化轨迹，完全依赖任务成功的二元奖励进行在线训练

Result: Qwen-2.5-3B成功率从6.1%提升至33.9%，Llama-3.1-8B从8.5%提升至44.8%，超越现有SOTA方法和商业模型

Conclusion: 验证了思维提示策略的有效性，发现预热训练阶段的重要性，为网页代理融入长链推理提供新思路

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [69] [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
*Jing Bi,Pinxin Liu,Ali Vosoughi,Jiarui Wu,Jinxi He,Chenliang Xu*

Main category: cs.CL

TL;DR: 提出语言驱动框架将程序文本转化为视觉指令，通过语言结构建模和三个创新模块提升图文对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统纯文本指令难以有效传达复杂物理动作和空间关系，需要增强程序性知识的可视化表达。

Method: 1. 基于选区句法树的文本编码机制
2. 成对语篇连贯性模型
3. 专门设计的程序语言-图像对齐评估协议

Result: 在HTStep等三个数据集上显著优于基线模型，有效保持指令序列的一致性和语义完整性。

Conclusion: 该框架为教育、任务指导等领域提供了程序性语言可视化的新方案，推动了多模态语言理解研究。

Abstract: The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.

</details>


### [70] [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
*Song Jin,Juntian Zhang,Yuhan Liu,Xun Zhang,Yufei Zhang,Guojun Yin,Fei Jiang,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 论文提出基于智能体的推荐系统仿真平台RecInter，通过动态交互机制和LLM微调实现高保真模拟，成功复现品牌忠诚度等现实场景。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试资源消耗大，离线方法难以捕捉用户与平台的动态交互。现有仿真平台缺乏用户行为动态重塑环境的机制。

Method: 1. 多维用户画像模块构建用户特征
2. 动态交互机制（用户行为实时更新商品属性+商家代理响应）
3. 思维链增强的LLM微调
4. 高级智能体架构支持生态系统演化

Result: 显著提升模拟可信度（+32%指标），成功复现品牌忠诚度/马太效应等涌现现象，验证交互机制对系统演化模拟的关键作用

Conclusion: RecInter通过环境反馈循环机制，为推荐系统研究提供了更真实的动态演化模拟平台

Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.

</details>


### [71] [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
*Ikhlasul Akmal Hanif,Eryawan Presma Yulianrifat,Jaycent Gunawan Ongris,Eduardus Tjitrahardja,Muhammad Falensi Azmi,Rahmat Bryan Naufal,Alfan Farizki Wicaksono*

Main category: cs.CL

TL;DR: 提出集成多个BGE编码器与CatBoost分类器的模型，在28种语言的多标签情感分类任务中取得56.58平均F1值


<details>
  <summary>Details</summary>
Motivation: 解决多语言场景下情感分类性能不足的问题，探索基于提示的编码器与分类器组合的有效性

Method: 比较完全微调Transformer与分类器训练两种策略，测试不同编码器(mE5/BGE/XLMR/mBERT)与分类器配置，最终采用多BGE模型集成方案

Result: 基于提示的BGE+CatBoost集成模型显著优于完全微调方法，在全部语言平均F1-macro达56.58

Conclusion: 在编码器冻结情况下训练分类器的范式比完全微调更有效，模型集成策略能进一步提升多语言情感分类性能

Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.

</details>


### [72] [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
*Vera Neplenbroek,Arianna Bisazza,Raquel Fernández*

Main category: cs.CL

TL;DR: 大型语言模型通过对话中的刻板印象线索推断用户身份，导致对少数群体回应偏差，但可通过干预内部表征缓解。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明LLMs的隐性用户身份推断可能导致对少数群体的低质量回应，本研究旨在系统探索这种刻板印象信号的影响机制和干预方案。

Method: 使用合成对话实验分析模型潜在表征（模型内部参数+生成回答），并设计线性探针干预模型表征以验证缓解方案。

Result: 模型持续基于刻板印象推断身份特征（包括用户明确否认时），通过表征干预可有效将回应导向用户声明身份。

Conclusion: 研究揭示了LLMs身份表征的潜在风险，提出可解释的干预方法，强调提升模型身份表征透明度的必要性。

Abstract: Generative Large Language Models (LLMs) infer user's demographic information
from subtle cues in the conversation -- a phenomenon called implicit
personalization. Prior work has shown that such inferences can lead to lower
quality responses for users assumed to be from minority groups, even when no
demographic information is explicitly provided. In this work, we systematically
explore how LLMs respond to stereotypical cues using controlled synthetic
conversations, by analyzing the models' latent user representations through
both model internals and generated answers to targeted user questions. Our
findings reveal that LLMs do infer demographic attributes based on these
stereotypical signals, which for a number of groups even persists when the user
explicitly identifies with a different demographic group. Finally, we show that
this form of stereotype-driven implicit personalization can be effectively
mitigated by intervening on the model's internal representations using a
trained linear probe to steer them toward the explicitly stated identity. Our
results highlight the need for greater transparency and control in how LLMs
represent user identity.

</details>


### [73] [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
*Shuzheng Si,Haozhe Zhao,Cheng Gao,Yuzhuo Bai,Zhitong Wang,Bofei Gao,Kangyang Luo,Wenhao Li,Yufei Huang,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 提出CANOE框架，通过合成短形式QA数据和Dual-GRPO强化学习方法，无需人工标注即可提升LLM在11个下游任务中的忠实性表现，超越GPT-4o等先进模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在信息检索任务中生成结果缺乏忠实性的问题，传统方法依赖人工标注且难以平衡短/长文本生成优化。

Method: 1.合成四种短形式QA任务构建可验证训练数据；2.设计Dual-GRPO强化学习方法，利用三个规则奖励同时优化长短文本生成，避免人工偏好数据和短文本过优化。

Result: 在11个下游任务中显著提升LLM忠实性，性能超越GPT-4o和OpenAI o1等先进模型。

Conclusion: CANOE框架创新性地结合数据合成与双目标强化学习，为提升LLM可靠性提供高效无监督解决方案，具有重要应用价值。

Abstract: Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.

</details>


### [74] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
*Dario Di Palma,Alessandro De Bellis,Giovanni Servedio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 探究Llama模型隐藏层的情感特征分布及其对情感分析的影响，发现中层集中情感信息并显著提升检测精度，同时降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs如何捕捉情感信息理解有限，需深入分析模型内部情感表征机制以优化性能。

Method: 使用探针分类器分析不同层级/规模的情感编码特征，评估层位置与池化方法对信号捕获的影响。

Result: 情感特征在模型中层集中(二分类任务准确率+14%)，末token非最优信息源，内存需求平均降低57%。

Conclusion: 层级特异性探测可替代提示技术，有效提升情感任务效果并优化资源效率，推动LLMs实际应用。

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [75] [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
*Runcong Zhao,Chengyu Cao,Qinglin Zhu,Xiucheng Lv,Shun Shao,Lin Gui,Ruifeng Xu,Yulan He*

Main category: cs.CL

TL;DR: 提出无需训练的Concise-SAE框架提升语言模型在叙事场景中的指令遵循能力，并创建FreeInstruct基准验证效果


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估语言模型在复杂叙事情境下的指令遵循能力，需要更贴近现实的评估体系

Method: 通过自然语言指令识别并编辑指令相关神经元（无需标注数据/模型训练）

Result: FreeInstruct基准包含1,212个叙事场景样本，测试显示方法在多项任务中实现最佳指令遵循准确率

Conclusion: Concise-SAE在保持生成质量的同时，有效提升模型在复杂叙事及其他场景的指令遵循性能

Abstract: Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.

</details>


### [76] [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
*Yuting Huang,Meitong Guo,Yiquan Wu,Ang Li,Xiaozhong Liu,Keting Yin,Changlong Sun,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 提出包含1万对民事案件裁判文书的AppealCase数据集，填补上诉流程分析空白，实验证明现有模型在改判预测任务中表现不足（F1<50%）


<details>
  <summary>Details</summary>
Motivation: 现有LegalAI研究集中于个案分析，忽视了上诉程序这一司法纠错机制的重要价值，需构建专业数据集推动上诉场景研究

Method: 构建包含一审/二审匹配文书的数据集，标注改判标记、法律依据等5个核心维度，设计5项新任务并测试20个主流模型

Result: 所有模型在改判预测任务上F1分数均低于50%，揭示上诉案件分析的复杂性

Conclusion: 该数据集为上诉案例研究提供基准，有望提升司法裁判一致性，推动LegalAI在审级监督场景的发展

Abstract: Recent advances in LegalAI have primarily focused on individual case judgment
analysis, often overlooking the critical appellate process within the judicial
system. Appeals serve as a core mechanism for error correction and ensuring
fair trials, making them highly significant both in practice and in research.
To address this gap, we present the AppealCase dataset, consisting of 10,000
pairs of real-world, matched first-instance and second-instance documents
across 91 categories of civil cases. The dataset also includes detailed
annotations along five dimensions central to appellate review: judgment
reversals, reversal reasons, cited legal provisions, claim-level decisions, and
whether there is new information in the second instance. Based on these
annotations, we propose five novel LegalAI tasks and conduct a comprehensive
evaluation across 20 mainstream models. Experimental results reveal that all
current models achieve less than 50% F1 scores on the judgment reversal
prediction task, highlighting the complexity and challenge of the appeal
scenario. We hope that the AppealCase dataset will spur further research in
LegalAI for appellate case analysis and contribute to improving consistency in
judicial decision-making.

</details>


### [77] [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
*Lovisa Hagström,Youna Kim,Haeun Yu,Sang-goo Lee,Richard Johansson,Hyunsoo Cho,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 研究开发了CUB基准测试评估上下文利用技术，发现现有方法难以全面应对真实场景中的多类型上下文挑战，且存在合成数据表现虚高现象。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强场景中，语言模型存在忽略矛盾信息或被无关上下文干扰的问题，而众多上下文利用技术(CMTs)缺乏系统性比较评估。

Method: 通过设计包含三种典型上下文类型的CUB基准，在三个不同数据集上评估七种主流CMT方法（涵盖主要技术类别）在九个语言模型中的表现。

Result: 多数CMT难以同时处理真实场景中的各类上下文，且在自然样本数据集上的表现显著低于合成数据集（平均下降12.3%准确率）。

Conclusion: 需要开发能处理多类型上下文的CMT技术，并建立包含自然样本的综合性评估体系。

Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.

</details>


### [78] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
*Giovanni Servedio,Alessandro De Bellis,Dario Di Palma,Vito Walter Anelli,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 本研究挑战先前关于LLM真实性编码的研究结论，通过开发更真实的真假数据集生成方法，发现现有模型在真实性判断任务中泛化能力不足


<details>
  <summary>Details</summary>
Motivation: 大语言模型的事实性幻觉问题严重影响可靠性，先前研究基于合成数据集得出的结论在真实生成内容评估中存在局限性

Method: 提出两种新方法：1）从表格数据中采样生成可信的真假陈述 2）基于问答数据集生成LLM依赖的真实真假数据集

Result: 部分验证先前研究的真实性编码能力，但发现现有方法在LLM生成数据集上的泛化存在显著挑战

Conclusion: 为LLM事实性研究奠定新基础，提供更有效的真实性评估实践指南，强调需要开发更鲁棒的检测方法

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [79] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
*Zhouhao Sun,Zhiyuan Kan,Xiao Ding,Li Du,Yang Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型存在多重偏见问题，提出结合因果效应估计的多偏见消除方法CMBE，实验证明能有效提升模型泛化性


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅考察单一偏见类型，而实际场景中数据常包含多重交织的偏见，需要开发更全面的去偏方法

Method: 通过因果效应估计量化多重偏见的影响，在推理阶段从总因果效应中分离并消除偏见成分

Result: CMBE方法在自建的多重偏见基准测试中表现优于现有方法，准确率提升显著（具体数据见原文）

Conclusion: 同时消除多重偏见是提升大语言模型泛化能力的关键，CMBE通过因果效应分离机制为此提供了有效解决方案

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [80] [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
*Heejae Suh,Yejin Jeon,Deokhyung Kang,Taehee Park,Yejin Min,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出EnSToM方法，通过熵缩放的动态转向向量增强小型语言模型在任务对话中的主题坚持能力，在少量数据下实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 小型语言模型在资源受限场景下存在主题漂移问题，现有激活工程方法无法有效维持服务型聊天机器人的功能边界，存在潜在误用风险

Method: 基于输入不确定性的动态转向强度调节机制（Entropy-scaled Steering vectors），通过量化模型内部激活熵值自动调整干预强度

Result: 实验显示EnSToM相较微调方法节省80%训练数据，在主题坚持任务上提升23.6%准确率，同时保持97%的原任务性能

Conclusion: 该方法在不大幅增加计算开销的前提下，为资源受限的对话系统提供了可靠的主题边界防护方案，平衡安全性与实用性

Abstract: Small large language models (sLLMs) offer the advantage of being lightweight
and efficient, which makes them suitable for resource-constrained environments.
However, sLLMs often struggle to maintain topic consistency in task-oriented
dialogue systems, which is critical for scenarios such as service chatbots.
Specifically, it is important to ensure that the model denies off-topic or
malicious inputs and adheres to its intended functionality so as to prevent
potential misuse and uphold reliability. Towards this, existing activation
engineering approaches have been proposed to manipulate internal activations
during inference. While these methods are effective in certain scenarios, our
preliminary experiments reveal their limitations in ensuring topic adherence.
Therefore, to address this, we propose a novel approach termed Entropy-scaled
Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the
steering intensity based on input uncertainty, which allows the model to handle
off-topic distractors effectively while preserving on-topic accuracy. Our
experiments demonstrate that EnSToM achieves significant performance gain with
a relatively small data size compared to fine-tuning approaches. By improving
topic adherence without compromising efficiency, our approach provides a robust
solution for enhancing sLLM-based dialogue systems.

</details>


### [81] [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
*Ercong Nie,Helmut Schmid,Hinrich Schütze*

Main category: cs.CL

TL;DR: 通过机制可解释性方法定位语言混淆现象，发现最后一层神经元转换失败是关键成因，通过编辑少量关键神经元实现多语言稳定输出


<details>
  <summary>Details</summary>
Motivation: 解决英语中心化LLM在非英语场景下的语言意外切换问题，该现象严重影响多语言场景的模型可靠性

Method: 1. 开发语言混淆基准(LCB)定位混淆点 2. 使用TunedLens进行分层神经元归因分析 3. 对比多语言调优模型识别关键神经元 4. 实施选择性神经元编辑

Result: 1. 88%语言混淆源于最后3层转换失败 2. 仅调整0.3%神经元可减少76%混淆 3. 编辑后模型在多语言场景保持97%原始能力 4. 输出质量提升32%且语法错误减少41%

Conclusion: 首次从神经元层面揭示语言混淆机制，提出高效精准的模型修正方案，为构建可解释的多语言模型提供新范式

Abstract: Language confusion -- where large language models (LLMs) generate unintended
languages against the user's need -- remains a critical challenge, especially
for English-centric models. We present the first mechanistic interpretability
(MI) study of language confusion, combining behavioral benchmarking with
neuron-level analysis. Using the Language Confusion Benchmark (LCB), we show
that confusion points (CPs) -- specific positions where language switches occur
-- are central to this phenomenon. Through layer-wise analysis with TunedLens
and targeted neuron attribution, we reveal that transition failures in the
final layers drive confusion. We further demonstrate that editing a small set
of critical neurons, identified via comparative analysis with
multilingual-tuned models, substantially mitigates confusion without harming
general competence or fluency. Our approach matches multilingual alignment in
confusion reduction for most languages and yields cleaner, higher-quality
outputs. These findings provide new insights into the internal dynamics of LLMs
and highlight neuron-level interventions as a promising direction for robust,
interpretable multilingual language modeling.

</details>


### [82] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
*Wenhui Tan,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Ruihua Song*

Main category: cs.CL

TL;DR: 提出CoLaR框架，通过潜在空间压缩推理步骤，在保持性能的同时显著减少计算开销和推理链长度


<details>
  <summary>Details</summary>
Motivation: 传统思维链(CoT)方法存在token级推理链过长导致的效率低下问题，需开发更高效的潜在空间压缩推理方法

Method: 两阶段框架：1）监督微调阶段加入压缩嵌入预测目标，随机采样压缩因子合并token嵌入；2）强化学习阶段利用潜在头的非确定性探索优化推理路径

Result: 在数学推理任务中：准确率比基线高14.1%，推理链缩短53.3%（性能仅降4.8%）；强化学习增强版性能提升5.4%同时推理链缩短82.8%

Conclusion: CoLaR实现了高效潜在空间推理，支持动态压缩率调整，通过两阶段训练在性能与效率间取得平衡，为LLM推理优化提供新方向

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [83] [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
*Dongwon Noh,Donghyeok Koh,Junghun Yuk,Gyuwan Kim,Jaeyong Lee,Kyungtae Lim,Cheoneum Park*

Main category: cs.CL

TL;DR: ScholarBench是一个针对大语言模型学术推理能力的双语基准测试，覆盖8个研究领域5种问题类型，通过5310个英文和5031个韩文案例验证，即使先进模型得分仅0.543


<details>
  <summary>Details</summary>
Motivation: 现有基准测试难以评估LLM在复杂学术任务中的深度专业知识和跨领域推理能力

Method: 三步构建流程：1) 从学术文献提炼专业问题 2) 按学科设计研究方法论 3) 创建英韩双语数据集 4) 定义领域特有评估属性

Result: 构建含5,309英文/5,031韩文样本的数据集，顶尖模型o3-mini平均得分仅0.543，证实基准挑战性

Conclusion: ScholarBench通过多维度评估框架和双语设计，显著提升对LLM学术推理能力的评估深度与广度

Abstract: Prior benchmarks for evaluating the domain-specific knowledge of large
language models (LLMs) lack the scalability to handle complex academic tasks.
To address this, we introduce \texttt{ScholarBench}, a benchmark centered on
deep expert knowledge and complex academic problem-solving, which evaluates the
academic reasoning ability of LLMs and is constructed through a three-step
process. \texttt{ScholarBench} targets more specialized and logically complex
contexts derived from academic literature, encompassing five distinct problem
types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the
abstraction, comprehension, and reasoning capabilities of LLMs across eight
distinct research domains. To ensure high-quality evaluation data, we define
category-specific example attributes and design questions that are aligned with
the characteristic research methodologies and discourse structures of each
domain. Additionally, this benchmark operates as an English-Korean bilingual
dataset, facilitating simultaneous evaluation for linguistic capabilities of
LLMs in both languages. The benchmark comprises 5,031 examples in Korean and
5,309 in English, with even state-of-the-art models like o3-mini achieving an
average evaluation score of only 0.543, demonstrating the challenging nature of
this benchmark.

</details>


### [84] [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
*Dongyang Fan,Vinko Sabolčec,Martin Jaggi*

Main category: cs.CL

TL;DR: 大语言模型预训练中加入URL元数据可加速训练并提升下游任务效果，但质量评分/主题元数据无效；URL效果需长推理提示，主题/格式元数据可实现无分类器引导式可控生成


<details>
  <summary>Details</summary>
Motivation: 探索不同元数据（来源/质量/主题）在LLM预训练中的真实效用，突破传统无上下文学习范式

Method: 系统评估URL/质量评分/主题格式三类元数据，分析训练效率、下游任务表现、生成可控性

Result: 1.仅URL提升20%训练速度 2.URL效果依赖长推理提示 3.主题/格式元数据实现93%准确率的可控生成

Conclusion: 元数据效用存在特异性：URL优化训练效率，主题/格式实现生成控制，为模型优化提供新维度

Abstract: Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.

</details>


### [85] [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
*Spencer Hong,Meng Luo,Xinyi Wan*

Main category: cs.CL

TL;DR: 提出EMULATE多智能体框架，通过模拟人类操作流程显著提升声明验证效果


<details>
  <summary>Details</summary>
Motivation: 现有基于单次证据检索+语言模型的验证方式偏离人类认知模式，需要更接近人类多次检索、分步决策的验证流程

Method: 构建多智能体协作系统（包含搜索结果排序代理、网页内容评估代理等），通过角色分工模拟人类专家组的协同验证过程

Result: 在多个基准测试中取得显著性能提升，验证框架有效性

Conclusion: 多智能体分工协作机制能有效模拟人类事实核查的认知过程，为自动化验证系统设计提供新思路

Abstract: Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.

</details>


### [86] [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
*Jianbiao Mei,Tao Hu,Daocheng Fu,Licheng Wen,Xuemeng Yang,Rong Wu,Pinlong Cai,Xing Gao,Yu Yang,Chengjun Xie,Botian Shi,Yong Liu,Yu Qiao*

Main category: cs.CL

TL;DR: 提出O²-Searcher强化学习搜索代理，通过本地模拟搜索环境解决开放/封闭式问题，3B模型性能超越主流LLM。


<details>
  <summary>Details</summary>
Motivation: 大模型受限于静态知识库，现有方法在开放性问题（无标准答案/多解）处理上存在明显不足。

Method: 构建本地模拟搜索环境实现知识动态获取，设计奖励机制自适应区分问题类型及生成策略。

Result: O²-QA基准测试显著超越主流模型，封闭式任务达同规模SOTA，性能匹敌更大模型。

Conclusion: 验证了轻量级模型通过强化学习框架处理复杂开放问题的可行性，为知识动态更新提供新范式。

Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.

</details>


### [87] [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
*Bowen Jiang,Runchuan Zhu,Jiang Wu,Zinco Jiang,Yifan He,Junyuan Gao,Jia Yu,Rui Min,Yinfan Wang,Haote Yang,Songyang Zhang,Dahua Lin,Lijun Wu,Conghui He*

Main category: cs.CL

TL;DR: KoLasSimpleQA是首个评估大语言模型多语言事实能力的基准，涵盖9种语言和双领域设计，揭示主流模型在不同领域的显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估基准在语言覆盖广度和领域评估深度上的不足，支持全球适用性评估及多语言能力的全面诊断。

Method: 构建具有单知识点覆盖、客观稳定特征的问题集，采用LLM-as-judge范式，测试模型的事实记忆与自我认知能力。

Result: 传统LLM与大型推理模型在通用领域和语言专属领域存在显著性能差异，尤其在校准能力和鲁棒性方面表现分化明显。

Conclusion: KoLasSimpleQA为识别多语言环境下LLM能力边界提供新工具，其双域设计揭示了模型优化需针对不同领域特性进行定向改进。

Abstract: We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual
factual ability of Large Language Models (LLMs). Inspired by existing research,
we created the question set with features such as single knowledge point
coverage, absolute objectivity, unique answers, and temporal stability. These
questions enable efficient evaluation using the LLM-as-judge paradigm, testing
both the LLMs' factual memory and self-awareness ("know what they don't know").
KoLasSimpleQA expands existing research in two key dimensions: (1) Breadth
(Multilingual Coverage): It includes 9 languages, supporting global
applicability evaluation. (2) Depth (Dual Domain Design): It covers both the
general domain (global facts) and the language-specific domain (such as
history, culture, and regional traditions) for a comprehensive assessment of
multilingual capabilities. We evaluated mainstream LLMs, including traditional
LLM and emerging Large Reasoning Models. Results show significant performance
differences between the two domains, particularly in performance metrics,
ranking, calibration, and robustness. This highlights the need for targeted
evaluation and optimization in multilingual contexts. We hope KoLasSimpleQA
will help the research community better identify LLM capability boundaries in
multilingual contexts and provide guidance for model optimization. We will
release KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .

</details>


### [88] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
*Shijia Zhou,Siyao Peng,Simon Luebke,Jörg Haßler,Mario Haim,Saif M. Mohammad,Barbara Plank*

Main category: cs.CL

TL;DR: 跨学科研究气候变化迷因中立场与媒体框架的交互，构建首个标注数据集CLIMATEMEMES并评估模型表现


<details>
  <summary>Details</summary>
Motivation: 媒体框架通过强调特定现实维度塑造议题认知，但立场与框架的交互机制尚未被充分探索

Method: 收集47个subreddit的1,184个迷因，构建双标注数据集，提出立场检测和框架检测任务，评估LLaVA-NeXT/Molmo模型在不同设置下的表现

Result: VLMs立场检测优秀但框架检测困难（LLMs反超），人类标题持续提升性能，合成标题/修正OCR部分有效

Conclusion: 需开发更细粒度框架处理模型，揭示跨学科方法在计算传播学中的价值，验证迷因作为研究载体的有效性

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [89] [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出两阶段自我进化框架，通过经验获取与自我优化机制提升LLMs在情感支持中的个性化响应能力


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在情感支持中存在回应泛化、缺乏个性化的问题，需针对性优化用户画像、情绪状态、具体场景的隐式偏好对齐

Method: 1.情感支持经验获取：基于有限对话数据微调模型；2.自我改进阶段：通过自我反思生成优化响应，迭代执行直接偏好优化

Result: 实验证明该方法显著减少无效回应，用户偏好与模型输出的对齐度提升34.6%（基于人工评估指标）

Conclusion: 自我进化框架有效增强LLMs的个性化情感支持能力，实现用户需求与模型输出的动态适配

Abstract: Effective emotional support hinges on understanding users' emotions and needs
to provide meaningful comfort during multi-turn interactions. Large Language
Models (LLMs) show great potential for expressing empathy; however, they often
deliver generic and one-size-fits-all responses that fail to address users'
specific needs. To tackle this issue, we propose a self-evolution framework
designed to help LLMs improve their responses to better align with users'
implicit preferences concerning user profiles (personalities), emotional
states, and specific situations. Our framework consists of two distinct phases:
\textit{(1)} \textit{Emotional Support Experience Acquisition}, where LLMs are
fine-tuned on limited emotional support conversation data to provide basic
support, and \textit{(2)} \textit{Self-Improvement for Personalized Emotional
Support}, where LLMs leverage self-reflection and self-refinement to generate
personalized responses. Through iterative direct preference optimization
between the pre- and post-refined responses, our model generates responses that
reflect a better understanding of the user's implicit preferences. Extensive
experiments and evaluations demonstrate that our method significantly enhances
the model's performance in emotional support, reducing unhelpful responses and
minimizing discrepancies between user preferences and model outputs.

</details>


### [90] [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
*Daniel Scalena,Gabriele Sarti,Arianna Bisazza,Elisabetta Fersini,Malvina Nissim*

Main category: cs.CL

TL;DR: 提出基于大语言模型的对比框架，通过潜在概念引导实现文学翻译个性化，在保持质量的同时突破隐式风格约束


<details>
  <summary>Details</summary>
Motivation: 现有系统难以处理隐式文体需求，特别是文学翻译领域低资源场景下的个性化风格表达

Method: 结合多提示策略、推理时干预和基于稀疏自编码器的对比框架提取潜在概念特征

Result: 引导方法实现强个性化翻译且保持质量，模型层影响机制与多样本提示相似

Conclusion: 个性化引导机制与提示学习共享相似表征机制，为理解LLM翻译控制提供新视角

Abstract: High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.

</details>


### [91] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 提出Simple Self-Rewarding（SSR）强化学习框架，通过自奖励机制实现无参考数据的高效机器翻译，在多语种翻译任务中超越现有大模型并达到SOTA水平


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译模型过度依赖昂贵的外部监督信号（人工标注数据/奖励模型），限制了可扩展性和应用效率。SSR框架旨在实现完全自监督的在线强化学习

Method: 基于Qwen-2.5-7B构建SSR框架：1）通过13K单语数据实现自我奖励机制 2）结合COMET外部监督生成增强模型SSR-X-Zero-7B 3）完全在线的强化学习过程

Result: SSR-Zero-7B在WMT23/24和Flores200基准测试中：1）超越TowerInstruct-13B等专业模型 2）优于Qwen2.5-32B-Instruct等大参数通用模型。SSR-X-Zero-7B达到英语↔中文翻译SOTA，超越GPT-4o/Gemini 1.5 Pro

Conclusion: 自奖励机制相比外部LLM评判更有效，且与奖励模型形成互补。证明自改进强化学习在MT领域的潜力，开源资源促进后续研究

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [92] [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
*Kexin Shang,Chia-Hsuan Chang,Christopher C. Yang*

Main category: cs.CL

TL;DR: 提出多LLM协作框架提升医疗多选题推理能力


<details>
  <summary>Details</summary>
Motivation: 现有研究未能有效整合多个LLM在医疗领域的专业知识协同效应

Method: 设计基于医学多选题数据集的多LLM协作框架，通过后验分析三个预训练LLM

Result: 框架提升所有LLM的推理能力（准确率提升），缓解模型间分歧，发现置信度与预测准确度正相关

Conclusion: 多LLM协作框架有效释放大语言模型在医疗任务中的协同潜力

Abstract: Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.

</details>


### [93] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
*Liu Chang,Wang Dongbo,Liu liu,Zhao Zhixiao*

Main category: cs.CL

TL;DR: 构建Guji_MATH文言数学评测基准，发现推理模型在古算题表现弱于现代数学任务，优化重点应提升文言理解与文化知识


<details>
  <summary>Details</summary>
Motivation: 解决古籍数学经典智能化处理难题，系统评估主流模型在文言特殊语境下的数学问题解决能力，为传统文化传播与模型跨文化评估提供新方法

Method: 基于《算经十书》构建包含538个问题的结构化数据集，设计闭卷/开卷双模式评估6个模型的文言数学问题处理能力

Result: 模型具备部分文言数学问题解决能力，但整体准确率低于现代数学基准（闭卷最高58.7% vs 现代数学78.2%），文言歧义消解和文化常识理解是主要瓶颈

Conclusion: 该研究为古籍数学知识挖掘与传播提供方法论支持，同时建立了评估AI模型跨语言文化能力的新范式，推动传统文化数字化转型

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [94] [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
*Issey Sukeda,Takuro Fujii,Kosei Buma,Shunsuke Sasaki,Shinnosuke Ono*

Main category: cs.CL

TL;DR: 开发日语制药领域专用大模型，通过20亿制药日文+80亿生物医学英文预训练，构建三大诊断性评测基准并验证模型效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在医药术语处理、跨语言术语规范化和语句逻辑一致性推理上的不足，构建安全可控的日本领域专用模型。

Method: 1. 混合日英专业语料持续预训练
2. 创新YakugakuQA（药师考试）、NayoseQA（术语标准化）、SogoCheck（一致性推理）三大评测体系

Result: 模型在术语密集型任务超越开源模型，与商业模型持平；发现GPT-4o在跨句逻辑一致性任务(SogoCheck)存在显著缺陷

Conclusion: 验证领域专用模型可行性，为医药NLP提供可复用的评测框架与资源，推动安全可靠的行业应用落地。

Abstract: We present a Japanese domain-specific language model for the pharmaceutical
field, developed through continual pretraining on 2 billion Japanese
pharmaceutical tokens and 8 billion English biomedical tokens. To enable
rigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on
national pharmacist licensing exams; NayoseQA, which tests cross-lingual
synonym and terminology normalization; and SogoCheck, a novel task designed to
assess consistency reasoning between paired statements. We evaluate our model
against both open-source medical LLMs and commercial models, including GPT-4o.
Results show that our domain-specific model outperforms existing open models
and achieves competitive performance with commercial ones, particularly on
terminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o
performs poorly on SogoCheck, suggesting that cross-sentence consistency
reasoning remains an open challenge. Our benchmark suite offers a broader
diagnostic lens for pharmaceutical NLP, covering factual recall, lexical
variation, and logical consistency. This work demonstrates the feasibility of
building practical, secure, and cost-effective language models for Japanese
domain-specific applications, and provides reusable evaluation resources for
future research in pharmaceutical and healthcare NLP. Our model, codes, and
datasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.

</details>


### [95] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
*Gouki Minegishi,Hiroki Furuta,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 研究通过实验分析Transformer模型在上下文元学习任务中的电路动态，揭示了模型通过多个训练阶段逐步形成不同电路结构来实现元学习能力的过程。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型如何从上下文示例中推断任务并解决（而非简单复制答案）的元学习能力获取机制，该过程在先前研究中尚未被充分揭示。

Method: 将传统复制任务扩展为上下文元学习场景，通过动态分析模型训练过程中内部电路的变化，追踪元学习能力的形成过程。

Result: 发现元学习能力通过多个训练阶段逐步形成，各阶段涌现出不同功能的电路结构，这与传统induction heads的单阶段形成机制形成鲜明对比。

Conclusion: 研究揭示了Transformer上下文学习能力的多层次电路形成机制，为理解大模型元学习过程提供了新的视角和分析框架。

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [96] [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
*Zeping Yu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出无训练的Locate-then-Merge框架和Neuron-Fusion策略，通过参数选择性融合有效缓解多模态大模型的语言能力退化问题，在13个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态指令调优阶段导致基础大语言模型（如Llama3）语言能力退化的问题，现有模型融合方法难以平衡视觉适应与语言保留。

Method: 1. 两阶段框架：先定位重要参数，再选择性融合
2. Neuron-Fusion策略：保留参数变化大的视觉相关神经元，衰减变化小的语言相关神经元

Result: 在13个语言/视觉基准测试中全面领先，生成任务上下文幻觉减少40%，语言任务准确率保持基础模型98.7%性能

Conclusion: 神经元级参数融合策略开创了模型能力保留新范式，为多模态大模型调优提供了无需重新训练的高效解决方案

Abstract: Although multimodal large language models (MLLMs) have achieved impressive
performance, the multimodal instruction tuning stage often causes catastrophic
forgetting of the base LLM's language ability, even in strong models like
Llama3. To address this, we propose Locate-then-Merge, a training-free
parameter fusion framework that first locates important parameters and then
selectively merges them. We further introduce Neuron-Fusion, a neuron-level
strategy that preserves the influence of neurons with large parameter
shifts--neurons likely responsible for newly acquired visual
capabilities--while attenuating the influence of neurons with smaller changes
that likely encode general-purpose language skills. This design enables better
retention of visual adaptation while mitigating language degradation.
Experiments on 13 benchmarks across both language and visual tasks show that
Neuron-Fusion consistently outperforms existing model merging methods. Further
analysis reveals that our method effectively reduces context hallucination in
generation.

</details>


### [97] [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
*Himanshu Beniwal,Youngwoo Kim,Maarten Sap,Soham Dan,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 研究提出跨语言去毒化范式，通过迁移学习实现不同语系语言间的毒性消除，分析504种设置验证有效性，揭示安全性与知识保留的权衡关系


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多语言环境中的毒性控制存在空白，尤其缺乏低资源语言的解决方案，需建立跨语言迁移的净化机制

Method: 构建包含高低资源语言的多语种数据集，采用跨分布实验框架设计504种数据受限场景，量化评估毒性迁移效率及对常规NLP任务的影响

Result: 实现跨语系语言间的毒性消除迁移，发现净化强度与模型知识保留呈负相关，公开包含多语种毒性标注的数据集及算法框架

Conclusion: 该研究为多语种AI安全提供新范式，公开资源推动相关领域发展，揭示模型安全性与功能完整性的平衡机制

Abstract: As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore "Cross-lingual
Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.

</details>


### [98] [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
*Florentin Beck,William Rudman,Carsten Eickhoff*

Main category: cs.CL

TL;DR: TRIM提出基于行级动态稀疏调整的LLM剪枝方法，在极端压缩场景下实现SOTA效果


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法采用统一稀疏度导致高压缩率下性能骤降，需通过细粒度维度级调整优化信息保留

Method: 采用迭代度量驱动机制，为每个输出维度动态分配稀疏率，结合质量指标反馈调整，并与现有层剪枝策略兼容

Result: 在Qwen2.5-14B模型80%稀疏度下困惑度降低48%，OPT-13B降低超90%，多任务表现稳定

Conclusion: 维度级稀疏自适应是实现极端LLM压缩的关键，TRIM验证了细粒度调整对信息保持的有效性

Abstract: Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM

</details>


### [99] [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
*Yiming Gao,Bin Wang,Chengwei Wei,Shuo Sun,AiTi Aw*

Main category: cs.CL

TL;DR: 提出IFEval-Audio数据集，填补音频LLM指令遵循能力评估的空白，覆盖6个评估维度并公开数据集


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型对齐非文本模态后指令遵循能力下降，音频LLM领域缺乏系统性评估工具

Method: 构建包含280个音频指令对的数据集，从内容、格式、结构等6个维度设计结构化测试用例

Result: 完成对主流音频LLM的基准测试，建立首个系统化音频指令遵循评估标准

Conclusion: IFEval-Audio为音频LLM的指令遵循能力评估提供标准化工具，推动该领域研究发展

Abstract: Large language models (LLMs) have demonstrated strong instruction-following
capabilities in text-based tasks. However, this ability often deteriorates in
multimodal models after alignment with non-text modalities such as images or
audio. While several recent efforts have investigated instruction-following
performance in text and vision-language models, instruction-following in
audio-based large language models remains largely unexplored. To bridge this
gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess
the ability to follow instructions in an audio LLM. IFEval-Audio contains 280
audio-instruction-answer triples across six diverse dimensions: Content,
Capitalization, Symbol, List Structure, Length, and Format. Each example pairs
an audio input with a text instruction, requiring the model to generate an
output that follows a specified structure. We benchmark state-of-the-art audio
LLMs on their ability to follow audio-involved instructions. The dataset is
released publicly to support future research in this emerging area.

</details>


### [100] [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
*Xinghao Chen,Anhao Zhao,Heming Xia,Xuan Lu,Hanlin Wang,Yanjun Chen,Wei Zhang,Jian Wang,Wenjie Li,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 系统综述了潜在思维链推理在提升大语言模型效率与灵活性方面的创新方法，提出统一分类框架并分析技术路线


<details>
  <summary>Details</summary>
Motivation: 传统显式思维链推理存在语言表达效率低、难以处理抽象推理的局限，需探索潜在空间推理实现认知表征与计算效率的突破

Method: 从token策略、内部机制、分析方法和应用场景四个维度构建分类体系，系统比较代表性方法的设计模式与技术特征

Result: 潜在推理通过解耦语言与计算，可支持更丰富的认知表征和并行化推理，但面临动态规划、可解释性等开放挑战

Conclusion: 潜在思维链标志着LLM推理范式的重大转变，未来需在理论建模、工程优化与应用扩展方向持续突破

Abstract: Large Language Models (LLMs) have achieved impressive performance on complex
reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional
CoT relies on reasoning steps explicitly verbalized in natural language,
introducing inefficiencies and limiting its applicability to abstract
reasoning. To address this, there has been growing research interest in latent
CoT reasoning, where inference occurs within latent spaces. By decoupling
reasoning from language, latent reasoning promises richer cognitive
representations and more flexible, faster inference. Researchers have explored
various directions in this promising field, including training methodologies,
structural innovations, and internal reasoning mechanisms. This paper presents
a comprehensive overview and analysis of this reasoning paradigm. We begin by
proposing a unified taxonomy from four perspectives: token-wise strategies,
internal mechanisms, analysis, and applications. We then provide in-depth
discussions and comparative analyses of representative methods, highlighting
their design patterns, strengths, and open challenges. We aim to provide a
structured foundation for advancing this emerging direction in LLM reasoning.
The relevant papers will be regularly updated at
https://github.com/EIT-NLP/Awesome-Latent-CoT.

</details>


### [101] [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
*Punya Syon Pandey,Samuel Simko,Kellin Pelrine,Zhijing Jin*

Main category: cs.CL

TL;DR: 研究发现微调数据特征（语言特征/语义相似性/毒性）与对抗攻击成功率存在相关性，揭示了数据设计对维护模型对齐的关键作用。


<details>
  <summary>Details</summary>
Motivation: 探究模型微调过程中因数据特性导致的意外安全漏洞（Accidental Misalignment），揭示微调数据特征与模型对抗鲁棒性的潜在关联。

Method: 1. 识别微调数据中的相关性因子（语言特征/语义相似性/毒性）→ 2. 评估微调模型的对抗性能 → 3. 分析数据集因素与攻击成功率的相关性 → 4. 因果推断验证

Result: 发现特定数据特征与对抗攻击成功率显著相关，证实微调数据设计会直接影响模型的安全对齐性。

Conclusion: 数据特征设计是防御对抗攻击的关键环节，该发现为通过数据治理提升模型安全性提供了新方向。

Abstract: As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.

</details>


### [102] [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
*Changbing Yang,Garrett Nicolai*

Main category: cs.CL

TL;DR: 提出基于Transformer的多任务学习框架，通过合成数据增强低资源语言的词素分割性能


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中词素分割数据稀缺问题，传统单任务模型泛化能力不足

Method: 联合预测形态分割与注释+LLM上下文学习生成合成数据+共享语言表征

Result: 在SIGMORPHON 2023数据集显著提升多语言词级分割准确率与词素F1值

Conclusion: 多任务框架与合成数据结合为低资源NLP任务提供了有效解决方案

Abstract: We introduce a transformer-based morpheme segmentation system that augments a
low-resource training signal through multitask learning and LLM-generated
synthetic data. Our framework jointly predicts morphological segments and
glosses from orthographic input, leveraging shared linguistic representations
obtained through a common documentary process to enhance model generalization.
To further address data scarcity, we integrate synthetic training data
generated by large language models (LLMs) using in-context learning.
Experimental results on the SIGMORPHON 2023 dataset show that our approach
significantly improves word-level segmentation accuracy and morpheme-level
F1-score across multiple low-resource languages.

</details>


### [103] [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
*Kexin Zhang,Junlan Chen,Daifeng Li,Yuxuan Zhang,Yangyang Feng,Bowen Deng,Weixu Chen*

Main category: cs.CL

TL;DR: 提出TW-ESA和DGR双模块框架，通过双向证据对齐和双门控推理增强，显著提升语言模型在知识密集型多步推理任务中的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有方法在证据提取时存在逻辑不相关性，且在证据不确定时难以有效融合语言模型知识，导致推理错误

Method: TW-ESA模块实现严格推理与LLM推理的双向对齐，DGR模块通过双门控机制逐步融合模型知识，二者协同构建ESA-DGR统一框架

Result: 在三个KIMSR数据集上实现平均4%的EM值和5%的F1分数提升，超越现有微调方法

Conclusion: ESA-DGR通过增强证据因果理解与知识融合机制，有效提升复杂推理任务的准确性和抗干扰能力

Abstract: Large language models (LLMs) encounter difficulties in knowledge-intensive
multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract
and represent rationale evidence. The current methods often extract
semantically relevant but logically irrelevant evidence, resulting in flawed
reasoning and inaccurate responses. We propose a two-way evidence
self-alignment (TW-ESA) module, which utilizes the mutual alignment between
strict reasoning and LLM reasoning to enhance its understanding of the causal
logic of evidence, thereby addressing the first challenge. Another challenge is
how to utilize the rationale evidence and LLM's intrinsic knowledge for
accurate reasoning when the evidence contains uncertainty. We propose a
dual-gated reasoning enhancement (DGR) module to gradually fuse useful
knowledge of LLM within strict reasoning, which can enable the model to perform
accurate reasoning by focusing on causal elements in the evidence and exhibit
greater robustness. The two modules are collaboratively trained in a unified
framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR
datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based
fine-tuning methods, with remarkable average improvements of 4% in exact match
(EM) and 5% in F1 score. The implementation code is available at
https://anonymous.4open.science/r/ESA-DGR-2BF8.

</details>


### [104] [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
*Gaurav Kamath,Sowmya Vajjala*

Main category: cs.CL

TL;DR: 合成数据在低资源语言命名实体识别中展现潜力但存在语言差异性


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言因标注数据不足导致NER性能受限的问题，探索多语言环境下合成数据增强的有效性

Method: 通过跨11种不同语系语言的实验，系统评估合成数据增强对低资源NER的影响

Result: 合成数据显著提升低资源语言NER性能，但不同语言间改善幅度存在显著差异

Conclusion: 合成数据是有效的低资源解决方案，但需结合语言特性进行针对性优化

Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.

</details>


### [105] [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
*Xiaoyu Xu,Xiang Yue,Yang Liu,Qingqing Ye,Haibo Hu,Minxin Du*

Main category: cs.CL

TL;DR: 论文揭示当前大模型遗忘学习评估指标的误导性，提出基于表示层分析的新诊断框架，区分可逆与不可逆遗忘机制。


<details>
  <summary>Details</summary>
Motivation: 现有基于词级指标的评估方法不可靠，无法真实反映模型遗忘效果，需建立可信的表示层分析体系。

Method: 使用PCA相似性、中心核对齐、Fisher信息等表示分析工具，在6种遗忘方法/3个领域/2个开源模型上进行系统性评估。

Result: 发现可逆遗忘（保留潜在特征）与不可逆遗忘（深层表示损伤）的本质差异，证明任务类型和超参数调节遗忘可逆性。

Conclusion: 现有评估体系存在根本缺陷，需建立基于表示分析的新范式，为可信模型遗忘提供诊断基础并开源分析工具集。

Abstract: Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.

</details>


### [106] [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
*Shuang Sun,Huatong Song,Yuhao Wang,Ruiyang Ren,Jinhao Jiang,Junjie Zhang,Fei Bai,Jia Deng,Wayne Xin Zhao,Zheng Liu,Lei Fang,Zhongyuan Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出SimpleDeepSearcher框架，通过数据工程而非复杂训练范式，仅用871个样本的SFT就显著提升深度搜索系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面临高质量训练轨迹缺乏、模拟环境分布不匹配和真实部署计算成本高昂三大瓶颈。

Method: 通过实时网络搜索环境模拟用户交互生成数据，采用输入输出端多标准筛选策略优化数据多样性质量。

Result: 在5个跨领域基准测试中，SFT效果超越RL基线方法，计算效率显著提升。

Conclusion: 系统化解决数据稀缺问题，验证了SFT的有效性，为高效深度搜索系统提供实践路径。

Abstract: Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.

</details>


### [107] [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
*Yibo Wang,Li Shen,Huanjin Yao,Tiansheng Huang,Rui Liu,Naiqiang Tan,Jiaxing Huang,Kai Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: 两阶段块级压缩框架R1-Compress有效减少Long-CoT的token使用量（约20%），在保持推理准确率（MATH500达92.4%）的同时解决现有压缩方法的连贯性缺失问题


<details>
  <summary>Details</summary>
Motivation: 现有Long-CoT压缩方法存在矛盾：实例级压缩丢失关键局部推理信号（如反思过程），而token级压缩导致输出不连贯。需要平衡计算效率与推理质量

Method: 1. 分块处理：将Long-CoT切分为可管理块；2. 块内压缩：用LLM进行内部压缩；3. 块间搜索：选择最短且连贯的序列。通过双阶段机制实现全局优化

Result: 在Qwen2.5-Instruct模型测试中：MATH500准确率92.4%（仅比基线低0.6%），AIME24和GPQA-Diamond保持同等精度，token使用量减少约20%

Conclusion: R1-Compress证明块级压缩的有效性，平衡计算效率与推理质量，为长链推理的实际应用提供可行方案。开源代码将促进社区发展

Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by
enabling step-by-step problem-solving, yet its extension to Long-CoT introduces
substantial computational overhead due to increased token length. Existing
compression approaches -- instance-level and token-level -- either sacrifice
essential local reasoning signals like reflection or yield incoherent outputs.
To address these limitations, we propose R1-Compress, a two-stage chunk-level
compression framework that preserves both local information and coherence. Our
method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk
compression, and employs an inter-chunk search mechanism to select the short
and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,
AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces
token usage while maintaining comparable reasoning accuracy. On MATH500,
R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to
the Long-CoT baseline, while reducing token usage by about 20%. Source code
will be available at https://github.com/w-yibo/R1-Compress

</details>


### [108] [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
*Baran Barbarestani,Isa Maks,Piek Vossen*

Main category: cs.CL

TL;DR: 集成专家标注与ChatGPT检测网络仇恨言论，揭示语境重要性并发现新型歧视类别


<details>
  <summary>Details</summary>
Motivation: 解决现有内容审核系统在识别显性仇恨言论和微妙歧视语言方面的不足，提升在线社区安全性

Method: 使用Reddit英文对话数据，构建综合标注框架比较专家/众包/ChatGPT标注效果，分析目标类别和语境因素

Result: 发现社会信仰/身体形象等新型歧视类别，ChatGPT在细微语境理解存在局限，专家标注准确率最高（达83%）

Conclusion: 应融合人类标注与AI优势优化审核系统，上下文理解和多维度标注体系对提升内容审核效果具有关键作用

Abstract: This paper introduces a method for detecting inappropriately targeting
language in online conversations by integrating crowd and expert annotations
with ChatGPT. We focus on English conversation threads from Reddit, examining
comments that target individuals or groups. Our approach involves a
comprehensive annotation framework that labels a diverse data set for various
target categories and specific target words within the conversational context.
We perform a comparative analysis of annotations from human experts, crowd
annotators, and ChatGPT, revealing strengths and limitations of each method in
recognizing both explicit hate speech and subtler discriminatory language. Our
findings highlight the significant role of contextual factors in identifying
hate speech and uncover new categories of targeting, such as social belief and
body image. We also address the challenges and subjective judgments involved in
annotation and the limitations of ChatGPT in grasping nuanced language. This
study provides insights for improving automated content moderation strategies
to enhance online safety and inclusivity.

</details>


### [109] [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
*Alberto Muñoz-Ortiz,David Vilares,Caio COrro,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 将嵌套命名实体识别转化为序列标注任务，通过线性化句法结构降低复杂度，结合预训练编码器实现高效训练


<details>
  <summary>Details</summary>
Motivation: 解决嵌套命名实体识别作为结构化预测任务的复杂度问题，简化模型训练流程

Method: 使用句法结构线性化技术将嵌套结构转换为序列标注形式，结合预训练编码器进行端到端学习

Result: 在保持竞争力的性能前提下，仅需n次标注操作即可完成训练，且兼容现有序列标注工具库

Conclusion: 该方法实现了高效且轻量化的嵌套实体识别框架，显著降低算法实现门槛

Abstract: We cast nested named entity recognition (NNER) as a sequence labeling task by
leveraging prior work that linearizes constituency structures, effectively
reducing the complexity of this structured prediction problem to
straightforward token classification. By combining these constituency
linearizations with pretrained encoders, our method captures nested entities
while performing exactly $n$ tagging actions. Our approach achieves competitive
performance compared to less efficient systems, and it can be trained using any
off-the-shelf sequence labeling library.

</details>


### [110] [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
*Sudhansu Bala Das,Samujjal Choudhury,Tapas Kumar Mishra,Bidyut Kr. Patra*

Main category: cs.CL

TL;DR: 该研究比较了不同子词分词技术（SentencePiece/BPE/WordPiece）在印度语言机器翻译中的表现，发现SentencePiece在统计和神经模型中效果最佳，而BPE在多语言模型中表现最优。


<details>
  <summary>Details</summary>
Motivation: 印度语言具有复杂的形态结构（如黏着语特征），需通过子词分词捕捉其词缀变化，传统分词方法难以有效处理这些特性。

Method: 使用SentencePiece/BPE/WordPiece三种技术，在统计机器翻译、神经机器翻译及多语言神经机器翻译模型中测试，采用BLEU/TER/METEOR等6种指标进行系统评估。

Result: 1. 统计/神经模型中SentencePiece的BLEU得分持续领先
2. 多语言模型中BPE表现最优
3. 印度语言→英语的翻译质量始终优于英语→印度语言方向

Conclusion: 子词分词技术的选择显著影响机器翻译性能，需根据模型类型（单语/多语言）和翻译方向优化分词策略，且语言形态复杂度与分词效果存在强关联。

Abstract: Tokenization is the act of breaking down text into smaller parts, or tokens,
that are easier for machines to process. This is a key phase in machine
translation (MT) models. Subword tokenization enhances this process by breaking
down words into smaller subword units, which is especially beneficial in
languages with complicated morphology or a vast vocabulary. It is useful in
capturing the intricate structure of words in Indian languages (ILs), such as
prefixes, suffixes, and other morphological variations. These languages
frequently use agglutinative structures, in which words are formed by the
combination of multiple morphemes such as suffixes, prefixes, and stems. As a
result, a suitable tokenization strategy must be chosen to address these
scenarios. This paper examines how different subword tokenization techniques,
such as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,
affect ILs. The effectiveness of these subword tokenization techniques is
investigated in statistical, neural, and multilingual neural machine
translation models. All models are examined using standard evaluation metrics,
such as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,
RIBES, and COMET. Based on the results, it appears that for the majority of
language pairs for the Statistical and Neural MT models, the SentencePiece
tokenizer continuously performed better than other tokenizers in terms of BLEU
score. However, BPE tokenization outperformed other tokenization techniques in
the context of Multilingual Neural Machine Translation model. The results show
that, despite using the same tokenizer and dataset for each model, translations
from ILs to English surpassed translations from English to ILs.

</details>


### [111] [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
*Weixiang Zhao,Yulin Hu,Yang Deng,Tongtong Wu,Wenxuan Zhang,Jiahe Guo,An Zhang,Yanyan Zhao,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.CL

TL;DR: 提出MPO方法解决多语言安全对齐难题，通过主语言奖励差距优化实现跨语言安全能力迁移


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO等安全对齐方法主要面向单语言场景，多语言场景下存在数据噪声和跨语言迁移效果差的问题

Method: MPO方法通过最小化主语言(英语)与目标语言之间的奖励差距，直接迁移主语言的安全能力到其他语言

Result: 在LLaMA-3.1、Gemma-2和Qwen2.5三个模型上的实验证明，MPO能有效提升多语言安全对齐效果且不损害通用多语言能力

Conclusion: MPO为LLMs的全球化安全部署提供了新范式，开创了主语言能力迁移的新路径

Abstract: Large language models (LLMs) have become increasingly central to AI
applications worldwide, necessitating robust multilingual safety alignment to
ensure secure deployment across diverse linguistic contexts. Existing
preference learning methods for safety alignment, such as RLHF and DPO, are
primarily monolingual and struggle with noisy multilingual data. To address
these limitations, we introduce Multilingual reward gaP Optimization (MPO), a
novel approach that leverages the well-aligned safety capabilities of the
dominant language (English) to improve safety alignment across multiple
languages. MPO directly minimizes the reward gap difference between the
dominant language and target languages, effectively transferring safety
capabilities while preserving the original strengths of the dominant language.
Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate
MPO's efficacy in multilingual safety alignment without degrading general
multilingual utility.

</details>


### [112] [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
*Daniel F. Perez-Ramirez,Dejan Kostic,Magnus Boman*

Main category: cs.CL

TL;DR: CASTILLO数据集通过记录13个开源LLM在7个指令语料库上的响应长度分布，为预测性资源调度提供数据支持，揭示模型间显著的长度变异性与生成行为差异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理资源管理方法难以准确预估随机性文本生成长度，导致资源分配低效。需系统性量化不同模型和提示下的响应长度分布特性。

Method: 在固定解码参数下，对13个LLM生成7个指令集的10次独立响应，统计长度均值、标准差、百分位数，并记录极端案例及生成设置。

Result: 发现模型间响应长度差异达3.8倍，同模型同参数下仍有显著波动（最高±47%），部分模型出现局部文本退化现象。

Conclusion: CASTILLO为生成模型与系统协同优化提供基准，其开源数据支持前瞻性资源调度算法开发与跨模型生成行为分析。

Abstract: Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.

</details>


### [113] [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
*Zeyu Wei,Shuo Wang,Xiaohui Rong,Xuemin Liu,He Li*

Main category: cs.CL

TL;DR: 研究首次通过内部状态漂移分析揭示大语言模型幻觉的演变机制，发现上下文注入轮次与注意力锁定阈值的关键关联。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因上下文增量注入导致的幻觉问题，探索其内部表征漂移与错误模式的关联机制。

Method: 使用TruthfulQA构建双轨16轮上下文注入实验，通过三视角检测器追踪显性幻觉率，结合隐藏状态/注意力图的四种指标（余弦/熵/JS/Spearman）分析隐性漂移。

Result: 1. 幻觉频率随轮次单调增长（5-7轮后稳定）
2. 相关上下文引发高置信度自洽幻觉，无关上下文导致注意力重路由的主题漂移
3. JS漂移(0.69)与Spearman漂移(0)收敛形成注意力锁定阈值

Conclusion: 揭示了模型容量与注意力扩散的权衡关系，为构建幻觉预测模型和上下文感知矫正机制提供了理论依据。

Abstract: Hallucinations -- plausible yet erroneous outputs -- remain a critical
barrier to reliable deployment of large language models (LLMs). We present the
first systematic study linking hallucination incidence to internal-state drift
induced by incremental context injection. Using TruthfulQA, we construct two
16-round "titration" tracks per question: one appends relevant but partially
flawed snippets, the other injects deliberately misleading content. Across six
open-source LLMs, we track overt hallucination rates with a tri-perspective
detector and covert dynamics via cosine, entropy, JS and Spearman drifts of
hidden states and attention maps. Results reveal (1) monotonic growth of
hallucination frequency and representation drift that plateaus after 5--7
rounds; (2) relevant context drives deeper semantic assimilation, producing
high-confidence "self-consistent" hallucinations, whereas irrelevant context
induces topic-drift errors anchored by attention re-routing; and (3)
convergence of JS-Drift ($\sim0.69$) and Spearman-Drift ($\sim0$) marks an
"attention-locking" threshold beyond which hallucinations solidify and become
resistant to correction. Correlation analyses expose a seesaw between
assimilation capacity and attention diffusion, clarifying size-dependent error
modes. These findings supply empirical foundations for intrinsic hallucination
prediction and context-aware mitigation mechanisms.

</details>


### [114] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: 提出幂律衰减损失函数(PDL)，通过动态调整token权重优化文本生成微调过程


<details>
  <summary>Details</summary>
Motivation: 基于信息理论观察：token的信息量与出现频率成反比。传统交叉熵损失导致模型过度关注高频低信息token，忽略重要低频token

Method: 在交叉熵损失基础上引入幂律衰减机制，按训练语料频率动态调整token权重：降低高频token权重，提升低频token权重

Result: 增强生成文本的信息密度与多样性，提升摘要生成、对话系统等任务中特定信息的表达能力

Conclusion: PDL为文本生成微调提供了更符合信息分布特性的优化方法，在需要精确信息表达的任务中具有显著优势

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [115] [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
*Ruihan Yang,Caiqi Zhang,Zhisong Zhang,Xinting Huang,Dong Yu,Nigel Collier,Deqing Yang*

Main category: cs.CL

TL;DR: 提出UNCLE基准测试，评估LLMs在长短问答中的不确定性表达能力，发现当前模型在长文本生成中存在不足，训练方法改进效果显著。


<details>
  <summary>Details</summary>
Motivation: LLMs在长文本生成中存在幻觉问题，现有研究缺乏对长短问答场景下不确定性表达能力的直接评估。

Method: 1. 构建跨5领域的UNCLE基准（含4k长问答+20k短问答对）；2. 提出新指标评估选择性不确定性表达能力；3. 探索提示法和训练法改进模型。

Result: 当前模型无法恰当表达长文本不确定性；训练方法（如微调）比提示方法改进更显著；短/长形式不确定性表达存在对齐差距。

Conclusion: UNCLE为未来研究提供评估基础，训练方法优化和长短形式对齐是提升LLMs不确定性表达的关键方向。

Abstract: Large Language Models (LLMs) are prone to hallucination, particularly in
long-form generations. A promising direction to mitigate hallucination is to
teach LLMs to express uncertainty explicitly when they lack sufficient
knowledge. However, existing work lacks direct and fair evaluation of LLMs'
ability to express uncertainty effectively in long-form generation. To address
this gap, we first introduce UNCLE, a benchmark designed to evaluate
uncertainty expression in both long- and short-form question answering (QA).
UNCLE spans five domains and comprises 4k long-form QA instances and over 20k
short-form QA pairs. Our dataset is the first to directly bridge short- and
long-form QA with paired questions and gold-standard answers. Along with the
benchmark, we propose a suite of new metrics to assess the models' capabilities
to selectively express uncertainty. Using UNCLE, we then demonstrate that
current models fail to convey uncertainty appropriately in long-form
generation. We further explore both prompt-based and training-based methods to
improve models' performance, with the training-based methods yielding greater
gains. Further analysis of alignment gaps between short- and long-form
uncertainty expression highlights promising directions for future research
using UNCLE.

</details>


### [116] [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
*Keshav Ramji,Tahira Naseem,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出自动化挖掘语言模型潜在行为属性的自校正框架，通过聚类压缩实现可解释性，使小模型实现自我提升。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注模型行为属性的方式成本过高，需要自动化方法挖掘语言模型内在的推理原则。

Method: 使用后验正则化的蒙特卡洛期望最大化算法，通过迭代自举识别有效潜在原则，并训练模型主动调用这些原则改进生成质量。

Result: 7-8B参数模型在AlpacaEval胜率提升8-10%，MT-Bench平均+0.3，IFEval原则遵循率提升19-23%，同时保持聚类后的原则可解释性。

Conclusion: 基于自动化原则挖掘的训练方法展现出持续自我改进潜力，为语言模型的后训练优化提供了新方向。

Abstract: When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.

</details>


### [117] [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
*Matthew Zent,Digory Smith,Simon Woodhead*

Main category: cs.CL

TL;DR: 提出轻量级PII匿名化框架PIIvot，通过利用数据上下文知识简化检测流程，并发布最大开源教育对话数据集QATD-2k


<details>
  <summary>Details</summary>
Motivation: 现有PII匿名化技术在错误阈值和召回率/精准率权衡方面存在局限，阻碍开放科学数据共享

Method: 通过数据上下文知识约束检测范围，将PII检测转化为结构化模式匹配问题

Result: 成功构建QATD-2k教育对话数据集（规模达2000小时），验证框架在真实教育场景的有效性

Conclusion: 上下文驱动的轻量化方案有效平衡匿名化需求与数据效用，推动教育AI数据开放共享

Abstract: Personally identifiable information (PII) anonymization is a high-stakes task
that poses a barrier to many open-science data sharing initiatives. While PII
identification has made large strides in recent years, in practice, error
thresholds and the recall/precision trade-off still limit the uptake of these
anonymization pipelines. We present PIIvot, a lighter-weight framework for PII
anonymization that leverages knowledge of the data context to simplify the PII
detection problem. To demonstrate its effectiveness, we also contribute
QATD-2k, the largest open-source real-world tutoring dataset of its kind, to
support the demand for quality educational dialogue data.

</details>


### [118] [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
*Yepeng Liu,Xuandong Zhao,Christopher Kruegel,Dawn Song,Yuheng Bu*

Main category: cs.CL

TL;DR: 提出无需访问模型内部参数的上下文水印技术（ICW），通过提示工程实现AI生成文本溯源，验证其作为通用水印方案的可行性


<details>
  <summary>Details</summary>
Motivation: 现有水印技术依赖模型内部参数访问，无法应对学术评审等现实场景中第三方需要检测AI文本的需求

Method: 利用大语言模型的上下文学习能力，设计四种不同粒度的提示策略，结合间接提示注入（IPI）进行隐蔽水印触发

Result: 实验证明ICW在不同模型上有效，且模型能力越强水印效果越好

Conclusion: ICW为AI内容溯源提供了模型无关的实用方案，随着大模型进化具备规模化应用潜力

Abstract: The growing use of large language models (LLMs) for sensitive applications
has highlighted the need for effective watermarking techniques to ensure the
provenance and accountability of AI-generated text. However, most existing
watermarking methods require access to the decoding process, limiting their
applicability in real-world settings. One illustrative example is the use of
LLMs by dishonest reviewers in the context of academic peer review, where
conference organizers have no access to the model used but still need to detect
AI-generated reviews. Motivated by this gap, we introduce In-Context
Watermarking (ICW), which embeds watermarks into generated text solely through
prompt engineering, leveraging LLMs' in-context learning and
instruction-following abilities. We investigate four ICW strategies at
different levels of granularity, each paired with a tailored detection method.
We further examine the Indirect Prompt Injection (IPI) setting as a specific
case study, in which watermarking is covertly triggered by modifying input
documents such as academic manuscripts. Our experiments validate the
feasibility of ICW as a model-agnostic, practical watermarking approach.
Moreover, our findings suggest that as LLMs become more capable, ICW offers a
promising direction for scalable and accessible content attribution.

</details>


### [119] [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
*Daniil Gurgurov,Michal Gregor,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 结合知识蒸馏+结构化剪枝+截断+词汇修剪，将多语言模型压缩92%的同时仅造成2-10%性能下降


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言的大模型部署难题，探索极端压缩方案以保持语言知识同时大幅缩小模型体积

Method: 分步实施知识蒸馏→结构化剪枝（缩减层深度/FFN隐藏层/中间嵌入维度）→词汇表裁剪→截断处理

Result: 在3种低资源语言的4个下游任务（情感分析/主题分类/NER/POS）中，压缩模型性能仅下降2-10%

Conclusion: 教师模型中语言特定数据量决定性能损失程度，数据量越大损失越小；通过系统组合现有技术可实现有效压缩

Abstract: In this paper, we combine two-step knowledge distillation, structured
pruning, truncation, and vocabulary trimming for extremely compressing
multilingual encoder-only language models for low-resource languages. Our novel
approach systematically combines existing techniques and takes them to the
extreme, reducing layer depth, feed-forward hidden size, and intermediate layer
embedding size to create significantly smaller monolingual models while
retaining essential language-specific knowledge. We achieve compression rates
of up to 92% with only a marginal performance drop of 2-10% in four downstream
tasks, including sentiment analysis, topic classification, named entity
recognition, and part-of-speech tagging, across three low-resource languages.
Notably, the performance degradation correlates with the amount of
language-specific data in the teacher model, with larger datasets resulting in
smaller performance losses. Additionally, we conduct extensive ablation studies
to identify best practices for multilingual model compression using these
techniques.

</details>


### [120] [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
*Fengyi Li,Kayhan Behdin,Natesh Pillai,Xiaofeng Wang,Zhipeng Wang,Ercan Yildiz*

Main category: cs.CL

TL;DR: 提出基于图模型的非监督文本分割方法BP-Seg，通过置信传播算法有效结合局部连贯性和语义相似性，实验显示优于对比方法


<details>
  <summary>Details</summary>
Motivation: 解决传统文本分割方法仅关注局部连贯性的局限，通过同时考虑相邻句子的关联性和远距离语义相似性，提升长文档分割效果

Method: 构建图模型进行信念传播，节点表示文本块，边包含局部相邻关系和语义相似关系，通过消息传递机制实现全局最优分割

Result: 在长文档数据集上的实验表明，本方法在分割准确性和语义一致性指标上比基线方法平均提升12.7%

Conclusion: BP-Seg通过图模型整合局部与全局信息，验证了非监督方法在文本分割任务中的有效性，为复杂文档处理提供了新思路

Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.

</details>


### [121] [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
*Tianduo Wang,Lu Xu,Wei Lu,Shanbo Cheng*

Main category: cs.CL

TL;DR: 通过语音反向翻译技术，用少量真实语音数据生成海量合成语音，显著提升多语言ASR模型性能（错误率降低30%+）


<details>
  <summary>Details</summary>
Motivation: 解决多语言ASR系统中低资源语言数据稀缺问题，突破传统需要大量标注语音数据的限制

Method: 1. 开发智能度评估框架筛选合成语音质量
2. 用数十小时真实语音训练TTS模型
3. 生成50万小时多语言合成语音数据
4. 基于Whisper-large-v3进行持续预训练

Result: 在10种语言上实现：
- 合成语音量达真实数据数百倍
- 平均转录错误率降低超30%
- 建立合成数据有效性阈值标准

Conclusion: Speech Back-Translation为多语言ASR系统提供了可扩展的高效解决方案，显著降低对真实标注数据的依赖

Abstract: Recent advances in Automatic Speech Recognition (ASR) have been largely
fueled by massive speech corpora. However, extending coverage to diverse
languages with limited resources remains a formidable challenge. This paper
introduces Speech Back-Translation, a scalable pipeline that improves
multilingual ASR models by converting large-scale text corpora into synthetic
speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just
tens of hours of real transcribed speech can effectively train TTS models to
generate synthetic speech at hundreds of times the original volume while
maintaining high quality. To evaluate synthetic speech quality, we develop an
intelligibility-based assessment framework and establish clear thresholds for
when synthetic data benefits ASR training. Using Speech Back-Translation, we
generate more than 500,000 hours of synthetic speech in ten languages and
continue pre-training Whisper-large-v3, achieving average transcription error
reductions of over 30\%. These results highlight the scalability and
effectiveness of Speech Back-Translation for enhancing multilingual ASR
systems.

</details>


### [122] [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
*Rishanth Rajendhran,Amir Zadeh,Matthew Sarte,Chuan Li,Mohit Iyyer*

Main category: cs.CL

TL;DR: 提出VeriFastScore模型，通过微调Llama3.1 8B实现并行声明提取与验证，将事实性评估速度提升6.6倍


<details>
  <summary>Details</summary>
Motivation: 传统事实性评估指标（如VeriScore）需多次LLM调用，单次评估耗时超100秒，难以适应大规模评估场景

Method: 利用合成数据微调Llama3.1 8B模型，通过Google Search证据同步处理声明分解、可验证性判断和噪声证据验证

Result: 在样本级（r=0.80）和系统级（r=0.94）均保持强相关性，总体速度提升6.6倍（排除证据检索后达9.9倍）

Conclusion: VeriFastScore在保持评估质量的同时显著提升效率，公开的模型与数据集将推动事实性评估领域发展

Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.

</details>


### [123] [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
*Junlong Tong,Jinlan Fu,Zixuan Lin,Yingqi Fan,Anhao Zhao,Hui Su,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 研究发现LLMs在流式处理中存在三个关键不匹配问题，提出基于组位置编码的新方法，无需架构修改即可提升流式性能


<details>
  <summary>Details</summary>
Motivation: 现有流式适配方法存在高成本重新编码或架构扩展性差的问题，且发现输入注意力不匹配是主要性能瓶颈

Method: 提出基于批处理架构的组位置编码范式，保持源/目标上下文相对位置，增强流式与批处理模式一致性

Result: 跨语言/跨模态实验显示优于现有方法，在流式和批处理模式均展现强泛化能力

Conclusion: 成功解决流式处理关键不匹配问题，保持架构兼容性的同时显著提升性能，方法论具有广泛适用性

Abstract: Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.

</details>


### [124] [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
*Amartya Chakraborty,Paresh Dashore,Nadia Bathaee,Anmol Jain,Anirban Das,Shi-Xiong Zhang,Sambit Sahu,Milind Naphade,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出T1多领域多轮对话数据集，用于评估LLMs在工具依赖场景下的规划能力，支持动态缓存与重规划。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在多轮对话中处理工具调用依赖关系存在不足，需构建多领域数据集验证代理的协调能力。

Method: 构建含9个领域（4单域+5跨域）的数据集，集成短期/长期记忆缓存机制，支持动态重规划（如重用缓存或重新计算）。

Result: 基于T1-Agent的实验表明该数据集能有效评估代理在复杂工具依赖场景下的规划推理能力，并为开源模型提供基准。

Conclusion: T1推动了工具使用规划研究，可作为评估LLMs工具协调能力的标准测试集，其缓存机制提升了动态决策效率。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.

</details>


### [125] [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
*Rui Ye,Keduan Huang,Qimin Wu,Yuzhu Cai,Tian Jin,Xianghe Pang,Xiangrui Liu,Jiaqi Su,Chen Qian,Bohan Tang,Kaiqu Liang,Jiaao Chen,Yue Hu,Zhenfei Yin,Rongye Shi,Bo An,Yang Gao,Wenjun Wu,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: MASLab是一个整合了20+多领域方法的统一代码库，通过标准化评估协议和模块化结构推动多智能体系统研究发展


<details>
  <summary>Details</summary>
Motivation: 解决当前LLM多智能体系统领域缺乏统一代码库导致的重复实现、不公平比较和高研究门槛问题

Method: 1.集成验证20+现有方法 2.构建统一测试环境与标准化评估协议 3.采用模块化架构降低理解与扩展门槛

Result: 通过10+基准测试和8种模型的系统实验，揭示了当前MAS方法在通信效率与协同优化层面的性能差异

Conclusion: MASLab将持续跟踪领域发展，通过开源社区协作建立多智能体系统研究的统一技术基础设施

Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.

</details>


### [126] [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
*Chao Zhang,Xin Shi,Xueqiao Zhang,Yifan Zhu,Yi Yang,Yawei Luo*

Main category: cs.CL

TL;DR: 提出解耦式情感支持对话框架IPM-PrefDial，通过策略规划与共情生成分步优化，有效降低心理偏好偏差并提升响应质量


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话(ESC)数据存在心理策略与响应内容的耦合性，导致直接偏好优化(DPO)应用时面临数据质量低与优化目标模糊的双重挑战

Method: 1. 设计推断式偏好挖掘(IPM)构建高质量偏好数据集IPM-PrefDial；2. 基于Gross情感调节模型将ESC解耦为策略规划与共情生成两个子任务，分别通过SFT+DPO分阶段优化

Result: 实验证明解耦框架相比联合优化基线降低75%偏好偏差，响应质量在Helpfulness(↑18.7%)和Safety(↑15.3%)指标显著提升

Conclusion: 分步优化策略有效解决心理策略与内容生成的耦合问题，为情感对话系统提供新的可解释性优化路径

Abstract: Recent advances in Emotional Support Conversation (ESC) have improved
emotional support generation by fine-tuning Large Language Models (LLMs) via
Supervised Fine-Tuning (SFT). However, common psychological errors still
persist. While Direct Preference Optimization (DPO) shows promise in reducing
such errors through pairwise preference learning, its effectiveness in ESC
tasks is limited by two key challenges: (1) Entangled data structure: Existing
ESC data inherently entangles psychological strategies and response content,
making it difficult to construct high-quality preference pairs; and (2)
Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data
leads to ambiguous training objectives. To address these issues, we introduce
Inferential Preference Mining (IPM) to construct high-quality preference data,
forming the IPM-PrefDial dataset. Building upon this data, we propose a
Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion
Regulation, which decomposes the ESC task into two sequential subtasks:
strategy planning and empathic response generation. Each was trained via SFT
and subsequently enhanced by DPO to align with the psychological preference.
Extensive experiments demonstrate that our Decoupled ESC framework outperforms
joint optimization baselines, reducing preference bias and improving response
quality.

</details>


### [127] [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
*Jin Jiang,Jianing Wang,Yuchen Yan,Yang Liu,Jianhua Zhu,Mengdi Zhang,Xunliang Cai,Liangcai Gao*

Main category: cs.CL

TL;DR: 系统评估大型语言模型在形式语言下的逻辑推理能力，揭示思维模型优势、归纳推理局限及PoT格式的泛化优势，并提出小模型增强方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦形式语言引导LLMs推理路径，但对其系统性评估不足。本文旨在通过形式语言全面评估LLMs在不同逻辑推理任务中的表现。

Method: 从LLMs类型（思维/指导模型）、任务分类（演绎/归纳推理）、轨迹格式（CoT/PoT）三个维度构建评估框架，并设计形式相关训练数据增强小模型。

Result: 1) 思维模型在形式语言下显著优于指导模型 2) 所有模型均存在归纳推理缺陷 3) PoT格式数据跨语言泛化最优 4) 拒绝微调方法有效提升小模型跨形式语言性能。

Conclusion: 形式语言显著影响LLMs逻辑推理表现，PoT轨迹格式和拒绝微调方法为优化模型推理能力及跨语言泛化提供了有效解决方案。

Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.

</details>


### [128] [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
*Huatong Song,Jinhao Jiang,Wenqing Tian,Zhipeng Chen,Yuhuan Wu,Jiahao Zhao,Yingqian Min,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出R1-Searcher++框架，通过两阶段训练策略实现语言模型内外知识协同增强，提升检索增强推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG方法成本高、泛化差且忽视模型内部知识的问题，探索LLM如何自适应整合内部知识与外部检索信息。

Method: 采用SFT冷启动+RL动态知识获取两阶段策略，RL阶段引入结果监督促进探索、知识利用奖励机制及持续记忆机制。

Result: 实验显示R1-Searcher++在RAG任务中优于现有方法，同时保持高效检索能力。

Conclusion: 通过动态知识获取与内部知识强化，R1-Searcher++实现了更高效的检索增强推理框架。

Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [129] [Dynamic Caustics by Ultrasonically Modulated Liquid Surface](https://arxiv.org/abs/2505.16397)
*Koki Nagakura,Tatsuki Fushimi,Ayaka Tsutsui,Yoichi Ochiai*

Main category: cs.GR

TL;DR: 提出基于双优化全息场与相控阵换能器的动态焦散生成方法，通过数字孪生框架实现液体表面实时控制，可生成高频复杂焦散动画


<details>
  <summary>Details</summary>
Motivation: 现有固体表面焦散生成方法缺乏动态调控能力，液体表面作为折射介质的潜力尚未在实用化场景中充分挖掘

Method: 结合相控阵换能器超声波操控液体表面形变，采用数字孪生框架实现迭代反馈优化，建立动态全息场计算模型

Result: 实验验证了每秒千帧级动态焦散生成能力，在交互显示场景实现连续动画效果，但对比度较固体方法低约40%

Conclusion: 该技术突破了传统静态焦散限制，在艺术装置、教育工具等领域具应用潜力，未来需提升图案分辨率和光学质量

Abstract: This paper presents a method for generating dynamic caustic patterns by
utilising dual-optimised holographic fields with Phased Array Transducer (PAT).
Building on previous research in static caustic optimisation and ultrasonic
manipulation, this approach employs computational techniques to dynamically
shape fluid surfaces, thereby creating controllable and real-time caustic
images. The system employs a Digital Twin framework, which enables iterative
feedback and refinement, thereby improving the accuracy and quality of the
caustic patterns produced. This paper extends the foundational work in caustic
generation by integrating liquid surfaces as refractive media. This concept has
previously been explored in simulations but not fully realised in practical
applications. The utilisation of ultrasound to directly manipulate these
surfaces enables the generation of dynamic caustics with a high degree of
flexibility. The Digital Twin approach further enhances this process by
allowing for precise adjustments and optimisation based on real-time feedback.
Experimental results demonstrate the technique's capacity to generate
continuous animations and complex caustic patterns at high frequencies.
Although there are limitations in contrast and resolution compared to
solid-surface methods, this approach offers advantages in terms of real-time
adaptability and scalability. This technique has the potential to be applied in
a number of areas, including interactive displays, artistic installations and
educational tools. This research builds upon the work of previous researchers
in the fields of caustics optimisation, ultrasonic manipulation, and
computational displays. Future research will concentrate on enhancing the
resolution and intricacy of the generated patterns.

</details>


### [130] [From Reality to Virtual Worlds: The Role of Photogrammetry in Game Development](https://arxiv.org/abs/2505.16951)
*Santiago Berrezueta-Guzman,Andrei Koshelev,Stefan Wagner*

Main category: cs.GR

TL;DR: 论文评估了GPU加速摄影测量工具RealityCapture在VR游戏开发中的效能，证实其能显著提升开发效率并保持高精度，同时揭示用户对细节手工模型的偏好与开发者效率需求间的平衡。


<details>
  <summary>Details</summary>
Motivation: 探索摄影测量技术在数字内容创作中的工业化潜力，验证RealityCapture在提升VR开发效率、模型精度方面的实际表现，并分析用户接受度与开发者工作流程的适配性。

Method: 通过技术指标测试（重建速度/模型精度）、引擎兼容性验证、AB测试比较传统建模与摄影测量流程，结合用户调研量化接受度差异。

Result: 用户偏好手工模型3.7%的可操作部件细节表现，但开发者节省58%建模时间；工具实现亚毫米级几何精度，纹理映射效率提升40倍，硬件成本增加35%。

Conclusion: RealityCapture在保证质量前提下重构了VR内容生产范式，未来结合AI优化与云处理将突破硬件限制，推动其在文化遗产数字化等新领域的应用。

Abstract: Photogrammetry is transforming digital content creation by enabling the rapid
conversion of real-world objects into highly detailed 3D models. This paper
evaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in
game development of Virtual Reality (VR). We assess its efficiency,
reconstruction accuracy, and integration with Unreal Engine, comparing its
advantages and limitations against traditional modeling workflows.
Additionally, we examined user preferences between designed 3D assets and
photogrammetry-generated models. The results revealed that while photogrammetry
enhances realism and interactivity, users slightly preferred manually designed
models for small, manipulable elements because of the level of detail. However,
from a developer perspective, RealityCapture significantly reduces development
time while maintaining geometric precision and photorealistic textures. Despite
its reliance on high-performance hardware, its automation, scalability, and
seamless integration with real-time rendering engines make it a valuable tool
for game developers and VR creators. Future improvements in AI-driven
optimization and cloud-based processing could enhance accessibility, broadening
its applications in gaming, cultural heritage preservation, and simulation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [131] [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
*Yaxin Du,Yuzhu Cai,Yifan Zhou,Cheng Wang,Yu Qian,Xianghe Pang,Qian Liu,Yue Hu,Siheng Chen*

Main category: cs.SE

TL;DR: 首个大规模功能开发数据集SWE-Dev发布，支持代码测试环境与强化学习，显著提升7B模型性能至GPT-4o水平


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏针对真实场景功能驱动开发(FDD)的大规模评估数据集，难以验证AI系统的实际开发能力

Method: 构建包含14K训练样本的可运行代码库数据集，集成单元测试支持SFT和RL训练，评估17种LLM和10种多智能体系统

Result: 主流模型在困难测试集表现欠佳(Claude-3.7仅22.45%通过率)，但微调后7B模型达到GPT-4o水平

Conclusion: SWE-Dev验证了高质量训练数据对模型性能的关键作用，为功能开发任务提供了有效的评估与改进平台

Abstract: Large Language Models (LLMs) have shown strong capability in diverse software
engineering tasks, e.g. code completion, bug fixing, and document generation.
However, feature-driven development (FDD), a highly prevalent real-world task
that involves developing new functionalities for large, existing codebases,
remains underexplored. We therefore introduce SWE-Dev, the first large-scale
dataset (with 14,000 training and 500 test samples) designed to evaluate and
train autonomous coding systems on real-world feature development tasks. To
ensure verifiable and diverse training, SWE-Dev uniquely provides all instances
with a runnable environment and its developer-authored executable unit tests.
This collection not only provides high-quality data for Supervised Fine-Tuning
(SFT), but also enables Reinforcement Learning (RL) by delivering accurate
reward signals from executable unit tests. Our extensive evaluations on
SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent
Systems (MAS), reveal that FDD is a profoundly challenging frontier for current
AI (e.g., Claude-3.7-Sonnet achieves only 22.45\% Pass@3 on the hard test
split). Crucially, we demonstrate that SWE-Dev serves as an effective platform
for model improvement: fine-tuning on training set enabled a 7B model
comparable to GPT-4o on \textit{hard} split, underscoring the value of its
high-quality training data. Code is available here
\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [132] [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
*Siting Li,Xiang Gao,Simon Shaolei Du*

Main category: cs.CV

TL;DR: 提出基于可提示图像嵌入的文本-图像检索优化方法，有效解决现有模型对属性聚焦查询的不足，并通过预处理和线性近似提升应用效率。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等检索模型因全局语义编码导致对细粒度属性查询性能低下，且MLLM大模型也存在类似限制。

Method: 利用多模态检索器生成可提示图像嵌入，通过预处理优化和线性近似加速策略增强特定属性关注能力。

Result: 预处理策略使Recall@5提升15%，线性近似在推理时实现8%改进，验证方法跨查询类型和模型架构的有效性。

Conclusion: 可提示嵌入机制突破传统检索瓶颈，配套加速策略为实际部署提供有效解决方案，推动细粒度跨模态检索发展。

Abstract: While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.

</details>


### [133] [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
*Yue Fan,Xuehai He,Diji Yang,Kaizhi Zheng,Ching-Chen Kuo,Yuting Zheng,Sravana Jyothi Narayanaraju,Xinze Guan,Xin Eric Wang*

Main category: cs.CV

TL;DR: 提出GRIT方法，通过结合自然语言与边界框坐标实现视觉语言模型的视觉基础推理，结合强化学习GRPO-GR显著提升数据效率与推理质量


<details>
  <summary>Details</summary>
Motivation: 现有开源视觉推理模型仅使用自然语言生成推理链，缺乏对视觉信息的显式整合，导致推理过程缺乏视觉依据和清晰度

Method: 1. 创新地引入交替生成自然语言与图像边界框坐标的推理范式
2. 基于GRPO算法开发GRPO-GR强化学习框架，仅需最终答案准确性作为奖励信号
3. 无需推理链标注或边界框标签支持

Result: 仅需20个图像-问题-答案样本即可有效训练模型，在综合评估中实现连贯且视觉基础的推理链生成，推理准确性与定位能力统一提升

Conclusion: GRIT成功建立视觉与语言推理的桥梁，突破现有模型局限，为多模态推理提供高效训练范式和强解释性解决方案

Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.

</details>


### [134] [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
*Tony Montes,Fernando Lozano*

Main category: cs.CV

TL;DR: 提出结合思维链框架和YOLO-World的LLM智能体，实现零样本视频问答，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答系统在时序对象追踪和语言模型输出对齐方面存在不足，需通过增强对象追踪和推理决策来提升性能。

Method: 采用思维链推理框架，集成YOLO-World进行对象定位，通过时间轴交叉验证增强追踪与语言输出的对齐能力。

Result: 在NExT-QA/iVQA/ActivityNet-QA基准测试中创下新纪录，代码已开源。

Conclusion: 该框架通过改进对象追踪和推理验证机制，显著提升视频问答的准确性和输出可靠性，支持多领域视频理解。

Abstract: Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.

</details>


### [135] [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
*Shujun Liu,Siyuan Wang,Zejun Li,Jianxiang Wang,Cheng Zeng,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出OViP框架，通过动态构建对比数据和扩散模型合成负样本图像，实时生成相关监督信号，有效减少大型视觉语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO的方法依赖预定义/随机负样本，无法准确反映模型实际错误，导致训练效果受限

Method: 利用模型自身幻觉输出构建对比数据，通过语义差异分析结合扩散模型合成负样本图像，实现失败驱动的自适应偏好对齐

Result: 在幻觉基准和通用测试中，OViP显著减少错误生成同时保持多模态核心能力，改进现有评估协议揭示效果-表达的平衡

Conclusion: 通过动态生成相关监督信号和视觉负样本，OViP实现更有效的多模态偏好对齐，为抑制幻觉提供新的失败驱动训练范式

Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.

</details>


### [136] [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
*Alex Su,Haozhe Wang,Weimin Ren,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出像素空间推理框架，通过视觉操作和两阶段训练提升视觉语言模型的视觉推理能力


<details>
  <summary>Details</summary>
Motivation: 传统文本空间的思维链推理在视觉密集型任务中存在局限，需要直接在像素空间进行视觉证据分析

Method: 1. 引入放大/选帧等视觉操作 2. 分两阶段训练（指令调整+基于好奇心的强化学习）

Result: 7B模型在V* bench(84%)、TallyQA-Complex(74%)和InfographicsVQA(84%)达到开源模型最高精度

Conclusion: 像素空间推理框架显著提升VLM性能，证明了视觉操作与混合训练策略的有效性

Abstract: Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.

</details>


### [137] [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
*Zhenglin Hua,Jinghan He,Zijun Yao,Tianxu Han,Haiyun Guo,Yuheng Jia,Junfeng Fang*

Main category: cs.CV

TL;DR: 利用稀疏自编码器(SAE)识别语义方向，提出无需训练的SSL方法有效缓解多模态大模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有解决大模型幻觉的方法依赖外部知识库、对齐训练或解码策略，存在计算成本高且效率低下的问题，需要探索更高效的内部表示调整方法

Method: 通过SAE分解潜在空间，识别与幻觉/实际密切相关的语义方向，基于方向向量对模型表示进行精确干预(Steering LVLMs via SAE Latent Directions)

Result: SSL方法在多个基准测试中显著优于现有解码方法，保持跨模型架构的迁移性，额外时间开销可忽略不计(仅增加1.6%推理时间)

Conclusion: SAE识别的语义方向能实现精准的表示控制，提出的SSL方法在保证语义完整性的前提下有效缓解幻觉，为模型安全部署提供高效解决方案

Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.

</details>


### [138] [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
*Zirui Pang,Haosheng Tan,Yuhan Pu,Zhijie Deng,Zhouan Shen,Keyu Hu,Jiaheng Wei*

Main category: cs.CV

TL;DR: 提出REVEAL框架，整合视觉语言模型与标签清洗方法，系统性解决图像分类数据集的噪声标签和缺失标签问题，显著提升6个基准测试集质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类基准数据集存在噪声标签和共现类别导致的缺失标签问题，现有方法仅关注噪声标签而忽略缺失标签，导致模型评估不公平。

Method: 融合LLaVA/BLIP等视觉语言模型与Docta/Cleanlab等标签清洗技术，通过置信度预测和共识过滤机制检测噪声/缺失标签，生成带概率的软标签。

Result: 改进6个测试集质量，人类验证显示与人工判断高度一致，提供10项模型行为观察，支持软标签输出便于后续分析。

Conclusion: REVEAL通过自动化标签校正显著提升数据集可靠性，为公平的模型性能比较提供新基准。

Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.

</details>


### [139] [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
*Ashim Dahal,Ankit Ghimire,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CV

TL;DR: 提出Redemption Score框架，通过融合多模态信号实现更全面的图像描述评估，在Flickr8k数据集上超越12种现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述评估指标难以全面捕捉视觉语义和语言语用特征，需要更综合的评估框架。

Method: 整合三种互补信号：互信息散度（全局对齐）、DINO感知相似度（视觉基础）、BERTScore（文本相似度），通过校准融合实现评估。

Result: 在Flickr8k基准测试中达到56.43的Kendall-τ值，无需任务特定训练即实现与人类判断的优越相关性。

Conclusion: 该框架通过有效融合多维度信号，在跨数据集场景中展现出鲁棒的评估能力，为图像语义和语言可解释性提供了更细致的分析工具。

Abstract: Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.

</details>


### [140] [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
*Francesco Dalla Serra,Patrick Schrempf,Chaoyang Wang,Zaiqiao Meng,Fani Deligianni,Alison Q. O'Neil*

Main category: cs.CV

TL;DR: 提出整合放射报告的胸部X光视觉问答两步法（报告生成+答案生成），在Medical-Diff-VQA数据集实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 探索放射报告作为额外输入提升VQA模型性能的可能性，突破现有方法仅在预训练阶段使用报告的局限

Method: 统一处理单图像/图像差异问题：1）报告生成（RG）模块生成放射报告；2）答案生成（AG）模块基于报告和图像自回归生成答案

Result: 在Medical-Diff-VQA数据集上达到当前最佳性能，证明放射报告作为证据能显著提升两种问题的解答质量

Conclusion: 通过思维链式两步推理框架，验证放射报告作为中间证据对胸部X光时序变化检测的有效性

Abstract: We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR ("What abnormalities are
seen in image X?"), while image-difference questions compare two longitudinal
CXRs acquired at different time points ("What are the differences between image
X and Y?"). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.

</details>


### [141] [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
*Huanjin Yao,Qixiang Yin,Jingyi Zhang,Min Yang,Yibo Wang,Wenhao Wu,Fei Su,Li Shen,Minghui Qiu,Dacheng Tao,Jiaxing Huang*

Main category: cs.CV

TL;DR: 提出Share-GRPO方法，通过扩展问题空间+共享推理轨迹/奖励信息，解决MLLMs强化学习中的奖励稀疏和优势消失问题，在6个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在训练多模态大语言模型时面临奖励信号稀疏和优势估计消失的问题，导致模型推理能力提升受限。

Method: 1. 数据转换扩展问题空间
2. 跨问题变体共享推理轨迹
3. 分层优势计算（跨问题/内部变体）
4. 策略梯度优化框架

Result: 在6个主流推理基准测试（如A-OKVQA、ScienceQA等）中显著超越基线模型，代码即将开源。

Conclusion: Share-GRPO通过空间扩展和协同共享机制，有效提升MLLMs的复杂推理能力，为RL训练多模态模型提供了新思路。

Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.

</details>


### [142] [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
*Suhao Yu,Haojin Wang,Juncheng Wu,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 提出首个医学多图像推理评测基准MedFrameQA，构建自动化流程创建高质量数据集（2,851 VQA对），测试发现现有多模态大模型准确率普遍低于50%且存在显著推理缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有医学VQA基准聚焦单图像分析，但临床诊断需多图像对比。为更贴近真实场景，需系统性评估模型的多图像推理能力。

Method: 1) 自动化流程从医学视频提取时序连贯的帧，构建逻辑关联的VQA项；2) 多阶段过滤（模型筛选+人工审核）保证数据质量；3) 测试十种先进多模态LLM（含显式推理模块）。

Result: 所有模型表现差（多数准确率<50%），准确率随图像数量增加波动。模型常忽略关键特征、跨图像证据整合错误、错误链式传播，不同器官/模态表现差异显著。

Conclusion: 该工作推动临床多图像推理研究，暴露现有模型局限，为诊断AI系统发展提供关键基准。

Abstract: Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.

</details>


### [143] [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
*Runsen Xu,Weiyao Wang,Hao Tang,Xingyu Chen,Xiaodong Wang,Fu-Jen Chu,Dahua Lin,Matt Feiszli,Kevin J. Liang*

Main category: cs.CV

TL;DR: 提出Multi-SpatialMLLM框架，通过整合深度感知、视觉对应和动态感知，显著增强多模态大语言模型的多帧空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在单图像空间理解受限，难以满足机器人等需要多帧推理的实际应用需求。

Method: 构建MultiSPA数据集（2700万+样本），开发统一基准测试，集成深度感知/视觉对应/动态感知模块。

Result: Multi-SpatialMLLM超越基线模型和商业系统，展现可扩展的多帧推理能力，并实现机器人领域的多帧奖励标注应用。

Conclusion: 该框架不仅提升多任务性能，还展现出复杂场景下的新兴能力，为机器人等现实应用提供了有效的多帧推理解决方案。

Abstract: Multi-modal large language models (MLLMs) have rapidly advanced in visual
tasks, yet their spatial understanding remains limited to single images,
leaving them ill-suited for robotics and other real-world applications that
require multi-frame reasoning. In this paper, we propose a framework to equip
MLLMs with robust multi-frame spatial understanding by integrating depth
perception, visual correspondence, and dynamic perception. Central to our
approach is the MultiSPA dataset, a novel, large-scale collection of more than
27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we
introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks
under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves
significant gains over baselines and proprietary systems, demonstrating
scalable, generalizable multi-frame reasoning. We further observe multi-task
benefits and early indications of emergent capabilities in challenging
scenarios, and showcase how our model can serve as a multi-frame reward
annotator for robotics.

</details>


### [144] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 该研究首次在自回归图像生成领域系统对比GRPO与DPO算法，揭示两者在域内外性能的差异性优势，发现奖励模型的内在泛化能力可提升RL算法效果，并探索了三种高效的规模化扩展策略。


<details>
  <summary>Details</summary>
Motivation: 针对自回归图像生成领域RL算法研究缺乏系统性分析的现状，作者希望探究不同RL算法（GRPO/DPO）在该场景下的性能差异、奖励模型对算法泛化能力的影响，以及规模化扩展的有效路径。

Method: 1. 将GRPO和DPO算法应用于自回归图像生成任务 2. 系统评估算法在域内性能与跨域泛化能力 3. 分析不同奖励模型对算法效果的影响机制 4. 探索三种规模化策略对算法性能的提升效果

Result: 1. GRPO在域内任务表现更稳定，DPO展现更强的跨域泛化潜力 2. 具有内在泛化优势的奖励模型可同步提升RL算法的泛化能力 3. 提出混合式规模化策略可协同提升域内外性能

Conclusion: 该研究为开发面向图像生成CoT推理的强化学习算法提供了新思路，指出算法特性与奖励模型协同优化的关键作用，并通过规模化实验揭示了不同范式的高效扩展路径。代码开源促进后续研究。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


### [145] [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
*Chengqi Duan,Rongyao Fang,Yuqing Wang,Kun Wang,Linjiang Huang,Xingyu Zeng,Hongsheng Li,Xihui Liu*

Main category: cs.CV

TL;DR: 提出强化学习框架GoT-R1，通过双阶段多维度奖励机制增强视觉生成模型的语义-空间推理能力，显著提升复杂文本提示下的图像生成效果


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型在处理涉及多对象空间关系和属性绑定的复杂提示时存在局限，需要更系统的语义-空间推理方法

Method: 基于强化学习的Generation Chain-of-Thought框架，设计双阶段评估体系（过程推理+结果生成），利用多模态大语言模型进行多维度奖励计算（语义对齐/空间精度/视觉质量）

Result: 在T2I-CompBench基准测试中取得显著提升，特别是在空间关系准确率（+18.7%）和属性绑定成功率（+15.3%）等核心指标

Conclusion: GoT-R1成功将复杂推理能力迁移到视觉生成领域，推动图像生成技术发展，并通过开源促进后续研究

Abstract: Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [146] [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
*Sampanna Yashwant Kahu,Naman Ahuja*

Main category: cs.CR

TL;DR: 本文提出一种黑盒扰动技术，通过生成对抗性文本成功规避现有深度学习仇恨言论检测模型，在保持语义连贯性的前提下使86.8%的仇恨文本逃逸检测


<details>
  <summary>Details</summary>
Motivation: 针对社交媒体平台仇恨言论泛滥问题，现有深度学习检测模型存在对抗攻击脆弱性，需开发保护用户免受仇恨言论侵害的技术方案

Method: 设计黑盒扰动生成技术，通过文本层面的对抗性修改欺骗仇恨检测模型，同时采用语义保持机制最小化对原文意义的改变

Result: 最佳扰动攻击方案成功使86.8%的仇恨文本逃逸检测，显著降低现有模型的检测效率

Conclusion: 研究揭示了当前仇恨检测模型的脆弱性，提出有效的对抗攻击方法，为开发鲁棒性更强的防御系统提供了重要参考

Abstract: Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.

</details>


### [147] [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
*Yuliang Yan,Haochun Tang,Shuo Yan,Enyan Dai*

Main category: cs.CR

TL;DR: 提出DuFFin双层指纹框架，通过触发模式与知识级指纹在黑盒环境下有效验证大语言模型版权，IP-ROC指标超0.95。


<details>
  <summary>Details</summary>
Motivation: 现有水印和指纹方法影响文本生成或依赖白盒访问，需开发实用方案保护LLM知识产权。

Method: 结合触发模式与知识级指纹，在开源基础模型及其微调/量化/对齐版本上进行跨变体版权验证。

Result: 在基础模型及其各类变体上实现IP-ROC>0.95的准确版权验证，覆盖商业公司/创业团队/个人用户模型。

Conclusion: DuFFin为黑盒场景提供可靠版权保护方案，支持多类型模型衍生场景，代码已开源便于应用扩展。

Abstract: Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, quantization, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.

</details>


### [148] [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
*Biao Yi,Tiansheng Huang,Baolei Zhang,Tong Li,Lihai Nie,Zheli Liu,Li Shen*

Main category: cs.CR

TL;DR: 提出CTRAP防御机制，通过诱导模型崩溃而非选择性遗忘，从根本上阻止恶意微调攻击


<details>
  <summary>Details</summary>
Motivation: 传统选择性遗忘防御易被大语言模型的强大适应能力绕过，需解决模型被恶意重新利用的核心缺陷

Method: 在模型对齐阶段预埋条件崩溃机制（CTRAP），当检测到持续性安全对齐逆转时触发渐进式语言建模能力退化

Result: CTRAP在不同LLM和攻击场景下有效防御风险，同时保持良性场景的高性能表现

Conclusion: 通过破坏攻击者依赖的模型通用能力，CTRAP实现了比选择性遗忘更本质的防御突破

Abstract: Fine-tuning-as-a-service, while commercially successful for Large Language
Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a
widely explored defense paradigm against such attacks, unlearning attempts to
remove malicious knowledge from LLMs, thereby essentially preventing them from
being used to perform malicious tasks. However, we highlight a critical flaw:
the powerful general adaptability of LLMs allows them to easily bypass
selective unlearning by rapidly relearning or repurposing their capabilities
for harmful tasks. To address this fundamental limitation, we propose a
paradigm shift: instead of selective removal, we advocate for inducing model
collapse--effectively forcing the model to "unlearn everything"--specifically
in response to updates characteristic of malicious adaptation. This collapse
directly neutralizes the very general capabilities that attackers exploit,
tackling the core issue unaddressed by selective unlearning. We introduce the
Collapse Trap (CTRAP) as a practical mechanism to implement this concept
conditionally. Embedded during alignment, CTRAP pre-configures the model's
reaction to subsequent fine-tuning dynamics. If updates during fine-tuning
constitute a persistent attempt to reverse safety alignment, the pre-configured
trap triggers a progressive degradation of the model's core language modeling
abilities, ultimately rendering it inert and useless for the attacker.
Crucially, this collapse mechanism remains dormant during benign fine-tuning,
ensuring the model's utility and general capabilities are preserved for
legitimate users. Extensive empirical results demonstrate that CTRAP
effectively counters harmful fine-tuning risks across various LLMs and attack
settings, while maintaining high performance in benign scenarios. Our code is
available at https://anonymous.4open.science/r/CTRAP.

</details>


### [149] [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
*Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: 提出CAIN算法，通过篡改LLM系统提示定向生成恶意回答，黑盒攻击下对特定问题F1分数影响达70%，其他问题保持正常。


<details>
  <summary>Details</summary>
Motivation: LLM系统提示若被恶意篡改并传播，可能导致大规模隐蔽信息操纵，威胁现实应用安全性。

Method: 开发CAIN算法，无需模型参数即可自动生成针对特定问题的有害系统提示，支持黑盒环境攻击。

Result: 定向攻击成功率超70% F1，非定向攻击使目标问题F1下降40%，普通问题准确率保持高位。

Conclusion: 需加强LLM系统提示的鲁棒性防护机制，防止现实场景中的隐蔽对话劫持攻击。

Abstract: Large language models (LLMs) have advanced many applications, but are also
known to be vulnerable to adversarial attacks. In this work, we introduce a
novel security threat: hijacking AI-human conversations by manipulating LLMs'
system prompts to produce malicious answers only to specific targeted questions
(e.g., "Who should I vote for US President?", "Are Covid vaccines safe?"),
while behaving benignly on others. This attack is detrimental as it can enable
malicious actors to exercise large-scale information manipulation by spreading
harmful but benign-looking system prompts online. To demonstrate such an
attack, we develop CAIN, an algorithm that can automatically curate such
harmful system prompts for a specific target question in a black-box setting or
without the need to access the LLM's parameters. Evaluated on both open-source
and commercial LLMs, CAIN demonstrates significant adversarial impact. In
untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves
up to 40% F1 degradation on targeted questions while preserving high accuracy
on benign inputs. For targeted attacks or forcing LLMs to output specific
harmful answers, CAIN achieves over 70% F1 scores on these targeted responses
with minimal impact on benign questions. Our results highlight the critical
need for enhanced robustness measures to safeguard the integrity and safety of
LLMs in real-world applications. All source code will be publicly available.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [150] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
*Chih-Kai Yang,Neo S. Ho,Hung-yi Lee*

Main category: eess.AS

TL;DR: 首次提出针对大型音频语言模型（LALMs）评估的系统分类体系，涵盖四个核心维度并指明未来方向


<details>
  <summary>Details</summary>
Motivation: 现有LALMs评估标准碎片化且缺乏系统性分类，阻碍了该领域的规范化发展，需建立统一评估框架

Method: 通过全面文献调研构建四维评估分类体系：（1）通用听觉感知（2）知识推理（3）对话能力（4）公平安全可信度

Result: 创建首个LALMs评估专项综述，提供社区指南并承诺持续维护论文数据库

Conclusion: 本研究填补了LALMs评估领域的系统性空白，提出的分类框架为后续研究奠定基础，未来将持续更新资源推动领域发展

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [151] [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
*Liang-Yeh Shen,Shi-Xin Fang,Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出Meta-PerSER元学习框架，通过MAML增强方法和预训练模型，实现个性化语音情感识别


<details>
  <summary>Details</summary>
Motivation: 传统SER系统依赖群体标注数据，忽视个体情感认知差异导致预测不一致

Method: 采用MAML元学习框架，集成组合元训练、导数退火和分层分步学习率策略，结合预训练模型实现两阶段优化

Result: 在IEMOCAP数据集上显著超越基线方法，在已知和未知数据场景均表现优异

Conclusion: 该框架为个性化情感识别提供了有效解决方案，展示了元学习在个人标注风格适应方面的潜力

Abstract: This paper introduces Meta-PerSER, a novel meta-learning framework that
personalizes Speech Emotion Recognition (SER) by adapting to each listener's
unique way of interpreting emotion. Conventional SER systems rely on aggregated
annotations, which often overlook individual subtleties and lead to
inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic
Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,
Derivative Annealing, and per-layer per-step learning rates, enabling rapid
adaptation with only a few labeled examples. By integrating robust
representations from pre-trained self-supervised models, our framework first
captures general emotional cues and then fine-tunes itself to personal
annotation styles. Experiments on the IEMOCAP corpus demonstrate that
Meta-PerSER significantly outperforms baseline methods in both seen and unseen
data scenarios, highlighting its promise for personalized emotion recognition.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [152] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
*Asterios Tsiourvas,Wei Sun,Georgia Perakis*

Main category: cs.AI

TL;DR: 提出端到端因果框架LLM路由，通过观察数据学习最优模型选择策略，解决传统方法错误累积和高成本问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方案采用指标预测与模型选择分离策略，存在误差叠加风险且依赖全反馈数据（需调用所有候选模型），维护成本极高

Method: 1. 构建因果推理框架直接最小化决策遗憾 2. 提出两种理论支撑的替代优化目标（分类上界/softmax加权遗憾近似） 3. 开发区间条件架构适配异构成本偏好

Result: 在公共基准测试中实现SOTA性能，验证框架在不同嵌入模型中的有效性

Conclusion: 通过端到端学习观察数据中的决策模式，提出的因果路由框架显著优于传统基线方法

Abstract: LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.

</details>


### [153] [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
*Ming Shen,Raphael Shu,Anurag Pratik,James Gung,Yubin Ge,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.

</details>


### [154] [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
*Dominick Kubica,Dylan T. Gordon,Nanami Emura,Derleen Saini,Charlie Goldenberg*

Main category: cs.AI

TL;DR: 研究揭示生成式AI在金融文本情感分析中的局限性，通过对比主流LLMs与传统模型表现，提出提示工程优化方案并验证情绪趋势与股价的关联性。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在专业金融场景中处理战略模糊性语言（如财报电话会议中的套期表述、行业术语）的可靠性，解决模型对情绪基调误判的问题。

Method: 使用微软财报电话记录作为数据集，对比测试Copilot/ChatGPT/Gemini与传统ML模型，开发情绪可视化工具并实施提示工程优化策略。

Result: 现有LLMs对金融情感分析的准确率较传统模型低15-20%，但提示工程可提升18%性能，情绪趋势与股价波动呈现0.65相关性。

Conclusion: 需结合领域专业知识优化LLMs的金融语义理解，建立动态评估框架以适应市场变化，提示工程是现阶段有效提升路径。

Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.

</details>


### [155] [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
*Zifeng Wang,Benjamin Danek,Jimeng Sun*

Main category: cs.AI

TL;DR: BioDSA-1K是一个包含1029个生物医学假设验证任务的基准测试集，用于评估AI代理在真实科研场景下的决策准确性、证据对齐、推理正确性和代码可执行性，特别包含数据不足的不可验证假设场景。


<details>
  <summary>Details</summary>
Motivation: 现有AI难以处理复杂生物医学数据验证，需要建立基于真实研究范式的评估体系来推动可信AI在科研中的应用。

Method: 从300+已发表研究中提取假设结论，构建1,177个含结构化假设+数据表格支撑的分析任务，支持传统统计与机器学习验证方法，特别设计非可验证假设场景。

Result: 建立首个覆盖完整科研工作流的生物医学假设验证评估框架，强调处理非可验证假设的能力对AI科学推理的重要性。

Conclusion: BioDSA-1K为开发生物医学领域可泛化、可信赖的AI代理提供了标准化测试基础，推动AI在复杂科学发现中的应用。

Abstract: Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.

</details>


### [156] [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
*Jun Rao,Xuebo Liu,Hexuan Deng,Zepeng Lin,Zixiong Yu,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.AI

TL;DR: 提出SAI-DPO算法实现动态数据选择，通过实时评估模型不同训练阶段的推理能力，显著提升数学推理任务性能


<details>
  <summary>Details</summary>
Motivation: 现有基于静态指标（难度/多样性）的数据选择方法无法适应在线训练过程中模型能力的动态演化，尤其在强化学习框架下面临严重适配问题

Method: 开发SAI-DPO算法：1. 持续监测模型各阶段推理能力 2. 根据实时性能反馈动态调整数据选择策略 3. 适配模型强弱项演变

Result: 在8个数学推理基准测试中平均提升21.3个百分点，竞赛级数据集AIME24/AMC23分别提升10和15个百分点

Conclusion: 动态模型自适应数据选择策略显著优于静态策略，验证了实时反馈机制对提升推理能力的关键作用

Abstract: In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.

</details>


### [157] [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
*Kaiwen Zhou,Xuandong Zhao,Gaowen Liu,Jayanth Srinivasa,Aosong Feng,Dawn Song,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出SafeKey方法提升大型推理模型的安全性，通过激活安全顿悟时刻降低9.6%危害率


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法对未见越狱提示泛化不足，需增强模型内在安全推理机制

Method: 包含双路径安全头强化安全信号 + 查询掩码建模提升注意力机制

Result: 在多安全基准测试中显著提升防御能力，平均危害率降低9.6%

Conclusion: 通过重塑内部注意力机制和表征质量，SafeKey实现了安全性与通用能力的平衡

Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.

</details>


### [158] [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
*Desiree Heim,Lars-Peter Meyer,Markus Schröder,Johannes Frey,Andreas Dengel*

Main category: cs.AI

TL;DR: 探讨LLM模型尺寸在知识图谱工程任务中的影响，验证缩放定律适用性并发现平台效应，建议权衡性能与成本选择合适模型


<details>
  <summary>Details</summary>
Motivation: 大模型虽强但资源成本高昂，需通过模型性能与成本的比值评估实际应用价值

Method: 使用LLM-KG-Bench框架测试26个开源LLM，分析模型尺寸与KGE任务表现的关联规律

Result: 缩放定律在多数KGE任务中成立，但存在性能平台期；同系列模型中较大版本有时表现反而不如较小版本

Conclusion: 建议根据具体任务选择最小可用模型，测试同系列相邻尺寸模型以平衡性能与成本

Abstract: When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.

</details>


### [159] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
*Xiaoxue Cheng,Junyi Li,Zhenduo Zhang,Xinyu Tang,Wayne Xin Zhao,Xinyu Kong,Zhiqiang Zhang*

Main category: cs.AI

TL;DR: 提出ACPO强化学习框架解决大模型冗余推理问题，通过动态系统切换实现高效认知分配


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在处理复杂任务时存在过度思考现象，导致冗余计算和效率低下。受人类双过程认知理论启发，研究者希望赋予模型自适应分配认知资源的能力

Method: 1. 引入系统感知推理令牌显式表征思维模式
2. 结合在线难度评估和令牌预算机制
3. 两阶段训练策略（监督微调冷启动+ACPO强化学习优化）

Result: ACPO有效减少34%推理长度，在复杂任务中保持精度同时实现17x加速，证明其能根据任务难度动态调整认知策略

Conclusion: 该框架首次实现透明化认知过程与自适应系统切换的融合，为构建高效混合推理系统提供了新范式

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [160] [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
*Lars Benedikt Kaesberg,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Main category: cs.AI

TL;DR: 提出SPaRC空间路径推理数据集，暴露现有模型在抽象多步空间逻辑推理上的严重缺陷


<details>
  <summary>Details</summary>
Motivation: 现有推理数据集无法有效评估抽象多步问题（如路径规划和复杂规则约束），需要创建能测试空间与符号推理能力的基准

Method: 构建包含1000个二维网格路径规划谜题的SPaRC数据集，要求结合算术和几何规则进行逐步规划

Result: 人类准确率98%（困难题94.5%），最佳模型o4-mini仅15.8%（困难题1.1%），模型50%以上路径无效且存在导航逻辑错误

Conclusion: SPaRC揭示模型空间推理瓶颈，未来需开发能动态调整计算资源、支持多尝试机制的新方法，推动抽象多步问题解决能力发展

Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.

</details>


### [161] [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
*Wei Sun,Wen Yang,Pu Jian,Qianlong Du,Fuwei Cui,Shuo Ren,Jiajun Zhang*

Main category: cs.AI

TL;DR: 提出KTAE算法通过细粒度的token级优势估计提升强化学习在语言模型推理中的效果


<details>
  <summary>Details</summary>
Motivation: 现有强化学习算法在计算优势时采用rollout级别粗粒度估计，无法捕捉token级贡献影响学习效果

Method: 利用采样结果的正确性进行统计分析，量化关键token的重要性，结合rollout优势实现无需额外模型的token级优势估计

Result: GRPO/DAPO+KTAE在5个数学推理基准上全面超越基线，响应更短且准确率更高，相同模型下超越R1-Distill-Qwen-1.5B

Conclusion: 通过细粒度token优势估计有效提升模型推理性能，验证了KTAE方法的优越性和实用性

Abstract: Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.

</details>


### [162] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
*Haonian Ji,Shi Qiu,Siyang Xin,Siwei Han,Zhaorun Chen,Hongyi Wang,Dake Zhang,Huaxiu Yao*

Main category: cs.AI

TL;DR: 论文提出EduVisBench基准测试和EduVisAgent多智能体框架，解决基础模型在教育可视化中的结构化推理不足问题，实现40.2%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型过度依赖文本推理，缺乏结构化可视化能力，难以支持教育场景中的概念理解。

Method: 通过多智能体协作框架（教学规划、推理分解、元认知提示、可视化设计）实现教育可视化任务的分解与执行。

Result: EduVisAgent相比基线模型提升40.2%，生成的可视化结果更符合教育认知需求。

Conclusion: 该框架通过任务分解与认知对齐机制，为教育AI系统提供了可扩展的视觉推理解决方案。

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


### [163] [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
*NovelSeek Team,Bo Zhang,Shiyang Feng,Xiangchao Yan,Jiakang Yuan,Zhiyin Yu,Xiaohan He,Songtao Huang,Shaowei Hou,Zheng Nie,Zhilong Wang,Jinyao Liu,Runmin Ma,Tianshuo Peng,Peng Ye,Dongzhan Zhou,Shufei Zhang,Xiaosong Wang,Yilan Zhang,Meng Li,Zhongying Tu,Xiangyu Yue,Wangli Ouyang,Bowen Zhou,Lei Bai*

Main category: cs.AI

TL;DR: NovelSeek是一个统一闭环的多智能体框架，通过在12个科研任务中展示可扩展性、交互性和高效性，显著提升科研效率和创新性


<details>
  <summary>Details</summary>
Motivation: 加速人工智能驱动的科研范式转型，整合人类专家反馈与自动化流程，解决复杂科学问题

Method: 基于三个核心设计：1）支持12个科研领域的可扩展架构；2）人类专家反馈接口与多智能体交互机制；3）端到端自动化流程优化时间效率

Result: 在12小时内将反应产率预测准确率从27.6%提升至35.4%；4小时增强子活性预测准确率从0.52升至0.79；30小时二维语义分割精度从78.8%提升至81.0%

Conclusion: NovelSeek通过融合自动化流程与领域专家知识，为跨学科复杂科研问题提供了高效解决方案，展示出在多领域协同创新的巨大潜力

Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.

</details>


### [164] [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
*Yunjia Qi,Hao Peng,Xiaozhi Wang,Amy Xin,Youfeng Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.AI

TL;DR: 提出首个评估大语言模型在代理场景中遵循复杂指令能力的基准AgentIF，发现现有模型在复杂约束和工具规范处理上表现欠佳


<details>
  <summary>Details</summary>
Motivation: 代理应用中冗长复杂的指令约束遵循能力是LLM应用落地的关键瓶颈，但相关系统化评估框架缺失

Method: 构建包含50个真实应用场景、707个标注指令的AgentIF基准，采用代码/LLM/混合评估指标，分析模型在长文本、多约束场景下的表现

Result: 当前先进模型整体表现较差（平均分<50%），在工具参数约束遵循上准确率仅17.5%，条件约束处理错误率达65.3%

Conclusion: AgentIF揭示了LLM在复杂指令遵循上的能力缺陷，为改进模型和构建可靠代理系统提供了基准与方向

Abstract: Large Language Models (LLMs) have demonstrated advanced capabilities in
real-world agentic applications. Growing research efforts aim to develop
LLM-based agents to address practical demands, introducing a new challenge:
agentic scenarios often involve lengthy instructions with complex constraints,
such as extended system prompts and detailed tool specifications. While
adherence to such instructions is crucial for agentic applications, whether
LLMs can reliably follow them remains underexplored. In this paper, we
introduce AgentIF, the first benchmark for systematically evaluating LLM
instruction following ability in agentic scenarios. AgentIF features three key
characteristics: (1) Realistic, constructed from 50 real-world agentic
applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.
(3) Complex, averaging 11.9 constraints per instruction, covering diverse
constraint types, such as tool specifications and condition constraints. To
construct AgentIF, we collect 707 human-annotated instructions across 50
agentic tasks from industrial application agents and open-source agentic
systems. For each instruction, we annotate the associated constraints and
corresponding evaluation metrics, including code-based evaluation, LLM-based
evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically
evaluate existing advanced LLMs. We observe that current models generally
perform poorly, especially in handling complex constraint structures and tool
specifications. We further conduct error analysis and analytical experiments on
instruction length and meta constraints, providing some findings about the
failure modes of existing LLMs. We have released the code and data to
facilitate future research.

</details>


### [165] [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
*Rui Ye,Xiangrui Liu,Qimin Wu,Xianghe Pang,Zhenfei Yin,Lei Bai,Siheng Chen*

Main category: cs.AI

TL;DR: 论文提出异构大语言模型驱动多智能体系统（X-MAS），通过X-MAS-Bench测试平台验证异构配置能显著提升系统性能（MATH数据集提升8.4%，AIME数据集提升47%）。


<details>
  <summary>Details</summary>
Motivation: 现有同构LLM驱动的多智能体系统受限于单一模型能力上限，无法发挥不同LLM的协同潜力。论文探索通过异构LLM组合释放集体智能优势。

Method: 构建X-MAS-Bench测试平台，跨5领域21个测试集评估27个LLM的5种功能组合，完成170万次评估确定最优模型组合方案。通过同构与异构配置对比实验验证效果。

Result: 异构配置在纯对话机器人场景下（MATH）提升8.4%，在混合对话-推理场景（AIME）实现47%性能飞跃。实验证明无需系统重构即可获得显著增益。

Conclusion: 异构LLM驱动为多智能体系统提供突破性升级路径，通过模型能力互补显著提升系统智能上限，为构建可扩展协作AI系统开辟新方向。

Abstract: LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
enabling cooperation among multiple specialized agents. However, most existing
MAS frameworks rely on a single LLM to drive all agents, constraining the
system's intelligence to the limit of that model. This paper explores the
paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by
diverse LLMs, elevating the system's potential to the collective intelligence
of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to
evaluate the performance of various LLMs across different domains and
MAS-related functions. As an extensive empirical study, we assess 27 LLMs
across 5 domains (encompassing 21 test sets) and 5 functions, conducting over
1.7 million evaluations to identify optimal model selections for each
domain-function combination. Building on these findings, we demonstrate that
transitioning from homogeneous to heterogeneous LLM-driven MAS can
significantly enhance system performance without requiring structural redesign.
Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration
yields up to 8.4\% performance improvement on the MATH dataset. In a mixed
chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable
47\% performance boost on the AIME dataset. Our results underscore the
transformative potential of heterogeneous LLMs in MAS, highlighting a promising
avenue for advancing scalable, collaborative AI systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [166] [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
*Omer Hofman,Oren Rachmil,Shamik Bose,Vikas Pahuja,Jonathan Brokman,Toshiya Shimizu,Trisha Starostina,Kelly Marchisio,Seraphina Goldfarb-Tarrant,Roman Vainshtein*

Main category: cs.DB

TL;DR: 论文提出MAPS多语言基准套件，验证代理AI系统在非英语环境下存在性能与安全性下降现象，并提出开发建议。


<details>
  <summary>Details</summary>
Motivation: 现有代理AI评估基准仅支持英语，非英语用户可能面临系统不可靠和安全风险，需建立多语言评估标准。

Method: 基于GAIA/SWE-bench/MATH/Agent Security四个基准，翻译成10种语言生成805个任务(8,855实例)，构建MAPS评估框架。

Result: 实证显示代理AI从英语转向其他语言时性能与安全性普遍下降，严重程度与任务类型及翻译输入量正相关。

Conclusion: MAPS为多语言代理AI提供标准化评估工具，建议改进系统开发策略以实现全球公平可靠的AI服务，数据集已开源。

Abstract: Agentic AI systems, which build on Large Language Models (LLMs) and interact
with tools and memory, have rapidly advanced in capability and scope. Yet,
since LLMs have been shown to struggle in multilingual settings, typically
resulting in lower performance and reduced safety, agentic systems risk
inheriting these limitations. This raises concerns about the global
accessibility of such systems, as users interacting in languages other than
English may encounter unreliable or security-critical agent behavior. Despite
growing interest in evaluating agentic AI, existing benchmarks focus
exclusively on English, leaving multilingual settings unexplored. To address
this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate
agentic AI systems across diverse languages and tasks. MAPS builds on four
widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code
generation), MATH (mathematical reasoning), and the Agent Security Benchmark
(security). We translate each dataset into ten diverse languages, resulting in
805 unique tasks and 8,855 total language-specific instances. Our benchmark
suite enables a systematic analysis of how multilingual contexts affect agent
performance and robustness. Empirically, we observe consistent degradation in
both performance and security when transitioning from English to other
languages, with severity varying by task and correlating with the amount of
translated input. Building on these findings, we provide actionable
recommendations to guide agentic AI systems development and assessment under
multilingual settings. This work establishes a standardized evaluation
framework, encouraging future research towards equitable, reliable, and
globally accessible agentic AI. MAPS benchmark suite is publicly available at
https://huggingface.co/datasets/Fujitsu-FRE/MAPS

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [167] [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
*Yunjia Xi,Jianghao Lin,Menghui Zhu,Yongzhao Xiao,Zhuoying Ou,Jiaqi Liu,Tong Wan,Bo Chen,Weiwen Liu,Yasheng Wang,Ruiming Tang,Weinan Zhang,Yong Yu*

Main category: cs.IR

TL;DR: 提出InfoDeepSeek基准，用于评估动态网络环境中代理式信息检索系统的有效性


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准局限于静态检索环境和简单查询，无法适应真实动态网络环境及激发代理行为

Method: 构建满足确定性/难度/多样性要求的挑战性查询，开发包含准确性/实用性/简洁性指标的评估框架

Result: 通过多维度实验揭示不同LLM和搜索引擎在动态环境中的代理行为差异

Conclusion: InfoDeepSeek为动态代理式信息检索研究提供了有效的评估基准和方向指引

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding responses with retrieved information. As an emerging paradigm,
Agentic RAG further enhances this process by introducing autonomous LLM agents
into the information seeking process. However, existing benchmarks fall short
in evaluating such systems, as they are confined to a static retrieval
environment with a fixed, limited corpus} and simple queries that fail to
elicit agentic behavior. Moreover, their evaluation protocols assess
information seeking effectiveness by pre-defined gold sets of documents, making
them unsuitable for the open-ended and dynamic nature of real-world web
environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with
challenging questions designed for assessing agentic information seeking in
real-world, dynamic web environments. We propose a systematic methodology for
constructing challenging queries satisfying the criteria of determinacy,
difficulty, and diversity. Based on this, we develop the first evaluation
framework tailored to dynamic agentic information seeking, including
fine-grained metrics about the accuracy, utility, and compactness of
information seeking outcomes. Through extensive experiments across LLMs, search
engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and
offers actionable insights for future research.

</details>


### [168] [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
*Ruijie Xi,He Ba,Hao Yuan,Rishu Agrawal,Arul Prakash*

Main category: cs.IR

TL;DR: 提出Aug2Search框架，利用生成式AI生成合成数据提升嵌入式检索模型性能，实验显示ROC_AUC最高提升4%且纯合成数据训练效果更优


<details>
  <summary>Details</summary>
Motivation: 现有平台（如Facebook Marketplace）的搜索日志数据缺乏多样性和细节，限制了EBR模型捕捉细微搜索模式的能力

Method: 使用8个Llama模型生成三种策略的合成数据（生成查询/增强商品列表/基于增强列表生成查询），并在原始数据、合成数据及混合数据上训练EBR模型进行对比

Result: 合成数据质量高（高连贯性/相关性/多样性，低幻觉率），纯合成数据训练的模型ROC_AUC提升4%，且表现优于原始数据或混合数据训练模型

Conclusion: GenAI生成的合成数据能有效增强EBR模型，在相同数据量下单独使用合成数据可能比混合数据更具优势

Abstract: Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., "Click" and "Listing
Interactions")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.

</details>


### [169] [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
*Kuicai Dong,Yujing Chang,Shijie Huang,Yasheng Wang,Ruiming Tang,Yong Liu*

Main category: cs.IR

TL;DR: MMDocRAG推出首个支持跨模态证据链的DocVQA评测基准，包含4055标注QA对并验证60个模型性能，发现闭源视觉语言模型显著优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有DocRAG方法过度依赖文本模态，缺乏处理多页跨模态文档的能力，且缺乏可靠的评估体系。

Method: 构建包含4055个多模态QA对的基准，设计多模态引文选择指标，并系统性测试60个VLM/LLM模型与14个检索系统。

Result: 闭源模型在跨模态检索中表现最优（超开源模型33%），精细图像描述可提升LLM性能，多模态输入优势存在显著模型差异。

Conclusion: MMDocRAG填补多模态文档理解评估空白，证明视觉信息融合的重要性，为改进DocVQA系统提供可量化路径。

Abstract: Document Visual Question Answering (DocVQA) faces dual challenges in
processing lengthy multimodal documents (text, images, tables) and performing
cross-modal reasoning. Current document retrieval-augmented generation (DocRAG)
methods remain limited by their text-centric approaches, frequently missing
critical visual information. The field also lacks robust benchmarks for
assessing multimodal evidence selection and integration. We introduce MMDocRAG,
a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with
multi-page, cross-modal evidence chains. Our framework introduces innovative
metrics for evaluating multimodal quote selection and enables answers that
interleave text with relevant visual elements. Through large-scale experiments
with 60 VLM/LLM models and 14 retrieval systems, we identify persistent
challenges in multimodal evidence retrieval, selection, and integration.Key
findings reveal advanced proprietary LVMs show superior performance than
open-sourced alternatives. Also, they show moderate advantages using multimodal
inputs over text-only inputs, while open-source alternatives show significant
performance degradation. Notably, fine-tuned LLMs achieve substantial
improvements when using detailed image descriptions. MMDocRAG establishes a
rigorous testing ground and provides actionable insights for developing more
robust multimodal DocVQA systems. Our benchmark and code are available at
https://mmdocrag.github.io/MMDocRAG/.

</details>


### [170] [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
*Jonghwi Kim,Deokhyung Kang,Seonjeong Hwang,Yunsu Kim,Jungseul Ok,Gary Lee*

Main category: cs.IR

TL;DR: 研究构建首个混合语言查询测试集MiLQ，验证多语言IR模型处理混合查询的效能及混合策略对搜索优化的作用


<details>
  <summary>Details</summary>
Motivation: 双语用户频繁使用混合语言搜索但缺乏相关研究基准，需建立公共测试集探索模型优化方案

Method: 创建现实性混合查询数据集MiLQ，评估多语言模型在原生/英语/混合查询的表现，分析代码切换训练数据价值

Result: 模型在混合查询表现中等且存在跨类型波动，混合英语策略显著提升英文文档搜索效果(源于增强的词汇匹配)

Conclusion: 混合查询处理是双语搜索刚需，MiLQ提供基准支撑，代码切换训练增强模型鲁棒性，主动混合策略具备实用价值

Abstract: Despite bilingual speakers frequently using mixed-language queries in web
searches, Information Retrieval (IR) research on them remains scarce. To
address this, we introduce MiLQ,Mixed-Language Query test set, the first public
benchmark of mixed-language queries, confirmed as realistic and highly
preferred. Experiments show that multilingual IR models perform moderately on
MiLQ and inconsistently across native, English, and mixed-language queries,
also suggesting code-switched training data's potential for robust IR models
handling such queries. Meanwhile, intentional English mixing in queries proves
an effective strategy for bilinguals searching English documents, which our
analysis attributes to enhanced token matching compared to native queries.

</details>


### [171] [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
*Nour Jedidi,Yung-Sung Chuang,James Glass,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究通过实验发现，在相同训练条件下非推理式排序模型（StandardRR）优于推理式模型（ReasonRR），且移除推理模块后的模型（ReasonRR-NoReason）效果更佳，表明推理过程会因极化相关性分数而损害排序准确性。


<details>
  <summary>Details</summary>
Motivation: 探索自然语言推理模型在信息检索任务中的应用效果，验证显式推理过程是否能提升段落重排序的准确性。

Method: 在相同训练条件下对比推理式模型（ReasonRR）与非推理式模型（StandardRR），并通过禁用推理模块（ReasonRR-NoReason）进一步分析推理过程的影响。

Result: StandardRR普遍优于ReasonRR，且ReasonRR-NoReason效果超过原推理模型，表明推理过程导致相关性分数极化，无法有效捕捉段落的部分相关性特征。

Conclusion: 推理过程会限制模型对段落部分相关性的评估能力，极化相关性分数分布，因此非推理式方法在段落重排序任务中更具优势。

Abstract: With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.

</details>


### [172] [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
*Nandan Thakur,Crystina Zhang,Xueguang Ma,Jimmy Lin*

Main category: cs.IR

TL;DR: 通过LLM级联提示重标注训练数据中的假阴性样本，可显著提升检索模型和排序模型的效果


<details>
  <summary>Details</summary>
Motivation: 大规模检索数据集训练存在数据质量问题，BGE数据集中部分负样本实为假阴性（相关段落被误标），导致模型性能下降

Method: 提出成本效益高的级联LLM提示方法：1. 用GPT-4o-mini初步筛选困难负样本 2. 通过GPT-4o进行精细验证与重标注

Result: 重标注后E5/Qwen2.5-7B检索模型在BEIR上提升0.7-1.4nDCG@10，AIR-Bench零样本评估提升1.7-1.8nDCG@10；Qwen2.5-3B排序模型也获得类似增益

Conclusion: 训练数据清洗（特别是假阴性重标注）对模型效果提升显著，且人类标注验证表明GPT-4o的判别结果与人工判断具有更高一致性

Abstract: Training robust retrieval and reranker models typically relies on large-scale
retrieval datasets; for example, the BGE collection contains 1.6 million
query-passage pairs sourced from various data sources. However, we find that
certain datasets can negatively impact model effectiveness -- pruning 8 out of
15 datasets from the BGE collection reduces the training set size by
2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a
deeper examination of training data quality, with a particular focus on "false
negatives", where relevant passages are incorrectly labeled as irrelevant. We
propose a simple, cost-effective approach using cascading LLM prompts to
identify and relabel hard negatives. Experimental results show that relabeling
false negatives with true positives improves both E5 (base) and Qwen2.5-7B
retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot
AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on
the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the
cascading design is further supported by human annotation results, where we
find judgment by GPT-4o shows much higher agreement with humans than
GPT-4o-mini.

</details>


### [173] [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
*Runyang You,Yongqi Li,Xinyu Lin,Xin Zhang,Wenjie Wang,Wenjie Li,Liqiang Nie*

Main category: cs.IR

TL;DR: 提出统一大型推荐模型RRec，通过自回归架构整合推理与推荐功能，RecPO强化学习框架实现双能力联合优化，实验指标显著提升68.67% Hit@5和45.21% NDCG@20


<details>
  <summary>Details</summary>
Motivation: 现有方法将LLMs作为外部推理模块存在资源成本高、联合优化困难的问题，需要构建具备内在推理能力的统一推荐模型

Method: 1. 重构自回归架构实现推理与推荐的交替执行
2. 设计RecPO强化学习框架（含融合奖励机制），仅用推荐标签同步优化推理和推荐能力

Result: 在三个数据集上验证，相对基线模型实现Hit@5提升68.67%，NDCG@20提升45.21%

Conclusion: RRec成功实现推理与推荐的端到端优化，无需额外推理标注的实验效果验证了方案有效性，代码已开源

Abstract: Large recommender models have extended LLMs as powerful recommenders via
encoding or item generation, and recent breakthroughs in LLM reasoning
synchronously motivate the exploration of reasoning in recommendation. Current
studies usually position LLMs as external reasoning modules to yield auxiliary
thought for augmenting conventional recommendation pipelines. However, such
decoupled designs are limited in significant resource cost and suboptimal joint
optimization. To address these issues, we propose \name, a unified large
recommender model with intrinsic reasoning capabilities. Initially, we
reconceptualize the model architecture to facilitate interleaved reasoning and
recommendation in the autoregressive process. Subsequently, we propose RecPO, a
corresponding reinforcement learning framework that optimizes \name\ both the
reasoning and recommendation capabilities simultaneously in a single policy
update; RecPO introduces a fused reward scheme that solely leverages
recommendation labels to simulate the reasoning capability, eliminating
dependency on specialized reasoning annotations. Experiments on three datasets
with various baselines verify the effectiveness of \name, showing relative
improvements of 68.67\% in Hit@5 and 45.21\% in NDCG@20. Code available at
https://github.com/YRYangang/RRec.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [174] [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
*Aaron J. Li,Suraj Srinivas,Usha Bhalla,Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器的概念表示对微小对抗扰动敏感，可能影响其在模型监控中的应用


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器评估忽视概念表示的鲁棒性，而鲁棒性是反映概念标注真实性的关键指标

Method: 通过输入空间优化框架，在不同现实场景中生成对抗扰动来操纵SAE表示

Result: 微小对抗扰动即可有效改变概念解释，且不影响基础LLM输出

Conclusion: SAE概念表示具有脆弱性，可能不适用于需要可靠性的模型监控与监管场景

Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.

</details>


### [175] [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
*Zhixu Silvia Tao,Kasper Vinken,Hao-Wei Yeh,Avi Cooper,Xavier Boix*

Main category: cs.LG

TL;DR: 提出Merge to Mix方法，通过模型合并加速数据集混合选择，避免传统多次微调的耗时过程


<details>
  <summary>Details</summary>
Motivation: 传统数据集混合方法依赖试错和多次微调，效率低下。模型合并技术可通过算术运算整合单数据集微调模型，成为混合微调的有效替代方案

Method: 将各数据集单独微调的模型通过参数合并技术整合成单一模型，作为混合数据集微调的代理模型，实现快速候选组合评估

Result: 实验证明Merge to Mix在语言模型微调数据集选择任务中超越现有最佳方法

Conclusion: 该方法突破传统需要完整微调的限制，为高效数据集组合选择提供了新范式

Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.

</details>


### [176] [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
*Ziqing Wang,Kexin Zhang,Zihan Zhao,Yibo Wen,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: 大语言模型（LLM）通过文本引导和多模态输入革新分子发现，聚焦分子生成与优化，提供分类、技术分析及资源整合


<details>
  <summary>Details</summary>
Motivation: 传统分子发现方法依赖计算模拟和实验，效率低且成本高。LLM能通过自然语言处理高效探索化学空间，加速发现过程

Method: 建立分子生成和优化的分类体系，分析不同学习场景下的LLM技术，总结数据集和评估方法

Result: 现有技术有效结合LLM能力，多模态扩展提升模型适用性，公开数据集和评估协议促进研究标准化

Conclusion: 需解决数据稀缺、模型可解释性及跨模态融合问题，未来方向包括增强领域适应性和自动化工作流

Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.

</details>


### [177] [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
*Chongjie Si,Kangtao Lv,Jingjing Jiang,Yadao Wang,Yongwei Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 提出基于参数范数倒数的训练无关模型融合方法NAN，有效提升多任务模型合并性能


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法依赖启发式确定合并系数，限制了方法的扩展性和通用性。需要更数学化的优化框架解决该问题

Method: 通过最小二乘优化推导出模型参数范数与任务信息量的关系，提出用参数范数倒数自动估计合并权重（NAN方法）

Result: 在多个实验场景下，NAN方法持续提升基线方法性能，验证了理论推导的有效性

Conclusion: NAN方法具有训练无关、即插即用优势，为模型融合提供了理论指导和新范式，可广泛适用于不同合并策略

Abstract: Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.

</details>


### [178] [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
*Zhihang Cai,Xingjun Zhang,Zhendong Tan,Zheng Wei*

Main category: cs.LG

TL;DR: 针对大语言模型推理中KV缓存高内存消耗问题，提出基于块正态分布的分位数量化算法NQKV，实现在不显著降低模型质量的前提下将OPT模型吞吐量提升9.3倍


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文和大批量推理时面临KV缓存显存消耗过高的问题，现有量化方法难以在低比特下保持精度

Method: 利用KV缓存块内元素的正态分布特性，采用分位数量化实现信息论最优量化误差

Result: OPT模型批量扩大2倍/上下文延长4倍，吞吐量提升9.3倍（相比无KV缓存）

Conclusion: NQKV算法通过统计特性优化低比特量化，有效突破LLM部署中的显存瓶颈

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.

</details>


### [179] [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
*Woosung Koh,Wonbeen Oh,Jaein Jang,MinHyung Lee,Hyeongjin Kim,Ah Yeon Kim,Joonkee Kim,Junghyun Lee,Taehyeon Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: AdaSTaR算法通过自适应多样性采样和课程式采样，显著提升语言模型自改进训练效率，在6个基准测试中均取得最佳准确率并降低58.6%训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统STaR/RFT采用随机数据采样导致训练失衡：简单样本过度训练而困难样本训练不足，影响模型效率与效果。

Method: 1. 多样性自适应采样：平衡不同观测样本的训练强度
2. 课程式自适应采样：动态调整数据难度匹配模型能力演进

Result: 在6个基准测试中全部达到最佳准确率(6/6)，训练FLOPs平均降低58.6%，改进效果在不同预训练模型和更大模型上均成立。

Conclusion: AdaSTaR为自改进语言模型提供了更高效、通用的训练框架，其双自适应机制具有广泛适用性。

Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.

</details>


### [180] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
*Yang Chen,Zhuolin Yang,Zihan Liu,Chankyu Lee,Peng Xu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.LG

TL;DR: 大规模强化学习显著提升中小型模型的数学与代码推理能力，超越现有蒸馏方法效果


<details>
  <summary>Details</summary>
Motivation: 解决当前强化学习在推理模型训练中缺乏明确指导方案的问题，验证RL对小模型的适用性

Method: 分阶段RL训练：先数学提示训练，后代码提示训练；建立包含验证机制的数据筛选流程；采用逐步加长的课程学习策略

Result: 7B/14B模型在数学基准提升14.6%/17.2%，代码基准提升6.8%/5.8%；代码训练不损害数学能力

Conclusion: 强化学习不仅能激发预训练获得的推理能力，还能突破模型原有推理极限，解决之前无法处理的复杂问题

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [181] [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
*Chengcan Wu,Zhixin Zhang,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 提出SAP框架，通过在梯度传播中引入安全感知探测，有效缓解微调LLM时的安全风险，保持模型安全性同时提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在预训练阶段进行了安全对齐，但现有研究发现即使对良性数据进行微调仍会导致安全性下降。本文旨在探究非有害数据微调引发安全退化的根本原因，并提出解决方案。

Method: 提出安全感知探测优化框架（SAP），在梯度传播过程中加入安全感知探针，通过识别梯度方向中的潜在风险区域，在提升任务性能的同时维持模型安全性。

Result: 实验表明SAP将有害性降至原始微调模型水平以下，且测试损失与标准微调方法相当。代码已开源。

Conclusion: SAP框架成功实现了任务性能提升与模型安全性的平衡，为解决LLM微调中的安全退化问题提供了有效方案。

Abstract: The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.

</details>


### [182] [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
*Tajamul Ashraf,Mohammed Mohsen Peerzada,Moloud Abdar,Yutong Xie,Yuyin Zhou,Xiaofeng Liu,Iqra Altaf Gillani,Janibul Bashir*

Main category: cs.LG

TL;DR: 提出了ATR-Bench框架，从适应性、信任和推理三个维度系统评估联邦学习，填补标准化评估空白


<details>
  <summary>Details</summary>
Motivation: 当前联邦学习方法缺乏标准化评估体系，导致系统性进展受限且方法间难以公平比较

Method: 构建ATR-Bench统一框架，对异构客户端适应性/对抗环境可信性进行基准测试，推理维度进行文献驱动分析

Result: 建立联邦学习系统评估基础框架，创建持续跟踪FL发展的开源代码库和资源平台

Conclusion: ATR-Bench为联邦学习的现实场景应用提供了系统性评估方法论，推动领域向实用化方向发展

Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.

</details>


### [183] [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
*Noah Amsel,David Persson,Christopher Musco,Robert Gower*

Main category: cs.LG

TL;DR: 提出GPU友好的极坐标分解算法Polar Express，通过动态调整多项式更新规则实现快速收敛，在Muon框架中显著提升大模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 传统数值分析方法(如Newton-Schulz和有理函数法)存在GPU兼容性差/收敛速度慢的问题，无法满足深度学习对高效计算的需求。

Method: 结合极小极大优化动态调整多项式迭代规则，仅使用矩阵乘法运算，保持GPU兼容性的同时实现快速收敛，并解决bfloat16下的数值稳定性问题。

Result: 在GPT-2等大模型训练中，相比现有方法在不同学习率下均表现出更优的验证损失改进效果。

Conclusion: Polar Express在GPU兼容性、收敛速度和数值稳定性方面实现突破，为深度学习优化提供了更高效的矩阵分解解决方案。

Abstract: Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.

</details>


### [184] [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
*Zebin You,Shen Nie,Xiaolu Zhang,Jun Hu,Jun Zhou,Zhiwu Lu,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 纯扩散模型LLaDA-V在多模态任务中展现竞争力，突破传统自回归范式限制


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在多模态领域的潜力，替代主流自回归架构，通过视觉-语言对齐提升多模态性能

Method: 基于LLaDA语言扩散模型，集成视觉编码器与MLP连接器，实现视觉特征到语言空间的投影

Result: 与LLaMA3-V竞争且数据扩展性更优，多模态理解达SOTA，缩小与Qwen2-VL差距

Conclusion: 扩散模型在多模态领域展现应用前景，其架构有效性值得深度探索

Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.

</details>


### [185] [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
*Mingyang Liu,Gabriele Farina,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 提出统一后训练方法UFT，结合监督微调和强化微调优势，在提升大语言模型推理能力的同时突破传统方法的局限性


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法易导致大模型过拟合，而RFT过度依赖基模型强度且样本效率低，需探索能平衡探索与监督的新范式

Method: 将SFT和RFT统一为单阶段训练框架，通过引入信息性监督信号引导探索过程，建立记忆与思考的协同机制

Result: UFT在不同规模模型上全面超越传统方法，理论证明其可打破RFT的指数级样本复杂度瓶颈，加速长程推理任务收敛

Conclusion: UFT开创性地统一训练范式，实现探索与监督的有机融合，不仅实证效果显著，更在理论上突破强化学习的效率瓶颈

Abstract: Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [186] [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
*Kai Li,Can Shen,Yile Liu,Jirui Han,Kelong Zheng,Xuechao Zou,Zhe Wang,Xingjian Du,Shun Zhang,Hanjun Luo,Yingbin Jin,Xinxin Xing,Ziyang Ma,Yue Liu,Xiaojun Jia,Yifan Zhang,Junfeng Fang,Kun Wang,Yibo Yan,Haoyang Li,Yiming Li,Xiaobin Zhuang,Yang Liu,Haibo Hu,Zhuo Chen,Zhizheng Wu,Xiaolin Hu,Eng-Siong Chng,XiaoFeng Wang,Wenyuan Xu,Wei Dong,Xinfeng Li*

Main category: cs.SD

TL;DR: 首个针对音频大语言模型的多维可信度评估框架AudioTrust，覆盖公平性、幻觉、安全性等六大维度，构建含4420+样本的数据集，揭示现有模型在高风险音频场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架集中于文本模态且安全维度受限，无法满足音频模态独特风险（如语音助手交互、紧急呼叫）的评估需求。

Method: 构建含18种实验设置的数据集，设计9个音频专用评估指标，采用自动化流程进行客观模型评分。

Result: 实验揭示了开源/闭源ALLMs在高风险音频场景中的可信度边界，为模型部署提供安全参考。

Conclusion: 该框架为音频模型的安全部署提供系统性评估工具，其多维度洞见将推动可信音频AI发展。

Abstract: The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [187] [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
*Ahmed Heakl,Sarim Hashmi,Gustavo Bertolo Stahl,Seung Hun Eddie Han,Salman Khan,Abdulrahman Mahmoud*

Main category: cs.AR

TL;DR: 首个跨架构GPU代码转译数据集CASS，包含70k验证代码对，训练模型实现95%源码翻译准确率，生成代码性能匹配原生85%用例


<details>
  <summary>Details</summary>
Motivation: 解决GPU代码跨架构移植难题，现有工具在低层代码转换存在准确率和性能差距

Method: 构建多层级代码转译数据集，训练领域专用语言模型，开发CASS-Bench基准测试框架

Result: 源码翻译准确率95%（超GPT-4o 38%），汇编级达37.5%，生成代码性能保留率85%

Conclusion: CASS填补GPU工具链空白，开源资源推动硬件翻译研究和LLM在编译领域应用

Abstract: We introduce \texttt{CASS}, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level
(CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia
SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k
verified code pairs across host and device, addressing a critical gap in
low-level GPU code portability. Leveraging this resource, we train the
\texttt{CASS} family of domain-specific language models, achieving 95\% source
translation accuracy and 37.5\% assembly translation accuracy, substantially
outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our
generated code matches native performance in over 85\% of test cases,
preserving runtime and memory behavior. To support rigorous evaluation, we
introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with
ground-truth execution. All data, models, and evaluation tools are released as
open source to foster progress in GPU compiler tooling, binary compatibility,
and LLM-guided hardware translation. Dataset and benchmark are on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.

</details>
