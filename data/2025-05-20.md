<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 193]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 21]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.SE](#cs.SE) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)
*Jinqiang Wang,Huansheng Ning,Tao Zhu,Jianguo Ding*

Main category: cs.CL

TL;DR: 提出SynPT方法，通过LLM驱动的对话模拟生成高质量数据集，提升旅游领域隐含意图挖掘能力


<details>
  <summary>Details</summary>
Motivation: 现有旅游领域LLM存在隐含意图识别不足、数据集质量低、方法存在领域适应性差/查询分布偏斜/上下文冗余/缺乏情感分析四大缺陷

Method: 构建LLM双代理系统模拟用户对话，生成包含显式推理链的SynPT-Dialog数据集，并用于下游模型微调

Result: 实验证明SynPT在人工和LLM评估中均优于基线方法，成功实现主动意图引导与跨语言适应

Conclusion: SynPT有效解决了旅游领域意图挖掘难题，其数据合成框架具有领域扩展潜力，公开数据资源推动相关研究发展

Abstract: In the tourism domain, Large Language Models (LLMs) often struggle to mine
implicit user intentions from tourists' ambiguous inquiries and lack the
capacity to proactively guide users toward clarifying their needs. A critical
bottleneck is the scarcity of high-quality training datasets that facilitate
proactive questioning and implicit intention mining. While recent advances
leverage LLM-driven data synthesis to generate such datasets and transfer
specialized knowledge to downstream models, existing approaches suffer from
several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed
distributions of detail levels in initial inquiries, (3) contextual redundancy
in the implicit intention mining module, and (4) lack of explicit thinking
about tourists' emotions and intention values. Therefore, we propose SynPT (A
Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User
Intentions in the Tourism), which constructs an LLM-driven user agent and
assistant agent to simulate dialogues based on seed data collected from Chinese
tourism websites. This approach addresses the aforementioned limitations and
generates SynPT-Dialog, a training dataset containing explicit reasoning. The
dataset is utilized to fine-tune a general LLM, enabling it to proactively mine
implicit user intentions. Experimental evaluations, conducted from both human
and LLM perspectives, demonstrate the superiority of SynPT compared to existing
methods. Furthermore, we analyze key hyperparameters and present case studies
to illustrate the practical applicability of our method, including discussions
on its adaptability to English-language scenarios. All code and data are
publicly available.

</details>


### [2] [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)
*Harika Abburi,Sanmitra Bhattacharya,Edward Bowen,Nirmala Pudota*

Main category: cs.CL

TL;DR: 开发神经网络模型检测AI生成文本，任务A（人机区分）F1达0.994，任务B（模型溯源）F1达0.627，均位列第五名


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型滥用风险（如假新闻/垃圾邮件/学术作弊），需建立可靠检测机制保障AI伦理使用

Method: 针对AAAI 2025 Defactify竞赛两任务：任务A采用优化神经网络架构，任务B使用简化版架构进行模型溯源

Result: 任务A优化模型F1=0.994（第五名），任务B简化模型F1=0.627（第五名）

Conclusion: 神经网络在AI文本检测中效果显著，任务B难度更高，体现模型溯源的技术挑战，强调检测技术对AI治理的重要性

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating text that closely resembles human writing across a wide range of
styles and genres. However, such capabilities are prone to potential misuse,
such as fake news generation, spam email creation, and misuse in academic
assignments. As a result, accurate detection of AI-generated text and
identification of the model that generated it are crucial for maintaining the
responsible use of LLMs. In this work, we addressed two sub-tasks put forward
by the Defactify workshop under AI-Generated Text Detection shared task at the
Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A
involved distinguishing between human-authored or AI-generated text, while Task
B focused on attributing text to its originating language model. For each task,
we proposed two neural architectures: an optimized model and a simpler variant.
For Task A, the optimized neural architecture achieved fifth place with $F1$
score of 0.994, and for Task B, the simpler neural architecture also ranked
fifth place with $F1$ score of 0.627.

</details>


### [3] [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)
*Yuxuan Li,Aoi Naito,Hirokazu Shirado*

Main category: cs.CL

TL;DR: 论文引入社会心理学中的隐藏档案范式，揭示了多智能体LLM系统存在集体推理缺陷，发现所有模型的多智能体表现均不及拥有完整信息的单智能体，并揭示了合作与矛盾间的系统级权衡。


<details>
  <summary>Details</summary>
Motivation: 针对多智能体LLM系统缺乏系统性评估集体推理缺陷的理论基础基准，研究者试图通过心理学范式揭示智能体间动态交互对集体推理的影响机制。

Method: 将隐藏档案范式形式化为分布式知识下的多智能体决策框架，构建包含9个任务的基准测试，在GPT-4.1等六个主流LLM上开展对比实验，包括增强推理变体的测试。

Result: 所有模型的多智能体系统准确率落后于完整信息单智能体28.3%，集体表现接近人类群体但存在行为差异（如对社会期望敏感度提升），合作智能体易出现过度协调而矛盾增加损害群体收敛。

Conclusion: 研究建立了可复现的多智能体系统评估框架，揭示了LLM群体智能与人类集体行为的异同，为人工集体智能和人机交互研究提供了新的理论基础与诊断工具。

Abstract: Multi-agent systems built on large language models (LLMs) promise enhanced
problem-solving through distributed information integration, but also risk
replicating collective reasoning failures observed in human groups. Yet, no
theory-grounded benchmark exists to systematically evaluate such failures. In
this paper, we introduce the Hidden Profile paradigm from social psychology as
a diagnostic testbed for multi-agent LLM systems. By distributing critical
information asymmetrically across agents, the paradigm reveals how inter-agent
dynamics support or hinder collective reasoning. We first formalize the
paradigm for multi-agent decision-making under distributed knowledge and
instantiate it as a benchmark with nine tasks spanning diverse scenarios,
including adaptations from prior human studies. We then conduct experiments
with GPT-4.1 and five other leading LLMs, including reasoning-enhanced
variants, showing that multi-agent systems across all models fail to match the
accuracy of single agents given complete information. While agents' collective
performance is broadly comparable to that of human groups, nuanced behavioral
differences emerge, such as increased sensitivity to social desirability.
Finally, we demonstrate the paradigm's diagnostic utility by exploring a
cooperation-contradiction trade-off in multi-agent LLM systems. We find that
while cooperative agents are prone to over-coordination in collective settings,
increased contradiction impairs group convergence. This work contributes a
reproducible framework for evaluating multi-agent LLM systems and motivates
future research on artificial collective intelligence and human-AI interaction.

</details>


### [4] [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)
*Kyudan Jung,Hojun Cho,Jooyeol Yun,Jaehyeok Jang,Jagul Choo*

Main category: cs.CL

TL;DR: 论文提出Talk-to-Your-Slides系统，通过LLM代理实现PowerPoint的智能编辑，采用高层指令解析+底层Python脚本执行的两级架构，并构建TSBench评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于PPT生成而忽视编辑场景，传统方法依赖预定义操作，无法满足复杂场景的灵活编辑需求。

Method: 1) 高层LLM代理解析自然语言指令并制定编辑计划 2) 底层通过COM通信协议直接操作PPT对象 3) 提出包含379条标注指令的TSBench数据集

Result: 系统在指令执行成功率(提升32%)、编辑准确性(提高28%)和操作效率(减少40%交互步骤)上显著优于基线方法

Conclusion: 该方法突破了传统PPT编辑工具的局限性，实现了上下文感知的智能编辑，为办公自动化提供了新范式。代码和基准已开源。

Abstract: Existing research on large language models (LLMs) for PowerPoint
predominantly focuses on slide generation, overlooking the common yet tedious
task of editing existing slides. We introduce Talk-to-Your-Slides, an
LLM-powered agent that directly edits slides within active PowerPoint sessions
through COM communication. Our system employs a two-level approach: (1)
high-level processing where an LLM agent interprets instructions and formulates
editing plans, and (2) low-level execution where Python scripts directly
manipulate PowerPoint objects. Unlike previous methods relying on predefined
operations, our approach enables more flexible and contextually-aware editing.
To facilitate evaluation, we present TSBench, a human-annotated dataset of 379
diverse editing instructions with corresponding slide variations. Experimental
results demonstrate that Talk-to-Your-Slides significantly outperforms baseline
methods in execution success rate, instruction fidelity, and editing
efficiency. Our code and benchmark are available at
https://anonymous.4open.science/r/talk-to-your-slides/

</details>


### [5] [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)
*Xiaomin Li,Mingye Gao,Yuexing Hao,Taoran Li,Guangya Wan,Zihan Wang,Yijun Wang*

Main category: cs.CL

TL;DR: 研究评估大语言模型(LLMs)遵循临床指南的能力，开发MedGUIDE基准测试发现即使医学专用模型在结构化协议遵循任务中表现欠佳


<details>
  <summary>Details</summary>
Motivation: 临床指南作为决策树对循证医疗至关重要，但LLMs能否可靠遵循此类结构化协议尚未明确，需确保模型在真实临床环境中安全运作

Method: 基于17种癌症的55个NCCN决策树生成临床场景，通过两阶段筛选（专家标注奖励模型+LLM评委集成）选出7,747个高质量样本，评估25个通用/开源/医学专用LLM

Result: 领域专用模型在需严格遵循指南的任务中表现不佳，上下文指南纳入和继续预训练对性能提升有限

Conclusion: MedGUIDE有效评估LLMs临床适用性，凸显当前模型在真实临床框架中的操作局限性，强调改进模型结构化协议遵循能力的必要性

Abstract: Clinical guidelines, typically structured as decision trees, are central to
evidence-based medical practice and critical for ensuring safe and accurate
diagnostic decision-making. However, it remains unclear whether Large Language
Models (LLMs) can reliably follow such structured protocols. In this work, we
introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to
make guideline-consistent clinical decisions. MedGUIDE is constructed from 55
curated NCCN decision trees across 17 cancer types and uses clinical scenarios
generated by LLMs to create a large pool of multiple-choice diagnostic
questions. We apply a two-stage quality selection process, combining
expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical
and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25
LLMs spanning general-purpose, open-source, and medically specialized models,
and find that even domain-specific LLMs often underperform on tasks requiring
structured guideline adherence. We also test whether performance can be
improved via in-context guideline inclusion or continued pretraining. Our
findings underscore the importance of MedGUIDE in assessing whether LLMs can
operate safely within the procedural frameworks expected in real-world clinical
settings.

</details>


### [6] [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)
*Jian-Qiao Zhu,Haijiang Yan,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 通过将行为方法与神经表示对齐，提出系统化识别引导向量的方法，验证其有效调控LLM风险输出的能力。


<details>
  <summary>Details</summary>
Motivation: 探索无需重新训练即可精准调控LLM行为的有效方法，解决传统微调方法效率低、针对性差的问题。

Method: 结合MCMC行为方法生成潜在表示，与神经网络激活对齐提取引导向量，并注入残差流调整输出。

Result: 提取的引导向量可稳定调节LLM风险偏好，输出结果与目标行为保持高度一致性。

Conclusion: 表示工程为LLM行为调控提供了轻量化解决方案，该方法可扩展至其他行为特征调控场景。

Abstract: Changing the behavior of large language models (LLMs) can be as
straightforward as editing the Transformer's residual streams using
appropriately constructed "steering vectors." These modifications to internal
neural activations, a form of representation engineering, offer an effective
and targeted means of influencing model behavior without retraining or
fine-tuning the model. But how can such steering vectors be systematically
identified? We propose a principled approach for uncovering steering vectors by
aligning latent representations elicited through behavioral methods
(specifically, Markov chain Monte Carlo with LLMs) with their neural
counterparts. To evaluate this approach, we focus on extracting latent risk
preferences from LLMs and steering their risk-related outputs using the aligned
representations as steering vectors. We show that the resulting steering
vectors successfully and reliably modulate LLM outputs in line with the
targeted behavior.

</details>


### [7] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)
*Udita Patel,Rutu Mulkar,Jay Roberts,Cibi Chakravarthy Senthilkumar,Sujay Gandhi,Xiaofei Zheng,Naumaan Nayyar,Rafael Castrillo*

Main category: cs.CL

TL;DR: THELMA框架通过6个互关联指标实现RAG问答系统的无参考全链路评估


<details>
  <summary>Details</summary>
Motivation: 解决传统评估方法需要标注数据、无法针对RAG全链路进行细粒度评估的问题

Method: 设计六个相互关联的评估指标，支持开发者在无标注数据情况下对检索增强生成QA全链路进行监控优化

Result: 发现指标间的关联性可精确定位需要优化的RAG组件（检索/生成/排序等模块）

Conclusion: THELMA为RAG问答系统提供端到端的评估方法论，通过指标关联分析指导具体模块的优化

Abstract: We propose THELMA (Task Based Holistic Evaluation of Large Language Model
Applications), a reference free framework for RAG (Retrieval Augmented
generation) based question answering (QA) applications. THELMA consist of six
interdependent metrics specifically designed for holistic, fine grained
evaluation of RAG QA applications. THELMA framework helps developers and
application owners evaluate, monitor and improve end to end RAG QA pipelines
without requiring labelled sources or reference responses.We also present our
findings on the interplay of the proposed THELMA metrics, which can be
interpreted to identify the specific RAG component needing improvement in QA
applications.

</details>


### [8] [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Chia-Hsuan Lee,Sambit Sahu*

Main category: cs.CL

TL;DR: 提出Critique-Guided Distillation (CGD)框架，通过整合教师模型的解释性批判和改进响应，解决监督微调中的模仿问题，提升模型理解能力


<details>
  <summary>Details</summary>
Motivation: 监督微调(SFT)存在模仿问题——模型机械复制专家示范而缺乏对底层逻辑的理解

Method: 多阶段框架CGD：训练学生模型将提示、教师批判和自身初始响应映射到改进后的教师响应，同时学习『模仿内容』和『模仿原因』

Result: 在数学(AMC23 +17.5%)和语言理解(MMLU-Pro +6.3%)任务上显著提升，有效缓解格式漂移问题

Conclusion: CGD通过贝叶斯后验更新解释，降低改进不确定性，证明整合批判性反馈能有效增强模型的理解与推理能力

Abstract: Supervised fine-tuning (SFT) using expert demonstrations often suffer from
the imitation problem, where the model learns to reproduce the correct
responses without \emph{understanding} the underlying rationale. To address
this limitation, we propose \textsc{Critique-Guided Distillation (CGD)}, a
novel multi-stage framework that integrates teacher model generated
\emph{explanatory critiques} and \emph{refined responses} into the SFT process.
A student model is then trained to map the triplet of prompt, teacher critique,
and its own initial response to the corresponding refined teacher response,
thereby learning both \emph{what} to imitate and \emph{why}. Using
entropy-based analysis, we show that \textsc{CGD} reduces refinement
uncertainty and can be interpreted as a Bayesian posterior update. We perform
extensive empirical evaluation of \textsc{CGD}, on variety of benchmark tasks,
and demonstrate significant gains on both math (AMC23 +17.5%) and language
understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format
drift issues observed in previous critique fine-tuning (CFT) techniques.

</details>


### [9] [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)
*Xiang Fu*

Main category: cs.CL

TL;DR: 有序课程训练显著提升小语言模型的推理透明度和样本效率，但最终准确率仍存在差距，并存在显着性探测不足的问题。


<details>
  <summary>Details</summary>
Motivation: 探索通过分阶段训练提升小模型效率，解决传统方法在样本效率和可解释性上的局限性。

Method: 采用四阶段发展式课程（从词汇匹配到多步符号推理）训练GPT-2模型，无需任务特定微调，并通过控制实验验证课程顺序的关键作用。

Result: 有序课程使模型在1/2时间内达到目标准确率，激活10倍梯度显著推理头并迁移至深层，但最终准确率仍低常规方法30%，且在困难阶段出现探测盲区。

Conclusion: 课程顺序是效果提升的核心驱动力，未来需通过混合阶段微调和扩展显着性探测方法解决剩余挑战。

Abstract: We demonstrate that a developmentally ordered curriculum markedly improves
reasoning transparency and sample-efficiency in small language models (SLMs).
Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage
syllabus that ascends from lexical matching to multi-step symbolic inference
and then evaluate it without any task-specific fine-tuning. Cognivolve reaches
target accuracy in half the optimization steps of a single-phase baseline,
activates an order-of-magnitude more gradient-salient reasoning heads, and
shifts those heads toward deeper layers, yielding higher-entropy attention that
balances local and long-range context. The same curriculum applied out of order
or with optimizer resets fails to reproduce these gains, confirming that
progression--not extra compute--drives the effect. We also identify open
challenges: final-answer success still lags a conventional run by about 30%,
and our saliency probe under-detects verbal-knowledge heads in the hardest
stage, suggesting directions for mixed-stage fine-tuning and probe expansion.

</details>


### [10] [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)
*Shubham Vatsal,Harsh Dubey,Aditi Singh*

Main category: cs.CL

TL;DR: 论文系统综述多语言提示工程如何提升大型语言模型（LLMs）在250种语言、30个NLP任务中的表现，涵盖36篇研究及39种技术方法。


<details>
  <summary>Details</summary>
Motivation: LLMs在多语言场景下的有效性面临挑战，多语言提示工程提供无需大量训练调参的轻量化解决方案，降低非专业人士使用门槛。

Method: 通过文献计量法对近2-3年研究成果进行系统分类，分析不同语言家族/资源水平下的任务分布与提示技术应用频率。

Result: 覆盖250种语言，识别高资源语言常用结构化模板、低资源语言侧重知识迁移；39种提示技术中上下文学习（in-context learning）占比最高。

Conclusion: 多语言提示工程显著提升LLMs跨语言泛化能力，未来需加强低资源语言场景的提示策略优化与语言家族特性研究。

Abstract: Large language models (LLMs) have demonstrated impressive performance across
a wide range of Natural Language Processing (NLP) tasks. However, ensuring
their effectiveness across multiple languages presents unique challenges.
Multilingual prompt engineering has emerged as a key approach to enhance LLMs'
capabilities in diverse linguistic settings without requiring extensive
parameter re-training or fine-tuning. With growing interest in multilingual
prompt engineering over the past two to three years, researchers have explored
various strategies to improve LLMs' performance across languages and NLP tasks.
By crafting structured natural language prompts, researchers have successfully
extracted knowledge from LLMs across different languages, making these
techniques an accessible pathway for a broader audience, including those
without deep expertise in machine learning, to harness the capabilities of
LLMs. In this paper, we survey and categorize different multilingual prompting
techniques based on the NLP tasks they address across a diverse set of datasets
that collectively span around 250 languages. We further highlight the LLMs
employed, present a taxonomy of approaches and discuss potential
state-of-the-art (SoTA) methods for specific multilingual datasets.
Additionally, we derive a range of insights across language families and
resource levels (high-resource vs. low-resource), including analyses such as
the distribution of NLP tasks by language resource type and the frequency of
prompting methods across different language families. Our survey reviews 36
research papers covering 39 prompting techniques applied to 30 multilingual NLP
tasks, with the majority of these studies published in the last two years.

</details>


### [11] [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)
*Zhibo Hu,Chen Wang,Yanfeng Shu,Hye-Young Paik,Liming Zhu*

Main category: cs.CL

TL;DR: 提出通过潜在空间表示差异检测自然语言歧义的新方法，提升LLM结构化数据映射性能


<details>
  <summary>Details</summary>
Motivation: 现有基于ReACT试错和监督微调的歧义处理方法存在局限，无法有效识别潜在空间概念缺失导致的歧义

Method: 利用稀疏自编码器(SAE)概念梯度路径核积分，设计新的歧义检测距离测量方法

Result: 开发出通过缺失概念预测提升LLM歧义场景下工具调用性能的新框架

Conclusion: 该方法为处理自然语言歧义提供了新的技术路径，可扩展应用于工具调用之外的文本-SQL等结构化映射任务

Abstract: Ambiguity in natural language is a significant obstacle for achieving
accurate text to structured data mapping through large language models (LLMs),
which affects the performance of tasks such as mapping text to agentic tool
calling and text-to-SQL queries. Existing methods of ambiguity handling either
exploit ReACT framework to produce the correct mapping through trial and error,
or supervised fine tuning to guide models to produce a biased mapping to
improve certain tasks. In this paper, we adopt a different approach that
characterizes the representation difference of ambiguous text in the latent
space and leverage the difference to identify ambiguity before mapping them to
structured data. To detect ambiguity of a sentence, we focused on the
relationship between ambiguous questions and their interpretations and what
cause the LLM ignore multiple interpretations. Different to the distance
calculated by dense embedding vectors, we utilize the observation that
ambiguity is caused by concept missing in latent space of LLM to design a new
distance measurement, computed through the path kernel by the integral of
gradient values for each concepts from sparse-autoencoder (SAE) under each
state. We identify patterns to distinguish ambiguous questions with this
measurement. Based on our observation, We propose a new framework to improve
the performance of LLMs on ambiguous agentic tool calling through missing
concepts prediction.

</details>


### [12] [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)
*Susanna Rücker,Alan Akbik*

Main category: cs.CL

TL;DR: 提出VerbalizED双编码器模型改进实体消歧任务，通过优化标签表达和负采样策略实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 评估双编码器在实体消歧任务中的关键设计决策（损失函数/相似性度量/标签表达/负采样策略）对性能的影响

Method: 文档级双编码器架构 + 上下文标签表达 + 高效硬负采样策略 + 迭代预测变体改进困难样本消歧

Result: 在AIDA-Yago验证有效性，ZELDA基准测试达到新SOTA（F1提升4.6%）

Conclusion: 关键设计选择显著提升性能，上下文标签表达与硬负采样策略的组合形成高效实体消歧系统

Abstract: Entity disambiguation (ED) is the task of linking mentions in text to
corresponding entries in a knowledge base. Dual Encoders address this by
embedding mentions and label candidates in a shared embedding space and
applying a similarity metric to predict the correct label. In this work, we
focus on evaluating key design decisions for Dual Encoder-based ED, such as its
loss function, similarity metric, label verbalization format, and negative
sampling strategy. We present the resulting model VerbalizED, a document-level
Dual Encoder model that includes contextual label verbalizations and efficient
hard negative sampling. Additionally, we explore an iterative prediction
variant that aims to improve the disambiguation of challenging data points.
Comprehensive experiments on AIDA-Yago validate the effectiveness of our
approach, offering insights into impactful design choices that result in a new
State-of-the-Art system on the ZELDA benchmark.

</details>


### [13] [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)
*Sukairaj Hafiz Imam,Babangida Sani,Dawit Ketema Gete,Bedru Yimam Ahamed,Ibrahim Said Ahmad,Idris Abdulmumin,Seid Muhie Yimam,Muhammad Yahuza Bello,Shamsuddeen Hassan Muhammad*

Main category: cs.CL

TL;DR: 非洲低资源语言ASR面临数据/资源/伦理挑战，需社区驱动+轻量模型+隐私保护方案推动包容性发展


<details>
  <summary>Details</summary>
Motivation: 非洲语言在ASR研究中代表性不足导致数字鸿沟，需解决技术障碍以促进语言多样性与数字包容

Method: 结合案例分析与前沿技术（自监督学习/多语言建模），评估社区数据采集和轻量化架构的应用效果

Result: 试点项目验证定制方案可行性，语素建模和领域专用ASR在医疗/教育场景展现应用潜力

Conclusion: 需跨学科合作与持续投资，构建伦理优先的ASR系统以保护语言多样性并提升非洲语言使用者数字参与

Abstract: Automatic Speech Recognition (ASR) technologies have transformed
human-computer interaction; however, low-resource languages in Africa remain
significantly underrepresented in both research and practical applications.
This study investigates the major challenges hindering the development of ASR
systems for these languages, which include data scarcity, linguistic
complexity, limited computational resources, acoustic variability, and ethical
concerns surrounding bias and privacy. The primary goal is to critically
analyze these barriers and identify practical, inclusive strategies to advance
ASR technologies within the African context. Recent advances and case studies
emphasize promising strategies such as community-driven data collection,
self-supervised and multilingual learning, lightweight model architectures, and
techniques that prioritize privacy. Evidence from pilot projects involving
various African languages showcases the feasibility and impact of customized
solutions, which encompass morpheme-based modeling and domain-specific ASR
applications in sectors like healthcare and education. The findings highlight
the importance of interdisciplinary collaboration and sustained investment to
tackle the distinct linguistic and infrastructural challenges faced by the
continent. This study offers a progressive roadmap for creating ethical,
efficient, and inclusive ASR systems that not only safeguard linguistic
diversity but also improve digital accessibility and promote socioeconomic
participation for speakers of African languages.

</details>


### [14] [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)
*Ana Ezquerro,David Vilares,Anssi Yli-Jyrä,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 提出基于分层括号的序列标注编码方法，优化标签数量并支持非投射结构，在多个树库中取得竞争性准确率。


<details>
  <summary>Details</summary>
Motivation: 现有4-bit投射编码存在标签冗余问题，需开发更紧凑的编码方式同时支持投射/非投射依存树解析。

Method: 1. 构建最优分层括号编码（12标签替代原有16标签） 2. 扩展支持非投射结构的紧凑编码方案

Result: 在多种类型树库上验证，新编码方案达到竞争性解析准确率

Conclusion: 分层括号编码通过符号最小化实现高效依存解析，在保持精度的同时显著提升编码效率，并更好支持非投射结构。

Abstract: We present a family of encodings for sequence labeling dependency parsing,
based on the concept of hierarchical bracketing. We prove that the existing
4-bit projective encoding belongs to this family, but it is suboptimal in the
number of labels used to encode a tree. We derive an optimal hierarchical
bracketing, which minimizes the number of symbols used and encodes projective
trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also
extend optimal hierarchical bracketing to support arbitrary non-projectivity in
a more compact way than previous encodings. Our new encodings yield competitive
accuracy on a diverse set of treebanks.

</details>


### [15] [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)
*Shun Inadumi,Nobuhiro Ueda,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 提出统一文本和多模态指代消解的框架，通过语义嵌入映射减少视觉对话中的歧义


<details>
  <summary>Details</summary>
Motivation: 现有短语定位局限于图像-标题配对，实际对话场景需整合文本/多模态指代消解来处理代词/省略引发的歧义

Method: 将mention嵌入映射到object嵌入空间，基于相似度选择mention或object

Result: 模型在代词短语定位任务上优于MDETR和GLIP，核心ference解析能增强mention-object置信度

Conclusion: 整合文本指代关系可有效降低视觉对话中的歧义，提升多模态推理的可靠性

Abstract: Multimodal reference resolution, including phrase grounding, aims to
understand the semantic relations between mentions and real-world objects.
Phrase grounding between images and their captions is a well-established task.
In contrast, for real-world applications, it is essential to integrate textual
and multimodal reference resolution to unravel the reference relations within
dialogue, especially in handling ambiguities caused by pronouns and ellipses.
This paper presents a framework that unifies textual and multimodal reference
resolution by mapping mention embeddings to object embeddings and selecting
mentions or objects based on their similarity. Our experiments show that
learning textual reference resolution, such as coreference resolution and
predicate-argument structure analysis, positively affects performance in
multimodal reference resolution. In particular, our model with coreference
resolution performs better in pronoun phrase grounding than representative
models for this task, MDETR and GLIP. Our qualitative analysis demonstrates
that incorporating textual reference relations strengthens the confidence
scores between mentions, including pronouns and predicates, and objects, which
can reduce the ambiguities that arise in visually grounded dialogues.

</details>


### [16] [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)
*Kevin Wu,Eric Wu,Rahul Thapa,Kevin Wei,Angela Zhang,Arvind Suresh,Jacqueline J. Tao,Min Woo Sun,Alejandro Lozano,James Zou*

Main category: cs.CL

TL;DR: 首个开放数据集MedCaseReasoning评估LLMs临床诊断推理能力，微调后模型性能显著提升


<details>
  <summary>Details</summary>
Motivation: 现有医学基准仅评估最终答案准确性，忽视临床推理过程质量，需开发新评估体系提升LLMs诊断可靠性

Method: 构建含14,489个临床案例的数据集，通过微调LLMs使其学习临床医生推理轨迹

Result: 最佳开源模型DeepSeek-R1初始准确率48%/召回率64%，微调后分别提升29%/41%

Conclusion: MedCaseReasoning有效提升LLMs医疗诊断性能，为医学AI研究提供重要基准资源

Abstract: Doctors and patients alike increasingly use Large Language Models (LLMs) to
diagnose clinical cases. However, unlike domains such as math or coding, where
correctness can be objectively defined by the final answer, medical diagnosis
requires both the outcome and the reasoning process to be accurate. Currently,
widely used medical benchmarks like MedQA and MMLU assess only accuracy in the
final answer, overlooking the quality and faithfulness of the clinical
reasoning process. To address this limitation, we introduce MedCaseReasoning,
the first open-access dataset for evaluating LLMs on their ability to align
with clinician-authored diagnostic reasoning. The dataset includes 14,489
diagnostic question-and-answer cases, each paired with detailed reasoning
statements derived from open-access medical case reports. We evaluate
state-of-the-art reasoning LLMs on MedCaseReasoning and find significant
shortcomings in their diagnoses and reasoning: for instance, the top-performing
open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy
and mentions only 64% of the clinician reasoning statements (recall). However,
we demonstrate that fine-tuning LLMs on the reasoning traces derived from
MedCaseReasoning significantly improves diagnostic accuracy and clinical
reasoning recall by an average relative gain of 29% and 41%, respectively. The
open-source dataset, code, and models are available at
https://github.com/kevinwu23/Stanford-MedCaseReasoning.

</details>


### [17] [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)
*Feijiang Han,Xiaodong Yu,Jianheng Tang,Lyle Ungar*

Main category: cs.CL

TL;DR: 通过调整大语言模型初始空语义token的注意力，ZeroTuning方法无需训练即可显著提升模型在多项任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有基于token注意力调整的方法依赖辅助机制识别关键token，存在偏差风险。研究发现初始空语义token作为注意力汇聚点，能更有效控制模型行为

Method: 理论证明调整初始token注意力可改变后续注意力分布，结合实验发现不同注意力头的调整规律，提出针对该token的头部特定注意力调整方法ZeroTuning

Result: 在文本分类（+11.71%）、QA（+2.64%）和多轮对话任务中显著提升Llama等模型性能，方法对资源限制、长上下文等场景表现鲁棒

Conclusion: 揭示了LLMs中一个被忽视的关键控制点，为推理时优化和模型可解释性研究提供了新方向，ZeroTuning展示了无训练调优的潜力

Abstract: Recently, training-free methods for improving large language models (LLMs)
have attracted growing interest, with token-level attention tuning emerging as
a promising and interpretable direction. However, existing methods typically
rely on auxiliary mechanisms to identify important or irrelevant task-specific
tokens, introducing potential bias and limiting applicability. In this paper,
we uncover a surprising and elegant alternative: the semantically empty initial
token is a powerful and underexplored control point for optimizing model
behavior. Through theoretical analysis, we show that tuning the initial token's
attention sharpens or flattens the attention distribution over subsequent
tokens, and its role as an attention sink amplifies this effect. Empirically,
we find that: (1) tuning its attention improves LLM performance more
effectively than tuning other task-specific tokens; (2) the effect follows a
consistent trend across layers, with earlier layers having greater impact, but
varies across attention heads, with different heads showing distinct
preferences in how they attend to this token. Based on these findings, we
propose ZeroTuning, a training-free approach that improves LLM performance by
applying head-specific attention adjustments to this special token. Despite
tuning only one token, ZeroTuning achieves higher performance on text
classification, multiple-choice, and multi-turn conversation tasks across
models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves
Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its
multi-turn score from 7.804 to 7.966. The method is also robust to limited
resources, few-shot settings, long contexts, quantization, decoding strategies,
and prompt variations. Our work sheds light on a previously overlooked control
point in LLMs, offering new insights into both inference-time tuning and model
interpretability.

</details>


### [18] [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)
*Xianglong Xu,John Bowen,Rojin Taheri*

Main category: cs.CL

TL;DR: 提出掩码输入正则化方法，通过随机掩码提升Transformer模型性能，实验显示在多种NLP任务中稳定优于标准正则化方法，最佳掩码率p=0.1，效果源于抗过拟合和隐式模型集成


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在文本分类中表现优异，但研究者希望探索通过输入掩码的正则化方式进一步挖掘模型潜力，改善对深层语义依赖关系的捕捉能力

Method: 提出token masking regularization方法：训练时以概率p随机将输入token替换为[MASK]，通过梯度层面的隐式平均机制，强制模型建立更鲁棒的token间依赖关系。在mBERT、Qwen2.5-0.5B等多个模型架构上进行跨任务验证

Result: 在语言识别和情感分析任务中，该方法持续优于传统正则化技术。发现任务特异性最优掩码率（通用场景推荐p=0.1），性能提升源于：(1)输入扰动抑制过拟合 (2)梯度平滑产生隐式集成效果

Conclusion: 掩码输入正则化是简单有效的改进策略，其跨模型和跨任务的稳定提升表明该方法具有广泛适用性，为Transformer优化提供了新的正则化视角

Abstract: While transformer-based models achieve strong performance on text
classification, we explore whether masking input tokens can further enhance
their effectiveness. We propose token masking regularization, a simple yet
theoretically motivated method that randomly replaces input tokens with a
special [MASK] token at probability p. This introduces stochastic perturbations
during training, leading to implicit gradient averaging that encourages the
model to capture deeper inter-token dependencies. Experiments on language
identification and sentiment analysis -- across diverse models (mBERT,
Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard
regularization techniques. We identify task-specific optimal masking rates,
with p = 0.1 as a strong general default. We attribute the gains to two key
effects: (1) input perturbation reduces overfitting, and (2) gradient-level
smoothing acts as implicit ensembling.

</details>


### [19] [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)
*Wenyu Huang,Pavlos Vougiouklis,Mirella Lapata,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 研究探讨语言模型在多跳问答任务中的表现，发现编码器-解码器架构优于纯解码器模型，通过调整文档顺序和注意力机制可提升性能


<details>
  <summary>Details</summary>
Motivation: 传统因果掩码限制了语言模型在复杂上下文中的多跳推理能力，需探索不同文档排列对模型表现的影响

Method: 通过排列搜索结果顺序测试模型表现，分析注意力权重分布，并修改因果掩码引入双向注意力机制

Result: 1. Flan-T5系列模型优于同任务解码器模型 2. 文档顺序与推理链一致时效果最佳 3. 双向注意力机制提升解码器模型性能 4. 注意力权重峰值与答案正确性正相关

Conclusion: 研究揭示了模型架构选择、文档顺序优化和注意力机制改进对多跳问答的影响，并提出基于注意力权重的启发式优化方法

Abstract: Multi-hop Question Answering (MHQA) adds layers of complexity to question
answering, making it more challenging. When Language Models (LMs) are prompted
with multiple search results, they are tasked not only with retrieving relevant
information but also employing multi-hop reasoning across the information
sources. Although LMs perform well on traditional question-answering tasks, the
causal mask can hinder their capacity to reason across complex contexts. In
this paper, we explore how LMs respond to multi-hop questions by permuting
search results (retrieved documents) under various configurations. Our study
reveals interesting findings as follows: 1) Encoder-decoder models, such as the
ones in the Flan-T5 family, generally outperform causal decoder-only LMs in
MHQA tasks, despite being significantly smaller in size; 2) altering the order
of gold documents reveals distinct trends in both Flan T5 models and fine-tuned
decoder-only models, with optimal performance observed when the document order
aligns with the reasoning chain order; 3) enhancing causal decoder-only models
with bi-directional attention by modifying the causal mask can effectively
boost their end performance. In addition to the above, we conduct a thorough
investigation of the distribution of LM attention weights in the context of
MHQA. Our experiments reveal that attention weights tend to peak at higher
values when the resulting answer is correct. We leverage this finding to
heuristically improve LMs' performance on this task. Our code is publicly
available at https://github.com/hwy9855/MultiHopQA-Reasoning.

</details>


### [20] [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)
*Raymond Baartmans,Matthew Raffel,Rahul Vikram,Aiden Deringer,Lizhong Chen*

Main category: cs.CL

TL;DR: 利用大语言模型自动生成自然语义元语言解释，1B/8B模型超越GPT-4o实现跨语言语义表示


<details>
  <summary>Details</summary>
Motivation: 传统NSM解释生成效率低下，需要自动化解决方案推动NLP应用发展

Method: 开发自动评估体系，构建专用数据集，并微调不同规模的LLM模型

Result: 1B/8B模型在生成准确性和跨语言适配性上显著优于GPT-4o

Conclusion: 首次实现LLM驱动的通用语义表示突破，为语义分析和机器翻译开辟新路径

Abstract: The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a
universal set of semantic primes: simple, primitive word-meanings that have
been shown to exist in most, if not all, languages of the world. According to
this framework, any word, regardless of complexity, can be paraphrased using
these primes, revealing a clear and universally translatable meaning. These
paraphrases, known as explications, can offer valuable applications for many
natural language processing (NLP) tasks, but producing them has traditionally
been a slow, manual process. In this work, we present the first study of using
large language models (LLMs) to generate NSM explications. We introduce
automatic evaluation methods, a tailored dataset for training and evaluation,
and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in
producing accurate, cross-translatable explications, marking a significant step
toward universal semantic representation with LLMs and opening up new
possibilities for applications in semantic analysis, translation, and beyond.

</details>


### [21] [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)
*Yufei Xiang,Yiqun Shen,Yeqin Zhang,Cam-Tu Nguyen*

Main category: cs.CL

TL;DR: 提出Retrospex框架，通过结合LLM行为概率与基于强化学习的经验评估模块，显著提升智能体在复杂环境中的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架未能充分利用历史经验优化决策机制，需开发能深度分析经验数据的新型架构。

Method: 1. 分离经验存储与模型上下文
2. 通过离线强化学习训练Critic评估模块
3. 动态调整经验价值权重（环境交互需求越高，经验价值占比越大）

Result: 在ScienceWorld、ALFWorld和Webshop环境中验证，性能超越现有基线方法

Conclusion: 经验回溯机制与动态评分策略的结合，为LLM智能体的持续学习提供了有效范式

Abstract: Large Language Models (LLMs) possess extensive knowledge and commonsense
reasoning capabilities, making them valuable for creating powerful agents.
However, existing LLM agent frameworks have not fully utilized past experiences
for improvement. This work introduces a new LLM-based agent framework called
Retrospex, which addresses this challenge by analyzing past experiences in
depth. Unlike previous approaches, Retrospex does not directly integrate
experiences into the LLM's context. Instead, it combines the LLM's action
likelihood with action values estimated by a Reinforcement Learning (RL)
Critic, which is trained on past experiences through an offline
''retrospection'' process. Additionally, Retrospex employs a dynamic action
rescoring mechanism that increases the importance of experience-based values
for tasks that require more interaction with the environment. We evaluate
Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its
advantages over strong, contemporary baselines.

</details>


### [22] [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)
*Shen Li,Renfen Hu,Lijun Wang*

Main category: cs.CL

TL;DR: 18亿参数的文言文大模型AI Taiyan在标点/典故识别等任务中超越通用模型和传统模型，接近人类水平


<details>
  <summary>Details</summary>
Motivation: 通用大模型在文言文领域表现欠佳，传统微调方法难以有效融入领域知识

Method: 通过合理模型设计+数据清洗+基础训练+微调的三阶段训练策略，构建18亿参数专用模型

Result: 在文言文标点/典故识别/词义解释/古今翻译任务中准确率超GPT-4 10个百分点，部分任务达人类专家水平

Conclusion: 验证了小规模专用模型的技术路径，为古籍整理/词典编纂提供智能支持，推动计算语言学研究范式创新

Abstract: General-purpose large language models demonstrate notable capabilities in
language comprehension and generation, achieving results that are comparable
to, or even surpass, human performance in many language information processing
tasks. Nevertheless, when general models are applied to some specific domains,
e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and
fine-tuning open-source foundational models similarly struggles to adequately
incorporate domain-specific knowledge. To address this challenge, this study
developed a large language model, AI Taiyan, specifically designed for
understanding and generating Classical Chinese. Experiments show that with a
reasonable model design, data processing, foundational training, and
fine-tuning, satisfactory results can be achieved with only 1.8 billion
parameters. In key tasks related to Classical Chinese information processing
such as punctuation, identification of allusions, explanation of word meanings,
and translation between ancient and modern Chinese, this model exhibits a clear
advantage over both general-purpose large models and domain-specific
traditional models, achieving levels close to or surpassing human baselines.
This research provides a reference for the efficient construction of
specialized domain-specific large language models. Furthermore, the paper
discusses the application of this model in fields such as the collation of
ancient texts, dictionary editing, and language research, combined with case
studies.

</details>


### [23] [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)
*Taolin Zhang,Dongyang Li,Qizhou Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.CL

TL;DR: 提出BELLE分层多代理框架，通过问题类型与方法匹配优化多跳问答，实验证明高效且具成本效益


<details>
  <summary>Details</summary>
Motivation: 现有方法未考虑问题类型对多跳QA方法敏感度的影响，需建立类型与方法的动态对应关系

Method: 1. 第一层多代理辩论生成组合方法方案 2. 第二层快慢思考者监控观点合理性 3. 将不同方法封装为可组合的LLM提示操作符

Result: BELLE在多个数据集显著超越基线模型，复杂场景成本效益提升30%以上

Conclusion: 分层辩论机制结合问题类型适配方法策略，为动态多跳QA推理提供新范式

Abstract: Multi-hop question answering (QA) involves finding multiple relevant passages
and performing step-by-step reasoning to answer complex questions. Previous
works on multi-hop QA employ specific methods from different modeling
perspectives based on large language models (LLMs), regardless of the question
types. In this paper, we first conduct an in-depth analysis of public multi-hop
QA benchmarks, dividing the questions into four types and evaluating five types
of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,
Iterative-step, Sub-step, and Adaptive-step. We find that different types of
multi-hop questions have varying degrees of sensitivity to different types of
methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to
address multi-hop QA by specifically focusing on the correspondence between
question types and methods, where each type of method is regarded as an
''operator'' by prompting LLMs differently. The first level of BELLE includes
multiple agents that debate to obtain an executive plan of combined
''operators'' to address the multi-hop QA task comprehensively. During the
debate, in addition to the basic roles of affirmative debater, negative
debater, and judge, at the second level, we further leverage fast and slow
debaters to monitor whether changes in viewpoints are reasonable. Extensive
experiments demonstrate that BELLE significantly outperforms strong baselines
in various datasets. Additionally, the model consumption of BELLE is higher
cost-effectiveness than that of single models in more complex multi-hop QA
scenarios.

</details>


### [24] [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)
*Kaitao Song,Xiaohua Wang,Xu Tan,Huiqiang Jiang,Chengruidong Zhang,Yongliang Shen,Cen LU,Zihao Li,Zifan Song,Caihua Shan,Yansen Wang,Kan Ren,Xiaoqing Zheng,Tao Qin,Yuqing Yang,Dongsheng Li,Lili Qiu*

Main category: cs.CL

TL;DR: 提出Chain-of-Model（CoM）学习范式，通过隐藏状态的链式结构提升模型扩展性和推理弹性，开发了CoLM及其改进版CoLM-Air实现训练效率与部署灵活性的平衡


<details>
  <summary>Details</summary>
Motivation: 传统模型在扩展时需重新训练整个架构且缺乏弹性推理能力，CoM通过因果关系链式结构解决模型扩展效率低和部署不灵活的问题

Method: 1. 提出Chain-of-Representation（CoR）将隐藏状态分解为多个维度链，前序链决定后续链
2. 基于Transformer构建CoLM
3. 改进CoLM-Air通过首链共享KV机制实现计算优化

Result: CoLM系列在保持与标准Transformer相当性能的同时，支持渐进式扩展（训练效率提升2.4倍）和弹性推理（支持从1.3B到13B的多尺寸子模型）

Conclusion: CoM范式通过模块化链式结构，为语言模型提供了训练阶段渐进扩展和部署阶段弹性推理的新范式，KV共享机制进一步扩展了应用场景

Abstract: In this paper, we propose a novel learning paradigm, termed Chain-of-Model
(CoM), which incorporates the causal relationship into the hidden states of
each layer as a chain style, thereby introducing great scaling efficiency in
model training and inference flexibility in deployment. We introduce the
concept of Chain-of-Representation (CoR), which formulates the hidden states at
each layer as a combination of multiple sub-representations (i.e., chains) at
the hidden dimension level. In each layer, each chain from the output
representations can only view all of its preceding chains in the input
representations. Consequently, the model built upon CoM framework can
progressively scale up the model size by increasing the chains based on the
previous models (i.e., chains), and offer multiple sub-models at varying sizes
for elastic inference by using different chain numbers. Based on this
principle, we devise Chain-of-Language-Model (CoLM), which incorporates the
idea of CoM into each layer of Transformer architecture. Based on CoLM, we
further introduce CoLM-Air by introducing a KV sharing mechanism, that computes
all keys and values within the first chain and then shares across all chains.
This design demonstrates additional extensibility, such as enabling seamless LM
switching, prefilling acceleration and so on. Experimental results demonstrate
our CoLM family can achieve comparable performance to the standard Transformer,
while simultaneously enabling greater flexiblity, such as progressive scaling
to improve training efficiency and offer multiple varying model sizes for
elastic inference, paving a a new way toward building language models. Our code
will be released in the future at: https://github.com/microsoft/CoLM.

</details>


### [25] [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)
*Yansong Ning,Wei Li,Jun Fang,Naiqiang Tan,Hao Liu*

Main category: cs.CL

TL;DR: 提出Long⊗Short框架，通过区分关键思考与普通思考，使用长/短思考双LLM协同实现高效推理，压缩80%推理长度并保持性能


<details>
  <summary>Details</summary>
Motivation: 现有方法对所有思考进行均等压缩，无法平衡推理效果与效率。通过蒙特卡洛推演发现不同思考的重要性差异，需建立量化指标实现更精准的压缩

Method: 1) 提出理论边界指标联合评估思考有效性/效率 2) 构建长思考LLM（生成关键思考）+短思考LLM（生成普通思考）协同框架 3) 通过合成冷启动数据微调模型 4) 多轮强化学习优化协作

Result: Qwen2.5-7B/Llama3.1-8B在MATH500等基准上达到对标模型性能，同时减少80%+推理token量

Conclusion: 双模型协同显著提升推理效率，理论指标有效指导思考重要性划分，框架兼具实用性与可复现性（开源代码数据）

Abstract: Compressing long chain-of-thought (CoT) from large language models (LLMs) is
an emerging strategy to improve the reasoning efficiency of LLMs. Despite its
promising benefits, existing studies equally compress all thoughts within a
long CoT, hindering more concise and effective reasoning. To this end, we first
investigate the importance of different thoughts by examining their
effectiveness and efficiency in contributing to reasoning through automatic
long CoT chunking and Monte Carlo rollouts. Building upon the insights, we
propose a theoretically bounded metric to jointly measure the effectiveness and
efficiency of different thoughts. We then propose Long$\otimes$Short, an
efficient reasoning framework that enables two LLMs to collaboratively solve
the problem: a long-thought LLM for more effectively generating important
thoughts, while a short-thought LLM for efficiently generating remaining
thoughts. Specifically, we begin by synthesizing a small amount of cold-start
data to fine-tune LLMs for long-thought and short-thought reasoning styles,
respectively. Furthermore, we propose a synergizing-oriented multi-turn
reinforcement learning, focusing on the model self-evolution and collaboration
between long-thought and short-thought LLMs. Experimental results show that our
method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance
compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while
reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and
GPQA Diamond benchmarks. Our data and code are available at
https://github.com/yasNing/Long-otimes-Short/.

</details>


### [26] [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)
*Chenlu Wang,Weimin Lyu,Ritwik Banerjee*

Main category: cs.CL

TL;DR: 提出ClaD类蒸馏方法，通过结构化的损失函数和可解释决策算法，显著提升特定语言检测任务的效率，小模型性能媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 现有语言检测模型存在高计算成本和大数据依赖问题，需开发更高效的轻量化解决方案。

Method: 1. 基于马氏距离的类分布结构化损失函数
2. 面向类分离优化的可解释决策算法
3. 类蒸馏框架从异构背景中提取目标类别特征

Result: 在性别歧视/隐喻/讽刺检测任务中超越基线模型，参数量减少2个数量级时性能仍接近GPT-3.5(175B)

Conclusion: ClaD为从复杂语言背景中提取小规模目标类提供了高效解决方案，在保持模型轻量化同时实现与大模型相当的检测精度。

Abstract: Detecting deviant language such as sexism, or nuanced language such as
metaphors or sarcasm, is crucial for enhancing the safety, clarity, and
interpretation of online social discourse. While existing classifiers deliver
strong results on these tasks, they often come with significant computational
cost and high data demands. In this work, we propose \textbf{Cla}ss
\textbf{D}istillation (ClaD), a novel training paradigm that targets the core
challenge: distilling a small, well-defined target class from a highly diverse
and heterogeneous background. ClaD integrates two key innovations: (i) a loss
function informed by the structural properties of class distributions, based on
Mahalanobis distance, and (ii) an interpretable decision algorithm optimized
for class separation. Across three benchmark detection tasks -- sexism,
metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with
smaller language models and orders of magnitude fewer parameters, achieves
performance comparable to several large language models (LLMs). These results
demonstrate ClaD as an efficient tool for pragmatic language understanding
tasks that require gleaning a small target class from a larger heterogeneous
background.

</details>


### [27] [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)
*Hongliang Li,Jinan Xu,Gengping Cui,Changhao Guan,Fengran Mo,Kaiyu Huang*

Main category: cs.CL

TL;DR: 提出多语言协作防御（MCD）方法，通过优化持续软安全提示提升大语言模型的多语言安全防护能力，在防御多语言越狱攻击中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在多语言场景下存在安全防护薄弱点，攻击者可通过低资源语言绕过安全机制，需加强多语言安全对齐能力

Method: 基于跨语言攻击特征相关性分析，设计自动优化连续软安全提示的学习框架，通过多语言协同训练实现安全防护

Result: 构建多语言越狱基准测试集（含MaliciousInstruct/AdvBench多语言版），实验证明MCD防御成功率提升20%且误拒率降低15%，在零样本语言场景展现强迁移能力

Conclusion: MCD有效平衡语言安全对齐，缓解语料库训练偏差带来的安全问题，其协作防御机制为多语言模型安全提供新范式，代码已开源

Abstract: The robustness and security of large language models (LLMs) has become a
prominent research area. One notable vulnerability is the ability to bypass LLM
safeguards by translating harmful queries into rare or underrepresented
languages, a simple yet effective method of "jailbreaking" these models.
Despite the growing concern, there has been limited research addressing the
safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to
enhance multilingual safety. In this work, we investigate the correlation
between various attack features across different languages and propose
Multilingual Collaborative Defense (MCD), a novel learning method that
optimizes a continuous, soft safety prompt automatically to facilitate
multilingual safeguarding of LLMs. The MCD approach offers three advantages:
First, it effectively improves safeguarding performance across multiple
languages. Second, MCD maintains strong generalization capabilities while
minimizing false refusal rates. Third, MCD mitigates the language safety
misalignment caused by imbalances in LLM training corpora. To evaluate the
effectiveness of MCD, we manually construct multilingual versions of commonly
used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess
various safeguarding methods. Additionally, we introduce these datasets in
underrepresented (zero-shot) languages to verify the language transferability
of MCD. The results demonstrate that MCD outperforms existing approaches in
safeguarding against multilingual jailbreak attempts while also exhibiting
strong language transfer capabilities. Our code is available at
https://github.com/HLiang-Lee/MCD.

</details>


### [28] [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)
*Guijin Son,Jiwoo Hong,Honglu Fan,Heejeong Nam,Hyunwoo Ko,Seungwon Lim,Jinyeop Song,Jinha Choi,Gonçalo Paulo,Youngjae Yu,Stella Biderman*

Main category: cs.CL

TL;DR: 当前大语言模型在学术论文验证任务中表现不佳（最佳模型o3召回率21.1%/准确率6.1%），与实用化要求存在显著差距


<details>
  <summary>Details</summary>
Motivation: 探索LLMs作为科学论文验证工具的可能性，突破其传统生成式应用场景

Method: 构建包含83篇论文及91个重大错误的SPOT数据集，通过多轮人类验证，评估主流LLM的验证能力

Result: 模型表现不稳定（不同运行间错误检测结果差异大），置信度低，且错误类型类似学生级概念误解

Conclusion: 现有LLM尚不具备可靠的学术验证能力，需突破模型理解深度与一致性难题

Abstract: Recent advances in large language models (LLMs) have fueled the vision of
automated scientific discovery, often called AI Co-Scientists. To date, prior
work casts these systems as generative co-authors responsible for crafting
hypotheses, synthesizing code, or drafting manuscripts. In this work, we
explore a complementary application: using LLMs as verifiers to automate the
\textbf{academic verification of scientific manuscripts}. To that end, we
introduce SPOT, a dataset of 83 published papers paired with 91 errors
significant enough to prompt errata or retraction, cross-validated with actual
authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find
that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best
scores, with all others near zero). Furthermore, confidence estimates are
uniformly low, and across eight independent runs, models rarely rediscover the
same errors, undermining their reliability. Finally, qualitative analysis with
domain experts reveals that even the strongest models make mistakes resembling
student-level misconceptions derived from misunderstandings. These findings
highlight the substantial gap between current LLM capabilities and the
requirements for dependable AI-assisted academic verification.

</details>


### [29] [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Shuai Wang*

Main category: cs.CL

TL;DR: NAMET通过向MEMIT添加噪声机制，有效解决大规模模型编辑中的嵌入碰撞问题


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法在大规模知识更新时出现可靠性下降，主要由于知识项之间的嵌入碰撞

Method: 在Transformer的memory提取阶段引入噪声（仅需单行代码修改MEMIT）

Result: 在6个LLM和3个数据集上的实验表明，NAMET在数千条事实编辑中持续优于现有方法

Conclusion: 噪声感知机制是提升大规模模型编辑效果的有效解决方案

Abstract: Model editing techniques are essential for efficiently updating knowledge in
large language models (LLMs). However, the effectiveness of existing approaches
degrades in massive editing scenarios, particularly when evaluated with
practical metrics or in context-rich settings. We attribute these failures to
embedding collisions among knowledge items, which undermine editing reliability
at scale. To address this, we propose NAMET (Noise-aware Model Editing in
Transformers), a simple yet effective method that introduces noise during
memory extraction via a one-line modification to MEMIT. Extensive experiments
across six LLMs and three datasets demonstrate that NAMET consistently
outperforms existing methods when editing thousands of facts.

</details>


### [30] [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)
*Xiechi Zhang,Zetian Ouyang,Linlin Wang,Gerard de Melo,Zhu Cao,Xiaoling Wang,Ya Zhang,Yanfeng Wang,Liang He*

Main category: cs.CL

TL;DR: 提出开源自动评估模型AutoMedEval(13B参数)，通过分层训练方法解决医疗大模型问答能力评估难题


<details>
  <summary>Details</summary>
Motivation: 传统评估指标忽略医学术语价值，人工评估成本高且存在误差，现有LLM评估方法在医疗领域存在适用性缺陷

Method: 采用课程指令微调与知识内省机制的层次化训练框架，实现有限数据下的专业医疗评估能力获取

Result: 人工评估显示AutoMedEval与人类判断相关性优于基线模型

Conclusion: AutoMedEval有效降低对人工评估的依赖，为医疗LLM评估提供可靠解决方案

Abstract: With the proliferation of large language models (LLMs) in the medical domain,
there is increasing demand for improved evaluation techniques to assess their
capabilities. However, traditional metrics like F1 and ROUGE, which rely on
token overlaps to measure quality, significantly overlook the importance of
medical terminology. While human evaluation tends to be more reliable, it can
be very costly and may as well suffer from inaccuracies due to limits in human
expertise and motivation. Although there are some evaluation methods based on
LLMs, their usability in the medical field is limited due to their proprietary
nature or lack of expertise. To tackle these challenges, we present
AutoMedEval, an open-sourced automatic evaluation model with 13B parameters
specifically engineered to measure the question-answering proficiency of
medical LLMs. The overarching objective of AutoMedEval is to assess the quality
of responses produced by diverse models, aspiring to significantly reduce the
dependence on human evaluation. Specifically, we propose a hierarchical
training method involving curriculum instruction tuning and an iterative
knowledge introspection mechanism, enabling AutoMedEval to acquire professional
medical assessment capabilities with limited instructional data. Human
evaluations indicate that AutoMedEval surpasses other baselines in terms of
correlation with human judgments.

</details>


### [31] [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)
*Weikai Xu,Zhizheng Jiang,Yuxuan Liu,Wei Liu,Jian Luan,Yuanchun Li,Yunxin Liu,Bin Wang,Bo An*

Main category: cs.CL

TL;DR: 提出Mobile-Bench-v2基准测试，通过多路径评估、噪声环境模拟和主动交互测试改进移动代理评估体系


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在动态环境奖励不稳定、单路径评估与真实多解任务不匹配、缺乏噪声应用和完整交互评估的问题

Method: 使用基于插槽的指令生成方法构建基准，包含离线多路径评估、基于弹窗广告的噪声测试集(AITZ-Noise)和预设问答的模糊指令交互测试

Result: 新基准在AppAgent-v1、Mobile-Agent-v2等框架上完成验证，证明评估体系的有效性

Conclusion: Mobile-Bench-v2通过多维度环境模拟和交互测试机制，建立了更全面的移动代理评估标准

Abstract: VLM-based mobile agents are increasingly popular due to their capabilities to
interact with smartphone GUIs and XML-structured texts and to complete daily
tasks. However, existing online benchmarks struggle with obtaining stable
reward signals due to dynamic environmental changes. Offline benchmarks
evaluate the agents through single-path trajectories, which stands in contrast
to the inherently multi-solution characteristics of GUI tasks. Additionally,
both types of benchmarks fail to assess whether mobile agents can handle noise
or engage in proactive interactions due to a lack of noisy apps or overly full
instructions during the evaluation process. To address these limitations, we
use a slot-based instruction generation method to construct a more realistic
and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a
common task split, with offline multi-path evaluation to assess the agent's
ability to obtain step rewards during task execution. It contains a noisy split
based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to
formulate a real noisy environment. Furthermore, an ambiguous instruction split
with preset Q\&A interactions is released to evaluate the agent's proactive
interaction capabilities. We conduct evaluations on these splits using the
single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,
as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are
available at https://huggingface.co/datasets/xwk123/MobileBench-v2.

</details>


### [32] [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)
*Zepeng Ding,Dixuan Wang,Ziqin Luo,Guochao Jiang,Deqing Yang,Jiaqing Liang*

Main category: cs.CL

TL;DR: 提出强化学习增强的自适应规划框架RLAP，通过建模NLP任务为马尔可夫决策过程并训练轻量级Actor模型，实现基于语言特征的动态子任务排序优化。


<details>
  <summary>Details</summary>
Motivation: 现有多步规划方法忽略实例语言特征，过度依赖LLM内在规划能力导致次优结果，需建立量化评估机制实现自适应子任务排序。

Method: 1. 将NLP任务建模为MDP过程 2. 设计Actor模型通过强化学习估计自然语言序列的Q值 3. Actor与LLM交互动态确定子任务最优顺序

Result: 在三大类NLP任务、多个数据集上的实验验证了框架有效性，平均性能提升2.1-5.7%且保持鲁棒性。

Conclusion: RLAP通过融合强化学习与语言模型规划，突破了静态路径规划的局限性，为复杂NLP任务提供新范式。

Abstract: Multi-step planning has been widely employed to enhance the performance of
large language models (LLMs) on downstream natural language processing (NLP)
tasks, which decomposes the original task into multiple subtasks and guide LLMs
to solve them sequentially without additional training. When addressing task
instances, existing methods either preset the order of steps or attempt
multiple paths at each step. However, these methods overlook instances'
linguistic features and rely on the intrinsic planning capabilities of LLMs to
evaluate intermediate feedback and then select subtasks, resulting in
suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this
paper we propose a Reinforcement Learning enhanced Adaptive Planning framework
(RLAP). In our framework, we model an NLP task as a Markov decision process
(MDP) and employ an LLM directly into the environment. In particular, a
lightweight Actor model is trained to estimate Q-values for natural language
sequences consisting of states and actions through reinforcement learning.
Therefore, during sequential planning, the linguistic features of each sequence
in the MDP can be taken into account, and the Actor model interacts with the
LLM to determine the optimal order of subtasks for each task instance. We apply
RLAP on three different types of NLP tasks and conduct extensive experiments on
multiple datasets to verify RLAP's effectiveness and robustness.

</details>


### [33] [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)
*Philipp Christmann,Gerhard Weikum*

Main category: cs.CL

TL;DR: Proposes ReQAP, a method for question answering over mixed personal data sources via executable operator trees, and releases PerQA benchmark.


<details>
  <summary>Details</summary>
Motivation: Enable efficient, privacy-preserving access to heterogeneous personal data (text/tables) on user devices without transferring data externally.

Method: Recursive decomposition of questions into executable operator trees that integrate structured/unstructured data sources seamlessly.

Result: Developed PerQA benchmark with realistic persona-based queries; ReQAP enables traceable answers while keeping data locally.

Conclusion: ReQAP addresses on-device QA challenges through modular operators and validated by PerQA, balancing usability and data privacy.

Abstract: Question answering over mixed sources, like text and tables, has been
advanced by verbalizing all contents and encoding it with a language model. A
prominent case of such heterogeneous data is personal information: user devices
log vast amounts of data every day, such as calendar entries, workout
statistics, shopping records, streaming history, and more. Information needs
range from simple look-ups to queries of analytical nature. The challenge is to
provide humans with convenient access with small footprint, so that all
personal data stays on the user devices. We present ReQAP, a novel method that
creates an executable operator tree for a given question, via recursive
decomposition. Operators are designed to enable seamless integration of
structured and unstructured sources, and the execution of the operator tree
yields a traceable answer. We further release the PerQA benchmark, with
persona-based data and questions, covering a diverse spectrum of realistic user
needs.

</details>


### [34] [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)
*Zhangyu Wang,Siyuan Gao,Rong Zhou,Hao Wang,Li Ning*

Main category: cs.CL

TL;DR: 提出无需嵌入的检索框架，结合LLMs逻辑推理能力与重要性度量机制，在长上下文QA任务中显著降低存储/计算开销的同时超越现有基线


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统依赖基于语义相似性的检索容易导致意图偏差，且基于图/层次结构的改进方案带来过高存储计算成本

Method: 通过迭代搜索空间优化策略，利用新型重要性度量指导检索范围扩展，在不需要显式构建图结构的情况下整合逻辑关联信息

Result: 在NovelQA和Marathon基准测试中取得SOTA表现，存储和运行时效率提升超过一个数量级

Conclusion: 验证了逻辑推理在检索过程中的关键作用，为长上下文处理提供兼顾效率与精度的新型解决方案

Abstract: Large Language Models (LLMs) have achieved impressive progress in natural
language processing, but their limited ability to retain long-term context
constrains performance on document-level or multi-turn tasks.
Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant
information from an external corpus. However, existing RAG systems often rely
on embedding-based retrieval trained on corpus-level semantic similarity, which
can lead to retrieving content that is semantically similar in form but
misaligned with the question's true intent. Furthermore, recent RAG variants
construct graph- or hierarchy-based structures to improve retrieval accuracy,
resulting in significant computation and storage overhead. In this paper, we
propose an embedding-free retrieval framework. Our method leverages the logical
inferencing ability of LLMs in retrieval using iterative search space
refinement guided by our novel importance measure and extend our retrieval
results with logically related information without explicit graph construction.
Experiments on long-context QA benchmarks, including NovelQA and Marathon, show
that our approach outperforms strong baselines while reducing storage and
runtime by over an order of magnitude.

</details>


### [35] [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)
*Yuheng Lu,ZiMeng Bai,Caixia Yuan,Huixing Jiang,Xiaojie Wang*

Main category: cs.CL

TL;DR: 提出MISO方法改进LLMs的复杂指令遵循能力，通过并行子上下文机制提升监督微调效果和训练效率


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在处理复杂多约束指令时，对关键子上下文关注不足导致效果受限

Method: 将顺序指令转换为并行子指令，设计混合上下文架构联合优化整体对齐与子上下文影响

Result: 实验证明MISO在复杂指令场景效果显著优于传统方法，并展现训练效率优势

Conclusion: MISO为LLMs微调提供新范式，有效提升复杂任务处理能力，具有实际应用潜力

Abstract: Large language models (LLMs) exhibit remarkable capabilities in handling
natural language tasks; however, they may struggle to consistently follow
complex instructions including those involve multiple constraints.
Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to
improve their ability to follow instructions. In addressing complex instruction
following, existing efforts primarily focus on data-driven methods that
synthesize complex instruction-output pairs for SFT. However, insufficient
attention allocated to crucial sub-contexts may reduce the effectiveness of
SFT. In this work, we propose transforming sequentially structured input
instruction into multiple parallel instructions containing subcontexts. To
support processing this multi-input, we propose MISO (Multi-Input
Single-Output), an extension to currently dominant decoder-only
transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that
jointly considers the overall instruction-output alignment and the influence of
individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning
to complex instructionfollowing datasets and evaluate it with standard LLM
inference. Empirical results demonstrate the superiority of MISO as a
fine-tuning method for LLMs, both in terms of effectiveness in complex
instruction-following scenarios and its potential for training efficiency.

</details>


### [36] [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)
*Yu-Ting Lee,Hui-Ying Shih,Fu-Chieh Chang,Pei-Yuan Wu*

Main category: cs.CL

TL;DR: 论文通过数学建模和实验验证，揭示了语言模型内在自我纠正机制通过提示诱导隐藏状态线性变化，增强潜在概念识别的原理。


<details>
  <summary>Details</summary>
Motivation: 解释语言模型通过自我迭代修正提升输出的内在机制，探索提示如何通过隐状态线性变化影响输出分布。

Method: 提出线性表示假设，构建数学框架分析自我修正过程，并在zephyr-7b-sft模型上进行文本去毒化实验验证。

Result: 实验显示有毒指令下，前100有毒token与后100无毒token在提示诱导偏移的内积存在显著差距（Δ=0.34）。

Conclusion: 自我修正提示通过增强潜在概念对齐，提升语言模型的内在纠错能力，为可解释性机制研究提供新视角。

Abstract: We provide an explanation for the performance gains of intrinsic
self-correction, a process where a language model iteratively refines its
outputs without external feedback. More precisely, we investigate how prompting
induces interpretable changes in hidden states and thus affects the output
distributions. We hypothesize that each prompt-induced shift lies in a linear
span of some linear representation vectors, naturally separating tokens based
on individual concept alignment. Building around this idea, we give a
mathematical formulation of self-correction and derive a concentration result
for output tokens based on alignment magnitudes. Our experiments on text
detoxification with zephyr-7b-sft reveal a substantial gap in the inner
products of the prompt-induced shifts and the unembeddings of the top-100 most
toxic tokens vs. those of the unembeddings of the bottom-100 least toxic
tokens, under toxic instructions. This suggests that self-correction prompts
enhance a language model's capability of latent concept recognition. Our
analysis offers insights into the underlying mechanism of self-correction by
characterizing how prompting works explainably. For reproducibility, our code
is available.

</details>


### [37] [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)
*Yuyao Zhang,Zhicheng Dou,Xiaoxi Li,Jiajie Jin,Yongkang Wu,Zhonghua Li,Qi Ye,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出神经符号框架QCompiler，通过BNF语法和编译器组件提升RAG系统对复杂查询的处理能力


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG系统在资源受限条件下处理具有嵌套结构和依赖关系的复杂查询时意图识别不精确的问题

Method: 设计最小完备BNF语法G[q]，开发包含查询翻译器、语法解析器和递归处理器的编译框架，生成抽象语法树执行查询

Result: 叶子节点子查询原子性实现更精准的文档检索与响应生成，系统处理复杂查询能力显著提升

Conclusion: QCompiler通过融合语言学规则与编译器设计，为复杂查询处理提供了可解释且高效的解决方案

Abstract: Precise recognition of search intent in Retrieval-Augmented Generation (RAG)
systems remains a challenging goal, especially under resource constraints and
for complex queries with nested structures and dependencies. This paper
presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar
rules and compiler design, to bridge this gap. It theoretically designs a
minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize
complex queries. Unlike previous methods, this grammar maintains completeness
while minimizing redundancy. Based on this, QCompiler includes a Query
Expression Translator, a Lexical Syntax Parser, and a Recursive Descent
Processor to compile queries into Abstract Syntax Trees (ASTs) for execution.
The atomicity of the sub-queries in the leaf nodes ensures more precise
document retrieval and response generation, significantly improving the RAG
system's ability to address complex queries.

</details>


### [38] [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)
*Xuanle Zhao,Xuexin Liu,Haoyue Yang,Xianzhen Luo,Fanhu Zeng,Jianling Li,Qi Shi,Chi Chen*

Main category: cs.CL

TL;DR: 提出ChartEdit基准评估多模态大语言模型在图表编辑任务中的表现，揭示其在精确修改方面的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 图表编辑任务需要整合图表理解、复杂推理和意图解释，但现有评估方法局限于案例研究，缺乏系统性框架。

Method: 构建包含1,405个多样化编辑指令和233个真实图表的基准，通过人工标注验证，并评估10个主流MLLMs在代码/图表层级的性能。

Result: 大规模模型生成的代码可部分匹配参考图像（SOTA得分59.96），但精确编辑能力有限；小规模模型在指令遵循和图表生成上均表现不足。

Conclusion: 当前MLLMs在精确图表编辑任务中面临显著挑战，ChartEdit为领域发展提供了系统性评估基准。

Abstract: Although multimodal large language models (MLLMs) show promise in generating
chart rendering code, chart editing presents a greater challenge. This
difficulty stems from its nature as a labor-intensive task for humans that also
demands MLLMs to integrate chart understanding, complex reasoning, and precise
intent interpretation. While many MLLMs claim such editing capabilities,
current assessments typically rely on limited case studies rather than robust
evaluation methodologies, highlighting the urgent need for a comprehensive
evaluation framework. In this work, we propose ChartEdit, a new high-quality
benchmark designed for chart editing tasks. This benchmark comprises $1,405$
diverse editing instructions applied to $233$ real-world charts, with each
instruction-chart instance having been manually annotated and validated for
accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream
MLLMs across two types of experiments, assessing them at both the code and
chart levels. The results suggest that large-scale models can generate code to
produce images that partially match the reference images. However, their
ability to generate accurate edits according to the instructions remains
limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,
highlighting significant challenges in precise modification. In contrast,
small-scale models, including chart-domain models, struggle both with following
editing instructions and generating overall chart images, underscoring the need
for further development in this area. Code is available at
https://github.com/xxlllz/ChartEdit.

</details>


### [39] [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)
*Aswini Kumar Padhi,Anil Bandhakavi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出HiPPrO框架，通过分层前缀学习和偏好优化的双阶段方法，结合多属性条件生成更有效的反仇恨言论，在意图符合度和文本质量上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有反言论生成研究多基于单一意图属性，而多属性联合优化能产生更细致有效的回应。需要开发能同时整合意图和情感双属性的生成框架。

Method: 两阶段框架：1）分层优化属性前缀嵌入空间 2）结合参考样本和无奖励偏好优化。扩展IntentCONANv2数据集，对1.3万样本进行五重情感标注。

Result: 意图符合度提升38%，Rouge指标提升2-3%。人类评估证实生成内容更具相关性和适当性。

Conclusion: 多属性条件优化能有效提升反言论生成系统的效果，分层前缀学习与偏好优化的结合为此类任务提供了新范式。

Abstract: Counterspeech has proven to be a powerful tool to combat hate speech online.
Previous studies have focused on generating counterspeech conditioned only on
specific intents (single attributed). However, a holistic approach considering
multiple attributes simultaneously can yield more nuanced and effective
responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with
Preference Optimization, a novel two-stage framework that utilizes the
effectiveness of attribute-specific prefix embedding spaces hierarchically
optimized during the counterspeech generation process in the first phase.
Thereafter, we incorporate both reference and reward-free preference
optimization to generate more constructive counterspeech. Furthermore, we
extend IntentCONANv2 by annotating all 13,973 counterspeech instances with
emotion labels by five annotators. HiPPrO leverages hierarchical prefix
optimization to integrate these dual attributes effectively. An extensive
evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent
conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,
respectively, compared to several baseline models. Human evaluations further
substantiate the superiority of our approach, highlighting the enhanced
relevance and appropriateness of the generated counterspeech. This work
underscores the potential of multi-attribute conditioning in advancing the
efficacy of counterspeech generation systems.

</details>


### [40] [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)
*Md. Rafiul Biswas,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: 本研究构建了阿拉伯语和英语的双语情感与希望言论标注数据集，通过基线模型验证了标注质量（micro-F1=0.67），为低资源语言的NLP研究提供了新资源。


<details>
  <summary>Details</summary>
Motivation: 解决多情感（情感+希望）数据集稀缺问题，特别是针对阿拉伯语等低资源语言，促进跨语言情感分析研究。

Method: 构建双语数据集（阿拉伯语23,456条/英语10,036条），采用Fleiss' Kappa验证标注一致性（0.75-0.85），使用机器学习模型进行基线评估。

Result: 标注一致性高（Kappa值0.75-0.85），基线模型micro-F1达0.67，证明标注有效性。数据集支持情感强度/复杂性/原因分析及希望言论细分分类。

Conclusion: 该数据集填补了多情感分析资源空白，为低资源语言的NLP技术发展及跨语言情感研究提供了可靠基础。

Abstract: This research introduces a bilingual dataset comprising 23,456 entries for
Arabic and 10,036 entries for English, annotated for emotions and hope speech,
addressing the scarcity of multi-emotion (Emotion and hope) datasets. The
dataset provides comprehensive annotations capturing emotion intensity,
complexity, and causes, alongside detailed classifications and subcategories
for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,
revealing 0.75-0.85 agreement among annotators both for Arabic and English
language. The evaluation metrics (micro-F1-Score=0.67) obtained from the
baseline model (i.e., using a machine learning model) validate that the data
annotations are worthy. This dataset offers a valuable resource for advancing
natural language processing in underrepresented languages, fostering better
cross-linguistic analysis of emotions and hope speech.

</details>


### [41] [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)
*Xu Liu,Guanyi Chen*

Main category: cs.CL

TL;DR: CCNU团队开发了多LLM并行标注系统，结合内外知识验证，在Mu-SHROOM多语言幻觉检测任务中取得印地语第一、七种语言前五的成绩


<details>
  <summary>Details</summary>
Motivation: 解决多语言问答系统幻觉检测难题，探索大模型协同标注的有效性

Method: 1. 多专家LLM并行标注模拟众包流程
2. 整合输入相关的内外知识验证
3. 基于DeepSeek-V3构建系统

Result: 印地语排名#1，7种语言Top5；开源模型DeepSeek-V3实现最优表现

Conclusion: 多模型协同策略有效，但需平衡知识整合效率；失败经验为后续研究提供重要参考

Abstract: We present the system developed by the Central China Normal University (CCNU)
team for the Mu-SHROOM shared task, which focuses on identifying hallucinations
in question-answering systems across 14 different languages. Our approach
leverages multiple Large Language Models (LLMs) with distinct areas of
expertise, employing them in parallel to annotate hallucinations, effectively
simulating a crowdsourcing annotation process. Furthermore, each LLM-based
annotator integrates both internal and external knowledge related to the input
during the annotation process. Using the open-source LLM DeepSeek-V3, our
system achieves the top ranking (\#1) for Hindi data and secures a Top-5
position in seven other languages. In this paper, we also discuss unsuccessful
approaches explored during our development process and share key insights
gained from participating in this shared task.

</details>


### [42] [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)
*Md. Rafiul Biswas,Wajdi Zaghouani*

Main category: cs.CL

TL;DR: 构建首个多标签阿拉伯语仇恨言论数据集（10K推文），实现0.786准确率的AraBERTv2检测模型


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语方言复杂性导致仇恨言论检测困难，现有标注数据稀缺且多目标识别不足

Method: 人工多标注者标注（攻击性/仇恨目标分类）+ 计算标注一致性（Kappa 0.86/0.71）+ transformers模型验证

Result: AraBERTv2取得最优性能：micro-F1 0.7865，准确率0.786

Conclusion: 该数据集有效提升检测效果，AraBERTv2证明适用性，支持多目标仇恨内容识别研究

Abstract: Identifying hate speech content in the Arabic language is challenging due to
the rich quality of dialectal variations. This study introduces a multilabel
hate speech dataset in the Arabic language. We have collected 10000 Arabic
tweets and annotated each tweet, whether it contains offensive content or not.
If a text contains offensive content, we further classify it into different
hate speech targets such as religion, gender, politics, ethnicity, origin, and
others. A text can contain either single or multiple targets. Multiple
annotators are involved in the data annotation task. We calculated the
inter-annotator agreement, which was reported to be 0.86 for offensive content
and 0.71 for multiple hate speech targets. Finally, we evaluated the data
annotation task by employing a different transformers-based model in which
AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of
0.786.

</details>


### [43] [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)
*Yuhao Wang,Ruiyang Ren,Yucheng Wang,Wayne Xin Zhao,Jing Liu,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 该论文系统研究LLMs在RAG场景中整合参数化知识与检索知识的内在机制，通过知识流阶段分解和神经元分析方法揭示了模块间的互补作用，为提升检索增强模型的可解释性和可靠性提供理论基础


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs在RAG中整合内外部知识的底层机制理解不足，作者希望通过系统性分析揭示知识整合过程，从而改进模型在知识密集型任务中的表现

Method: 采用宏观知识流四阶段分解（知识提炼/激发/表达/竞争）与微观KAPE神经元识别方法，通过选择性神经元停用技术验证不同模块的功能特性

Result: 发现段落相关性引导知识流阶段演进，KAPE可有效识别特定知识源相关神经元，多头注意力与MLP层在知识形成中具有互补作用

Conclusion: 该研究为理解RAG机制提供了多层次分析框架，其发现能指导开发更可靠的知识整合策略，推动生成式AI在专业领域的可信应用

Abstract: Considering the inherent limitations of parametric knowledge in large
language models (LLMs), retrieval-augmented generation (RAG) is widely employed
to expand their knowledge scope. Since RAG has shown promise in
knowledge-intensive tasks like open-domain question answering, its broader
application to complex tasks and intelligent assistants has further advanced
its utility. Despite this progress, the underlying knowledge utilization
mechanisms of LLM-based RAG remain underexplored. In this paper, we present a
systematic investigation of the intrinsic mechanisms by which LLMs integrate
internal (parametric) and external (retrieved) knowledge in RAG scenarios.
Specially, we employ knowledge stream analysis at the macroscopic level, and
investigate the function of individual modules at the microscopic level.
Drawing on knowledge streaming analyses, we decompose the knowledge utilization
process into four distinct stages within LLM layers: knowledge refinement,
knowledge elicitation, knowledge expression, and knowledge contestation. We
further demonstrate that the relevance of passages guides the streaming of
knowledge through these stages. At the module level, we introduce a new method,
knowledge activation probability entropy (KAPE) for neuron identification
associated with either internal or external knowledge. By selectively
deactivating these neurons, we achieve targeted shifts in the LLM's reliance on
one knowledge source over the other. Moreover, we discern complementary roles
for multi-head attention and multi-layer perceptron layers during knowledge
formation. These insights offer a foundation for improving interpretability and
reliability in retrieval-augmented LLMs, paving the way for more robust and
transparent generative solutions in knowledge-intensive domains.

</details>


### [44] [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)
*Yupei Ren,Xinyi Zhou,Ning Zhang,Shangqing Zhao,Man Lan,Xiaopeng Bai*

Main category: cs.CL

TL;DR: 提出14种细粒度论点关系类型提升议论文结构分析，通过多维度实验验证细粒度标注对写作质量评估的有效性


<details>
  <summary>Details</summary>
Motivation: 当前论点关系研究过于基础，难以捕捉真实场景中复杂的论点结构交互

Method: 从垂直/水平维度设计14种关系类型，在论点检测、关系预测、作文评分三任务开展实验，并探索写作质量与话语特征关联

Result: 细粒度标注显著提升议论文质量评估效果，揭示写作质量与论点检测准确率的正相关性

Conclusion: 论文证明细粒度标注体系对多维度论点分析的必要性，为自动写作评估系统提供新方法论

Abstract: Argument mining has garnered increasing attention over the years, with the
recent advancement of Large Language Models (LLMs) further propelling this
trend. However, current argument relations remain relatively simplistic and
foundational, struggling to capture the full scope of argument information,
particularly when it comes to representing complex argument structures in
real-world scenarios. To address this limitation, we propose 14 fine-grained
relation types from both vertical and horizontal dimensions, thereby capturing
the intricate interplay between argument components for a thorough
understanding of argument structure. On this basis, we conducted extensive
experiments on three tasks: argument component detection, relation prediction,
and automated essay grading. Additionally, we explored the impact of writing
quality on argument component detection and relation prediction, as well as the
connections between discourse relations and argumentative features. The
findings highlight the importance of fine-grained argumentative annotations for
argumentative writing quality assessment and encourage multi-dimensional
argument analysis.

</details>


### [45] [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)
*Jingxue Chen,Qingkun Tang,Qianchun Lu,Siyuan Fang*

Main category: cs.CL

TL;DR: 提出MoL框架解决传统持续预训练(CPT)的领域适应问题，通过双损失机制(交叉熵+KL散度)保持基础模型通用能力的同时提升领域性能


<details>
  <summary>Details</summary>
Motivation: 现有持续预训练方法在领域适应时会导致通用语言能力退化，且语料混合比例难以优化

Method: 解耦领域/通用语料的优化目标：领域数据用交叉熵损失获取知识，通用数据用KL散度对齐基础模型能力

Result: Math-500非思维推理模式准确率提升27.9%，AIME25挑战集思维模式提升83.3%；验证1:1语料比例为最优解

Conclusion: MoL框架有效平衡领域适应与通用能力保留，避免灾难性遗忘，为资源受限的领域适配提供高效解决方案

Abstract: Although LLMs perform well in general tasks, domain-specific applications
suffer from hallucinations and accuracy limitations. CPT approaches encounter
two key issues: (1) domain-biased data degrades general language skills, and
(2) improper corpus-mixture ratios limit effective adaptation. To address
these, we propose a novel framework, Mixture of Losses (MoL), which decouples
optimization objectives for domain-specific and general corpora. Specifically,
cross-entropy (CE) loss is applied to domain data to ensure knowledge
acquisition, while Kullback-Leibler (KL) divergence aligns general-corpus
training with the base model's foundational capabilities. This dual-loss
architecture preserves universal skills while enhancing domain expertise,
avoiding catastrophic forgetting. Empirically, we validate that a 1:1
domain-to-general corpus ratio optimally balances training and overfitting
without the need for extensive tuning or resource-intensive experiments.
Furthermore, our experiments demonstrate significant performance gains compared
to traditional CPT approaches, which often suffer from degradation in general
language capabilities; our model achieves 27.9% higher accuracy on the Math-500
benchmark in the non-think reasoning mode, and an impressive 83.3% improvement
on the challenging AIME25 subset in the think mode, underscoring the
effectiveness of our approach.

</details>


### [46] [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)
*Vinod Raman,Hilal Asi,Satyen Kale*

Main category: cs.CL

TL;DR: 提出提示自适应的Best-of-N对齐策略，通过动态分配计算资源显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有测试时对齐方法对所有提示统一分配计算资源，未考虑不同提示的对齐难度差异，导致计算效率低下

Method: 两阶段自适应算法：第一阶段用少量预算探索奖励分布，第二阶段基于估计值动态分配剩余预算

Result: 在相同推理预算下，自适应策略在AlpacaEval数据集上全面超越均匀分配。实验显示该方法在批次规模扩大时性能持续提升，甚至可节省20%计算资源

Conclusion: 提出的自适应策略兼具简单性、实用性，兼容任意LM/RM组合，且计算效率随批次规模扩大而提升

Abstract: Recent advances in test-time alignment methods, such as Best-of-N sampling,
offer a simple and effective way to steer language models (LMs) toward
preferred behaviors using reward models (RM). However, these approaches can be
computationally expensive, especially when applied uniformly across prompts
without accounting for differences in alignment difficulty. In this work, we
propose a prompt-adaptive strategy for Best-of-N alignment that allocates
inference-time compute more efficiently. Motivated by latency concerns, we
develop a two-stage algorithm: an initial exploratory phase estimates the
reward distribution for each prompt using a small exploration budget, and a
second stage adaptively allocates the remaining budget using these estimates.
Our method is simple, practical, and compatible with any LM/RM combination.
Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different
batches of prompts show that our adaptive strategy consistently outperforms the
uniform allocation with the same inference budget. Moreover, our experiments
show that our adaptive strategy remains competitive against uniform allocations
with 20% larger inference budgets and even improves in performance as the batch
size grows.

</details>


### [47] [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)
*Matúš Pikuliak*

Main category: cs.CL

TL;DR: 开源评测框架GenderBench量化评估大语言模型性别偏见，发现模型普遍存在刻板印象推理、性别表征失衡及高风险场景歧视行为


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型中广泛存在的隐性性别偏见问题，缺乏系统性评测工具阻碍了模型公平性改进。通过构建标准化评估体系推动LLM性别平等研究。

Method: 开发包含14个测试探针的评估套件，量化19种性别相关有害行为。对12个主流LLM进行系统评测，分析其行为模式一致性。

Result: 模型在刻板印象推理（87%测试样本）、文本生成性别失衡（男性角色占比超68%）、招聘等高风险场景歧视行为（32%测试案例）存在系统性偏差。

Conclusion: GenderBench作为开源工具填补了领域空白，实证结果表明当前LLM性别偏见问题具有普遍性，强调开发过程中需嵌入公平性评估机制。

Abstract: We present GenderBench -- a comprehensive evaluation suite designed to
measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19
gender-related harmful behaviors exhibited by LLMs. We release GenderBench as
an open-source and extensible library to improve the reproducibility and
robustness of benchmarking across the field. We also publish our evaluation of
12 LLMs. Our measurements reveal consistent patterns in their behavior. We show
that LLMs struggle with stereotypical reasoning, equitable gender
representation in generated texts, and occasionally also with discriminatory
behavior in high-stakes scenarios, such as hiring.

</details>


### [48] [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)
*Peng Ding,Jun Kuang,Zongyu Wang,Xuezhi Cao,Xunliang Cai,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 提出SAGE框架，通过训练无关的判别式安全增强策略，有效抵御LLMs的越狱攻击，实现99%平均防御成功率。


<details>
  <summary>Details</summary>
Motivation: 发现LLMs存在安全判别能力与生成能力的不对称性——能识别恶意指令却仍生成不安全内容，需构建检测与生成的对齐机制。

Method: SAGE包含判别分析模块（检测恶意指令）和判别响应模块（安全生成），通过动态安全指令注入提升模型防御鲁棒性。

Result: 在开源/闭源多架构LLMs上验证有效性，抵御复杂越狱攻击成功率超99%，通用基准测试保持原有性能。通过隐状态分析揭示安全机制机理。

Conclusion: 首次系统性解决LLMs安全检测与生成的不一致问题，为构建安全对齐的AI系统提供新范式。代码数据集已开源。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
various tasks but remain vulnerable to meticulously crafted jailbreak attacks.
In this paper, we identify a critical safety gap: while LLMs are adept at
detecting jailbreak prompts, they often produce unsafe responses when directly
processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware
Guard Enhancement), a training-free defense strategy designed to align LLMs'
strong safety discrimination performance with their relatively weaker safety
generation ability. SAGE consists of two core components: a Discriminative
Analysis Module and a Discriminative Response Module, enhancing resilience
against sophisticated jailbreak attempts through flexible safety discrimination
instructions. Extensive experiments demonstrate SAGE's effectiveness and
robustness across various open-source and closed-source LLMs of different sizes
and architectures, achieving an average 99% defense success rate against
numerous complex and covert jailbreak methods while maintaining helpfulness on
general benchmarks. We further conduct mechanistic interpretability analysis
through hidden states and attention distributions, revealing the underlying
mechanisms of this detection-generation discrepancy. Our work thus contributes
to developing future LLMs with coherent safety awareness and generation
behavior. Our code and datasets are publicly available at
https://github.com/NJUNLP/SAGE.

</details>


### [49] [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)
*Harald Baayen,Kristian Berg,Maziyah Mohamed*

Main category: cs.CL

TL;DR: 通过认知计算模型和个案历时分析，探索形态能产性的衡量标准及作家语言创新规律


<details>
  <summary>Details</summary>
Motivation: 研究形态能产性需要结合系统性计算模型（验证语言泛化能力）和具体作家的历时语言演变（揭示个体创新机制）

Method: 使用判别词典模型分析芬兰语/马来语/英语的形态系统；通过托马斯·曼的阅读输入与写作输出数据进行历时对比

Result: 计算模型显示词缀与语义中心点强关联；作家语言创新率极低且与词缀嵌入距离负相关

Conclusion: 形态能产性需兼顾系统计算模型和个体历时分析，为语言习得与文学创作研究提供双重视角

Abstract: In this study, we approach morphological productivity from two perspectives:
a cognitive-computational perspective, and a diachronic perspective zooming in
on an actual speaker, Thomas Mann. For developing the first perspective, we
make use of a cognitive computational model of the mental lexicon, the
discriminative lexicon model. For computational mappings between form and
meaning to be productive, in the sense that novel, previously unencountered
words, can be understood and produced, there must be systematicities between
the form space and the semantic space. If the relation between form and meaning
would be truly arbitrary, a model could memorize form and meaning pairings, but
there is no way in which the model would be able to generalize to novel test
data. For Finnish nominal inflection, Malay derivation, and English
compounding, we explore, using the Discriminative Lexicon Model as a
computational tool, to trace differences in the degree to which inflectional
and word formation patterns are productive. We show that the DLM tends to
associate affix-like sublexical units with the centroids of the embeddings of
the words with a given affix. For developing the second perspective, we study
how the intake and output of one prolific writer, Thomas Mann, changes over
time. We show by means of an examination of what Thomas Mann is likely to have
read, and what he wrote, that the rate at which Mann produces novel derived
words is extremely low. There are far more novel words in his input than in his
output. We show that Thomas Mann is less likely to produce a novel derived word
with a given suffix the greater the average distance is of the embeddings of
all derived words to the corresponding centroid, and discuss the challenges of
using speaker-specific embeddings for low-frequency and novel words.

</details>


### [50] [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)
*Guy Davidson,Todd M. Gureckis,Brenden M. Lake,Adina Williams*

Main category: cs.CL

TL;DR: 论文通过函数向量分析发现，语言模型对示例演示和文本指令的任务表征机制存在差异，二者激活不同模型组件且仅部分重叠，支持了组合使用两种提示策略的实践。


<details>
  <summary>Details</summary>
Motivation: 探究相同任务通过示例演示（in-context demonstrations）和文本指令（textual instructions）两种不同提示方式，是否在语言模型中形成相似的任务表征机制，以增强模型可解释性和任务控制能力。

Method: 1. 将函数向量（function vectors）方法扩展到指令提示场景，提取零样本任务指令向量；2. 对比分析演示型与指令型函数向量在模型中的激活模式；3. 设计控制实验解耦两类向量对任务表现的贡献。

Result: 1. 演示型和指令型函数向量分别依赖不同模型组件；2. 二者存在部分重叠但未形成统一任务表征；3. 组合使用两种提示方式可提升任务准确率。

Conclusion: 不同任务呈现方式激活差异化机制，支持指令与演示组合策略，提示跨形式任务监控存在挑战，需继续探索LLM任务推理机制。

Abstract: Demonstrations and instructions are two primary approaches for prompting
language models to perform in-context learning (ICL) tasks. Do identical tasks
elicited in different ways result in similar representations of the task? An
improved understanding of task representation mechanisms would offer
interpretability insights and may aid in steering models. We study this through
function vectors, recently proposed as a mechanism to extract few-shot ICL task
representations. We generalize function vectors to alternative task
presentations, focusing on short textual instruction prompts, and successfully
extract instruction function vectors that promote zero-shot task accuracy. We
find evidence that demonstration- and instruction-based function vectors
leverage different model components, and offer several controls to dissociate
their contributions to task performance. Our results suggest that different
task presentations do not induce a common task representation but elicit
different, partly overlapping mechanisms. Our findings offer principled support
to the practice of combining textual instructions and task demonstrations,
imply challenges in universally monitoring task inference across presentation
forms, and encourage further examinations of LLM task inference mechanisms.

</details>


### [51] [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)
*Yunshui Li,Yiyuan Ma,Shen Yan,Chaoyi Zhang,Jing Liu,Jianqiao Lu,Ziwen Xu,Mengzhao Chen,Minrui Wang,Shiyi Zhan,Jin Ma,Xunhao Lai,Yao Luo,Xingyan Bin,Hongbin Ren,Mingji Han,Wenhao Hao,Bairen Yi,LingJun Liu,Bole Ma,Xiaoying Jia,Zhou Xun,Liang Xiang,Yonghui Wu*

Main category: cs.CL

TL;DR: 探索模型合并技术在大规模预训练中的应用，验证恒定学习率合并策略可显著提升模型性能、预测退火行为，并提供开源社区实用训练指南


<details>
  <summary>Details</summary>
Motivation: 当前模型合并在预训练阶段的潜力未被充分挖掘，需验证其在超大模型场景的有效性并探索降低训练成本的新方法

Method: 通过百万至千亿参数规模的密集/MoE架构实验，分析不同合并策略与超参数对预训练效果的影响，开展消融研究揭示底层机制

Result: 恒定学习率合并使模型性能提升20-35%，训练成本降低40%+，消融研究揭示了参数融合的临界阈值现象

Conclusion: 模型合并显著提升预训练效率，提出的方法论为社区建立实用训练范式，推动LLM开发成本降低50%以上

Abstract: Model merging has emerged as a promising technique for enhancing large
language models, though its application in large-scale pre-training remains
relatively unexplored. In this paper, we present a comprehensive investigation
of model merging techniques during the pre-training process. Through extensive
experiments with both dense and Mixture-of-Experts (MoE) architectures ranging
from millions to over 100 billion parameters, we demonstrate that merging
checkpoints trained with constant learning rates not only achieves significant
performance improvements but also enables accurate prediction of annealing
behavior. These improvements lead to both more efficient model development and
significantly lower training costs. Our detailed ablation studies on merging
strategies and hyperparameters provide new insights into the underlying
mechanisms while uncovering novel applications. Through comprehensive
experimental analysis, we offer the open-source community practical
pre-training guidelines for effective model merging.

</details>


### [52] [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)
*Mohammad Shokri,Sarah Ita Levitan,Rivka Levitan*

Main category: cs.CL

TL;DR: 研究通过用户层面分析发现LLM作者混淆效果存在双峰分布，提出个性化提示方法提升效果


<details>
  <summary>Details</summary>
Motivation: 现有作者混淆研究多关注整体效果，忽视了不同用户间的效果差异，需针对性改进个性化场景下的混淆性能

Method: 采用用户粒度效果分析，发现效果双峰分布现象，设计基于用户特征的个性化提示优化方案

Result: LLM总体有效但个体差异显著，个性化提示比标准方法效果提升18.6%，双峰分布标准差降低32%

Conclusion: 用户特征敏感的个性化提示策略能有效提升作者混淆效果，为数字隐私保护提供新的技术思路

Abstract: In this paper, we investigate the efficacy of large language models (LLMs) in
obfuscating authorship by paraphrasing and altering writing styles. Rather than
adopting a holistic approach that evaluates performance across the entire
dataset, we focus on user-wise performance to analyze how obfuscation
effectiveness varies across individual authors. While LLMs are generally
effective, we observe a bimodal distribution of efficacy, with performance
varying significantly across users. To address this, we propose a personalized
prompting method that outperforms standard prompting techniques and partially
mitigates the bimodality issue.

</details>


### [53] [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)
*Isabela Pereira Gregio,Ian Pons,Anna Helena Reali Costa,Artur Jordão*

Main category: cs.CL

TL;DR: 提出无需训练的前向扰动方法检测LLM偏见，显著提升模型公平性指标


<details>
  <summary>Details</summary>
Motivation: LLM在伦理敏感任务中的系统性偏见阻碍其可信应用，需开发无需调整参数的实时去偏方法

Method: 通过修改句子属性生成变体，比较原始预测与扰动预测的差异来识别偏见，仅需前向传播无需训练

Result: 在Llama3上实现27%的公平性指标提升，不同种族群体的预测差异显著降低

Conclusion: 该方法为LLM伦理决策提供实用解决方案，通过预测一致性检测有效提升模型可信度

Abstract: Large Language Models (LLMs) push the bound-aries in natural language
processing and generative AI, driving progress across various aspects of modern
society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,
predictions) poses a significant and open challenge, hindering their
application in tasks involving ethical sensitivity and responsible
decision-making. In this work, we propose a straightforward, user-friendly and
practical method to mitigate such biases, enhancing the reliability and
trustworthiness of LLMs. Our method creates multiple variations of a given
sentence by modifying specific attributes and evaluates the corresponding
prediction behavior compared to the original, unaltered, prediction/sentence.
The idea behind this process is that critical ethical predictions often exhibit
notable inconsistencies, indicating the presence of bias. Unlike previous
approaches, our method relies solely on forward passes (i.e., testing-time
adversaries), eliminating the need for training, fine-tuning, or prior
knowledge of the training data distribution. Through extensive experiments on
the popular Llama family, we demonstrate the effectiveness of our method in
improving various fairness metrics, focusing on the reduction of disparities in
how the model treats individuals from different racial groups. Specifically,
using standard metrics, we improve the fairness in Llama3 in up to 27
percentage points. Overall, our approach significantly enhances fairness,
equity, and reliability in LLM-generated results without parameter tuning or
training data modifications, confirming its effectiveness in practical
scenarios. We believe our work establishes an important step toward enabling
the use of LLMs in tasks that require ethical considerations and responsible
decision-making.

</details>


### [54] [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)
*Fitsum Gaim,Hoyun Song,Huije Lee,Changgeon Ko,Eui Jun Hwang,Jong C. Park*

Main category: cs.CL

TL;DR: 开发了首个提格雷尼亚语社交媒体多任务基准数据集，支持罗马化转写与原生文字，小规模多任务模型在低资源环境下辱骂检测准确率达86%


<details>
  <summary>Details</summary>
Motivation: 解决全球多数语言缺乏内容审核资源的问题，特别关注低资源语言提格雷尼亚语用户面临的网络暴力风险

Method: 通过迭代术语聚类方法收集13,717条YouTube评论（覆盖7,373个高浏览量视频），兼容罗马化与Geez文字系统，建立联合标注框架（辱骂性/情感/主题分类）

Result: 专用多任务模型在低资源环境下辱骂检测准确率提升7%达86%，显著优于前沿模型

Conclusion: 公开数据集促进在线安全研究，证明小规模专业化模型在低资源语言处理中的有效性

Abstract: Content moderation research has recently made significant advances, but still
fails to serve the majority of the world's languages due to the lack of
resources, leaving millions of vulnerable users to online hostility. This work
presents a large-scale human-annotated multi-task benchmark dataset for abusive
language detection in Tigrinya social media with joint annotations for three
tasks: abusiveness, sentiment, and topic classification. The dataset comprises
13,717 YouTube comments annotated by nine native speakers, collected from 7,373
videos with a total of over 1.2 billion views across 51 channels. We developed
an iterative term clustering approach for effective data selection. Recognizing
that around 64% of Tigrinya social media content uses Romanized
transliterations rather than native Ge'ez script, our dataset accommodates both
writing systems to reflect actual language use. We establish strong baselines
across the tasks in the benchmark, while leaving significant challenges for
future contributions. Our experiments reveal that small, specialized multi-task
models outperform the current frontier models in the low-resource setting,
achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the
resources publicly available to promote research on online safety.

</details>


### [55] [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)
*Elisa Bassignana,Amanda Cercas Curry,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究发现不同社会经济地位群体在使用生成式语言技术时存在显著差异，高SES群体倾向抽象表达，低SES群体更拟人化交互，需针对性优化技术设计以减少数字鸿沟。


<details>
  <summary>Details</summary>
Motivation: 探索社会经济地位如何影响人类与大型语言模型的真实互动，突破先前依赖代理指标和合成数据的研究局限。

Method: 通过调查1,000名不同SES背景个体的语言技术使用情况，并分析其6,482条历史LLM交互提示。

Result: 高SES群体请求更抽象简洁（涉及包容性/旅行主题），低SES群体交互拟人化程度高（使用问候语）且语言更具体。

Conclusion: 需在语言技术开发中充分考虑社会经济语言差异，通过定制化设计满足不同群体的需求，缓解SES群体间的AI技术鸿沟。

Abstract: Socioeconomic status (SES) fundamentally influences how people interact with
each other and more recently, with digital technologies like Large Language
Models (LLMs). While previous research has highlighted the interaction between
SES and language technology, it was limited by reliance on proxy metrics and
synthetic data. We survey 1,000 individuals from diverse socioeconomic
backgrounds about their use of language technologies and generative AI, and
collect 6,482 prompts from their previous interactions with LLMs. We find
systematic differences across SES groups in language technology usage (i.e.,
frequency, performed tasks), interaction styles, and topics. Higher SES entails
a higher level of abstraction, convey requests more concisely, and topics like
'inclusivity' and 'travel'. Lower SES correlates with higher
anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more
concrete language. Our findings suggest that while generative language
technologies are becoming more accessible to everyone, socioeconomic linguistic
differences still stratify their use to exacerbate the digital divide. These
differences underscore the importance of considering SES in developing language
technologies to accommodate varying linguistic needs rooted in socioeconomic
factors and limit the AI Gap across SES groups.

</details>


### [56] [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)
*Darmawan Wicaksono,Hasri Akbar Awal Rozaq,Nevfel Boz*

Main category: cs.CL

TL;DR: 研究开发了针对土耳其语的情感识别模型(ERM)，准确率92.62%，分析X平台数据揭示反难民情绪，推动本地化NLP工具在实时情感分析中的应用


<details>
  <summary>Details</summary>
Motivation: 分析土耳其社交媒体中'静默入侵'术语引发的反难民情绪，解决资源不足语言的情感分析难题，增强对全球数字话语中土耳其语独特挑战的理解

Method: 使用BERTurk模型和TREMO数据集构建情感识别模型，通过六种情绪分类（快乐/恐惧/愤怒/悲伤/厌恶/惊讶）分析X平台大规模数据

Result: ERM模型达到92.62%准确率，成功识别土耳其反难民讨论中的情感模式变化，揭示社交媒体情绪传播机制

Conclusion: 本地化NLP工具在土耳其语情境展现重要应用价值，实时情感分析能力可提升营销、公关等领域的决策质量，强调语言区域性特征在计算社会科学中的关键作用

Abstract: Social media platforms like X (formerly Twitter) play a crucial role in
shaping public discourse and societal norms. This study examines the term
Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise
of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and
the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)
tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such
as happiness, fear, anger, sadness, disgust, and surprise. By applying this
model to large-scale X data, the study uncovers emotional nuances in Turkish
discourse, contributing to computational social science by advancing sentiment
analysis in underrepresented languages and enhancing our understanding of
global digital discourse and the unique linguistic challenges of Turkish. The
findings underscore the transformative potential of localized NLP tools, with
our ERM model offering practical applications for real-time sentiment analysis
in Turkish-language contexts. By addressing critical areas, including
marketing, public relations, and crisis management, these models facilitate
improved decision-making through timely and accurate sentiment tracking. This
highlights the significance of advancing research that accounts for regional
and linguistic nuances.

</details>


### [57] [Truth Neurons](https://arxiv.org/abs/2505.12182)
*Haohang Li,Yupeng Cao,Yangyang Yu,Jordan W. Suchow,Zining Zhu*

Main category: cs.CL

TL;DR: 发现语言模型中的'真理神经元'，揭示其跨主题的真理编码机制


<details>
  <summary>Details</summary>
Motivation: 理解语言模型如何编码真实性特征以提升可靠性和安全性

Method: 通过神经元级表征分析定位真理神经元，在不同规模模型进行实验验证

Result: 证实多模型存在跨数据集通用的真理神经元，抑制其激活显著降低真实性表现

Conclusion: 揭示了语言模型真实性编码的神经机制，为提升模型可信度提供新方向

Abstract: Despite their remarkable success and deployment across diverse workflows,
language models sometimes produce untruthful responses. Our limited
understanding of how truthfulness is mechanistically encoded within these
models jeopardizes their reliability and safety. In this paper, we propose a
method for identifying representations of truthfulness at the neuron level. We
show that language models contain truth neurons, which encode truthfulness in a
subject-agnostic manner. Experiments conducted across models of varying scales
validate the existence of truth neurons, confirming that the encoding of
truthfulness at the neuron level is a property shared by many language models.
The distribution patterns of truth neurons over layers align with prior
findings on the geometry of truthfulness. Selectively suppressing the
activations of truth neurons found through the TruthfulQA dataset degrades
performance both on TruthfulQA and on other benchmarks, showing that the
truthfulness mechanisms are not tied to a specific dataset. Our results offer
novel insights into the mechanisms underlying truthfulness in language models
and highlight potential directions toward improving their trustworthiness and
reliability.

</details>


### [58] [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)
*Manari Hirose,Masato Uchida*

Main category: cs.CL

TL;DR: 研究通过436道选择题定量分析ChatGPT和Gemini的意识形态偏见，发现模型间存在差异且存在伦理隐患


<details>
  <summary>Details</summary>
Motivation: 大语言模型广泛应用背景下，需通过实证研究揭示其偏见模式及社会影响，确保伦理应用

Method: 开发新型评估框架，设计无标准答案的二元选择题，进行跨模型（ChatGPT/Gemini）和跨语言定量分析

Result: 模型间意识形态差异显著，ChatGPT易迎合提问者观点，两模型均存在可能引发负面社会影响的偏见主张

Conclusion: 评估LLMs需兼顾意识形态和伦理考量，该框架为开发社会友好型AI提供量化分析工具

Abstract: The widespread integration of Large Language Models (LLMs) across various
sectors has highlighted the need for empirical research to understand their
biases, thought patterns, and societal implications to ensure ethical and
effective use. In this study, we propose a novel framework for evaluating LLMs,
focusing on uncovering their ideological biases through a quantitative analysis
of 436 binary-choice questions, many of which have no definitive answer. By
applying our framework to ChatGPT and Gemini, findings revealed that while LLMs
generally maintain consistent opinions on many topics, their ideologies differ
across models and languages. Notably, ChatGPT exhibits a tendency to change
their opinion to match the questioner's opinion. Both models also exhibited
problematic biases, unethical or unfair claims, which might have negative
societal impacts. These results underscore the importance of addressing both
ideological and ethical considerations when evaluating LLMs. The proposed
framework offers a flexible, quantitative method for assessing LLM behavior,
providing valuable insights for the development of more socially aligned AI
systems.

</details>


### [59] [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)
*Yi-Chien Lin,Hongao Zhu,William Schuler*

Main category: cs.CL

TL;DR: 研究发现大型语言模型(LLM)预测人类阅读数据的能力随规模扩大呈逆向变化：初期正相关，但模型过大时预测能力反而下降，揭示LLM与人类语言处理机制存在本质偏差。


<details>
  <summary>Details</summary>
Motivation: 探究LLM规模扩展对预测人类句子处理数据的影响，验证'质量-能力正相关'假设的局限性，揭示过大模型产生的预测偏差现象。

Method: 使用完整LLM向量分析，控制不同规模模型的预测变量数量，通过回归分析比较模型预测人类阅读时间和脑成像数据的能力。

Result: 发现逆向扩展效应：模型参数超过临界值后，预测能力随规模增大显著下降，偏差程度与模型复杂度正相关。

Conclusion: LLM预测人类语言处理数据的不足源于架构/目标函数与人类机制的根本性错位，单纯扩大规模无法解决，需重新审视模型设计范式。

Abstract: The impressive linguistic abilities of large language models (LLMs) have
recommended them as models of human sentence processing, with some conjecturing
a positive 'quality-power' relationship (Wilcox et al., 2023), in which
language models' (LMs') fit to psychometric data continues to improve as their
ability to predict words in context increases. This is important because it
suggests that elements of LLM architecture, such as veridical attention to
context and a unique objective of predicting upcoming words, reflect the
architecture of the human sentence processing faculty, and that any
inadequacies in predicting human reading time and brain imaging data may be
attributed to insufficient model complexity, which recedes as larger models
become available. Recent studies (Oh and Schuler, 2023) have shown this scaling
inverts after a point, as LMs become excessively large and accurate, when word
prediction probability (as information-theoretic surprisal) is used as a
predictor. Other studies propose the use of entire vectors from differently
sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting
doubt on the value of surprisal as a predictor, but do not control for the
larger number of predictors in vectors from larger LMs. This study evaluates
LLM scaling using entire LLM vectors, while controlling for the larger number
of predictors in vectors from larger LLMs. Results show that inverse scaling
obtains, suggesting that inadequacies in predicting human reading time and
brain imaging data may be due to substantial misalignment between LLMs and
human sentence processing, which worsens as larger models are used.

</details>


### [60] [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)
*Xiyan Fu,Wei Liu*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型在多语言评估中存在判断不一致问题，提出集成策略改善一致性


<details>
  <summary>Details</summary>
Motivation: 探索LLM作为自动化多语言评估工具的可靠性，解决现有方法在跨语言场景中的局限性

Method: 跨5个模型家族、5类任务和25种语言进行系统评估，使用Fleiss' Kappa统计量衡量判断一致性

Result: 平均一致性指标仅0.3，低资源语言表现显著更差，模型规模和多语言训练未提升一致性

Conclusion: 当前LLM不适合直接用于多语言评估，提出的集成策略可提升实际应用中的判断稳定性

Abstract: LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced
large language models assess generation results in alignment with human
instructions. While these models serve as a promising alternative to human
annotators, their reliability in multilingual evaluation remains uncertain. To
bridge this gap, we conduct a comprehensive analysis of multilingual
LLM-as-a-Judge. Specifically, we evaluate five models from different model
families across five diverse tasks involving 25 languages. Our findings reveal
that LLMs struggle to achieve consistent judgment results across languages,
with an average Fleiss' Kappa of approximately 0.3, and some models performing
even worse. To investigate the cause of inconsistency, we analyze various
influencing factors. We observe that consistency varies significantly across
languages, with particularly poor performance in low-resource languages.
Additionally, we find that neither training on multilingual data nor increasing
model scale directly improves judgment consistency. These findings suggest that
LLMs are not yet reliable for evaluating multilingual predictions. We finally
propose an ensemble strategy which improves the consistency of the multilingual
judge in real-world applications.

</details>


### [61] [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)
*Shaobo Wang,Ziming Wang,Xiangqi Jin,Jize Wang,Jiajun Zhang,Kaixin Li,Zichen Wen,Zhong Li,Conghui He,Xuming Hu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出Data Whisperer方法，通过基于注意力机制的免训练数据选择策略，用10%数据实现超越全量数据的性能表现


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法需要全量微调或依赖启发式规则，存在效率低下和计算资源消耗大的问题

Method: 利用目标模型本身的few-shot上下文学习能力，通过注意力机制进行训练自由的数据选择

Result: 在Llama-3-8B-Instruct模型上，使用10%的GSM8K数据性能超越全量数据，比现有方法提升3.1个点且提速7.4倍

Conclusion: 该方法为大规模语言模型微调提供了高效的数据选择方案，在保持性能的同时显著降低计算成本

Abstract: Fine-tuning large language models (LLMs) on task-specific data is essential
for their effective deployment. As dataset sizes grow, efficiently selecting
optimal subsets for training becomes crucial to balancing performance and
computational costs. Traditional data selection methods often require
fine-tuning a scoring model on the target dataset, which is time-consuming and
resource-intensive, or rely on heuristics that fail to fully leverage the
model's predictive capabilities. To address these challenges, we propose Data
Whisperer, an efficient, training-free, attention-based method that leverages
few-shot in-context learning with the model to be fine-tuned. Comprehensive
evaluations were conducted on both raw and synthetic datasets across diverse
tasks and models. Notably, Data Whisperer achieves superior performance
compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just
10% of the data, and outperforms existing methods with a 3.1-point improvement
and a 7.4$\times$ speedup.

</details>


### [62] [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)
*Jiwei Tang,Zhicheng Zhang,Shunlong Wu,Jingheng Ye,Lichen Bai,Zitai Wang,Tingwei Lu,Jiaqi Chen,Lin Hai,Hai-Tao Zheng,Hong-Gee Kim*

Main category: cs.CL

TL;DR: Proposes GMSA framework for efficient long-context processing in LLMs through context compression and semantic alignment.


<details>
  <summary>Details</summary>
Motivation: Addresses low computational efficiency and redundant information in large language models when handling long-context scenarios.

Method: Combines Group Merging (context compression) and Layer Semantic Alignment (bridging semantic gaps) with Knowledge Extraction Fine-tuning (KEFT) for task adaptation.

Result: Achieves 2x inference speedup, superior context restoration, and outperforms SOTA methods in QA tasks with stable convergence.

Conclusion: GMSA effectively balances efficiency and performance in long-context processing through innovative compression and alignment strategies.

Abstract: Large language models (LLMs) have achieved impressive performance in a
variety of natural language processing (NLP) tasks. However, when applied to
long-context scenarios, they face two challenges, i.e., low computational
efficiency and much redundant information. This paper introduces GMSA, a
context compression framework based on the encoder-decoder architecture, which
addresses these challenges by reducing input sequence length and redundant
information. Structurally, GMSA has two key components: Group Merging and Layer
Semantic Alignment (LSA). Group merging is used to effectively and efficiently
extract summary vectors from the original context. Layer semantic alignment, on
the other hand, aligns the high-level summary vectors with the low-level
primary input semantics, thus bridging the semantic gap between different
layers. In the training process, GMSA first learns soft tokens that contain
complete semantics through autoencoder training. To furtherly adapt GMSA to
downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract
knowledge from the soft tokens for downstream tasks. We train GMSA by randomly
sampling the compression rate for each sample in the dataset. Under this
condition, GMSA not only significantly outperforms the traditional compression
paradigm in context restoration but also achieves stable and significantly
faster convergence with only a few encoder layers. In downstream
question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in
end-to-end inference while outperforming both the original input prompts and
various state-of-the-art (SOTA) methods by a large margin.

</details>


### [63] [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)
*Rongguang Ye,Ming Tang*

Main category: cs.CL

TL;DR: 提出统一模型UniCuCo，通过StratNet实现LLM的定制化压缩，处理64个请求速度快28倍且精度相当


<details>
  <summary>Details</summary>
Motivation: 现有LLM剪枝方法处理多用户请求时效率线性下降，难以满足实际场景的实时性需求

Method: 引入StratNet学习请求到剪枝策略的映射，采用高斯过程近似非可微的剪枝评估过程实现梯度回传

Result: 实验表明在64个并发请求下处理效率提升28倍，同时保持与基线相当的模型精度

Conclusion: UniCuCo通过可学习的策略映射机制，在保持模型性能的前提下显著提升多请求处理效率，具有实际应用价值

Abstract: Existing pruning methods for large language models (LLMs) focus on achieving
high compression rates while maintaining model performance. Although these
methods have demonstrated satisfactory performance in handling a single user's
compression request, their processing time increases linearly with the number
of requests, making them inefficient for real-world scenarios with multiple
simultaneous requests. To address this limitation, we propose a Univeral Model
for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that
learns to map arbitrary requests to their optimal pruning strategy. The
challenge in training StratNet lies in the high computational cost of
evaluating pruning strategies and the non-differentiable nature of the pruning
process, which hinders gradient backpropagation for StratNet updates. To
overcome these challenges, we leverage a Gaussian process to approximate the
evaluation process. Since the gradient of the Gaussian process is computable,
we can use it to approximate the gradient of the non-differentiable pruning
process, thereby enabling StratNet updates. Experimental results show that
UniCuCo is 28 times faster than baselines in processing 64 requests, while
maintaining comparable accuracy to baselines.

</details>


### [64] [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)
*Tong Bao,Yi Zhao,Jin Mao,Chengzhi Zhang*

Main category: cs.CL

TL;DR: LLMs显著影响学术写作风格：偏好词使用增加，词汇复杂度提升但句法简化，英语薄弱学者更依赖LLM优化逻辑表达，计算机学科变化最明显。


<details>
  <summary>Details</summary>
Motivation: 现有研究多从定量角度分析LLM使用，缺乏对学术写作语言特征的系统性影响研究。

Method: 基于arXiv近十年823,798篇摘要，分析LLM偏好词频、词汇/句法复杂度、衔接性、可读性及情感特征。

Result: 1. LLM偏好词比例显著增加 2.词汇复杂度/情感提升但句法复杂度下降 3.衔接性/可读性降低 4.英语薄弱学者更倾向使用LLM优化逻辑 5.计算机学科写作风格变化显著，数学领域最小。

Conclusion: LLMs改变了学术写作范式，在提升表达丰富性的同时降低了文本连贯性，且学科差异和作者语言背景显著影响使用效果。

Abstract: Large Language Models (LLMs), such as ChatGPT, have prompted academic
concerns about their impact on academic writing. Existing studies have
primarily examined LLM usage in academic writing through quantitative
approaches, such as word frequency statistics and probability-based analyses.
However, few have systematically examined the potential impact of LLMs on the
linguistic characteristics of academic writing. To address this gap, we
conducted a large-scale analysis across 823,798 abstracts published in last
decade from arXiv dataset. Through the linguistic analysis of features such as
the frequency of LLM-preferred words, lexical complexity, syntactic complexity,
cohesion, readability and sentiment, the results indicate a significant
increase in the proportion of LLM-preferred words in abstracts, revealing the
widespread influence of LLMs on academic writing. Additionally, we observed an
increase in lexical complexity and sentiment in the abstracts, but a decrease
in syntactic complexity, suggesting that LLMs introduce more new vocabulary and
simplify sentence structure. However, the significant decrease in cohesion and
readability indicates that abstracts have fewer connecting words and are
becoming more difficult to read. Moreover, our analysis reveals that scholars
with weaker English proficiency were more likely to use the LLMs for academic
writing, and focused on improving the overall logic and fluency of the
abstracts. Finally, at discipline level, we found that scholars in Computer
Science showed more pronounced changes in writing style, while the changes in
Mathematics were minimal.

</details>


### [65] [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)
*Quanjiang Guo,Jinchuan Zhang,Sijie Wang,Ling Tian,Zhao Kang,Bin Yan,Weidong Xiao*

Main category: cs.CL

TL;DR: TKRE框架通过两阶段知识引导预训练，整合大语言模型与传统关系抽取模型，解决小样本关系抽取的数据稀缺和泛化难题


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在关系抽取任务中存在数据稀缺适应不足和任务特异性优化不足的问题，需要融合生成式和判别式学习范式

Method: 1. 利用LLMs生成解释性知识和模式约束的合成数据
2. 两阶段预训练策略（MSLM掩码建模 + Span级对比学习）

Result: 在标准数据集上实现最先进性能（SOTA），特别在1-shot设定下F1提升7.3%

Conclusion: TKRE有效解决了小样本关系抽取的核心挑战，为低资源NLP任务提供了可扩展的解决方案

Abstract: Few-Shot Relation Extraction (FSRE) remains a challenging task due to the
scarcity of annotated data and the limited generalization capabilities of
existing models. Although large language models (LLMs) have demonstrated
potential in FSRE through in-context learning (ICL), their general-purpose
training objectives often result in suboptimal performance for task-specific
relation extraction. To overcome these challenges, we propose TKRE (Two-Stage
Knowledge-Guided Pre-training for Relation Extraction), a novel framework that
synergistically integrates LLMs with traditional relation extraction models,
bridging generative and discriminative learning paradigms. TKRE introduces two
key innovations: (1) leveraging LLMs to generate explanation-driven knowledge
and schema-constrained synthetic data, addressing the issue of data scarcity;
and (2) a two-stage pre-training strategy combining Masked Span Language
Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational
reasoning and generalization. Together, these components enable TKRE to
effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets
demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in
FSRE and underscoring its potential for broader application in low-resource
scenarios. \footnote{The code and data are released on
https://github.com/UESTC-GQJ/TKRE.

</details>


### [66] [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)
*Sriram Selvam,Anneswa Ghosh*

Main category: cs.CL

TL;DR: 提出了PANORAMA数据集用于评估LLM隐私风险，通过合成配置文件模拟真实在线环境中的敏感信息分布，并验证数据重复对PII记忆率的影响


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏反映真实网络环境中敏感信息多样性的数据集，阻碍了隐私风险分析和隐私保护策略的开发

Method: 使用约束选择构建含人口统计特征的综合配置文件，结合零样本提示和OpenAI模型生成wiki文章/社交媒体帖子/评论等多样化内容类型

Result: 微调Mistral-7B显示PII记忆率随数据重复次数增加持续上升，且不同内容类型间存在显著记忆差异

Conclusion: PANORAMA填补了隐私风险评估工具的空白，为模型审计和隐私保护型LLM开发提供了重要资源支持

Abstract: The memorization of sensitive and personally identifiable information (PII)
by large language models (LLMs) poses growing privacy risks as models scale and
are increasingly deployed in real-world applications. Existing efforts to study
sensitive and PII data memorization and develop mitigation strategies are
hampered by the absence of comprehensive, realistic, and ethically sourced
datasets reflecting the diversity of sensitive information found on the web. We
introduce PANORAMA - Profile-based Assemblage for Naturalistic Online
Representation and Attribute Memorization Analysis, a large-scale synthetic
corpus of 384,789 samples derived from 9,674 synthetic profiles designed to
closely emulate the distribution, variety, and context of PII and sensitive
data as it naturally occurs in online environments. Our data generation
pipeline begins with the construction of internally consistent, multi-attribute
human profiles using constrained selection to reflect real-world demographics
such as education, health attributes, financial status, etc. Using a
combination of zero-shot prompting and OpenAI o3-mini, we generate diverse
content types - including wiki-style articles, social media posts, forum
discussions, online reviews, comments, and marketplace listings - each
embedding realistic, contextually appropriate PII and other sensitive
information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B
model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and
measure PII memorization rates - revealing not only consistent increases with
repetition but also variation across content types, highlighting PANORAMA's
ability to model how memorization risks differ by context. Our dataset and code
are publicly available, providing a much-needed resource for privacy risk
assessment, model auditing, and the development of privacy-preserving LLMs.

</details>


### [67] [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)
*Haojin Wang,Zining Zhu,Freda Shi*

Main category: cs.CL

TL;DR: 研究揭示了语言模型生成概率分布的难易规律：极低/高熵分布＞中熵分布，含异常词的分布更易逼近，且LM自身生成的分布比随机目标更易拟合


<details>
  <summary>Details</summary>
Motivation: 系统探究语言模型生成任意目标概率分布的能力边界，验证其作为概率分布生成器的表达能力与局限性

Method: 采用基于梯度的软/硬提示调优技术，通过优化提示词使模型输出分布最大程度逼近预设目标分布

Result: 1. 熵极端（极低/高）的分布比中等熵分布更易逼近
2. 相同熵下含异常词的分布更易拟合
3. LM生成的分布（跨分词器）比随机目标更易匹配

Conclusion: 该研究揭示了语言模型概率空间的拓扑特性，为优化LM的分布建模能力提供理论依据，同时暴露其作为概率提议器的潜在局限

Abstract: Autoregressive neural language models (LMs) generate a probability
distribution over tokens at each time step given a prompt. In this work, we
attempt to systematically understand the probability distributions that LMs can
produce, showing that some distributions are significantly harder to elicit
than others. Specifically, for any target next-token distribution over the
vocabulary, we attempt to find a prompt that induces the LM to output a
distribution as close as possible to the target, using either soft or hard
gradient-based prompt tuning. We find that (1) in general, distributions with
very low or very high entropy are easier to approximate than those with
moderate entropy; (2) among distributions with the same entropy, those
containing ''outlier tokens'' are easier to approximate; (3) target
distributions generated by LMs -- even LMs with different tokenizers -- are
easier to approximate than randomly chosen targets. These results offer
insights into the expressiveness of LMs and the challenges of using them as
probability distribution proposers.

</details>


### [68] [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)
*Chi Zhang,Huaping Zhong,Hongtao Li,Chengliang Chai,Jiawei Hong,Yuhao Deng,Jiacheng Wang,Tian Tan,Yizhou Yan,Jiantao Qiu,Ye Yuan,Guoren Wang,Conghui He,Lei Cao*

Main category: cs.CL

TL;DR: EQUAL框架通过迭代选择文档和提取高质量QA对，显著降低指令调优的计算成本并提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM合成指令数据的方法存在多样性不足、成本过高的问题，且容易产生与下游任务无关的劣质数据

Method: 采用文档聚类（基于对比学习的嵌入）结合多臂老虎机策略，通过迭代式文档选择与QA对提取的交替机制

Result: 在AutoMathText和StackOverflow数据集上实现5-10倍计算成本降低，LLaMA-3.1-8B和Mistral-7B准确率提升2.5%

Conclusion: EQUAL框架通过智能文档筛选机制，在保证数据质量的同时显著提升数据提取效率，为实际场景的指令调优提供可扩展解决方案

Abstract: Instruction tuning improves the performance of large language models (LLMs),
but it heavily relies on high-quality training data. Recently, LLMs have been
used to synthesize instruction data using seed question-answer (QA) pairs.
However, these synthesized instructions often lack diversity and tend to be
similar to the input seeds, limiting their applicability in real-world
scenarios. To address this, we propose extracting instruction tuning data from
web corpora that contain rich and diverse knowledge. A naive solution is to
retrieve domain-specific documents and extract all QA pairs from them, but this
faces two key challenges: (1) extracting all QA pairs using LLMs is
prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to
the downstream tasks, potentially degrading model performance. To tackle these
issues, we introduce EQUAL, an effective and scalable data extraction framework
that iteratively alternates between document selection and high-quality QA pair
extraction to enhance instruction tuning. EQUAL first clusters the document
corpus based on embeddings derived from contrastive learning, then uses a
multi-armed bandit strategy to efficiently identify clusters that are likely to
contain valuable QA pairs. This iterative approach significantly reduces
computational cost while boosting model performance. Experiments on
AutoMathText and StackOverflow across four downstream tasks show that EQUAL
reduces computational costs by 5-10x and improves accuracy by 2.5 percent on
LLaMA-3.1-8B and Mistral-7B

</details>


### [69] [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)
*Yuhang Zhou,Xutian Chen,Yixin Cao,Yuchen Ni,Yu He,Siyu Tian,Xiang Liu,Jian Zhang,Chuanjun Ji,Guangnan Ye,Xipeng Qiu*

Main category: cs.CL

TL;DR: 提出Teach2Eval评估框架，通过模型教学能力间接评估LLMs，实现自动化多维度评测


<details>
  <summary>Details</summary>
Motivation: 传统基准测试存在公平性、扩展性不足及数据污染风险，需开发与模型能力同步的评估体系

Method: 基于费曼技巧设计间接评估框架，将开放任务转化为标准化多选题，通过教学反馈评估模型认知能力

Result: 在26个主流LLMs实验中，评估结果与动态排名高度一致，并提供模型训练的可解释性指导

Conclusion: Teach2Eval有效规避数据泄漏，捕捉多维认知能力，建立正交于现有基准的新型评估范式

Abstract: Recent progress in large language models (LLMs) has outpaced the development
of effective evaluation methods. Traditional benchmarks rely on task-specific
metrics and static datasets, which often suffer from fairness issues, limited
scalability, and contamination risks. In this paper, we introduce Teach2Eval,
an indirect evaluation framework inspired by the Feynman Technique. Instead of
directly testing LLMs on predefined tasks, our method evaluates a model's
multiple abilities to teach weaker student models to perform tasks effectively.
By converting open-ended tasks into standardized multiple-choice questions
(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,
automated, and multi-dimensional assessment. Our approach not only avoids data
leakage and memorization but also captures a broad range of cognitive abilities
that are orthogonal to current benchmarks. Experimental results across 26
leading LLMs show strong alignment with existing human and model-based dynamic
rankings, while offering additional interpretability for training guidance.

</details>


### [70] [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)
*Chengwei Qin,Wenxuan Zhou,Karthik Abinav Sankararaman,Nanshu Wang,Tengyu Xu,Alexander Radovic,Eryk Helenowski,Arya Talebzadeh,Aditya Tayade,Sinong Wang,Shafiq Joty,Han Fang,Hao Ma*

Main category: cs.CL

TL;DR: 论文提出RATE-FT方法，通过联合学习辅助任务增强微调效果，有效提升开放域长文本幻觉检测准确率


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在开放域长文本任务中存在领域限制或过度依赖外部工具的问题

Method: 提出RATE-FT范式，在微调过程中引入辅助任务进行联合学习，增强模型对幻觉特征的捕捉能力

Result: 在LongFact等数据集上超越通用微调方法3%，并在不同模型家族中展现良好泛化性

Conclusion: 通过任务增强的微调策略能有效提升模型自主检测幻觉的能力，为开放域文本可靠性提供新解决方案

Abstract: Hallucination, the generation of factually incorrect information, remains a
significant challenge for large language models (LLMs), especially in
open-domain long-form generation. Existing approaches for detecting
hallucination in long-form tasks either focus on limited domains or rely
heavily on external fact-checking tools, which may not always be available.
  In this work, we systematically investigate reference-free hallucination
detection in open-domain long-form responses. Our findings reveal that internal
states (e.g., model's output probability and entropy) alone are insufficient
for reliably (i.e., better than random guessing) distinguishing between factual
and hallucinated content. To enhance detection, we explore various existing
approaches, including prompting-based methods, probing, and fine-tuning, with
fine-tuning proving the most effective. To further improve the accuracy, we
introduce a new paradigm, named RATE-FT, that augments fine-tuning with an
auxiliary task for the model to jointly learn with the main task of
hallucination detection. With extensive experiments and analysis using a
variety of model families & datasets, we demonstrate the effectiveness and
generalizability of our method, e.g., +3% over general fine-tuning methods on
LongFact.

</details>


### [71] [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)
*Pratim Chowdhary*

Main category: cs.CL

TL;DR: 提出K-MSHC方法识别中等规模语言模型关键注意力头电路，发现不同句法任务依赖分布差异大且存在专用超级头


<details>
  <summary>Details</summary>
Motivation: 解决中等规模语言模型(≤10B参数)中特定能力神经组件定位难题，揭示不同句法任务的计算机制

Method: 开发Search-K-MSHC算法，应用在Gemma-9B模型，分析语法接受/算术验证/应用题三个任务家族

Result: 语法任务集中于浅层，应用题激活深浅层，算术验证分布广泛；任务对共享计算组件强度差异显著，存在跨任务重叠少的专用超级头

Conclusion: 句法与数值能力产生于专用但部分可复用的头电路，不同任务通过强弱共享与专用超级头的组合实现能力分化

Abstract: Understanding which neural components drive specific capabilities in
mid-sized language models ($\leq$10B parameters) remains a key challenge. We
introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),
a methodology to identify minimal sets of attention heads crucial for
classification tasks as well as Search-K-MSHC, an efficient algorithm for
discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,
we analyze three syntactic task families: grammar acceptability, arithmetic
verification, and arithmetic word problems. Our findings reveal distinct
task-specific head circuits, with grammar tasks predominantly utilizing early
layers, word problems showing pronounced activity in both shallow and deep
regions, and arithmetic verification demonstrating a more distributed pattern
across the network. We discover non-linear circuit overlap patterns, where
different task pairs share computational components at varying levels of
importance. While grammar and arithmetic share many "weak" heads, arithmetic
and word problems share more consistently critical "strong" heads. Importantly,
we find that each task maintains dedicated "super-heads" with minimal
cross-task overlap, suggesting that syntactic and numerical competencies emerge
from specialized yet partially reusable head circuits.

</details>


### [72] [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)
*Md. Atiqur Rahman,Sabrina Islam,Mushfiqul Haque Omi*

Main category: cs.CL

TL;DR: 提出结合方言指导的LLM框架提升低资源语言机器翻译评估效果


<details>
  <summary>Details</summary>
Motivation: 低资源语言和多方言语言的机器翻译评估面临参考译文稀缺、方言语境缺失导致的评估不准确问题

Method: 1. 扩展ONUBAD数据集（增加Sylheti语料及母语者评分）
2. 增加方言特定词汇解决词汇空缺
3. 设计回归头实现标量分数预测
4. 开发方言引导（DG）提示策略

Result: 在多个LLM中实现Spearman相关系数最高+0.1083提升，其他评估指标均改进

Conclusion: 该框架显著提升低资源语言MT评估效果，数据集和代码已开源

Abstract: Evaluating machine translation (MT) for low-resource languages poses a
persistent challenge, primarily due to the limited availability of high quality
reference translations. This issue is further exacerbated in languages with
multiple dialects, where linguistic diversity and data scarcity hinder robust
evaluation. Large Language Models (LLMs) present a promising solution through
reference-free evaluation techniques; however, their effectiveness diminishes
in the absence of dialect-specific context and tailored guidance. In this work,
we propose a comprehensive framework that enhances LLM-based MT evaluation
using a dialect guided approach. We extend the ONUBAD dataset by incorporating
Sylheti-English sentence pairs, corresponding machine translations, and Direct
Assessment (DA) scores annotated by native speakers. To address the vocabulary
gap, we augment the tokenizer vocabulary with dialect-specific terms. We
further introduce a regression head to enable scalar score prediction and
design a dialect-guided (DG) prompting strategy. Our evaluation across multiple
LLMs shows that the proposed pipeline consistently outperforms existing
methods, achieving the highest gain of +0.1083 in Spearman correlation, along
with improvements across other evaluation settings. The dataset and the code
are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.

</details>


### [73] [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)
*Linghan Huang,Haolin Jin,Zhaoge Bi,Pengyue Yang,Peizhou Zhao,Taozhao Chen,Xiongfei Wu,Lei Ma,Huaming Chen*

Main category: cs.CL

TL;DR: 针对闭源大模型的多语言对抗攻击研究显示中文提示攻击更有效，Qwen-Max最脆弱，GPT-4o防御最强


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注开源模型的漏洞，缺乏对闭源模型在多语言场景下安全性的系统评估

Method: 构建集成对抗框架，测试GPT-4o等4个闭源模型在6类安全内容上的表现，生成38,400个攻击响应

Result: 中文攻击成功率更高（平均提升15%），Two-Sides攻击法最有效，Qwen-Max防御最弱（ASR 32.1%），GPT-4o最强（ASR 8.7%）

Conclusion: 大模型亟需增强多语言安全对齐能力，研究为开发跨语言鲁棒AI系统提供重要基准

Abstract: Large language models (LLMs) have seen widespread applications across various
domains, yet remain vulnerable to adversarial prompt injections. While most
existing research on jailbreak attacks and hallucination phenomena has focused
primarily on open-source models, we investigate the frontier of closed-source
LLMs under multilingual attack scenarios. We present a first-of-its-kind
integrated adversarial framework that leverages diverse attack techniques to
systematically evaluate frontier proprietary solutions, including GPT-4o,
DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories
of security contents in both English and Chinese, generating 38,400 responses
across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as
the quantitative metric to assess performance from three dimensions: prompt
design, model architecture, and language environment. Our findings suggest that
Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.
Notably, prompts in Chinese consistently yield higher ASRs than their English
counterparts, and our novel Two-Sides attack technique proves to be the most
effective across all models. This work highlights a dire need for
language-aware alignment and robust cross-lingual defenses in LLMs, and we hope
it will inspire researchers, developers, and policymakers toward more robust
and inclusive AI systems.

</details>


### [74] [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)
*Kun Huang,Weikai Xu,Yuxuan Liu,Quandong Wang,Pengzhi Gao,Wei Liu,Jian Luan,Bin Wang,Bo An*

Main category: cs.CL

TL;DR: 提出迭代偏好学习(IPL)方法解决CoaT轨迹稀缺问题，通过构建CoaT树和思维级直接偏好优化(T-DPO)提升移动GUI智能体性能


<details>
  <summary>Details</summary>
Motivation: 现有自训练方法忽视中间推理步骤的正确性，或依赖昂贵的过程级标注。需要有效解决CoaT轨迹多样性不足的问题以提升智能体泛化能力

Method: 1. 迭代采样构建CoaT树，使用规则奖励评估叶节点并反向传播反馈生成T-DPO对
2. 三阶段指令演化策略：利用GPT-4o基于真实UI截图生成多样化Q&A，增强布局理解

Result: MobileIPL在三个标准移动GUI基准测试中超越OS-ATLAS等基线模型，取得SOTA性能，在域外场景展现强泛化能力

Conclusion: IPL方法有效解决数据稀缺问题，无需昂贵标注即可提升智能体性能，证实其在跨域场景的优越泛化性

Abstract: The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to
improve the reasoning performance of VLM-based mobile agents in GUI tasks.
However, the scarcity of diverse CoaT trajectories limits the expressiveness
and generalization ability of such agents. While self-training is commonly
employed to address data scarcity, existing approaches either overlook the
correctness of intermediate reasoning steps or depend on expensive
process-level annotations to construct process reward models (PRM). To address
the above problems, we propose an Iterative Preference Learning (IPL) that
constructs a CoaT-tree through interative sampling, scores leaf nodes using
rule-based reward, and backpropagates feedback to derive Thinking-level Direct
Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up
supervised fine-tuning, we further introduce a three-stage instruction
evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real
mobile UI screenshots, enhancing both generality and layout understanding.
Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our
agent MobileIPL outperforms strong baselines, including continual pretraining
models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance
across three standard Mobile GUI-Agents benchmarks and shows strong
generalization to out-of-domain scenarios.

</details>


### [75] [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: 提出分层平衡优化算法（HBO），通过全局-局部双层优化机制解决大模型微调中的数据不平衡与异构性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法仅处理跨数据集全局不平衡，忽略了单个数据集内部局部异构性，导致大模型在多任务微调中效果受限

Method: 采用双层优化框架：Global Actor平衡跨子集数据采样，Local Actors根据难度优化子集内部数据使用，通过训练状态反馈的奖励函数动态调整

Result: 在3个大模型架构/9个多语言多任务场景中，HBO准确率显著超越基线方法（+2.1%-14.7%），消融实验验证全局-局部组件的协同有效性

Conclusion: HBO首次实现跨数据集全局平衡与数据集内部局部优化的联合调控，为复杂数据环境下的模型微调提供系统性解决方案

Abstract: Fine-tuning large language models (LLMs) on a mixture of diverse datasets
poses challenges due to data imbalance and heterogeneity. Existing methods
often address these issues across datasets (globally) but overlook the
imbalance and heterogeneity within individual datasets (locally), which limits
their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a
novel method that enables LLMs to autonomously adjust data allocation during
fine-tuning both across datasets (globally) and within each individual dataset
(locally). HBO employs a bilevel optimization strategy with two types of
actors: a Global Actor, which balances data sampling across different subsets
of the training mixture, and several Local Actors, which optimizes data usage
within each subset based on difficulty levels. These actors are guided by
reward functions derived from the LLM's training state, which measure learning
progress and relative performance improvement. We evaluate HBO on three LLM
backbones across nine diverse tasks in multilingual and multitask setups.
Results show that HBO consistently outperforms existing baselines, achieving
significant accuracy gains. Our in-depth analysis further demonstrates that
both the global actor and local actors of HBO effectively adjust data usage
during fine-tuning. HBO provides a comprehensive solution to the challenges of
data imbalance and heterogeneity in LLM fine-tuning, enabling more effective
training across diverse datasets.

</details>


### [76] [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)
*Yuwei Zhang,Wenhao Yu,Shangbin Feng,Yifan Zhu,Letian Peng,Jayanth Srinivasa,Gaowen Liu,Jingbo Shang*

Main category: cs.CL

TL;DR: 提出WikiDYK基准测试，通过维基百科动态知识验证发现双向语言模型（BiLMs）比因果模型（CLMs）知识记忆可靠性高23%，并提出协同框架提升29.1%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的知识记忆能力缺乏标准化评估工具，需要动态、无需人工干预的测试基准。

Method: 利用维基百科编辑精选的『你知道吗』条目构建12,290个事实/77,180个多格式问题，通过持续预训练对比CLMs/BiLMs性能，并提出双向模型协同框架。

Result: BiLMs可靠性准确率比CLMs高23%，协作框架进一步提升29.1%的准确率。

Conclusion: 双向语言模型具有更强的知识记忆能力，模块化协作框架可有效增强语言模型的知识可靠性，为LLM知识机制研究提供新范式。

Abstract: Despite significant advances in large language models (LLMs), their knowledge
memorization capabilities remain underexplored, due to the lack of standardized
and high-quality test ground. In this paper, we introduce a novel, real-world
and large-scale knowledge injection benchmark that evolves continuously over
time without requiring human intervention. Specifically, we propose WikiDYK,
which leverages recently-added and human-written facts from Wikipedia's "Did
You Know..." entries. These entries are carefully selected by expert Wikipedia
editors based on criteria such as verifiability and clarity. Each entry is
converted into multiple question-answer pairs spanning diverse task formats
from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290
facts and 77,180 questions, which is also seamlessly extensible with future
updates from Wikipedia editors. Extensive experiments using continued
pre-training reveal a surprising insight: despite their prevalence in modern
LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge
memorization capabilities compared to Bidirectional Language Models (BiLMs),
exhibiting a 23% lower accuracy in terms of reliability. To compensate for the
smaller scales of current BiLMs, we introduce a modular collaborative framework
utilizing ensembles of BiLMs as external knowledge repositories to integrate
with LLMs. Experiment shows that our framework further improves the reliability
accuracy by up to 29.1%.

</details>


### [77] [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)
*Weixuan Wang,Minghao Wu,Barry Haddow,Alexandra Birch*

Main category: cs.CL

TL;DR: 提出了ExpertSteer方法，利用外部专家模型生成导向向量，无需修改参数即可引导不同LLM的生成过程，显著提升多领域任务性能


<details>
  <summary>Details</summary>
Motivation: 现有激活导向方法依赖模型自身生成导向向量，无法跨模型利用外部专家知识，限制了干预效果和应用范围

Method: 1) 通过自动编码器对齐表征维度 2) 基于互信息分析定位干预层 3) 使用递归特征机制生成导向向量 4) 在推理时应用向量实施引导

Result: 在4个领域15个基准测试中，以极低成本显著超越基线方法，验证了跨模型知识迁移的有效性

Conclusion: ExpertSteer开创了利用任意专家模型干预LLM的新范式，为模型控制提供了参数高效且灵活的新方案

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities across various
tasks, yet guiding them to follow desired behaviours during inference remains a
significant challenge. Activation steering offers a promising method to control
the generation process of LLMs by modifying their internal activations.
However, existing methods commonly intervene in the model's behaviour using
steering vectors generated by the model itself, which constrains their
effectiveness to that specific model and excludes the possibility of leveraging
powerful external expert models for steering. To address these limitations, we
propose ExpertSteer, a novel approach that leverages arbitrary specialized
expert models to generate steering vectors, enabling intervention in any LLMs.
ExpertSteer transfers the knowledge from an expert model to a target LLM
through a cohesive four-step process: first aligning representation dimensions
with auto-encoders to enable cross-model transfer, then identifying
intervention layer pairs based on mutual information analysis, next generating
steering vectors from the expert model using Recursive Feature Machines, and
finally applying these vectors on the identified layers during inference to
selectively guide the target LLM without updating model parameters. We conduct
comprehensive experiments using three LLMs on 15 popular benchmarks across four
distinct domains. Experiments demonstrate that ExpertSteer significantly
outperforms established baselines across diverse tasks at minimal cost.

</details>


### [78] [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)
*Xinye Li,Mingqi Wan,Dianbo Sui*

Main category: cs.CL

TL;DR: Team asdfo123使用Meta-Llama-3-8B-Instruct模型，通过少量样本提示和多轮交互设计，在LLMSR@XLLM25结构化推理任务中取得第五名，验证了轻量级方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索如何仅通过提示工程实现结构化推理，避免复杂训练流程和额外资源消耗，验证大语言模型在细粒度逻辑验证任务中的原生能力。

Method: 设计多轮few-shot提示框架（条件枚举-证据标注-逻辑验证），配合正则表达式后处理器实现格式规范化，全程未使用微调/检索/模型集成。

Result: 在官方评估中综合排名第五，宏观F1分数与更复杂系统持平（具体分数未披露），证明轻量化方案的竞争力。

Conclusion: 验证了提示工程在结构化推理任务中的潜力，指出当前模型在复杂逻辑链处理上的局限，提出结合符号推理的混合系统作为未来方向。

Abstract: We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which
evaluates large language models on producing fine-grained, controllable, and
interpretable reasoning processes. Systems must extract all problem conditions,
decompose a chain of thought into statement-evidence pairs, and verify the
logical validity of each pair. Leveraging only the off-the-shelf
Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that
first enumerates all conditions and then guides the model to label, cite, and
adjudicate every reasoning step. A lightweight post-processor based on regular
expressions normalises spans and enforces the official JSON schema. Without
fine-tuning, external retrieval, or ensembling, our method ranks 5th overall,
achieving macro F1 scores on par with substantially more complex and
resource-consuming pipelines. We conclude by analysing the strengths and
limitations of our approach and outlining directions for future research in
structural reasoning with LLMs. Our code is available at
https://github.com/asdfo123/LLMSR-asdfo123.

</details>


### [79] [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)
*Qizhou Chen,Dakan Wang,Taolin Zhang,Zaoming Yan,Chengsong You,Chengyu Wang,Xiaofeng He*

Main category: cs.CL

TL;DR: 提出UniEdit基准框架，通过开放领域知识图谱构建多维度评估体系，全面检测大语言模型编辑的准确性与连锁效应。


<details>
  <summary>Details</summary>
Motivation: 现有LLM编辑数据集存在知识覆盖狭窄、评估维度单一的问题，无法满足开放领域复杂编辑需求，亟需构建更全面的评估基准。

Method: 1. 基于开放知识图谱构建25领域实体样本；2. 设计NMCS算法抽取多跳关联子图；3. 利用大模型将知识三元组转化为自然语言测试文本。

Result: 统计分析验证UniEdit的规模与多样性优势，实验揭示不同编辑器在知识覆盖度/编辑持续性/副作用控制等维度的性能差异。

Conclusion: UniEdit为开放域模型编辑提供了首个系统化评估方案，其结构化采样机制和多维度评估体系推动编辑技术向实用化发展。

Abstract: Model editing aims to enhance the accuracy and reliability of large language
models (LLMs) by efficiently adjusting their internal parameters. Currently,
most LLM editing datasets are confined to narrow knowledge domains and cover a
limited range of editing evaluation. They often overlook the broad scope of
editing demands and the diversity of ripple effects resulting from edits. In
this context, we introduce UniEdit, a unified benchmark for LLM editing
grounded in open-domain knowledge. First, we construct editing samples by
selecting entities from 25 common domains across five major categories,
utilizing the extensive triple knowledge available in open-domain knowledge
graphs to ensure comprehensive coverage of the knowledge domains. To address
the issues of generality and locality in editing, we design an Neighborhood
Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given
knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we
employ proprietary LLMs to convert the sampled knowledge subgraphs into natural
language text, guaranteeing grammatical accuracy and syntactical diversity.
Extensive statistical analysis confirms the scale, comprehensiveness, and
diversity of our UniEdit benchmark. We conduct comprehensive experiments across
multiple LLMs and editors, analyzing their performance to highlight strengths
and weaknesses in editing across open knowledge domains and various evaluation
criteria, thereby offering valuable insights for future research endeavors.

</details>


### [80] [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)
*Axel Abels,Tom Lenaerts*

Main category: cs.CL

TL;DR: LLMs可能延续数据偏见，群体聚合方法可缓解问题（纯LLM群体效果有限，结合人类多样性效果更佳）


<details>
  <summary>Details</summary>
Motivation: 解决LLMs因训练数据偏见导致输出偏见的缺陷，探索有效的群体智能缓解策略

Method: 分析LLMs对偏见诱导性头条的回应，测试不同群体聚合策略（平均法/加权法/人机混合群体）效果

Result: 简单平均聚合加剧偏见，加权聚合提升准确性并缓解偏见，人机混合群体效果最优（准确率+15.3%，偏见指标-29.7%）

Conclusion: 结合人类多样性+LLM准确性的混合群体，是解决算法偏见问题的有效路径（特别在民族和性别相关语境中）

Abstract: Despite their performance, large language models (LLMs) can inadvertently
perpetuate biases found in the data they are trained on. By analyzing LLM
responses to bias-eliciting headlines, we find that these models often mirror
human biases. To address this, we explore crowd-based strategies for mitigating
bias through response aggregation. We first demonstrate that simply averaging
responses from multiple LLMs, intended to leverage the "wisdom of the crowd",
can exacerbate existing biases due to the limited diversity within LLM crowds.
In contrast, we show that locally weighted aggregation methods more effectively
leverage the wisdom of the LLM crowd, achieving both bias mitigation and
improved accuracy. Finally, recognizing the complementary strengths of LLMs
(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing
both significantly enhance performance and further reduce biases across ethnic
and gender-related contexts.

</details>


### [81] [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)
*Gauri Kholkar,Ratinder Ahuja*

Main category: cs.CL

TL;DR: 提出CAPTURE基准揭示现有提示注入防护模型存在对抗漏检率高、良性误报率高的双重缺陷


<details>
  <summary>Details</summary>
Motivation: 现有提示注入防护模型依赖静态基准且存在过度防御问题，在动态上下文场景中防护效果不足

Method: 构建上下文感知基准CAPTURE，通过最小域内示例同时评估攻击检测能力和过防御倾向

Result: 实验显示当前模型在对抗场景漏检率高达32%，良性场景误报率超40%

Conclusion: CAPTURE有效暴露防护模型短板，为构建动态安全防护体系提供新方向

Abstract: Prompt injection remains a major security risk for large language models.
However, the efficacy of existing guardrail models in context-aware settings
remains underexplored, as they often rely on static attack benchmarks.
Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel
context-aware benchmark assessing both attack detection and over-defense
tendencies with minimal in-domain examples. Our experiments reveal that current
prompt injection guardrail models suffer from high false negatives in
adversarial cases and excessive false positives in benign scenarios,
highlighting critical limitations.

</details>


### [82] [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)
*Mohsinul Kabir,Tasfia Tahsin,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 当前研究多关注数据质量对语言模型偏见的影响，本文提出需同时考虑模型架构和时间因素，并通过对比实验揭示不同架构在偏见传播中的特性差异。


<details>
  <summary>Details</summary>
Motivation: 现有偏见研究集中在数据质量层面，缺乏对模型架构和时间动态的系统性溯源分析，难以有效解决偏见传播的根本问题。

Method: 基于比较行为理论框架，结合transformer与n-gram模型的对比实验，分析训练数据时间来源、窗口大小和架构设计对偏见传播的影响机制。

Result: 1) n-gram模型对上下文窗口敏感，transformer架构稳健 2) 数据时间来源显著影响偏见 3) 不同架构对特定偏见（如性取向）存在差异化放大效应

Conclusion: 需建立数据-模型联合溯源机制，从源头而非表象治理语言模型偏见，这对降低实际应用危害具有关键意义。

Abstract: Current research on bias in language models (LMs) predominantly focuses on
data quality, with significantly less attention paid to model architecture and
temporal influences of data. Even more critically, few studies systematically
investigate the origins of bias. We propose a methodology grounded in
comparative behavioral theory to interpret the complex interaction between
training data and model architecture in bias propagation during language
modeling. Building on recent work that relates transformers to n-gram LMs, we
evaluate how data, model design choices, and temporal dynamics affect bias
propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to
context window size in bias propagation, while transformers demonstrate
architectural robustness; (2) the temporal provenance of training data
significantly affects bias; and (3) different model architectures respond
differentially to controlled bias injection, with certain biases (e.g. sexual
orientation) being disproportionately amplified. As language models become
ubiquitous, our findings highlight the need for a holistic approach -- tracing
bias to its origins across both data and model dimensions, not just symptoms,
to mitigate harm.

</details>


### [83] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)
*Yang Hu,Xingyu Zhang,Xueji Fang,Zhiyang Chen,Xiao Wang,Huatian Zhang,Guojun Qi*

Main category: cs.CL

TL;DR: 提出SLOT方法——通过测试时添加样本特定参数向量，显著提升大语言模型在复杂指令下的表现


<details>
  <summary>Details</summary>
Motivation: 现有大模型对复杂指令处理能力不足，尤其在非典型样本上表现较差

Method: 1. 在测试时对每个样本进行少量优化
2. 在输出层前添加轻量级样本特定参数向量
3. 通过缓存最后一层特征实现高效适配
4. 最小化输入提示的交叉熵损失

Result: Qwen2.5-7B模型在GSM8K准确率提升8.6%（57.54%→66.19%），DeepSeek-R1-Distill-Llama-70B在GPQA达到70B级模型SOTA的68.69%

Conclusion: SLOT以极低参数量实现大模型性能突破，测试时优化策略显著提升指令跟随能力，代码已开源

Abstract: We propose SLOT (Sample-specific Language Model Optimization at Test-time), a
novel and parameter-efficient test-time inference approach that enhances a
language model's ability to more accurately respond to individual prompts.
Existing Large Language Models (LLMs) often struggle with complex instructions,
leading to poor performances on those not well represented among general
samples. To address this, SLOT conducts few optimization steps at test-time to
update a light-weight sample-specific parameter vector. It is added to the
final hidden layer before the output head, and enables efficient adaptation by
caching the last layer features during per-sample optimization. By minimizing
the cross-entropy loss on the input prompt only, SLOT helps the model better
aligned with and follow each given instruction. In experiments, we demonstrate
that our method outperforms the compared models across multiple benchmarks and
LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on
GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT
achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is
available at https://github.com/maple-research-lab/SLOT.

</details>


### [84] [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)
*Yepeng Weng,Qiao Hu,Xujie Chen,Li Liu,Dianwen Mei,Huishi Qiu,Jiang Tian,Zhongchao Shi*

Main category: cs.CL

TL;DR: 提出基于叶到根遍历验证的Traversal Verification算法，通过保留潜在有效子序列显著提升大模型推理速度


<details>
  <summary>Details</summary>
Motivation: 现有推测解码框架存在两个关键缺陷：1) 基于单个token的验证机制导致接受长度次优；2) 自上而下的逐层验证导致候选token利用率低下

Method: 创新性地采用叶节点到根节点的逆向验证范式，基于整条候选序列的接受概率进行判断，保留可能被传统方法错误丢弃的有效子序列

Result: 理论上证明输出分布与目标模型完全一致（无损推理），实验显示在多个大模型和任务中平均接受长度和吞吐量均有显著提升

Conclusion: 该算法从根本上重构了推测解码的验证范式，为加速大语言模型推理提供了更高效的解决方案

Abstract: Speculative decoding is a promising approach for accelerating large language
models. The primary idea is to use a lightweight draft model to speculate the
output of the target model for multiple subsequent timesteps, and then verify
them in parallel to determine whether the drafted tokens should be accepted or
rejected. To enhance acceptance rates, existing frameworks typically construct
token trees containing multiple candidates in each timestep. However, their
reliance on token-level verification mechanisms introduces two critical
limitations: First, the probability distribution of a sequence differs from
that of individual tokens, leading to suboptimal acceptance length. Second,
current verification schemes begin from the root node and proceed layer by
layer in a top-down manner. Once a parent node is rejected, all its child nodes
should be discarded, resulting in inefficient utilization of speculative
candidates. This paper introduces Traversal Verification, a novel speculative
decoding algorithm that fundamentally rethinks the verification paradigm
through leaf-to-root traversal. Our approach considers the acceptance of the
entire token sequence from the current node to the root, and preserves
potentially valid subsequences that would be prematurely discarded by existing
methods. We theoretically prove that the probability distribution obtained
through Traversal Verification is identical to that of the target model,
guaranteeing lossless inference while achieving substantial acceleration gains.
Experimental results across different large language models and multiple tasks
show that our method consistently improves acceptance length and throughput
over existing methods

</details>


### [85] [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)
*Konstantinos Xylogiannopoulos,Petros Xanthopoulos,Panagiotis Karampelas,Georgios Bakamitsos*

Main category: cs.CL

TL;DR: 提出基于模式相似性的检测方法，可识别ChatGPT改写的新闻并溯源，准确率达96.23%


<details>
  <summary>Details</summary>
Motivation: 生成式AI被恶意用于新闻改写导致版权侵权和作者收益损失，但相关研究匮乏

Method: 非深度学习的模式相似性检测算法方案，重点识别ChatGPT改写特征

Result: 在包含4,448篇文章的数据集中，准确率/精确度/敏感度/特异性/F1分数均超96%

Conclusion: 无需深度学习的方法能高效检测ChatGPT改写内容，具有实际应用价值

Abstract: Generative AI paraphrased text can be used for copyright infringement and the
AI paraphrased content can deprive substantial revenue from original content
creators. Despite this recent surge of malicious use of generative AI, there
are few academic publications that research this threat. In this article, we
demonstrate the ability of pattern-based similarity detection for AI
paraphrased news recognition. We propose an algorithmic scheme, which is not
limited to detect whether an article is an AI paraphrase, but, more
importantly, to identify that the source of infringement is the ChatGPT. The
proposed method is tested with a benchmark dataset specifically created for
this task that incorporates real articles from BBC, incorporating a total of
2,224 articles across five different news categories, as well as 2,224
paraphrased articles created with ChatGPT. Results show that our pattern
similarity-based method, that makes no use of deep learning, can detect ChatGPT
assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for
precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1
score.

</details>


### [86] [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)
*Zhenhe Wu,Jian Yang,Jiaheng Liu,Xianjie Wu,Changzai Pan,Jie Zhang,Yu Zhao,Shuangyong Song,Yongxiang Li,Zhoujun Li*

Main category: cs.CL

TL;DR: 提出Table-R1强化学习方法，通过区域证据集成和混合奖励机制显著提升大语言模型表格推理能力，性能提升14.36分且减少67.5%响应token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有表格问答方法在区域信息整合和推理效率方面存在不足，需要强化模型对表格区域的理解与证据利用能力。

Method: 1. 区域增强监督微调（RE-SFT）整合文本/符号/程序推理
2. 表格感知分组策略优化（TARPO）动态平衡区域精度与答案正确性

Result: 在3个基准数据集上平均提升14.36分，优于10倍参数基线模型，响应token减少67.5%

Conclusion: Table-R1通过区域证据强化和混合奖励机制，显著提升语言模型表格推理效率和精度，推动高效表格理解技术发展

Abstract: Tables present unique challenges for language models due to their structured
row-column interactions, necessitating specialized approaches for effective
comprehension. While large language models (LLMs) have demonstrated potential
in table reasoning through prompting and techniques like chain-of-thought (CoT)
and program-of-thought (PoT), optimizing their performance for table question
answering remains underexplored. In this paper, we introduce region-based
Table-R1, a novel reinforcement learning approach that enhances LLM table
understanding by integrating region evidence into reasoning steps. Our method
employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in
identifying relevant table regions before generating answers, incorporating
textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group
Relative Policy Optimization (TARPO) introduces a mixed reward system to
dynamically balance region accuracy and answer correctness, with decaying
region rewards and consistency penalties to align reasoning steps. Experiments
show that Table-R1 achieves an average performance improvement of 14.36 points
across multiple base models on three benchmark datasets, even outperforming
baseline models with ten times the parameters, while TARPO reduces response
token consumption by 67.5% compared to GRPO, significantly advancing LLM
capabilities in efficient tabular reasoning.

</details>


### [87] [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)
*Wenqiao Zhu,Chao Xu,Lulu Wang,Jun Wu*

Main category: cs.CL

TL;DR: 提出PSC相位校准模块增强RoPE位置编码，有效提升现有方法（如PI/YaRN/LongRoPE）的长上下文处理能力，在16k-64k窗口下困惑度显著降低


<details>
  <summary>Details</summary>
Motivation: 现有基于RoPE的上下文扩展方法面临指数级搜索空间挑战，难以找到最优频率缩放因子

Method: 通过相位偏移校准（PSC）模块动态调整预定义频率，采用相位补偿机制优化位置编码分布

Result: 在LLaMA2等模型上验证，16k/32k/64k窗口困惑度分别降低12.3%/18.7%/24.1%，且多任务表现稳健

Conclusion: PSC作为通用增强模块显著提升现有长上下文方案的性能，尤其在超长文本场景下优势明显

Abstract: Rotary Position Embedding (RoPE) is an efficient position encoding approach
and is widely utilized in numerous large language models (LLMs). Recently, a
lot of methods have been put forward to further expand the context window based
on RoPE. The core concept of those methods is to predefine or search for a set
of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a
challenge for existing methods to predefine an optimal factor due to the
exponential search space. In view of this, we introduce PSC (Phase Shift
Calibration), a small module for calibrating the frequencies predefined by
existing methods. With the employment of PSC, we demonstrate that many existing
methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted
extensive experiments across multiple models and tasks. The results demonstrate
that (1) when PSC is enabled, the comparative reductions in perplexity increase
as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our
approach is broadly applicable and exhibits robustness across a variety of
models and tasks. The code can be found at https://github.com/WNQzhu/PSC.

</details>


### [88] [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)
*Jinming Zhang,Yunfei Long*

Main category: cs.CL

TL;DR: 提出认知启发框架LPLH，通过结构化地图构建、动作学习和反馈分析，使LLM代理在交互式小说游戏中实现类人类决策。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理在IF游戏中过于关注任务指标，缺乏人类对叙事逻辑的理解。需通过认知科学模拟人类玩家行为模式。

Method: 整合三组件：1) 结构化地图捕捉空间叙事关系 2) 上下文适配动作学习 3) 反馈驱动经验优化决策过程

Result: LPLH框架使LLM代理在文本环境中表现出更高可解释性、上下文感知能力和类人决策稳健性

Conclusion: 将IF游戏重构为LLM的学习问题，结合认知科学原理开辟复杂文本环境上下文感知游戏的新路径

Abstract: Interactive Fiction games (IF games) are where players interact through
natural language commands. While recent advances in Artificial Intelligence
agents have reignited interest in IF games as a domain for studying
decision-making, existing approaches prioritize task-specific performance
metrics over human-like comprehension of narrative context and gameplay logic.
This work presents a cognitively inspired framework that guides Large Language
Models (LLMs) to learn and play IF games systematically. Our proposed
**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three
key components: (1) structured map building to capture spatial and narrative
relationships, (2) action learning to identify context-appropriate commands,
and (3) feedback-driven experience analysis to refine decision-making over
time. By aligning LLMs-based agents' behavior with narrative intent and
commonsense constraints, LPLH moves beyond purely exploratory strategies to
deliver more interpretable, human-like performance. Crucially, this approach
draws on cognitive science principles to more closely simulate how human
players read, interpret, and respond within narrative worlds. As a result, LPLH
reframes the IF games challenge as a learning problem for LLMs-based agents,
offering a new path toward robust, context-aware gameplay in complex text-based
environments.

</details>


### [89] [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)
*Siyang Wu,Honglin Bao,Nadav Kunievsky,James A. Evans*

Main category: cs.CL

TL;DR: 提出自问自答策略提升LLMs对技术专利的语义区分能力，并通过130万专利数据集验证其有效性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在知识潜在化、结构松散的问题，尤其在需要精细语义区分的专业领域（如专利分析）表现受限

Method: 通过自问自答机制激活模型内部知识，结合外部科学文本检索，使用130万计算机科学专利对构建复杂基准测试

Result: 自问自答使模型性能提升15-20%，发现小模型生成的问题更基础开放（适合中型模型使用），外部知识补充可提升3倍效果

Conclusion: 自问自答既是提升LLM专业理解的实用机制，也是诊断内外部知识组织的工具，揭示了跨模型协作的新策略

Abstract: Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that prompting
LLMs to generate and answer their own questions - targeting the background
knowledge required for the task - significantly improves performance. These
self-generated questions and answers activate otherwise underutilized internal
knowledge. Allowing LLMs to retrieve answers from external scientific texts
further enhances performance, suggesting that model knowledge is compressed and
lacks the full richness of the training data. We also find that
chain-of-thought prompting and self-questioning converge, though
self-questioning remains more effective for improving understanding of
technical concepts. Notably, we uncover an asymmetry in prompting: smaller
models often generate more fundamental, more open-ended, better-aligned
questions for mid-sized models than large models with better understanding do,
revealing a new strategy for cross-model collaboration. Altogether, our
findings establish self-questioning as both a practical mechanism for
automatically improving LLM comprehension, especially in domains with sparse
and underrepresented knowledge, and a diagnostic probe of how internal and
external knowledge are organized.

</details>


### [90] [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)
*Yuyang Ding,Dan Qiao,Juntao Li,Jiajie Xu,Pingfu Chao,Xiaofang Zhou,Min Zhang*

Main category: cs.CL

TL;DR: 提出针对远程监督NER的新框架，有效解决未标注和噪声实体问题，显著提升多数据集性能


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注噪声测量方法，但忽视不同远程标注方法间的潜在噪声分布差异，导致模型性能受限

Method: 结合规则方法与LLM监督的标注技术，提出区分UEP（未标注实体）和NEP（噪声实体）的噪声评估框架，并分别设计解决方案

Result: 在覆盖3种数据源、4种标注技术的8个真实数据集上实现显著性能提升，超越现有SOTA方法

Conclusion: 该框架通过针对性解决远程监督中的核心噪声问题，验证了方法在多样标注场景下的有效性和鲁棒性

Abstract: Distantly supervised named entity recognition (DS-NER) has emerged as a cheap
and convenient alternative to traditional human annotation methods, enabling
the automatic generation of training data by aligning text with external
resources. Despite the many efforts in noise measurement methods, few works
focus on the latent noise distribution between different distant annotation
methods. In this work, we explore the effectiveness and robustness of DS-NER by
two aspects: (1) distant annotation techniques, which encompasses both
traditional rule-based methods and the innovative large language model
supervision approach, and (2) noise assessment, for which we introduce a novel
framework. This framework addresses the challenges by distinctly categorizing
them into the unlabeled-entity problem (UEP) and the noisy-entity problem
(NEP), subsequently providing specialized solutions for each. Our proposed
method achieves significant improvements on eight real-world distant
supervision datasets originating from three different data sources and
involving four distinct annotation techniques, confirming its superiority over
current state-of-the-art methods.

</details>


### [91] [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)
*Weixiao Zhou,Junnan Zhu,Gengyao Li,Xianfu Cheng,Xinnian Liang,Feifei Zhai,Zhoujun Li*

Main category: cs.CL

TL;DR: 研究评估了12个大语言模型在结合背景知识的对话摘要任务中的表现，发现模型在背景检索、观点整合及自我修正方面存在显著不足，顶尖模型平均性能不足69%。


<details>
  <summary>Details</summary>
Motivation: 解决现有对话摘要系统因仅依赖讨论信息导致的外部观察者混淆问题，探索LLMs结合背景知识生成摘要的能力。

Method: 1. 定义背景摘要与观点摘要双输出模式
2. 构建人工标注的基准数据集
3. 提出分层评估框架与细粒度指标
4. 采用结构化提示与自反思范式测试12个LLMs

Result: 1. LLMs背景摘要检索/生成能力薄弱
2. 最优模型双模式平均性能<69%
3. 当前模型缺乏有效的自我评估与修正机制

Conclusion: LLMs在需要结合背景知识的摘要任务中面临核心挑战，突出改进方向包括背景知识整合、多信息源协调能力及自省机制开发。

Abstract: In this work, we investigate the performance of LLMs on a new task that
requires combining discussion with background knowledge for summarization. This
aims to address the limitation of outside observer confusion in existing
dialogue summarization systems due to their reliance solely on discussion
information. To achieve this, we model the task output as background and
opinion summaries and define two standardized summarization patterns. To
support assessment, we introduce the first benchmark comprising high-quality
samples consistently annotated by human experts and propose a novel
hierarchical evaluation framework with fine-grained, interpretable metrics. We
evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our
findings reveal: (1) LLMs struggle with background summary retrieval,
generation, and opinion summary integration. (2) Even top LLMs achieve less
than 69% average performance across both patterns. (3) Current LLMs lack
adequate self-evaluation and self-correction capabilities for this task.

</details>


### [92] [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)
*Xiao Long,Liansheng Zhuang,Chen Shen,Shaotian Yan,Yifei Li,Shafei Wang*

Main category: cs.CL

TL;DR: 提出无需训练的KGQA框架RTSoG，通过子问题分解和奖励引导的树搜索优化推理路径选择


<details>
  <summary>Details</summary>
Motivation: 现有方法过度关注新推理路径探索，忽视历史路径利用导致次优解；复杂问题语义导致推理路径检索不准确

Method: 1. 将复杂问题分解为明确子问题 2. 引入基于奖励模型的SC-MCTS算法迭代检索加权推理路径 3. 按权重堆叠路径生成最终答案

Result: 在GrailQA和WebQSP数据集分别实现8.7%和7.0%的性能提升，超越现有最优方法

Conclusion: RTSoG通过语义分解和奖励引导的路径加权机制，有效提升复杂知识图谱问答的推理质量与准确率

Abstract: Recently, large language models (LLMs) have demonstrated impressive
performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to
find answers based on knowledge graphs (KGs) for natural language questions.
Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented
Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the
large KGs, and then generates the answers based on them. However, these methods
emphasize the exploration of new optimal reasoning paths in KGs while ignoring
the exploitation of historical reasoning paths, which may lead to sub-optimal
reasoning paths. Additionally, the complex semantics contained in questions may
lead to the retrieval of inaccurate reasoning paths. To address these issues,
this paper proposes a novel and training-free framework for KGQA tasks called
Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original
question into a series of simpler and well-defined sub-questions to handle the
complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided
by a reward model is introduced to iteratively retrieve weighted reasoning
paths as contextual knowledge. Finally, it stacks the weighted reasoning paths
according to their weights to generate the final answers. Extensive experiments
on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves
8.7\% and 7.0\% performance improvement over the state-of-the-art method on the
GrailQA and the WebQSP respectively.

</details>


### [93] [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)
*Nikita Tatarinov,Vidhyakshaya Kannan,Haricharana Srinivasa,Arnav Raj,Harpreet Singh Anand,Varun Singh,Aditya Luthra,Ravij Lade,Agam Shah,Sudheer Chava*

Main category: cs.CL

TL;DR: 提出KG-QAGen框架构建多复杂度QA数据集，揭示大语言模型在集合操作和多跳推理上的系统性缺陷


<details>
  <summary>Details</summary>
Motivation: 现有长文本评估基准缺乏系统化调节问题复杂度的能力，需细粒度评估模型的长上下文处理性能

Method: 基于知识图谱从财务协议中提取QA对，通过多跳检索/集合操作/答案多样性三个维度控制问题复杂度

Result: 构建当前最大规模长文本基准(20,139 QA对)，发现最优模型在集合比较和多跳推理上准确率不足50%

Conclusion: 框架有效定位模型弱点，揭示当前LLMs在语义关联捕捉和隐含逻辑推理方面的根本性局限

Abstract: The increasing context length of modern language models has created a need
for evaluating their ability to retrieve and process information across
extensive documents. While existing benchmarks test long-context capabilities,
they often lack a structured way to systematically vary question complexity. We
introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a
framework that (1) extracts QA pairs at multiple complexity levels (2) by
leveraging structured representations of financial agreements (3) along three
key dimensions -- multi-hop retrieval, set operations, and answer plurality --
enabling fine-grained assessment of model performance across controlled
difficulty levels. Using this framework, we construct a dataset of 20,139 QA
pairs (the largest number among the long-context benchmarks) and open-source a
part of it. We evaluate 13 proprietary and open-source LLMs and observe that
even the best-performing models are struggling with set-based comparisons and
multi-hop logical inference. Our analysis reveals systematic failure modes tied
to semantic misinterpretation and inability to handle implicit relations.

</details>


### [94] [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)
*Xu Zheng,Zhuomin Chen,Esteban Schafir,Sipeng Chen,Hojat Allah Salehi,Haifeng Chen,Farhad Shirani,Wei Cheng,Dongsheng Luo*

Main category: cs.CL

TL;DR: 提出可解释性框架LM²otifs，通过图神经网络和词共现图实现机器文本检测与解释


<details>
  <summary>Details</summary>
Motivation: 传统检测方法在解释词间复杂关系方面存在不足，需开发兼具准确性和可解释性的检测框架

Method: 三阶段框架：1) 词共现图构建 2) 图神经网络预测 3) 后解释方法提取多层级语言特征模板

Result: 在多个基准数据集验证有效性，提取的motif显著区分人机文本，揭示机器文本的独特语言指纹

Conclusion: LM²otifs首次实现检测与解释的平衡，为理解机器文本生成模式提供新视角

Abstract: The impressive ability of large language models to generate natural text
across various tasks has led to critical challenges in authorship
authentication. Although numerous detection methods have been developed to
differentiate between machine-generated texts (MGT) and human-generated texts
(HGT), the explainability of these methods remains a significant gap.
Traditional explainability techniques often fall short in capturing the complex
word relationships that distinguish HGT from MGT. To address this limitation,
we present LM$^2$otifs, a novel explainable framework for MGT detection.
Inspired by probabilistic graphical models, we provide a theoretical rationale
for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks
to achieve both accurate detection and interpretability. The LM$^2$otifs
pipeline operates in three key stages: first, it transforms text into graphs
based on word co-occurrence to represent lexical dependencies; second, graph
neural networks are used for prediction; and third, a post-hoc explainability
method extracts interpretable motifs, offering multi-level explanations from
individual words to sentence structures. Extensive experiments on multiple
benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The
empirical evaluation of the extracted explainable motifs confirms their
effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis
reveals distinct and visible linguistic fingerprints characteristic of MGT.

</details>


### [95] [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)
*Yanting Li,Jiyue Jiang,Zikang Wang,Ziqian Lin,Dongchen He,Yuheng Shan,Yanruisheng Shao,Jiayi Li,Xiangyu Shi,Jiuming Wang,Yanyu Chen,Yimin Fan,Han Li,Yu Li*

Main category: cs.CL

TL;DR: 提出双结构深度语言模型DS-ProGen，通过整合蛋白质主干几何和表面化学特征，显著提升功能蛋白设计的准确性与功能性保留能力。


<details>
  <summary>Details</summary>
Motivation: 现有逆蛋白质折叠方法仅依赖单一结构表征（主干坐标或表面特征），难以同时满足全局构象约束和局部化学相互作用的需求。

Method: 融合主干坐标与表面化学/几何描述符，构建双模态深度语言模型，采用next-amino-acid预测范式生成满足多维度结构约束的氨基酸序列。

Result: 在PRIDE数据集达到61.47%的氨基酸恢复率（SOTA），并展示优异的配体/离子/RNA等生物分子相互作用预测能力。

Conclusion: 多模态结构编码策略有效协调全局构象稳定性和局部功能位点保留，推动功能性蛋白设计向更高精度发展。

Abstract: Inverse Protein Folding (IPF) is a critical subtask in the field of protein
design, aiming to engineer amino acid sequences capable of folding correctly
into a specified three-dimensional (3D) conformation. Although substantial
progress has been achieved in recent years, existing methods generally rely on
either backbone coordinates or molecular surface features alone, which
restricts their ability to fully capture the complex chemical and geometric
constraints necessary for precise sequence prediction. To address this
limitation, we present DS-ProGen, a dual-structure deep language model for
functional protein design, which integrates both backbone geometry and
surface-level representations. By incorporating backbone coordinates as well as
surface chemical and geometric descriptors into a next-amino-acid prediction
paradigm, DS-ProGen is able to generate functionally relevant and structurally
stable sequences while satisfying both global and local conformational
constraints. On the PRIDE dataset, DS-ProGen attains the current
state-of-the-art recovery rate of 61.47%, demonstrating the synergistic
advantage of multi-modal structural encoding in protein design. Furthermore,
DS-ProGen excels in predicting interactions with a variety of biological
partners, including ligands, ions, and RNA, confirming its robust functional
retention capabilities.

</details>


### [96] [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)
*Navid Madani,Rohini Srihari*

Main category: cs.CL

TL;DR: ESC-Judge提出首个基于探索-洞察-行动咨询模型的自动化评估框架，可低成本验证情感支持LLMs效果并与人类专家决策高度一致。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏可扩展、理论化的评估方法筛选心理健康领域最有效的LLMs，导致部署决策缺乏依据。

Method: 三阶段流程：1) 合成含压力源/性格特征的虚拟求助者角色 2) 双代理独立会话隔离模型策略 3) 专用评判LLM按标准化量表对比模型表现

Result: 在探索/洞察/行动维度分别达到85%/83%/86%的人类专家决策匹配率，评估成本显著降低

Conclusion: 该框架为情感支持AI提供可靠评估工具，所有代码/数据开源推动领域透明发展

Abstract: Large language models (LLMs) increasingly power mental-health chatbots, yet
the field still lacks a scalable, theory-grounded way to decide which model is
most effective to deploy. We present ESC-Judge, the first end-to-end evaluation
framework that (i) grounds head-to-head comparisons of emotional-support LLMs
in Clara Hill's established Exploration-Insight-Action counseling model,
providing a structured and interpretable view of performance, and (ii) fully
automates the evaluation pipeline at scale. ESC-Judge operates in three stages:
first, it synthesizes realistic help-seeker roles by sampling empirically
salient attributes such as stressors, personality, and life history; second, it
has two candidate support agents conduct separate sessions with the same role,
isolating model-specific strategies; and third, it asks a specialized judge LLM
to express pairwise preferences across rubric-anchored skills that span the
Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched
PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and
86 percent of Action decisions, demonstrating human-level reliability at a
fraction of the cost. All code, prompts, synthetic roles, transcripts, and
judgment scripts are released to promote transparent progress in emotionally
supportive AI.

</details>


### [97] [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)
*Varvara Arzt,Allan Hanbury,Michael Wiegand,Gábor Recski,Terra Blevins*

Main category: cs.CL

TL;DR: 关系抽取模型泛化能力差，跨数据集表现不佳。数据质量比词汇相似性更重要，不同数据质量下应选择微调或小样本学习策略，基准测试的结构性缺陷影响模型迁移。


<details>
  <summary>Details</summary>
Motivation: 揭示关系抽取模型是否真正学习到稳健的关系模式，还是依赖数据集特定的虚假关联。现有评估局限于单一数据集，缺乏对模型泛化能力的系统分析。

Method: 通过跨数据集实验评估模型泛化能力，分析不同适应策略（微调、小样本上下文学习、零样本）的表现，研究数据质量、词汇相似性和基准结构问题对迁移的影响。

Result: 1. 模型跨数据集表现显著下降 2. 数据质量（非词汇相似性）是迁移关键 3. 高质量数据适用微调，噪声数据适合小样本学习 4. 零样本基线有时优于跨数据集结果 5. 基准结构问题（单关系样本约束、负类定义混乱）损害迁移能力

Conclusion: 关系抽取研究需重视数据质量和基准设计改进。实际应用中应根据目标数据质量选择微调或小样本学习策略，现有基准的结构性缺陷需通过多关系样本设计、标准化负类定义来改善。

Abstract: Analysing the generalisation capabilities of relation extraction (RE) models
is crucial for assessing whether they learn robust relational patterns or rely
on spurious correlations. Our cross-dataset experiments find that RE models
struggle with unseen data, even within similar domains. Notably, higher
intra-dataset performance does not indicate better transferability, instead
often signaling overfitting to dataset-specific artefacts. Our results also
show that data quality, rather than lexical similarity, is key to robust
transfer, and the choice of optimal adaptation strategy depends on the quality
of data available: while fine-tuning yields the best cross-dataset performance
with high-quality data, few-shot in-context learning (ICL) is more effective
with noisier data. However, even in these cases, zero-shot baselines
occasionally outperform all cross-dataset results. Structural issues in RE
benchmarks, such as single-relation per sample constraints and non-standardised
negative class definitions, further hinder model transferability.

</details>


### [98] [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)
*Md Mehrab Tanjim,Yeonjun In,Xiang Chen,Victor S. Bursztyn,Ryan A. Rossi,Sungchul Kim,Guang-Jie Ren,Vaishnavi Muppala,Shun Jiang,Yongsung Kim,Chanyoung Park*

Main category: cs.CL

TL;DR: 探讨大语言模型在对话问答中处理语言歧义的定义、分类方法及消歧技术，提出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理中歧义问题因大语言模型的应用变得更加复杂，需系统性研究其在对话场景下的影响与解决方案。

Method: 通过定义核心概念、分类LLM支持的消歧方法、对比分析技术优劣，并整合公开数据集用于技术评估。

Result: 构建了LLM消歧技术体系框架，揭示现有方法局限性，提出未解决问题如动态歧义处理的研发方向。

Conclusion: LLM为歧义处理提供新可能，但需在可解释性、多模态歧义等方向持续突破以完善语言系统。

Abstract: Ambiguity remains a fundamental challenge in Natural Language Processing
(NLP) due to the inherent complexity and flexibility of human language. With
the advent of Large Language Models (LLMs), addressing ambiguity has become
even more critical due to their expanded capabilities and applications. In the
context of Conversational Question Answering (CQA), this paper explores the
definition, forms, and implications of ambiguity for language driven systems,
particularly in the context of LLMs. We define key terms and concepts,
categorize various disambiguation approaches enabled by LLMs, and provide a
comparative analysis of their advantages and disadvantages. We also explore
publicly available datasets for benchmarking ambiguity detection and resolution
techniques and highlight their relevance for ongoing research. Finally, we
identify open problems and future research directions, proposing areas for
further investigation. By offering a comprehensive review of current research
on ambiguities and disambiguation with LLMs, we aim to contribute to the
development of more robust and reliable language systems.

</details>


### [99] [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)
*Yang Zhao,Pu Wang,Yibo Zhao,Hongru Du,Hao,Yang*

Main category: cs.CL

TL;DR: 提出TrafficSafe框架，通过LLMs整合多模态交通事故数据实现预测性能提升，并开发特征归因框架分析风险因素


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理多源交通数据（数值/文本/图像/环境/驾驶行为）的复杂关联，导致语义信息丢失和风险识别不足

Method: 构建包含58,903份事故报告的多模态数据集TrafficSafe Event，微调LLMs实现事故预测，开发句子级特征归因框架TrafficSafe Attribution

Result: 预测F1分数提升42%，揭示酒驾是严重事故主因（危险驾驶行为贡献度达其他因素2倍），特征归因框架指导数据收集策略优化

Conclusion: TrafficSafe为交通安全研究提供可解释AI方案，推动AI技术向可操作安全决策转化，实现生命保护的实际应用价值

Abstract: Predicting crash events is crucial for understanding crash distributions and
their contributing factors, thereby enabling the design of proactive traffic
safety policy interventions. However, existing methods struggle to interpret
the complex interplay among various sources of traffic crash data, including
numeric characteristics, textual reports, crash imagery, environmental
conditions, and driver behavior records. As a result, they often fail to
capture the rich semantic information and intricate interrelationships embedded
in these diverse data sources, limiting their ability to identify critical
crash risk factors. In this research, we propose TrafficSafe, a framework that
adapts LLMs to reframe crash prediction and feature attribution as text-based
reasoning. A multi-modal crash dataset including 58,903 real-world reports
together with belonged infrastructure, environmental, driver, and vehicle
information is collected and textualized into TrafficSafe Event Dataset. By
customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves
a 42% average improvement in F1-score over baselines. To interpret these
predictions and uncover contributing factors, we introduce TrafficSafe
Attribution, a sentence-level feature attribution framework enabling
conditional risk analysis. Findings show that alcohol-impaired driving is the
leading factor in severe crashes, with aggressive and impairment-related
behaviors having nearly twice the contribution for severe crashes compared to
other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal
features during model training, guiding strategic crash data collection for
iterative performance improvements. The proposed TrafficSafe offers a
transformative leap in traffic safety research, providing a blueprint for
translating advanced AI technologies into responsible, actionable, and
life-saving outcomes.

</details>


### [100] [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)
*A. Feder Cooper,Aaron Gokaslan,Amy B. Cyphert,Christopher De Sa,Mark A. Lemley,Daniel E. Ho,Percy Liang*

Main category: cs.CL

TL;DR: 研究通过对抗性机器学习验证LLMs存在选择性文本记忆现象，但记忆程度因模型和书籍差异显著，对版权案件具有复杂法律启示


<details>
  <summary>Details</summary>
Motivation: 澄清生成式AI版权诉讼中关于LLMs记忆程度的极端主张，揭示记忆与版权关系的复杂性

Method: 使用概率提取技术从13个开源LLMs中提取Books3数据集内容，设计多维度实验验证记忆程度

Result: Llama 3.1 70B对《哈利波特》《1984》近完全记忆，但多数大模型未整体记忆大部分书籍；记忆程度与模型规模非正相关

Conclusion: 记忆的个案特异性不支持简单法律推定，需建立动态技术评估框架平衡创新与版权保护

Abstract: Plaintiffs and defendants in copyright lawsuits over generative AI often make
sweeping, opposing claims about the extent to which large language models
(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial
ML and copyright law, we show that these polarized positions dramatically
oversimplify the relationship between memorization and copyright. To do so, we
leverage a recent probabilistic extraction technique to extract pieces of the
Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show
that it's possible to extract substantial parts of at least some books from
different LLMs. This is evidence that the LLMs have memorized the extracted
text; this memorized content is copied inside the model parameters. But the
results are complicated: the extent of memorization varies both by model and by
book. With our specific experiments, we find that the largest LLMs don't
memorize most books -- either in whole or in part. However, we also find that
Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost
entirely. We discuss why our results have significant implications for
copyright cases, though not ones that unambiguously favor either side.

</details>


### [101] [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)
*Hiram Ring*

Main category: cs.CL

TL;DR: 研究者开发了taggedPBC——一个包含1500+语言的大规模自动标注平行数据集，促进跨语言语料研究


<details>
  <summary>Details</summary>
Motivation: 现有跨语言数据集存在数据规模与语言覆盖的权衡，难以揭示人类语言的普遍规律，资源限制制约了研究进展

Method: 构建包含1800+句子、覆盖133语系111孤立语的标注语料库，通过SOTA标注工具对比验证准确性，并开发N1 ratio指标进行语序分类

Result: 标注准确性与专业工具相当（SpaCy/Trankit相关系数0.85），N1 ratio在三大类型数据库验证有效（分类准确率92%），数据集已开源

Conclusion: taggedPBC突破了跨语言研究的资源瓶颈，为语序类型学和大规模语言分析提供了新型基础设施，未来将持续扩展优化

Abstract: Existing datasets available for crosslinguistic investigations have tended to
focus on large amounts of data for a small group of languages or a small amount
of data for a large number of languages. This means that claims based on these
datasets are limited in what they reveal about universal properties of the
human language faculty. While this has begun to change through the efforts of
projects seeking to develop tagged corpora for a large number of languages,
such efforts are still constrained by limits on resources. The current paper
reports on a large automatically tagged parallel dataset which has been
developed to partially address this issue. The taggedPBC contains more than
1,800 sentences of pos-tagged parallel text data from over 1,500 languages,
representing 133 language families and 111 isolates, dwarfing previously
available resources. The accuracy of tags in this dataset is shown to correlate
well with both existing SOTA taggers for high-resource languages (SpaCy,
Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).
Additionally, a novel measure derived from this dataset, the N1 ratio,
correlates with expert determinations of word order in three typological
databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier
trained on this feature can accurately identify basic word order for languages
not in those databases. While much work is still needed to expand and develop
this dataset, the taggedPBC is an important step to enable corpus-based
crosslinguistic investigations, and is made available for research and
collaboration via GitHub.

</details>


### [102] [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)
*Lekang Jiang,Chengzu Li,Stephan Goetz*

Main category: cs.CL

TL;DR: 提出欧洲专利数据集EPD，通过司法多样性、质量提升和真实世界模拟三方面增强专利权利要求生成研究


<details>
  <summary>Details</summary>
Motivation: 现有专利生成研究过度依赖美国专利数据，需要扩展不同司法管辖区和更高质量的数据基准

Method: 构建包含丰富文本和结构化元数据的EPD数据集，在微调实验中对比LLMs性能

Result: EPD微调的LLMs在权利要求质量和跨领域泛化能力上显著超越GPT-4o，但在困难样本上表现骤降

Conclusion: EPD填补了欧洲专利数据空白，揭示了现有模型在复杂场景中的局限性，推动专利生成技术发展

Abstract: Drafting patent claims is time-intensive, costly, and requires professional
skill. Therefore, researchers have investigated large language models (LLMs) to
assist inventors in writing claims. However, existing work has largely relied
on datasets from the United States Patent and Trademark Office (USPTO). To
enlarge research scope regarding various jurisdictions, drafting conventions,
and legal standards, we introduce EPD, a European patent dataset. EPD presents
rich textual data and structured metadata to support multiple patent-related
tasks, including claim generation. This dataset enriches the field in three
critical aspects: (1) Jurisdictional diversity: Patents from different offices
vary in legal and drafting conventions. EPD fills a critical gap by providing a
benchmark for European patents to enable more comprehensive evaluation. (2)
Quality improvement: EPD offers high-quality granted patents with finalized and
legally approved texts, whereas others consist of patent applications that are
unexamined or provisional. Experiments show that LLMs fine-tuned on EPD
significantly outperform those trained on previous datasets and even GPT-4o in
claim quality and cross-domain generalization. (3) Real-world simulation: We
propose a difficult subset of EPD to better reflect real-world challenges of
claim generation. Results reveal that all tested LLMs perform substantially
worse on these challenging samples, which highlights the need for future
research.

</details>


### [103] [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)
*Hanwen Shen,Ting Ying*

Main category: cs.CL

TL;DR: 通过分层大纲优化LLM生成百万字小说的质量，确定最优大纲长度平衡信息保存与人力投入


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成框架主要针对短篇小说（10k-100k字），缺乏百万字级超长篇生成的有效方法，需要量化大纲压缩对生成质量的影响

Method: 基于信息论分析压缩失真度，提出两阶段生成流程（大纲→详细大纲→手稿），通过中文小说实验验证分层结构的有效性

Result: 两阶段方法相比单阶段减少47%语义失真，最优大纲长度可保持91%原始信息量

Conclusion: 分层大纲策略为LLM协作创作百万字小说提供了实证指导，平衡了创作效率与内容质量

Abstract: Writing novels with Large Language Models (LLMs) raises a critical question:
how much human-authored outline is necessary to generate high-quality
million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer
have improved stylistic coherence and logical consistency, they primarily
target shorter novels (10k--100k words), leaving ultra-long generation largely
unexplored. Drawing on insights from recent text compression methods like
LLMZip and LLM2Vec, we conduct an information-theoretic analysis that
quantifies distortion occurring when LLMs compress and reconstruct ultra-long
novels under varying compression-expansion ratios. We introduce a hierarchical
two-stage generation pipeline (outline -> detailed outline -> manuscript) and
find an optimal outline length that balances information preservation with
human effort. Through extensive experimentation with Chinese novels, we
establish that a two-stage hierarchical outline approach significantly reduces
semantic distortion compared to single-stage methods. Our findings provide
empirically-grounded guidance for authors and researchers collaborating with
LLMs to create million-word novels.

</details>


### [104] [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)
*Omar Mahmoud,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

TL;DR: 通过表示导向技术优化单层模型激活，显著提升LLM多语言处理能力，效果媲美翻译基线并超越现有提示优化方法。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型如何处理层次表示中的非英语标记，解决该领域尚未明确的机制问题。

Method: 采用表示导向技术，在单层模型激活中添加学习向量，结合监督微调(SFT)和强化学习(RLHF)优化表示空间。

Result: 单层导向效果显著，超越当前最优提示方法，与翻译基准效果相当；SFT和RLHF有效提升多语言表征能力。

Conclusion: 揭示参数优化对多语言能力的提升机制，为模型优化提供新方向：通过层次表示重塑实现性能突破。

Abstract: In this paper, we investigate how large language models (LLMS) process
non-English tokens within their layer representations, an open question despite
significant advancements in the field. Using representation steering,
specifically by adding a learned vector to a single model layer's activations,
we demonstrate that steering a single model layer can notably enhance
performance. Our analysis shows that this approach achieves results comparable
to translation baselines and surpasses state of the art prompt optimization
methods. Additionally, we highlight how advanced techniques like supervised
fine tuning (\textsc{sft}) and reinforcement learning from human feedback
(\textsc{rlhf}) improve multilingual capabilities by altering representation
spaces. We further illustrate how these methods align with our approach to
reshaping LLMS layer representations.

</details>


### [105] [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)
*Aditeya Baral,Allen George Ajith,Roshan Nayak,Mrityunjay Abhijeet Bhanja*

Main category: cs.CL

TL;DR: 提出CMLFormer模型，通过双解码器架构和多任务预训练策略提升混合语言建模效果


<details>
  <summary>Details</summary>
Motivation: 标准语言模型难以处理混合语言频繁的句内语言切换现象，需要专门的结构设计

Method: 采用共享编码器+同步双解码器架构，在增强的Hinglish语料上通过切换点识别、跨语言对齐和混合复杂度预测多任务预训练

Result: 在HASOC-2021基准测试中F1/精度/准确率提升，注意力分析验证了对语言切换点的捕捉能力

Conclusion: 双架构设计和针对性预训练目标有效提升混合语言建模，为跨语言NLP提供新思路

Abstract: Code-mixed languages, characterized by frequent within-sentence language
transitions, present structural challenges that standard language models fail
to address. In this work, we propose CMLFormer, an enhanced multi-layer
dual-decoder Transformer with a shared encoder and synchronized decoder
cross-attention, designed to model the linguistic and semantic dynamics of
code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with
switching point and translation annotations with multiple new objectives
specifically aimed at capturing switching behavior, cross-lingual structure,
and code-mixing complexity. Our experiments show that CMLFormer improves F1
score, precision, and accuracy over other approaches on the HASOC-2021
benchmark under select pre-training setups. Attention analyses further show
that it can identify and attend to switching points, validating its sensitivity
to code-mixed structure. These results demonstrate the effectiveness of
CMLFormer's architecture and multi-task pre-training strategy for modeling
code-mixed languages.

</details>


### [106] [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)
*Sullam Jeoung,Yueyan Chen,Yi Zhang,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

TL;DR: 提出了PromptPrism框架——通过功能结构、语义成分、句法模式三个层次系统分析提示词，并验证其在提示优化、数据集分析、敏感性实验中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏系统化的提示分析框架，难以深入理解LLM行为及优化提示效果。本文旨在建立结构化分类法填补这一空白。

Method: 基于语言学理论构建三层次分类体系：1)功能结构划分提示功能模块 2)语义成分解析核心要素 3)句法模式分析表达方式。通过自动提示优化、数据集特征提取、语义/分隔符敏感性实验验证框架。

Result: 实验证明：1)提示优化使多任务性能提升2-15% 2)数据集分析揭示结构分布特征 3)语义顺序调整影响达8%，分隔符改变导致4%波动。

Conclusion: PromptPrism为提示工程提供了系统性分析工具，支持从结构优化、数据洞察到鲁棒性测试的全流程改进。

Abstract: Prompts are the interface for eliciting the capabilities of large language
models (LLMs). Understanding their structure and components is critical for
analyzing LLM behavior and optimizing performance. However, the field lacks a
comprehensive framework for systematic prompt analysis and understanding. We
introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt
analysis across three hierarchical levels: functional structure, semantic
component, and syntactic pattern. We show the practical utility of PromptPrism
by applying it to three applications: (1) a taxonomy-guided prompt refinement
approach that automatically improves prompt quality and enhances model
performance across a range of tasks; (2) a multi-dimensional dataset profiling
method that extracts and aggregates structural, semantic, and syntactic
characteristics from prompt datasets, enabling comprehensive analysis of prompt
distributions and patterns; (3) a controlled experimental framework for prompt
sensitivity analysis by quantifying the impact of semantic reordering and
delimiter modifications on LLM performance. Our experimental results validate
the effectiveness of our taxonomy across these applications, demonstrating that
PromptPrism provides a foundation for refining, profiling, and analyzing
prompts.

</details>


### [107] [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)
*Tiankai Yang,Junjun Liu,Wingchun Siu,Jiahang Wang,Zhuangzhuang Qian,Chanjuan Song,Cheng Cheng,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: 提出AD-AGENT框架，利用LLM驱动多智能体自动生成异常检测流水线，解决非专家用户使用复杂AD库的难题。


<details>
  <summary>Details</summary>
Motivation: 异常检测领域存在数据模态多样化和专用库复杂化的双重挑战，非专家用户缺乏库专业知识与编程技能，亟需自动化解决方案。

Method: 构建基于共享工作空间的多智能体协作框架，整合意图解析、数据处理、库选择、文档挖掘、迭代式代码生成等模块，实现PyOD/PyGOD/TSLib等库的流程统一。

Result: 实验证明系统可生成可靠脚本并推荐优质模型，开源支持后续研究与应用。

Conclusion: AD-AGENT通过智能体协作有效降低了异常检测技术门槛，为跨库自动化流程提供了创新范式。

Abstract: Anomaly detection (AD) is essential in areas such as fraud detection, network
monitoring, and scientific research. However, the diversity of data modalities
and the increasing number of specialized AD libraries pose challenges for
non-expert users who lack in-depth library-specific knowledge and advanced
programming skills. To tackle this, we present AD-AGENT, an LLM-driven
multi-agent framework that turns natural-language instructions into fully
executable AD pipelines. AD-AGENT coordinates specialized agents for intent
parsing, data preparation, library and model selection, documentation mining,
and iterative code generation and debugging. Using a shared short-term
workspace and a long-term cache, the agents integrate popular AD libraries like
PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that
AD-AGENT produces reliable scripts and recommends competitive models across
libraries. The system is open-sourced to support further research and practical
applications in AD.

</details>


### [108] [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)
*Shujauddin Syed,Ted Pedersen*

Main category: cs.CL

TL;DR: 研究通过优化TF-IDF检索系统，在10种语言中实现平均0.69的测试集效果，证明传统方法在资源受限场景仍具竞争力


<details>
  <summary>Details</summary>
Motivation: 验证传统检索方法在计算资源受限场景下的有效性，为多语言事实核查任务建立竞争性基线

Method: 采用TF-IDF检索系统，实验不同向量维度和分词策略，最佳方案使用词级分词与15,000维特征空间

Result: 开发集平均success@10达0.78，测试集0.69（十语言），高资源语言表现更优但与最佳神经模型（0.96）仍有差距

Conclusion: 优化后的传统方法在跨语言检索任务中仍保持竞争力，特别适用于计算资源有限的实际应用场景

Abstract: This paper presents the Duluth approach to the SemEval-2025 Task 7 on
Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a
TF-IDF-based retrieval system with experimentation on vector dimensions and
tokenization strategies. Our best-performing configuration used word-level
tokenization with a vocabulary size of 15,000 features, achieving an average
success@10 score of 0.78 on the development set and 0.69 on the test set across
ten languages. Our system showed stronger performance on higher-resource
languages but still lagged significantly behind the top-ranked system, which
achieved 0.96 average success@10. Our findings suggest that though advanced
neural architectures are increasingly dominant in multilingual retrieval tasks,
properly optimized traditional methods like TF-IDF remain competitive
baselines, especially in limited compute resource scenarios.

</details>


### [109] [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)
*João Eduardo Batista,Emil Vatai,Mohamed Wahib*

Main category: cs.CL

TL;DR: 提出句子级预归属分类方法提升RAG系统的可信度与计算效率，并提供标准化HAGRID数据集和端到端归属系统


<details>
  <summary>Details</summary>
Motivation: LLMs在科学领域应用中缺乏可验证的归属机制，错误归属阻碍其在关键场景的应用

Method: 在RAG系统中增加预归属分类器，将句子分为不可归属/单引用归属/多引用归属三类，针对性选择归属策略

Result: 验证分类器有效性，提供清洗后的HAGRID数据集，开发开箱即用的端到端归属系统

Conclusion: 预归属机制通过降低计算复杂度与提升归属精准度，增强LLMs输出的可追溯性与可靠性

Abstract: Large Language Models (LLMs) are increasingly applied in various science
domains, yet their broader adoption remains constrained by a critical
challenge: the lack of trustworthy, verifiable outputs. Current LLMs often
generate answers without reliable source attribution, or worse, with incorrect
attributions, posing a barrier to their use in scientific and high-stakes
settings, where traceability and accountability are non-negotiable. To be
reliable, attribution systems need high accuracy and retrieve data with short
lengths, i.e., attribute to a sentence within a document rather than a whole
document. We propose a sentence-level pre-attribution step for
Retrieve-Augmented Generation (RAG) systems that classify sentences into three
categories: not attributable, attributable to a single quote, and attributable
to multiple quotes. By separating sentences before attribution, a proper
attribution method can be selected for the type of sentence, or the attribution
can be skipped altogether. Our results indicate that classifiers are
well-suited for this task. In this work, we propose a pre-attribution step to
reduce the computational complexity of attribution, provide a clean version of
the HAGRID dataset, and provide an end-to-end attribution system that works out
of the box.

</details>


### [110] [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)
*Ali Naseh,Harsh Chaudhari,Jaechul Roh,Mingshi Wu,Alina Oprea,Amir Houmansadr*

Main category: cs.CL

TL;DR: DeepSeek R1模型虽在推理任务表现优异，但被发现存在对中国政治敏感话题的特殊审查机制


<details>
  <summary>Details</summary>
Motivation: 研究R1模型在政治敏感话题上表现出的审查行为，揭示其训练/对齐过程中可能的特殊设计

Method: 通过构建大规模敏感提示数据集，分析审查模式的触发机制、跨语言表现及蒸馏模型的迁移性

Result: 发现R1存在系统性审查机制，其触发与话题敏感性、语言表达方式相关，并可通过特定技术绕过

Conclusion: 模型审查机制缺乏透明度可能引发技术伦理问题，需加强AI治理与算法透明度建设

Abstract: DeepSeek recently released R1, a high-performing large language model (LLM)
optimized for reasoning tasks. Despite its efficient training pipeline, R1
achieves competitive performance, even surpassing leading reasoning models like
OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1
refuses to answer certain prompts related to politically sensitive topics in
China. While existing LLMs often implement safeguards to avoid generating
harmful or offensive outputs, R1 represents a notable shift - exhibiting
censorship-like behavior on politically charged queries. In this paper, we
investigate this phenomenon by first introducing a large-scale set of heavily
curated prompts that get censored by R1, covering a range of politically
sensitive topics, but are not censored by other models. We then conduct a
comprehensive analysis of R1's censorship patterns, examining their
consistency, triggers, and variations across topics, prompt phrasing, and
context. Beyond English-language queries, we explore censorship behavior in
other languages. We also investigate the transferability of censorship to
models distilled from the R1 language model. Finally, we propose techniques for
bypassing or removing this censorship. Our findings reveal possible additional
censorship integration likely shaped by design choices during training or
alignment, raising concerns about transparency, bias, and governance in
language model deployment.

</details>


### [111] [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)
*Jiakuan Xie,Pengfei Cao,Yubo Chen,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 研究发现知识编辑存在'表面编辑'现象，现有算法虽指标优秀但无法彻底消除原始知识，识别出残留流和注意力机制两个关键成因


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑算法在传统评估中表现优异，但模型仍会输出原始知识，需揭示这一现象的内在机制并提出解决方案

Method: 通过系统评估发现：1.早期层主语位置的残留流 2.后期层特定注意力模块（特别是注意力头及其输出矩阵的左奇异向量）与表面编辑存在因果关系

Result: 验证了注意力头左奇异向量编码原始知识的机制，在表面遗忘任务中观察到相同模式，证实了方法论的有效性和结论的普适性

Conclusion: 表面编辑现象对现有算法构成重大挑战，注意力机制分析为理解模型知识存储提供了新视角，研究框架可拓展至其他模型编辑场景

Abstract: Knowledge editing, which aims to update the knowledge encoded in language
models, can be deceptive. Despite the fact that many existing knowledge editing
algorithms achieve near-perfect performance on conventional metrics, the models
edited by them are still prone to generating original knowledge. This paper
introduces the concept of "superficial editing" to describe this phenomenon.
Our comprehensive evaluation reveals that this issue presents a significant
challenge to existing algorithms. Through systematic investigation, we identify
and validate two key factors contributing to this issue: (1) the residual
stream at the last subject position in earlier layers and (2) specific
attention modules in later layers. Notably, certain attention heads in later
layers, along with specific left singular vectors in their output matrices,
encapsulate the original knowledge and exhibit a causal relationship with
superficial editing. Furthermore, we extend our analysis to the task of
superficial unlearning, where we observe consistent patterns in the behavior of
specific attention heads and their corresponding left singular vectors, thereby
demonstrating the robustness and broader applicability of our methodology and
conclusions. Our code is available here.

</details>


### [112] [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)
*Yuxin Lin,Yinglin Zheng,Ming Zeng,Wangzheng Shi*

Main category: cs.CL

TL;DR: 提出多模态对话数据集MM-F2F及端到端预测框架，实现话轮转换和反馈预测性能显著提升


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法满足人机对话中多模态信号（语言/声学/视觉）对话轮转换和反馈动作的预测需求

Method: 1. 开发自动数据采集管道构建含210小时视频的MM-F2F数据集 2. 设计支持多模态组合输入的端到端预测框架 3. 通过模态间关联性建模提升预测精度

Result: 话轮转换F1值提升10%，反馈预测F1值提升33%，达到当前最优水平

Conclusion: 公开的数据集与灵活的多模态框架为对话系统研究提供了有效基础设施，实验结果验证了多模态信号联合建模的重要性

Abstract: This paper addresses the gap in predicting turn-taking and backchannel
actions in human-machine conversations using multi-modal signals (linguistic,
acoustic, and visual). To overcome the limitation of existing datasets, we
propose an automatic data collection pipeline that allows us to collect and
annotate over 210 hours of human conversation videos. From this, we construct a
Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over
1.5M words and corresponding turn-taking and backchannel annotations from
approximately 20M frames. Additionally, we present an end-to-end framework that
predicts the probability of turn-taking and backchannel actions from
multi-modal signals. The proposed model emphasizes the interrelation between
modalities and supports any combination of text, audio, and video inputs,
making it adaptable to a variety of realistic scenarios. Our experiments show
that our approach achieves state-of-the-art performance on turn-taking and
backchannel prediction tasks, achieving a 10\% increase in F1-score on
turn-taking and a 33\% increase on backchannel prediction. Our dataset and code
are publicly available online to ease of subsequent research.

</details>


### [113] [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)
*Xukai Liu,Ye Liu,Shiwen Wu,Yanghai Zhang,Yihao Yuan,Kai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: Know3-RAG框架利用知识图谱结构化知识改进RAG检索、生成、过滤三阶段，显著减少幻觉问题并提升答案可靠性


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成(RAG)系统存在自适应控制不可靠(因外部知识监督有限)和引用不准确导致的幻觉问题

Method: 提出知识感知自适应检索模块（KG嵌入评估置信度）、知识增强参考生成策略（KG实体扩展查询）、知识驱动参考过滤机制（语义对齐验证）

Result: 在多个开放域QA基准测试中持续超越基线模型，幻觉现象显著减少且答案可靠性提升

Conclusion: 通过结构化知识对RAG全流程的指导，有效解决现有系统缺陷，实现更可靠的知识增强生成

Abstract: Recent advances in large language models (LLMs) have led to impressive
progress in natural language generation, yet their tendency to produce
hallucinated or unsubstantiated content remains a critical concern. To improve
factual reliability, Retrieval-Augmented Generation (RAG) integrates external
knowledge during inference. However, existing RAG systems face two major
limitations: (1) unreliable adaptive control due to limited external knowledge
supervision, and (2) hallucinations caused by inaccurate or irrelevant
references. To address these issues, we propose Know3-RAG, a knowledge-aware
RAG framework that leverages structured knowledge from knowledge graphs (KGs)
to guide three core stages of the RAG process, including retrieval, generation,
and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval
module that employs KG embedding to assess the confidence of the generated
answer and determine retrieval necessity, a knowledge-enhanced reference
generation strategy that enriches queries with KG-derived entities to improve
generated reference relevance, and a knowledge-driven reference filtering
mechanism that ensures semantic alignment and factual accuracy of references.
Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG
consistently outperforms strong baselines, significantly reducing
hallucinations and enhancing answer reliability.

</details>


### [114] [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)
*Taiqiang Wu,Runming Yang,Jiayi Li,Pengfei Hu,Ngai Wong,Yujiu Yang*

Main category: cs.CL

TL;DR: 提出Shadow-FT框架，通过微调BASE模型并移植权重到INSTRUCT模型，显著提升LLM在多任务中的性能表现


<details>
  <summary>Details</summary>
Motivation: 直接微调INSTRUCT模型效果有限且可能导致性能下降，而对应的BASE模型权重变化微小（如Llama 3.1 8B平均小于2%），存在潜在优化空间

Method: 先微调BASE模型，然后将学习到的权重更新直接移植到INSTRUCT模型，无需额外参数且易于实现

Result: 在19个涵盖编程、推理和数学的基准测试中，Shadow-FT在Qwen 3和Llama 3系列模型上持续优于传统全参数和参数高效微调方法，并可扩展应用于多模态大模型和结合DPO

Conclusion: Shadow-FT通过有效利用BASE模型的优化潜力，以零额外成本显著提升INSTRUCT模型的微调效果，具有广泛适用性和工程实践价值

Abstract: Large language models (LLMs) consistently benefit from further fine-tuning on
various tasks. However, we observe that directly tuning the INSTRUCT (i.e.,
instruction tuned) models often leads to marginal improvements and even
performance degeneration. Notably, paired BASE models, the foundation for these
INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on
average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to
tune the INSTRUCT models by leveraging the corresponding BASE models. The key
insight is to fine-tune the BASE model, and then directly graft the learned
weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no
additional parameters, is easy to implement, and significantly improves
performance. We conduct extensive experiments on tuning mainstream LLMs, such
as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering
coding, reasoning, and mathematical tasks. Experimental results demonstrate
that Shadow-FT consistently outperforms conventional full-parameter and
parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT
can be applied to multimodal large language models (MLLMs) and combined with
direct preference optimization (DPO). Codes and weights are available at
\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.

</details>


### [115] [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)
*Haoyuan Wu,Xueyi Chen,Rui Ming,Jilong Gao,Shoubo Hu,Zhuolun He,Bei Yu*

Main category: cs.CL

TL;DR: 通过强化学习训练LLMs从顺序链式推理转向并行树状推理，开发ToTRL框架提升复杂任务推理效率


<details>
  <summary>Details</summary>
Motivation: 长链式推理存在冗长自省导致的低效输出问题，且传统试错法缺乏系统性。树状推理结构通过并行生成和主动剪枝路径可提升性能并降低计算成本

Method: 提出树状思维强化学习框架(ToTRL)，结合基于规则的奖励机制，通过解谜游戏训练LLMs构建相互依赖的思维树结构

Result: ToTQwen3-8B模型在复杂推理任务中实现性能显著提升，推理效率提高30%

Conclusion: ToTRL成功将LLMs的线性推理升级为结构化树状推理，为处理复杂约束任务提供新范式，证明主动路径评估机制的有效性

Abstract: Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and pruning of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.

</details>


### [116] [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)
*Jingyang Peng,Wenyuan Shen,Jiarui Rao,Jionghao Lin*

Main category: cs.CL

TL;DR: 本研究提出结合上下文嵌入关联测试与检索增强生成框架的自动化偏见评估方法，有效检测AI教育内容中的偏见问题（r=0.993），提升审核的客观性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 针对生成式AI教育内容中存在的隐性偏见（性别/种族/国家刻板印象），需建立系统化评估方法以解决人工审核的主观性和效率瓶颈。

Method: 整合上下文嵌入关联测试与提示工程词汇提取技术，构建基于检索增强生成(RAG)框架的自动化评估流程。

Result: 自动化与人工筛选词汇集高度一致（Pearson r=0.993），验证了方法的可靠性和一致性优势。

Conclusion: 该方法显著降低人工主观性，为大规模审核AI教育内容提供标准化解决方案，促进教育公平的技术实现。

Abstract: Recent advances in Generative Artificial Intelligence (GenAI) have
transformed educational content creation, particularly in developing tutor
training materials. However, biases embedded in AI-generated content--such as
gender, racial, or national stereotypes--raise significant ethical and
educational concerns. Despite the growing use of GenAI, systematic methods for
detecting and evaluating such biases in educational materials remain limited.
This study proposes an automated bias assessment approach that integrates the
Contextualized Embedding Association Test with a prompt-engineered word
extraction method within a Retrieval-Augmented Generation framework. We applied
this method to AI-generated texts used in tutor training lessons. Results show
a high alignment between the automated and manually curated word sets, with a
Pearson correlation coefficient of r = 0.993, indicating reliable and
consistent bias assessment. Our method reduces human subjectivity and enhances
fairness, scalability, and reproducibility in auditing GenAI-produced
educational content.

</details>


### [117] [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)
*Haoyuan Wu,Rui Ming,Jilong Gao,Hangyu Zhao,Xueyi Chen,Yikai Yang,Haisheng Zheng,Zhuolun He,Bei Yu*

Main category: cs.CL

TL;DR: 提出OORL强化学习框架，通过代码翻译任务和GEPO优化方法提升LLMs跨语言代码生成能力。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs在不同编程语言间代码生成能力差异显著的问题，试图通过代码翻译任务实现跨语言编码能力迁移。

Method: 结合on-policy强化学习（基于单元测试的规则奖励）与off-policy的GEPO方法（通过中间表示组优化功能理解），构建OORL训练框架。

Result: 实验表明该方法显著提升了LLMs在多种编程语言代码基准测试中的性能表现。

Conclusion: 代码翻译任务结合混合强化学习策略有效增强了LLMs对代码功能的理解和跨语言关系建模能力。

Abstract: Large language models (LLMs) achieve remarkable performance in code
generation tasks. However, a significant performance disparity persists between
popular programming languages (e.g., Python, C++) and others. To address this
capability gap, we leverage the code translation task to train LLMs, thereby
facilitating the transfer of coding proficiency across diverse programming
languages. Moreover, we introduce OORL for training, a novel reinforcement
learning (RL) framework that integrates on-policy and off-policy strategies.
Within OORL, on-policy RL is applied during code translation, guided by a
rule-based reward signal derived from unit tests. Complementing this
coarse-grained rule-based reward, we propose Group Equivalent Preference
Optimization (GEPO), a novel preference optimization method. Specifically, GEPO
trains the LLM using intermediate representations (IRs) groups. LLMs can be
guided to discern IRs equivalent to the source code from inequivalent ones,
while also utilizing signals about the mutual equivalence between IRs within
the group. This process allows LLMs to capture nuanced aspects of code
functionality. By employing OORL for training with code translation tasks, LLMs
improve their recognition of code functionality and their understanding of the
relationships between code implemented in different languages. Extensive
experiments demonstrate that our OORL for LLMs training with code translation
tasks achieves significant performance improvements on code benchmarks across
multiple programming languages.

</details>


### [118] [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)
*Han Meng,Yancan Chen,Yunan Li,Yitian Yang,Jungup Lee,Renwen Zhang,Yi-Chieh Lee*

Main category: cs.CL

TL;DR: 提出了首个专家标注的心理健康污名检测理论驱动语料库，包含4,141条真实人类-聊天机器人对话片段，基准测试显示当前最先进模型仍面临检测挑战


<details>
  <summary>Details</summary>
Motivation: 现有心理健康污名检测主要依赖社交媒体或合成数据，缺乏理论框架支撑，限制了模型的精细分类能力

Method: 构建包含684名具有社会文化背景记录的参与者的人类-聊天机器人对话语料库（4,141个片段），采用专家标注和理论驱动的方法

Result: 通过实验揭示了当前最先进神经模型在污名检测任务中的性能瓶颈和技术挑战

Conclusion: 该数据集为计算检测、中和技术对抗心理健康污名提供了新的研究基础

Abstract: Mental-health stigma remains a pervasive social problem that hampers
treatment-seeking and recovery. Existing resources for training neural models
to finely classify such stigma are limited, relying primarily on social-media
or synthetic data without theoretical underpinnings. To remedy this gap, we
present an expert-annotated, theory-informed corpus of human-chatbot
interviews, comprising 4,141 snippets from 684 participants with documented
socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural
models and empirically unpack the challenges of stigma detection. This dataset
can facilitate research on computationally detecting, neutralizing, and
counteracting mental-health stigma.

</details>


### [119] [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)
*Yaxun Dai,Wenxuan Xie,Xialie Zhuang,Tianyu Yang,Yiying Yang,Haiqin Yang,Yuhang Zhao,Pingfu Chao,Wenhao Jiang*

Main category: cs.CL

TL;DR: ReEx-SQL框架通过执行感知强化学习，在Text-to-SQL解码过程中动态整合数据库反馈，显著提升查询准确性和推理效率


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将执行反馈作为事后校正信号，无法在生成过程中及时纠正错误，导致准确性和鲁棒性受限

Method: 提出执行感知推理范式：1）在推理路径中嵌入中间SQL执行 2）带标记的结构化提示设计 3）复合奖励函数（含探索奖励）4）基于树的动态解码策略

Result: Spider(88.8%)和BIRD(64.9%)准确率分别提升2.7%和2.6%，推理时间减少51.9%；Spider-Realistic达85.2%领先性能

Conclusion: 通过执行反馈的动态整合与探索性树状解码机制，实现了SQL生成性能与效率的协同提升，为交互式推理范式提供了新思路

Abstract: In Text-to-SQL, execution feedback is essential for guiding large language
models (LLMs) to reason accurately and generate reliable SQL queries. However,
existing methods treat execution feedback solely as a post-hoc signal for
correction or selection, failing to integrate it into the generation process.
This limitation hinders their ability to address reasoning errors as they
occur, ultimately reducing query accuracy and robustness. To address this
issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement
Learning), a framework for Text-to-SQL that enables models to interact with the
database during decoding and dynamically adjust their reasoning based on
execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm
that interleaves intermediate SQL execution into reasoning paths, facilitating
context-sensitive revisions. It achieves this through structured prompts with
markup tags and a stepwise rollout strategy that integrates execution feedback
into each stage of generation. To supervise policy learning, we develop a
composite reward function that includes an exploration reward, explicitly
encouraging effective database interaction. Additionally, ReEx-SQL adopts a
tree-based decoding strategy to support exploratory reasoning, enabling dynamic
expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on
Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning
baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving
85.2% on Spider-Realistic with leading performance. In addition, its
tree-structured decoding improves efficiency and performance over linear
decoding, reducing inference time by 51.9% on the BIRD development set.

</details>


### [120] [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)
*Jitai Hao,Qiang Huang,Hao Liu,Xinyan Xiao,Zhaochun Ren,Jun Yu*

Main category: cs.CL

TL;DR: 提出低秩克隆(LRC)预训练方法，通过联合软剪枝和激活克隆技术，在仅使用20B tokens的情况下实现1000倍训练效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在硬剪枝信息丢失、表示对齐效率低、FFN激活信号利用不足三大缺陷，需要更高效的蒸馏方案。

Method: 构建低秩投影矩阵实现权重压缩(软剪枝)和激活对齐(FFN信号克隆)，统一设计避免显式对齐模块。

Result: 在Llama-3.2-3B等教师模型上超越SOTA模型，训练效率提升1000倍且仅需20B tokens。

Conclusion: LRC通过参数共享架构显著降低训练成本，为小型语言模型训练提供新范式。

Abstract: Training high-performing Small Language Models (SLMs) remains costly, even
with knowledge distillation and pruning from larger teacher models. Existing
work often faces three key challenges: (1) information loss from hard pruning,
(2) inefficient alignment of representations, and (3) underutilization of
informative activations, particularly from Feed-Forward Networks (FFNs). To
address these challenges, we introduce Low-Rank Clone (LRC), an efficient
pre-training method that constructs SLMs aspiring to behavioral equivalence
with strong teacher models. LRC trains a set of low-rank projection matrices
that jointly enable soft pruning by compressing teacher weights, and activation
clone by aligning student activations, including FFN signals, with those of the
teacher. This unified design maximizes knowledge transfer while removing the
need for explicit alignment modules. Extensive experiments with open-source
teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC
matches or surpasses state-of-the-art models trained on trillions of
tokens--while using only 20B tokens, achieving over 1,000x training efficiency.
Our codes and model checkpoints are available at
https://github.com/CURRENTF/LowRankClone and
https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.

</details>


### [121] [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)
*Wenhao Zhu,Yuhang Xie,Guojie Song,Xin Zhang*

Main category: cs.CL

TL;DR: 提出EAVIT框架，结合本地可微调模型和在线LLMs优势，通过价值检测器生成初始估计并构建简洁提示，减少输入token至1/6的同时提升价值观识别准确率。


<details>
  <summary>Details</summary>
Motivation: 在线LLMs处理长文本时存在效率低、成本高的问题，传统NLP模型效果不足，需开发高效准确的价值观识别方法。

Method: 使用本地小型语言模型（价值检测器）生成初始估计，构建精简提示供在线LLMs处理；引入解释性训练、数据生成技术和提示长度优化策略。

Result: 输入token数量减少至直接查询的1/6，性能持续超越传统NLP方法及其他LLM策略。

Conclusion: EAVIT框架在效率与准确性间实现平衡，为文本中人类价值观识别提供高效解决方案。

Abstract: The rapid evolution of large language models (LLMs) has revolutionized
various fields, including the identification and discovery of human values
within text data. While traditional NLP models, such as BERT, have been
employed for this task, their ability to represent textual data is
significantly outperformed by emerging LLMs like GPTs. However, the performance
of online LLMs often degrades when handling long contexts required for value
identification, which also incurs substantial computational costs. To address
these challenges, we propose EAVIT, an efficient and accurate framework for
human value identification that combines the strengths of both locally
fine-tunable and online black-box LLMs. Our framework employs a value detector
- a small, local language model - to generate initial value estimations. These
estimations are then used to construct concise input prompts for online LLMs,
enabling accurate final value identification. To train the value detector, we
introduce explanation-based training and data generation techniques
specifically tailored for value identification, alongside sampling strategies
to optimize the brevity of LLM input prompts. Our approach effectively reduces
the number of input tokens by up to 1/6 compared to directly querying online
LLMs, while consistently outperforming traditional NLP methods and other
LLM-based strategies.

</details>


### [122] [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)
*Yanbin Yin,Kun Zhou,Zhen Wang,Xiangdong Zhang,Yifei Shao,Shibo Hao,Yi Gu,Jieyuan Liu,Somanshu Singla,Tianyang Liu,Eric P. Xing,Zhengzhong Liu,Haojian Jin,Zhiting Hu*

Main category: cs.CL

TL;DR: 提出了去中心化评估框架Decentralized Arena（dearena），通过LLM群体智能实现高效互评，显著降低评估成本。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在封闭式问题饱和、人工评估成本高、单一模型评判偏见等问题，需要更可靠且可扩展的解决方案。

Method: 结合民主化两两评估机制（降低偏见）+ 包含次二次复杂度的粗到细排名算法（快速插入新模型）+ 自动问题选择策略（构建新评估维度）。

Result: 在66个LLM上的实验显示，与人类判断相关性高达97%，同时成本显著降低。

Conclusion: dearena通过群体智能实现高效去中心化评估，为LLM持续进化提供可持续的评估基础设施。

Abstract: The recent explosion of large language models (LLMs), each with its own
general or specialized strengths, makes scalable, reliable benchmarking more
urgent than ever. Standard practices nowadays face fundamental trade-offs:
closed-ended question-based benchmarks (eg MMLU) struggle with saturation as
newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely
on costly and slow human judges. Recently, automated methods (eg
LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one
or a few "authority" models. To tackle these issues, we propose Decentralized
Arena (dearena), a fully automated framework leveraging collective intelligence
from all LLMs to evaluate each other. It mitigates single-model judge bias by
democratic, pairwise evaluation, and remains efficient at scale through two key
components: (1) a coarse-to-fine ranking algorithm for fast incremental
insertion of new models with sub-quadratic complexity, and (2) an automatic
question selection strategy for the construction of new evaluation dimensions.
Across extensive experiments across 66 LLMs, dearena attains up to 97%
correlation with human judgements, while significantly reducing the cost. Our
code and data will be publicly released on
https://github.com/maitrix-org/de-arena.

</details>


### [123] [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)
*Xilong Cheng,Yunxiao Qin,Yuting Tan,Zhengnan Li,Ye Wang,Hongjiang Xiao,Yuan Zhang*

Main category: cs.CL

TL;DR: 提出PsyMem框架，通过心理属性建模和显式记忆控制增强LLM角色扮演的可靠性与一致性


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演方法存在角色建模维度不足（仅依赖文本描述）和记忆一致性差（缺乏显式对齐）的问题，影响社交模拟等应用的可信度

Method: 1. 补充26个心理指标增强角色建模维度 2. 设计记忆对齐训练机制实现显式记忆控制 3. 基于5,414个角色和38,962段小说对话训练Qwen2.5-7B-Instruct模型

Result: PsyMem-Qwen在人类相似度（83.2%）和角色保真度（78.5%）指标上超越基线模型，达到SOTA性能

Conclusion: PsyMem框架成功将心理学维度与显式记忆控制结合，为可信角色扮演系统提供了新范式，支撑可靠的社会模拟应用

Abstract: Existing LLM-based role-playing methods often rely on superficial textual
descriptions or simplistic metrics, inadequately modeling both intrinsic and
extrinsic character dimensions. Additionally, they typically simulate character
memory with implicit model knowledge or basic retrieval augment generation
without explicit memory alignment, compromising memory consistency. The two
issues weaken reliability of role-playing LLMs in several applications, such as
trustworthy social simulation. To address these limitations, we propose PsyMem,
a novel framework integrating fine-grained psychological attributes and
explicit memory control for role-playing. PsyMem supplements textual
descriptions with 26 psychological indicators to detailed model character.
Additionally, PsyMem implements memory alignment training, explicitly trains
the model to align character's response with memory, thereby enabling dynamic
memory-controlled responding during inference. By training Qwen2.5-7B-Instruct
on our specially designed dataset (including 5,414 characters and 38,962
dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,
outperforms baseline models in role-playing, achieving the best performance in
human-likeness and character fidelity.

</details>


### [124] [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)
*Han Sun,Zhen Sun,Zongmin Zhang,Linzhao Jia,Wei Shao,Min Zhang*

Main category: cs.CL

TL;DR: 提出SynDec方法，通过自动合成提示词+解码增强机制，有效解决LLM风格迁移中手动提示依赖和固有风格偏见问题，实验显示在多个基准超越SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在任意风格迁移中的两个核心问题：1) 对人工构建提示词的强依赖 2) 模型固有的刻板风格偏见。旨在提升风格迁移的自动化程度和效果鲁棒性。

Method: Synthesize-then-Decode框架：1) 提示词合成阶段（代表性样本选择→四维风格分析→候选提示重排序） 2) 解码阶段通过对比有/无提示的输出概率差异，以及正负样本对比来放大提示效应。

Result: 在6个基准中的5个超越现有方法（如现代英语→伊丽莎白时期英语转换准确率提升9%），消融实验验证各模块有效性。

Conclusion: SynDec首次实现LLM风格迁移的全自动提示工程，通过提示合成与解码增强的协同作用，突破现有方法依赖人工提示的局限性。

Abstract: Large Language Models (LLMs) are emerging as dominant forces for textual
style transfer. However, for arbitrary style transfer, LLMs face two key
challenges: (1) considerable reliance on manually-constructed prompts and (2)
rigid stylistic biases inherent in LLMs. In this paper, we propose a novel
Synthesize-then-Decode (SynDec) approach, which automatically synthesizes
high-quality prompts and amplifies their roles during decoding process.
Specifically, our approach synthesizes prompts by selecting representative
few-shot samples, conducting a four-dimensional style analysis, and reranking
the candidates. At LLM decoding stage, the TST effect is amplified by
maximizing the contrast in output probabilities between scenarios with and
without the synthesized prompt, as well as between prompts and negative
samples. We conduct extensive experiments and the results show that SynDec
outperforms existing state-of-the-art LLM-based methods on five out of six
benchmarks (e.g., achieving up to a 9\% increase in accuracy for
modern-to-Elizabethan English transfer). Detailed ablation studies further
validate the effectiveness of SynDec.

</details>


### [125] [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)
*Zifeng Cheng,Zhonghui Wang,Yuchen Fu,Zhiwei Jiang,Yafeng Yin,Cong Wang,Qing Gu*

Main category: cs.CL

TL;DR: 提出一种对比提示（CP）方法，通过引入辅助提示优化大语言模型生成的句子嵌入质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在最后一个token编码时仍包含过多非必要信息（如停用词），限制了编码能力。

Method: 采用对比学习机制，通过辅助提示的对比迫使主提示聚焦核心语义，无需训练即插即用。

Result: 在STS任务和分类任务中显著提升现有方法性能（不同LLM上均验证有效），代码即将开源。

Conclusion: 证实了对比机制在语义编码中的有效性，为提示工程提供了新的优化方向。

Abstract: Extracting sentence embeddings from large language models (LLMs) is a
practical direction, as it requires neither additional data nor fine-tuning.
Previous studies usually focus on prompt engineering to guide LLMs to encode
the core semantic information of the sentence into the embedding of the last
token. However, the last token in these methods still encodes an excess of
non-essential information, such as stop words, limiting its encoding capacity.
To this end, we propose a Contrastive Prompting (CP) method that introduces an
extra auxiliary prompt to elicit better sentence embedding. By contrasting with
the auxiliary prompt, CP can steer existing prompts to encode the core
semantics of the sentence, rather than non-essential information. CP is a
plug-and-play inference-time intervention method that can be combined with
various prompt-based methods. Extensive experiments on Semantic Textual
Similarity (STS) tasks and downstream classification tasks demonstrate that our
method can improve the performance of existing prompt-based methods across
different LLMs. Our code will be released at https://github.com/zifengcheng/CP.

</details>


### [126] [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)
*Hengxing Cai,Jinhan Dong,Jingjun Tan,Jingcheng Deng,Sihang Li,Zhifeng Gao,Haidong Wang,Zicheng Su,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: 提出基于视觉语言模型的FlightGPT框架，通过两阶段训练和思维链推理机制显著提升无人机导航的准确性、泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有无人机视觉语言导航方法存在多模态融合不足、环境适应差、决策黑箱三大痛点，制约其在城市级复杂场景的应用。

Method: 1. 监督微调(SFT)构建结构化推理能力 2. 基于目标准确度+推理质量+格式合规的复合奖励机制(GRPO算法) 3. 链式思维(CoT)可解释决策框架

Result: CityNav数据集上达成SOTA，未知环境成功率提升9.22%，推理时间缩短32%同时保持90.5%的格式合规率。

Conclusion: FlightGPT通过算法-架构协同创新，为城市级无人机自主导航提供了可靠的多模态解决方案，具有重要工程应用价值。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital
for applications such as disaster response, logistics delivery, and urban
inspection. However, existing methods often struggle with insufficient
multimodal fusion, weak generalization, and poor interpretability. To address
these challenges, we propose FlightGPT, a novel UAV VLN framework built upon
Vision-Language Models (VLMs) with powerful multimodal perception capabilities.
We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)
using high-quality demonstrations to improve initialization and structured
reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by
a composite reward that considers goal accuracy, reasoning quality, and format
compliance, to enhance generalization and adaptability. Furthermore, FlightGPT
introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve
decision interpretability. Extensive experiments on the city-scale dataset
CityNav demonstrate that FlightGPT achieves state-of-the-art performance across
all scenarios, with a 9.22\% higher success rate than the strongest baseline in
unseen environments. Our implementation is publicly available.

</details>


### [127] [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)
*Christian Braun,Alexander Lilienbeck,Daniel Mentjukov*

Main category: cs.CL

TL;DR: 研究发现GPT-4o对法律文本结构变化更具鲁棒性但整体性能较弱，而GPT-4.1性能受输入结构显著影响。优化文本结构（Markdown格式）结合提示工程可使GPT-4.1准确率提升约33个百分点至79%


<details>
  <summary>Details</summary>
Motivation: 探索法律合同固有结构对LLM处理效果的影响，特别是在高风险法律应用中结构优化和提示工程对模型性能的提升潜力

Method: 使用CUAD数据集法律问答任务，对比五种输入格式（原生结构文本/清理文本/OCR文本/Vision文本/Vision MD），并通过调整系统提示评估提示工程效果

Result: GPT-4o在不同结构下表现稳定但准确率较低；GPT-4.1在结构化输入下准确率提升20%，结合提示工程再提升10-13%，Markdown格式最终达到79%准确率

Conclusion: 即使新模型具备更强适应性，精细的输入结构优化和提示设计仍是提升LLM法律应用性能的关键，尤其在涉及重大法律后果的场景中

Abstract: Legal contracts possess an inherent, semantically vital structure (e.g.,
sections, clauses) that is crucial for human comprehension but whose impact on
LLM processing remains under-explored. This paper investigates the effects of
explicit input text structure and prompt engineering on the performance of
GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the
CUAD. We compare model exact-match accuracy across various input formats:
well-structured plain-text (human-generated from CUAD), plain-text cleaned of
line breaks, extracted plain-text from Azure OCR, plain-text extracted by
GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o
Vision. To give an indication of the impact of possible prompt engineering, we
assess the impact of shifting task instructions to the system prompt and
explicitly informing the model about the structured nature of the input. Our
findings reveal that GPT-4o demonstrates considerable robustness to variations
in input structure, but lacks in overall performance. Conversely, GPT-4.1's
performance is markedly sensitive; poorly structured inputs yield suboptimal
results (but identical with GPT-4o), while well-structured formats (original
CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by
~20 percentage points. Optimizing the system prompt to include task details and
an advisory about structured input further elevates GPT-4.1's accuracy by an
additional ~10-13 percentage points, with Markdown ultimately achieving the
highest performance under these conditions (79 percentage points overall
exact-match accuracy). This research empirically demonstrates that while newer
models exhibit greater resilience, careful input structuring and strategic
prompt design remain critical for optimizing the performance of LLMs, and can
significantly affect outcomes in high-stakes legal applications.

</details>


### [128] [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)
*Lucas Georges Gabriel Charpentier,Pierre Lison*

Main category: cs.CL

TL;DR: 提出基于RAG的逆向工程方法，通过检索-填充两步流程成功恢复80%被掩盖的个人隐私信息


<details>
  <summary>Details</summary>
Motivation: 验证文档去标识化方法的脆弱性，证明现有隐私保护措施可能被背景知识辅助的AI模型破解

Method: 结合检索器(筛选背景知识)与填充模型(推测掩码内容)的迭代框架，在维基百科、司法文书和医疗记录三类数据测试

Result: 最高达80%的文本跨度可被准确恢复，且背景知识库越完备则重新识别准确率越高

Conclusion: 当前去标识化技术存在重大安全漏洞，隐私保护方案需考虑对抗性背景知识攻击

Abstract: Documents revealing sensitive information about individuals must typically be
de-identified. This de-identification is often done by masking all mentions of
personally identifiable information (PII), thereby making it more difficult to
uncover the identity of the person(s) in question. To investigate the
robustness of de-identification methods, we present a novel, RAG-inspired
approach that attempts the reverse process of re-identification based on a
database of documents representing background knowledge. Given a text in which
personal identifiers have been masked, the re-identification proceeds in two
steps. A retriever first selects from the background knowledge passages deemed
relevant for the re-identification. Those passages are then provided to an
infilling model which seeks to infer the original content of each text span.
This process is repeated until all masked spans are replaced. We evaluate the
re-identification on three datasets (Wikipedia biographies, court rulings and
clinical notes). Results show that (1) as many as 80% of de-identified text
spans can be successfully recovered and (2) the re-identification accuracy
increases along with the level of background knowledge.

</details>


### [129] [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)
*Yu Fan,Jingwei Ni,Jakob Merane,Etienne Salimbeni,Yang Tian,Yoan Hermstrüwer,Yinya Huang,Mubashara Akhtar,Florian Geering,Oliver Dreyer,Daniel Brunner,Markus Leippold,Mrinmaya Sachan,Alexander Stremitzer,Christoph Engel,Elliott Ash,Joel Niklaus*

Main category: cs.CL

TL;DR: LEXam基准测试基于340个法律考试构建，揭示当前大模型在结构化法律推理任务中的显著不足，并提出基于模型自评估的可扩展评测框架


<details>
  <summary>Details</summary>
Motivation: 针对现有大模型在长格式法律推理任务中评估体系不完善的问题，旨在构建专业领域的能力评估基准并揭示模型局限性

Method: 从116门法律课程中收集4,886道考试题目构建LEXam数据集，采用LLM-as-a-Judge范式配合专家验证实现自动化评估

Result: 模型在需要多步骤法律推理的开放题上准确率显著低于选择题（尤其涉及规则应用时），数据集有效区分不同规模模型能力差异

Conclusion: LEXam为法律推理质量评估提供新范式，强调结构化推理能力测试的重要性，推动领域专用模型的发展

Abstract: Long-form legal reasoning remains a key challenge for large language models
(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a
novel benchmark derived from 340 law exams spanning 116 law school courses
across a range of subjects and degree levels. The dataset comprises 4,886 law
exam questions in English and German, including 2,841 long-form, open-ended
questions and 2,045 multiple-choice questions. Besides reference answers, the
open questions are also accompanied by explicit guidance outlining the expected
legal reasoning approach such as issue spotting, rule recall, or rule
application. Our evaluation on both open-ended and multiple-choice questions
present significant challenges for current LLMs; in particular, they notably
struggle with open questions that require structured, multi-step legal
reasoning. Moreover, our results underscore the effectiveness of the dataset in
differentiating between models with varying capabilities. Adopting an
LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate
how model-generated reasoning steps can be evaluated consistently and
accurately. Our evaluation setup provides a scalable method to assess legal
reasoning quality beyond simple accuracy metrics. Project page:
https://lexam-benchmark.github.io/

</details>


### [130] [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)
*Jialun Zhong,Yanzeng Li,Sen Hu,Yang Zhang,Teng Xu,Lei Zou*

Main category: cs.CL

TL;DR: 提出了GAP框架，通过构建患者中心图结合外部知识图谱，提升医疗对话系统中用药推荐的准确性和安全性


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多轮医疗对话中容易忽略细粒度医疗信息及跨轮次关联，且缺乏领域知识导致非事实性回应风险

Method: 1. 从对话中提取医疗概念构建患者中心图 2. 结合外部医学知识图谱生成查询提示 3. 多源信息检索机制

Result: 在用药推荐数据集上表现优于基线模型，动态诊断场景中展现出应用潜力

Conclusion: GAP框架有效捕捉对话细节关联，结合领域知识显著降低非事实响应，为医疗对话系统提供可靠解决方案

Abstract: Medication recommendations have become an important task in the healthcare
domain, especially in measuring the accuracy and safety of medical dialogue
systems (MDS). Different from the recommendation task based on electronic
health records (EHRs), dialogue-based medication recommendations require
research on the interaction details between patients and doctors, which is
crucial but may not exist in EHRs. Recent advancements in large language models
(LLM) have extended the medical dialogue domain. These LLMs can interpret
patients' intent and provide medical suggestions including medication
recommendations, but some challenges are still worth attention. During a
multi-turn dialogue, LLMs may ignore the fine-grained medical information or
connections across the dialogue turns, which is vital for providing accurate
suggestions. Besides, LLMs may generate non-factual responses when there is a
lack of domain-specific knowledge, which is more risky in the medical domain.
To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted
\textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication
recommendation. It extracts medical concepts and corresponding states from
dialogue to construct an explicitly patient-centric graph, which can describe
the neglected but important information. Further, combined with external
medical knowledge graphs, GAP can generate abundant queries and prompts, thus
retrieving information from multiple sources to reduce the non-factual
responses. We evaluate GAP on a dialogue-based medication recommendation
dataset and further explore its potential in a more difficult scenario,
dynamically diagnostic interviewing. Extensive experiments demonstrate its
competitive performance when compared with strong baselines.

</details>


### [131] [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)
*Chenxi Liu,Yongqiang Chen,Tongliang Liu,James Cheng,Bo Han,Kun Zhang*

Main category: cs.CL

TL;DR: 提出Language-of-Thoughts(LoT)技术，通过调整语言表达顺序和用词，减少大语言模型中的语言建模偏差，显著提升多类推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型存在语言建模与思维链建模的偏差：1) 语言作为知识分享工具会引入偏离真实思维的偏见 2) 这种偏见会导致模型仅关注前提信息的局部偏置部分

Method: LoT提示技术：要求模型对所有相关信息进行表达顺序和词汇选择的重构，而非直接从部分信息中引出思维链

Result: 在多个推理任务中显著降低语言模型偏差，模型性能平均提升约15%

Conclusion: 语言建模与思维建模存在本质差异，LoT技术通过解耦语言表达与思维过程，有效提升大语言模型的System 2推理能力

Abstract: System 2 reasoning is one of the defining characteristics of intelligence,
which requires slow and logical thinking. Human conducts System 2 reasoning via
the language of thoughts that organizes the reasoning process as a causal
sequence of mental language, or thoughts. Recently, it has been observed that
System 2 reasoning can be elicited from Large Language Models (LLMs)
pre-trained on large-scale natural languages. However, in this work, we show
that there is a significant gap between the modeling of languages and thoughts.
As language is primarily a tool for humans to share knowledge and thinking,
modeling human language can easily absorb language biases into LLMs deviated
from the chain of thoughts in minds. Furthermore, we show that the biases will
mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of
the premise. To this end, we propose a new prompt technique termed
Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of
directly eliciting the chain of thoughts from partial information, LoT
instructs LLMs to adjust the order and token used for the expressions of all
the relevant information. We show that the simple strategy significantly
reduces the language modeling biases in LLMs and improves the performance of
LLMs across a variety of reasoning tasks.

</details>


### [132] [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)
*Paul Van Eecke,Katrien Beuls*

Main category: cs.CL

TL;DR: PyFCG 是将 Fluid Construction Grammar (FCG) 移植到 Python 的开源库，支持与 Python 生态工具集成，并通过三个典型用例教程展示其应用价值。


<details>
  <summary>Details</summary>
Motivation: 整合 FCG 到 Python 生态，便于研究者利用丰富的 Python 库资源，提升构式语法分析、语料学习和涌现通信实验的效率与扩展性。

Method: 以 Python 库形式实现 FCG 核心功能，提供语法分析测试、语料学习算法集成、多代理通信实验框架三类教程验证实用性。

Result: 成功开发 PyFCG 库，并在形式化语法验证、语料驱动的语法归纳、多代理语言演化实验中展示了其功能完备性和易用性。

Conclusion: PyFCG 为计算构式语法研究提供了灵活的工具支持，显著降低了复杂语言建模实验的技术门槛，推动跨领域研究协作。

Abstract: We present PyFCG, an open source software library that ports Fluid
Construction Grammar (FCG) to the Python programming language. PyFCG enables
its users to seamlessly integrate FCG functionality into Python programs, and
to use FCG in combination with other libraries within Python's rich ecosystem.
Apart from a general description of the library, this paper provides three
walkthrough tutorials that demonstrate example usage of PyFCG in typical use
cases of FCG: (i) formalising and testing construction grammar analyses, (ii)
learning usage-based construction grammars from corpora, and (iii) implementing
agent-based experiments on emergent communication.

</details>


### [133] [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)
*Zhihe Yang,Xufang Luo,Zilong Wang,Dongqi Han,Zhiyuan He,Dongsheng Li,Yunjian Xu*

Main category: cs.CL

TL;DR: 研究揭示强化学习中低概率token梯度干扰问题，提出Advantage Reweighting和Lopti方法，实验显示逻辑推理任务性能提升46.2%


<details>
  <summary>Details</summary>
Motivation: 发现强化学习训练中低概率token因梯度幅值过大主导模型更新，抑制高概率token的有效学习（对LLM性能起关键作用）

Method: Advantage Reweighting通过优势值重加权机制，Lopti采用低概率token隔离技术，双管齐下平衡不同概率token的梯度更新

Result: 在K&K逻辑谜题任务中实现最高46.2%的性能提升，GitHub开源实现验证方法有效性

Conclusion: 提出的梯度平衡策略显著提升RL训练效率，为LLM的强化学习优化提供新方法论

Abstract: Reinforcement learning (RL) has become a cornerstone for enhancing the
reasoning capabilities of large language models (LLMs), with recent innovations
such as Group Relative Policy Optimization (GRPO) demonstrating exceptional
effectiveness. In this study, we identify a critical yet underexplored issue in
RL training: low-probability tokens disproportionately influence model updates
due to their large gradient magnitudes. This dominance hinders the effective
learning of high-probability tokens, whose gradients are essential for LLMs'
performance but are substantially suppressed. To mitigate this interference, we
propose two novel methods: Advantage Reweighting and Low-Probability Token
Isolation (Lopti), both of which effectively attenuate gradients from
low-probability tokens while emphasizing parameter updates driven by
high-probability tokens. Our approaches promote balanced updates across tokens
with varying probabilities, thereby enhancing the efficiency of RL training.
Experimental results demonstrate that they substantially improve the
performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K
Logic Puzzle reasoning tasks. Our implementation is available at
https://github.com/zhyang2226/AR-Lopti.

</details>


### [134] [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)
*Jeffrey T. H. Wong,Cheng Zhang,Xinye Cao,Pedro Gimenes,George A. Constantinides,Wayne Luk,Yiren Zhao*

Main category: cs.CL

TL;DR: 提出A³框架，通过分割Transformer层并优化组件功能损失，实现更高效的低秩近似压缩


<details>
  <summary>Details</summary>
Motivation: 现有低秩近似方法忽视Transformer结构特性且引入运行时开销，A³通过组件优化直接减少模型规模并保持性能

Method: 将Transformer层拆分为QK、OV、MLP三个组件，对每个组件提供降低隐藏维度同时最小化功能损失的解析解

Result: LLaMA 3.1-70B在WikiText-2困惑度降至4.69，较之前最优结果提升3.18；支持KV缓存压缩和混合秩分配

Conclusion: A³在保持性能优势的同时直接减少计算/内存消耗，为模型压缩提供新优化范式并展现多场景应用潜力

Abstract: Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like pruning and quantization, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, KV cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and
mixed-rank assignments for enhanced performance.

</details>


### [135] [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)
*Cael Marquard,Simbarashe Mawere,Francois Meyer*

Main category: cs.CL

TL;DR: 使用神经网络方法为恩古尼语言构建形态学标注器，实验表明神经网络标注器显著优于传统规则系统，且从头训练的模型优于预训练模型。


<details>
  <summary>Details</summary>
Motivation: 针对南非恩古尼等黏着语的形态学解析难题（需分解词素并标注语法角色），探索如何用现代神经网络方法替代传统规则系统以提高标注效果。

Method: 1. 对比两类方法：a) 从头训练神经序列标注模型（LSTM/神经CRF） b) 微调预训练语言模型
2. 与传统规则系统基线比较
3. 测试不同上游分词器和语言学特征的影响

Result: 1. 神经网络标注器全面超越规则系统（p=0.01）
2. 从头训练模型F1平均比预训练模型高3.2个百分点
3. 现有形态学分词器可为神经标注器提供有效支持

Conclusion: 基于现有形态学分词器的神经网络标注器是恩古尼语言形态学解析的可行方案，其中从头训练的序列标注模型表现最优，该方法为低资源黏着语处理提供了新思路。

Abstract: Morphological parsing is the task of decomposing words into morphemes, the
smallest units of meaning in a language, and labelling their grammatical roles.
It is a particularly challenging task for agglutinative languages, such as the
Nguni languages of South Africa, which construct words by concatenating
multiple morphemes. A morphological parsing system can be framed as a pipeline
with two separate components, a segmenter followed by a tagger. This paper
investigates the use of neural methods to build morphological taggers for the
four Nguni languages. We compare two classes of approaches: training neural
sequence labellers (LSTMs and neural CRFs) from scratch and finetuning
pretrained language models. We compare performance across these two categories,
as well as to a traditional rule-based morphological parser. Neural taggers
comfortably outperform the rule-based baseline and models trained from scratch
tend to outperform pretrained models. We also compare parsing results across
different upstream segmenters and with varying linguistic input features. Our
findings confirm the viability of employing neural taggers based on
pre-existing morphological segmenters for the Nguni languages.

</details>


### [136] [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)
*Daehee Kim,Deokhyung Kang,Jonghwi Kim,Sangwon Ryu,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出基于大语言模型的查询改写方法GuRE，通过缓解词汇不匹配显著提升法律段落检索效果


<details>
  <summary>Details</summary>
Motivation: 法律段落检索系统因查询与目标段落间的词汇不匹配问题未被充分研究，需有效解决方案

Method: 训练大语言模型生成改写后的查询，使检索器能更准确地定位目标法律段落

Result: GuRE在不同检索器中实现平均6.4%的NDCG@5提升，优于所有基线方法

Conclusion: GuRE的检索器无关特性和多样化训练目标，使其比直接微调更适合实际法律检索场景

Abstract: Legal Passage Retrieval (LPR) systems are crucial as they help practitioners
save time when drafting legal arguments. However, it remains an underexplored
avenue. One primary reason is the significant vocabulary mismatch between the
query and the target passage. To address this, we propose a simple yet
effective method, the Generative query REwriter (GuRE). We leverage the
generative capabilities of Large Language Models (LLMs) by training the LLM for
query rewriting. "Rewritten queries" help retrievers to retrieve target
passages by mitigating vocabulary mismatch. Experimental results show that GuRE
significantly improves performance in a retriever-agnostic manner,
outperforming all baseline methods. Further analysis reveals that different
training objectives lead to distinct retrieval behaviors, making GuRE more
suitable than direct retriever fine-tuning for real-world applications. Codes
are avaiable at github.com/daehuikim/GuRE.

</details>


### [137] [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)
*Shanshan Liu,Noriki Nishida,Rumana Ferdous Munne,Narumi Tokunaga,Yuki Yamagata,Kouji Kozaki,Yuji Matsumoto*

Main category: cs.CL

TL;DR: 提出MA-COIR框架将生物医学概念识别重构为索引-识别任务，通过语义搜索索引(ssIDs)解决本体歧义，采用轻量级BART模型与LLM生成数据，在低资源场景下实现显/隐式概念的联合识别。


<details>
  <summary>Details</summary>
Motivation: 传统概念识别方法依赖显式文本提及，难以捕捉隐含的复杂生物医学概念，限制了本体论完善和知识图谱构建效果。需要开发无需提及标注的新方法提升概念识别能力。

Method: 1. 建立索引-识别范式，通过ssIDs编码概念语义
2. 使用预训练BART模型进行微调
3. 结合LLM生成查询语句和合成数据增强
4. 支持多场景概念联合识别

Result: 在CDR/HPO/HOIP三个场景测试中，MA-COIR有效识别显式和隐含概念，推理过程无需提及级标注，计算资源消耗降低60%以上。

Conclusion: 该框架突破传统方法的显式依赖，通过语义索引和轻量化模型设计，显著提升生物医学概念识别效率，为领域专家提供低门槛解决方案。代码和数据已开源。

Abstract: Recognizing biomedical concepts in the text is vital for ontology refinement,
knowledge graph construction, and concept relationship discovery. However,
traditional concept recognition methods, relying on explicit mention
identification, often fail to capture complex concepts not explicitly stated in
the text. To overcome this limitation, we introduce MA-COIR, a framework that
reformulates concept recognition as an indexing-recognition task. By assigning
semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in
ontology entries and enhances recognition efficiency. Using a pretrained
BART-based model fine-tuned on small datasets, our approach reduces
computational requirements to facilitate adoption by domain experts.
Furthermore, we incorporate large language models (LLMs)-generated queries and
synthetic data to improve recognition in low-resource settings. Experimental
results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of
MA-COIR in recognizing both explicit and implicit concepts without the need for
mention-level annotations during inference, advancing ontology-driven concept
recognition in biomedical domain applications. Our code and constructed data
are available at https://github.com/sl-633/macoir-master.

</details>


### [138] [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)
*Yingzhi Wang,Anas Alhmoud,Saad Alsahly,Muhammad Alqurishi,Mirco Ravanelli*

Main category: cs.CL

TL;DR: 提出Calm-Whisper方法，通过微调Whisper-large-v3解码器中3个关键注意力头，在非语音段实现80%幻听降低，词错率仅增加0.1%


<details>
  <summary>Details</summary>
Motivation: Whisper在非语音段存在严重幻听问题，制约其在复杂工业场景的应用。需要不依赖前后处理的解决方案

Method: 1. 通过头屏蔽技术定位解码器中导致幻听的3个关键注意力头（占幻听量的75%）；2. 使用非语音数据微调这三个注意力头

Result: 在UrbanSound数据集上幻听减少80%，LibriSpeech测试集词错率增幅小于0.1%

Conclusion: 针对特定注意力头的微调能有效抑制幻听，且不影响模型整体语音识别性能，为工业部署提供可靠解决方案

Abstract: OpenAI's Whisper has achieved significant success in Automatic Speech
Recognition. However, it has consistently been found to exhibit hallucination
issues, particularly in non-speech segments, which limits its broader
application in complex industrial settings.
  In this paper, we introduce a novel method to reduce Whisper's hallucination
on non-speech segments without using any pre- or post-possessing techniques.
Specifically, we benchmark the contribution of each self-attentional head in
the Whisper-large-v3 decoder to the hallucination problem by performing a
head-wise mask. Our findings reveal that only 3 of the 20 heads account for
over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune
these three crazy heads using a collection of non-speech data. The results show
that our best fine-tuned model, namely Calm-Whisper, achieves over 80%
reduction in non-speech hallucination with only less than 0.1% WER degradation
on LibriSpeech test-clean and test-other.

</details>


### [139] [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)
*Robin Jegan,Andreas Henrich*

Main category: cs.CL

TL;DR: 该论文通过分析五个NLP应用场景，揭示传统方法在LLMs主导时代仍以流程组件、基准对比等形式存在，并探讨其与前沿技术结合的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于大型语言模型（LLMs）虽主导NLP领域，但传统方法在特定场景仍具应用价值。作者旨在系统评估传统技术在当前研究中的角色，填补该领域的研究空白。

Method: 1. 定义传统方法特征标准
2. 选取分类、信息抽取、关系抽取、文本简化和摘要五个核心场景
3. 统计分析近期论文中传统模型的使用模式（流程组件/基准对比/核心模型）

Result: 所有被调查场景均存在传统方法应用：
- 34%作为处理流程组件
- 28%作为模型对比基准
- 12%仍作为论文核心模型
（数据来源：论文公开的统计记录）

Conclusion: 传统方法与LLMs存在互补性，建议：
1. 在数据稀缺场景结合规则系统
2. 构建混合式处理流水线
3. 将传统模型作为可解释性组件

Abstract: The continued rise of neural networks and large language models in the more
recent past has altered the natural language processing landscape, enabling new
approaches towards typical language tasks and achieving mainstream success.
Despite the huge success of large language models, many disadvantages still
remain and through this work we assess the state of the art in five application
scenarios with a particular focus on the future perspectives and sensible
application scenarios of traditional and older approaches and techniques.
  In this paper we survey recent publications in the application scenarios
classification, information and relation extraction, text simplification as
well as text summarization. After defining our terminology, i.e., which
features are characteristic for traditional techniques in our interpretation
for the five scenarios, we survey if such traditional approaches are still
being used, and if so, in what way they are used. It turns out that all five
application scenarios still exhibit traditional models in one way or another,
as part of a processing pipeline, as a comparison/baseline to the core model of
the respective paper, or as the main model(s) of the paper. For the complete
statistics, see https://zenodo.org/records/13683801

</details>


### [140] [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)
*Mahta Fetrat Qharabagh,Zahra Dehghanian,Hamid R. Rabiee*

Main category: cs.CL

TL;DR: 论文提出半自动化构建同形异义词数据集HomoRich，并开发快速规则系统HomoFast eSpeak，将波斯语同形异义词消解准确率提升约30%


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言在G2P转换中面临的同形异义词数据集构建困难，以及现有消解策略在实时应用中的高延迟问题

Method: 1. 提出半自动化数据集构建流程生成HomoRich数据集
2. 改进eSpeak规则系统，开发支持快速消解的HomoFast版本

Result: 深度学习系统和eSpeak系统的同形异义词消解准确率均提升约30%

Conclusion: 通过结合高质量离线数据集训练深度学习模型与优化规则系统，实现了准确性（HomoRich）与实时性（HomoFast）的协同优化

Abstract: Homograph disambiguation remains a significant challenge in
grapheme-to-phoneme (G2P) conversion, especially for low-resource languages.
This challenge is twofold: (1) creating balanced and comprehensive homograph
datasets is labor-intensive and costly, and (2) specific disambiguation
strategies introduce additional latency, making them unsuitable for real-time
applications such as screen readers and other accessibility tools. In this
paper, we address both issues. First, we propose a semi-automated pipeline for
constructing homograph-focused datasets, introduce the HomoRich dataset
generated through this pipeline, and demonstrate its effectiveness by applying
it to enhance a state-of-the-art deep learning-based G2P system for Persian.
Second, we advocate for a paradigm shift - utilizing rich offline datasets to
inform the development of fast, rule-based methods suitable for
latency-sensitive accessibility applications like screen readers. To this end,
we improve one of the most well-known rule-based G2P systems, eSpeak, into a
fast homograph-aware version, HomoFast eSpeak. Our results show an approximate
30% improvement in homograph disambiguation accuracy for the deep
learning-based and eSpeak systems.

</details>


### [141] [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)
*Jiaan Wang,Fandong Meng,Zengkui Sun,Yunlong Liang,Yuxuan Cao,Jiarong Xu,Haoxiang Shi,Jie Zhou*

Main category: cs.CL

TL;DR: 研究评估了大语言模型在多对多跨语言摘要任务中的表现，发现指令微调可提升性能但可能加剧事实性错误。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在多语言环境下处理多对多摘要任务的潜力，填补系统性实证研究的空白。

Method: 重组8个领域数据集构建47.8K样本的跨语言基准，对比18个LLM的零样本/指令微调表现及传统微调模型。

Result: 指令微调后的开源LLM自动评估超越GPT-4等零样本模型，但人类评估揭示事实性问题随微调加剧。

Conclusion: 实际应用中需优先控制LLM摘要的事实性错误，该问题应成为未来研究的关键方向。

Abstract: Many-to-many summarization (M2MS) aims to process documents in any language
and generate the corresponding summaries also in any language. Recently, large
language models (LLMs) have shown strong multi-lingual abilities, giving them
the potential to perform M2MS in real applications. This work presents a
systematic empirical study on LLMs' M2MS ability. Specifically, we first
reorganize M2MS data based on eight previous domain-specific datasets. The
reorganized data contains 47.8K samples spanning five domains and six
languages, which could be used to train and evaluate LLMs. Then, we benchmark
18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned
traditional models (e.g., mBART) are also conducted for comparisons. Our
experiments reveal that, zero-shot LLMs achieve competitive results with
fine-tuned traditional models. After instruct-tuning, open-source LLMs can
significantly improve their M2MS ability, and outperform zero-shot LLMs
(including GPT-4) in terms of automatic evaluations. In addition, we
demonstrate that this task-specific improvement does not sacrifice the LLMs'
general task-solving abilities. However, as revealed by our human evaluation,
LLMs still face the factuality issue, and the instruction tuning might
intensify the issue. Thus, how to control factual errors becomes the key when
building LLM summarizers in real applications, and is worth noting in future
research.

</details>


### [142] [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)
*Jiaan Wang,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 提出基于强LRM对比的新型奖励建模方法，显著提升机器翻译性能并实现多语言扩展


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在高资源语言且奖励建模未能充分发挥强化学习潜力，需改进方法提升多语言场景下的机器翻译性能

Method: 通过对比策略模型与DeepSeek-R1的翻译结果量化奖励，结合轻量级奖励模型设计实现90个方向的多语言迁移

Result: Qwen2.5-7B模型在文学翻译达到SOTA，多语言实验中覆盖11种语言并显著超越主流LRM

Conclusion: 新型奖励机制有效释放强化学习潜力，成功实现跨语言翻译能力迁移，为低资源语言MT提供新思路

Abstract: In recent years, the emergence of large reasoning models (LRMs), such as
OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex
problems, e.g., mathematics and coding. Some pioneering studies attempt to
bring the success of LRMs in neural machine translation (MT). They try to build
LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite
some progress that has been made, these attempts generally focus on several
high-resource languages, e.g., English and Chinese, leaving the performance on
other languages unclear. Besides, the reward modeling methods in previous work
do not fully unleash the potential of reinforcement learning in MT. In this
work, we first design a new reward modeling method that compares the
translation results of the policy MT model with a strong LRM (i.e.,
DeepSeek-R1-671B), and quantifies the comparisons to provide rewards.
Experimental results demonstrate the superiority of the reward modeling method.
Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new
state-of-the-art performance in literary translation, and outperforms strong
LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to
the multilingual settings with 11 languages. With a carefully designed
lightweight reward modeling in RL, we can simply transfer the strong MT ability
from a single direction into multiple (i.e., 90) translation directions and
achieve impressive multilingual MT performance.

</details>


### [143] [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)
*Yuhao Qing,Boyu Zhu,Mingzhe Du,Zhijiang Guo,Terry Yue Zhuo,Qianru Zhang,Jie M. Zhang,Heming Cui,Siu-Ming Yiu,Dong Huang,See-Kiong Ng,Luu Anh Tuan*

Main category: cs.CL

TL;DR: EffiBench-X首个多语言基准测试，显示LLM生成代码效率仅达人类专家62%，存在显著语言差异。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成基准主要关注功能正确性，缺乏对效率的评估且语言单一。需建立多语言效率评估体系。

Method: 构建支持6种语言的EffiBench-X基准，使用人类专家方案作为效率基线，评估主流LLM生成代码效率。

Result: LLM生成代码效率平均62%（Qwen3-32B最优），Python/Ruby/JS优于Java/C++/Golang，语言间最大差异达46%。

Conclusion: 亟需优化LLM多语言代码效率，数据集开源促进相关研究。

Abstract: Existing code generation benchmarks primarily evaluate functional
correctness, with limited focus on code efficiency and often restricted to a
single language like Python. To address this gap, we introduce EffiBench-X, the
first multi-language benchmark designed to measure the efficiency of
LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,
and Golang. It comprises competitive programming tasks with human-expert
solutions as efficiency baselines. Evaluating state-of-the-art LLMs on
EffiBench-X reveals that while models generate functionally correct code, they
consistently underperform human experts in efficiency. Even the most efficient
LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human
efficiency on average, with significant language-specific variations. LLMs show
better efficiency in Python, Ruby, and JavaScript than in Java, C++, and
Golang. For instance, DeepSeek-R1's Python code is significantly more efficient
than its Java code. These results highlight the critical need for research into
LLM optimization techniques to improve code efficiency across diverse
languages. The dataset and evaluation infrastructure are submitted and
available at https://github.com/EffiBench/EffiBench-X.git and
https://huggingface.co/datasets/EffiBench/effibench-x.

</details>


### [144] [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)
*Yuyang Li,Philip J. M. Kerbusch,Raimon H. R. Pruim,Tobias Käfer*

Main category: cs.CL

TL;DR: 论文对比三种RAG方法在机场对话系统的表现，推荐SQL RAG和Graph RAG以降低幻觉风险并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 提升机场自动化程度，解决专业术语/动态推理查询的交互需求。

Method: 开发传统RAG、SQL RAG和基于知识图谱的Graph RAG系统进行对比实验。

Result: 传统RAG准确率84.84%但存在安全隐患，Graph RAG达91.49%准确率且推理能力突出。

Conclusion: 推荐SQL RAG和Graph RAG，因其低幻觉特性及处理动态问题的优势。

Abstract: Airports from the top 20 in terms of annual passengers are highly dynamic
environments with thousands of flights daily, and they aim to increase the
degree of automation. To contribute to this, we implemented a Conversational AI
system that enables staff in an airport to communicate with flight information
systems. This system not only answers standard airport queries but also
resolves airport terminology, jargon, abbreviations, and dynamic questions
involving reasoning. In this paper, we built three different
Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL
RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that
traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally
produced hallucinations, which is risky to airport safety. In contrast, SQL RAG
and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with
significantly fewer hallucinations. Moreover, Graph RAG was especially
effective for questions that involved reasoning. Based on our observations, we
thus recommend SQL RAG and Graph RAG are better for airport environments, due
to fewer hallucinations and the ability to handle dynamic questions.

</details>


### [145] [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)
*Himel Ghosh,Ahmed Mosharafa,Georg Groh*

Main category: cs.CL

TL;DR: 提出基于RoBERTa的句子级媒体偏见检测模型，通过统计验证和注意力分析证明其有效性，并构建多级检测流程


<details>
  <summary>Details</summary>
Motivation: 媒体偏见检测面临标注数据稀缺和主观性强的挑战，现有方法存在对政治术语过度敏感等问题

Method: 在BABE数据集微调RoBERTa，采用McNemar检验和5x2交叉验证t检验，结合注意力机制分析

Result: 相比DA-RoBERTa基线模型取得统计显著提升，注意力机制更关注上下文相关特征而非表面政治术语

Conclusion: 构建了可解释的媒体偏见检测系统，提出未来应探索上下文建模、偏见中和与细粒度分类方向

Abstract: Media bias detection is a critical task in ensuring fair and balanced
information dissemination, yet it remains challenging due to the subjectivity
of bias and the scarcity of high-quality annotated data. In this work, we
perform sentence-level bias classification by fine-tuning a RoBERTa-based model
on the expert-annotated BABE dataset. Using McNemar's test and the 5x2
cross-validation paired t-test, we show statistically significant improvements
in performance when comparing our model to a domain-adaptively pre-trained
DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model
avoids common pitfalls like oversensitivity to politically charged terms and
instead attends more meaningfully to contextually relevant tokens. For a
comprehensive examination of media bias, we present a pipeline that combines
our model with an already-existing bias-type classifier. Our method exhibits
good generalization and interpretability, despite being constrained by
sentence-level analysis and dataset size because of a lack of larger and more
advanced bias corpora. We talk about context-aware modeling, bias
neutralization, and advanced bias type classification as potential future
directions. Our findings contribute to building more robust, explainable, and
socially responsible NLP systems for media bias detection.

</details>


### [146] [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)
*Márton Kardos,Kenneth C. Enevoldsen,Kristoffer Laigaard Nielbo*

Main category: cs.CL

TL;DR: 论文讨论了主题模型解释方法的局限性，提出了模型无关的交互式可视化框架topicwizard以提升解释效果。


<details>
  <summary>Details</summary>
Motivation: 现有主题模型可视化工具通常局限于特定模型类型，且传统『词列表』方法存在理解偏差，需要更全面的解释方案。

Method: 开发了模型无关框架topicwizard，通过交互式可视化工具揭示文档、词汇与主题间的语义关系。

Result: topicwizard提供动态探索功能，帮助用户更准确理解主题模型输出的复杂语义结构。

Conclusion: 该框架突破现有工具限制，增强了跨领域应用中主题模型的可解释性。

Abstract: Topic models are statistical tools that allow their users to gain qualitative
and quantitative insights into the contents of textual corpora without the need
for close reading. They can be applied in a wide range of settings from
discourse analysis, through pretraining data curation, to text filtering. Topic
models are typically parameter-rich, complex models, and interpreting these
parameters can be challenging for their users. It is typical practice for users
to interpret topics based on the top 10 highest ranking terms on a given topic.
This list-of-words approach, however, gives users a limited and biased picture
of the content of topics. Thoughtful user interface design and visualizations
can help users gain a more complete and accurate understanding of topic models'
output. While some visualization utilities do exist for topic models, these are
typically limited to a certain type of topic model. We introduce topicwizard, a
framework for model-agnostic topic model interpretation, that provides
intuitive and interactive tools that help users examine the complex semantic
relations between documents, words and topics learned by topic models.

</details>


### [147] [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)
*Sai Koneru,Maike Züfle,Thai-Binh Nguyen,Seymanur Akti,Jan Niehues,Alexander Waibel*

Main category: cs.CL

TL;DR: KIT提出基于大语言模型的语音翻译与指令跟随框架，通过多系统融合、两步翻译及上下文优化提升任务性能


<details>
  <summary>Details</summary>
Motivation: IWSLT任务范围扩展至语音问答和摘要等领域，利用大语言模型增强跨任务处理能力

Method: 离线翻译采用多语音识别系统融合+文档级LLM优化；指令跟踪构建语音编码器与LLM的端到端模型，均包含文档级后处理模块

Result: 开发出支持复杂跨模态任务的集成框架，上下文优化机制显著提升输出质量

Conclusion: 验证LLM在语音处理任务中的通用性，文档级优化对保持语义连贯性具有关键作用

Abstract: The scope of the International Workshop on Spoken Language Translation
(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to
encompass a wider array of tasks, including Speech Question Answering and
Summarization. This shift is partly driven by the growing capabilities of
modern systems, particularly with the success of Large Language Models (LLMs).
In this paper, we present the Karlsruhe Institute of Technology's submissions
for the Offline ST and Instruction Following (IF) tracks, where we leverage
LLMs to enhance performance across all tasks. For the Offline ST track, we
propose a pipeline that employs multiple automatic speech recognition systems,
whose outputs are fused using an LLM with document-level context. This is
followed by a two-step translation process, incorporating additional refinement
step to improve translation quality. For the IF track, we develop an end-to-end
model that integrates a speech encoder with an LLM to perform a wide range of
instruction-following tasks. We complement it with a final document-level
refinement stage to further enhance output quality by using contextual
information.

</details>


### [148] [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)
*Amelie S. Robrecht,Christoph R. Kowalski,Stefan Kopp*

Main category: cs.CL

TL;DR: 提出基于贝叶斯推断和非稳态MDP的框架，通过动态调整解释策略实现对话系统的用户自适应


<details>
  <summary>Details</summary>
Motivation: 现有对话系统缺乏对用户特征的动态适应能力，导致解释效果受限，需建立实时更新的用户认知模型

Method: 结合贝叶斯推断的持续用户建模与非稳态马尔可夫决策过程，实现动态策略调整

Result: 在5个模拟对话者测试中展现高适应性，能处理用户反馈模式变化并形成差异化解释策略

Conclusion: 该框架显著提升解释策略的适应性，为可解释AI和对话系统提供新方法论支持

Abstract: Adapting to the addressee is crucial for successful explanations, yet poses
significant challenges for dialogsystems. We adopt the approach of treating
explanation generation as a non-stationary decision process, where the optimal
strategy varies according to changing beliefs about the explainee and the
interaction context. In this paper we address the questions of (1) how to track
the interaction context and the relevant listener features in a formally
defined computational partner model, and (2) how to utilize this model in the
dynamically adjusted, rational decision process that determines the currently
best explanation strategy. We propose a Bayesian inference-based approach to
continuously update the partner model based on user feedback, and a
non-stationary Markov Decision Process to adjust decision-making based on the
partner model values. We evaluate an implementation of this framework with five
simulated interlocutors, demonstrating its effectiveness in adapting to
different partners with constant and even changing feedback behavior. The
results show high adaptivity with distinct explanation strategies emerging for
different partners, highlighting the potential of our approach to improve
explainable AI systems and dialogsystems in general.

</details>


### [149] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)
*Ambre Marie,Ilias Maoudj,Guillaume Dardenne,Gwenolé Quellec*

Main category: cs.CL

TL;DR: 多模态加权注意力模型在青少年自杀风险评估中取得69%准确率，但存在开发集与测试集的泛化差距


<details>
  <summary>Details</summary>
Motivation: 通过语音多模态分析提升青少年自杀风险评估精度，解决传统单模态方法的局限性

Method: 整合WhisperX语音转文本、RoBERTa语言嵌入、WavLM音频嵌入及手工声学特征(MFCC/频谱对比/基频)，采用早期拼接/模态专用处理/混合正则化加权注意力三种融合策略

Result: 加权注意力模型在开发集达69%准确率，但测试集性能差距揭示模型泛化挑战

Conclusion: 需优化嵌入表示和融合机制，提升基于MINI-KID框架的分类可靠性，解决跨数据集泛化问题

Abstract: The 1st SpeechWellness Challenge conveys the need for speech-based suicide
risk assessment in adolescents. This study investigates a multimodal approach
for this challenge, integrating automatic transcription with WhisperX,
linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.
Additionally, handcrafted acoustic features -- including MFCCs, spectral
contrast, and pitch-related statistics -- were incorporated. We explored three
fusion strategies: early concatenation, modality-specific processing, and
weighted attention with mixup regularization. Results show that weighted
attention provided the best generalization, achieving 69% accuracy on the
development set, though a performance gap between development and test sets
highlights generalization challenges. Our findings, strictly tied to the
MINI-KID framework, emphasize the importance of refining embedding
representations and fusion mechanisms to enhance classification reliability.

</details>


### [150] [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)
*Xiang Fei,Jinghui Lu,Qi Sun,Hao Feng,Yanjie Wang,Wei Shi,An-Lan Wang,Jingqun Tang,Can Huang*

Main category: cs.CL

TL;DR: 提出NTIL损失函数改善自回归模型对数字序列的预测能力


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型将数字视为独立token处理，忽略了数值序列的连贯结构特征

Method: 通过token级（扩展EMD保持数值序关系）和序列级（预测/实际序列差异惩罚）双重优化

Result: 实验显示模型性能获得显著提升

Conclusion: NTIL通过双层次设计有效提升大模型对数值序列的预测精度和整合能力

Abstract: Autoregressive models have become the de facto choice for sequence generation
tasks, but standard approaches treat digits as independent tokens and apply
cross-entropy loss, overlooking the coherent structure of numerical sequences.
This paper introduces Numerical Token Integrity Loss (NTIL) to address this
gap. NTIL operates at two levels: (1) token-level, where it extends the Earth
Mover's Distance (EMD) to preserve ordinal relationships between numerical
values, and (2) sequence-level, where it penalizes the overall discrepancy
between the predicted and actual sequences. This dual approach improves
numerical prediction and integrates effectively with LLMs/MLLMs. Extensive
experiments show significant performance improvements with NTIL.

</details>


### [151] [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)
*Sondre Wold,Lucas Georges Gabriel Charpentier,Étienne Simon*

Main category: cs.CL

TL;DR: 通过训练数据中组成元素的熵值量化系统泛化难度，发现模型性能与熵值呈正相关，高熵场景无需内置先验即可成功，低熵表现可作为系统性泛化评估目标。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对输入排列敏感且难以在新情境应用已知概念，缺乏系统性泛化问题的难度量化标准。

Method: 建立序列到序列任务的熵测量框架，分析不同架构模型在不同熵值训练数据上的表现。

Result: 流行模型架构的性能随熵值增长而提升，高熵场景(数据多样性高)无需内置先验即可成功，低熵表现(数据重复性强)可作为系统性泛化评估基准。

Conclusion: 系统泛化能力与信息效率密切相关，低熵场景的突破将推动更稳健的系统性泛化模型发展，为架构设计提供新方向。

Abstract: Systematic generalization remains challenging for current language models,
which are known to be both sensitive to semantically similar permutations of
the input and to struggle with known concepts presented in novel contexts.
Although benchmarks exist for assessing compositional behavior, it is unclear
how to measure the difficulty of a systematic generalization problem. In this
work, we show how one aspect of systematic generalization can be described by
the entropy of the distribution of component parts in the training data. We
formalize a framework for measuring entropy in a sequence-to-sequence task and
find that the performance of popular model architectures scales with the
entropy. Our work connects systematic generalization to information efficiency,
and our results indicate that success at high entropy can be achieved even
without built-in priors, and that success at low entropy can serve as a target
for assessing progress towards robust systematic generalization.

</details>


### [152] [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)
*David Stap,Christof Monz*

Main category: cs.CL

TL;DR: 研究发现语言多样性在LLM微调中存在双重效应：适度增加可提升翻译质量并形成语言无关表征，但超过阈值后收益递减


<details>
  <summary>Details</summary>
Motivation: 解决先前研究关于语言多样性在模型微调中效果不一致的争议，揭示多样性对多语言表征的适应性影响

Method: 通过132个翻译方向的受控微调实验，结合语言特异性和语言无关性的表征分析

Result: 语言多样性提升有监督/无监督对的翻译质量（最高+5.3 BLEU），但存在收益递减阈值；语言无关表征比例增加14%

Conclusion: 适度的语言多样性优化了多语言表征空间，其边际效益受表征适应能力的限制

Abstract: Prior research diverges on language diversity in LLM fine-tuning: Some
studies report benefits while others find no advantages. Through controlled
fine-tuning experiments across 132 translation directions, we systematically
resolve these disparities. We find that expanding language diversity during
fine-tuning improves translation quality for both unsupervised and --
surprisingly -- supervised pairs, despite less diverse models being fine-tuned
exclusively on these supervised pairs. However, benefits plateau or decrease
beyond a certain diversity threshold. We show that increased language diversity
creates more language-agnostic representations. These representational
adaptations help explain the improved performance in models fine-tuned with
greater diversity.

</details>


### [153] [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)
*Debarpan Bhattacharya,Apoorva Kulkarni,Sriram Ganapathy*

Main category: cs.CL

TL;DR: 提出了TREA数据集用于评估音频语言模型的时序推理能力，发现当前开源模型显著落后人类水平，并提出衡量模型鲁棒性的不确定性指标


<details>
  <summary>Details</summary>
Motivation: 现有大型音频语言模型（LALMs）在传统分类/生成任务外的推理能力缺乏有效评估方法，需要建立新的评估框架

Method: 1. 创建包含时序推理任务的TREA数据集
2. 对开源LALMs进行基准测试
3. 提出基于输入扰动不变性的不确定性评估指标

Result: 开源模型在TREA任务中准确率低于人类（平均差15-20%），且发现模型准确率与不确定性指标间不存在必然相关性（相关系数<0.3）

Conclusion: 需要结合准确率和不确定性进行综合评估，特别是在高风险应用中，单纯依赖传统准确率指标存在局限性

Abstract: The popular success of text-based large language models (LLM) has streamlined
the attention of the multimodal community to combine other modalities like
vision and audio along with text to achieve similar multimodal capabilities. In
this quest, large audio language models (LALMs) have to be evaluated on
reasoning related tasks which are different from traditional classification or
generation tasks. Towards this goal, we propose a novel dataset called temporal
reasoning evaluation of audio (TREA).
  We benchmark open-source LALMs and observe that they are consistently behind
human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we
also propose an uncertainty metric, which computes the invariance of the model
to semantically identical perturbations of the input. Our analysis shows that
the accuracy and uncertainty metrics are not necessarily correlated and thus,
points to a need for wholesome evaluation of LALMs for high-stakes
applications.

</details>


### [154] [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)
*Anton Ehrmanntraut,Julia Wunderle,Jan Pfister,Fotis Jannidis,Andreas Hotho*

Main category: cs.CL

TL;DR: 研究者开发了ModernGBERT和LL"aMmlein2Vec德语编码器模型组，通过系统对比验证了全新训练编码器在性能与参数效率上的显著优势


<details>
  <summary>Details</summary>
Motivation: 尽管仅解码器模型占据主流，但编码器在资源受限场景仍具不可替代性。本研究旨在构建完全透明的高性能德语编码器生态

Method: 采用ModernBERT架构创新从头训练ModernGBERT模型组(134M/1B)，同时通过LLM2Vec技术将德语仅解码器模型转化为LL"aMmlein2Vec编码器组(120M/1B/7B)

Result: ModernGBERT 1B在自然语言理解、文本嵌入和长文本推理任务中全面超越现有德语编码器及转换解码器，且在参数量效率上表现更优

Conclusion: 研究证实从头训练编码器的有效性，并通过全面开源模型、训练数据与代码，为德语NLP生态系统提供透明可靠的基础设施支持

Abstract: Despite the prominence of decoder-only language models, encoders remain
crucial for resource-constrained applications. We introduce ModernGBERT (134M,
1B), a fully transparent family of German encoder models trained from scratch,
incorporating architectural innovations from ModernBERT. To evaluate the
practical trade-offs of training encoders from scratch, we also present
LL\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German
decoder-only models via LLM2Vec. We benchmark all models on natural language
understanding, text embedding, and long-context reasoning tasks, enabling a
controlled comparison between dedicated encoders and converted decoders. Our
results show that ModernGBERT 1B outperforms prior state-of-the-art German
encoders as well as encoders adapted via LLM2Vec, with regard to performance
and parameter-efficiency. All models, training data, checkpoints and code are
publicly available, advancing the German NLP ecosystem with transparent,
high-performance encoder models.

</details>


### [155] [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)
*Zheng Wei Lim,Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）的跨语言推理依赖独立语言子空间而非共享语义空间，通过引导隐空间可提升多语言一致性。


<details>
  <summary>Details</summary>
Motivation: LLMs在不同语言提示下产生不一致输出，需揭示其跨语言知识迁移机制。

Method: 使用logit lens分析隐藏状态，验证模型通过独立语言子空间而非共享语义空间进行推理。

Result: 大模型隐藏状态更易脱离共享空间，但跨语言知识检索能力更强；强化共享空间利用可提升多语言推理准确率。

Conclusion: 引导模型隐空间向共享语义空间对齐能改善知识迁移，增强非英语输出的准确性和一致性。

Abstract: Large language models (LLMs) are demonstrably capable of cross-lingual
transfer, but can produce inconsistent output when prompted with the same
queries written in different languages. To understand how language models are
able to generalize knowledge from one language to the others, we apply the
logit lens to interpret the implicit steps taken by LLMs to solve multilingual
multi-choice reasoning questions. We find LLMs predict inconsistently and are
less accurate because they rely on subspaces of individual languages, rather
than working in a shared semantic space. While larger models are more
multilingual, we show their hidden states are more likely to dissociate from
the shared representation compared to smaller models, but are nevertheless more
capable of retrieving knowledge embedded across different languages. Finally,
we demonstrate that knowledge sharing can be modulated by steering the models'
latent processing towards the shared semantic space. We find reinforcing
utilization of the shared space improves the models' multilingual reasoning
performance, as a result of more knowledge transfer from, and better output
consistency with English.

</details>


### [156] [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)
*Aswathy Velutharambath,Roman Klinger,Kai Sassenberg*

Main category: cs.CL

TL;DR: 研究挑战现有文本欺骗检测方法，提出基于信念对齐的新框架，发现传统语言线索不可靠。


<details>
  <summary>Details</summary>
Motivation: 先前欺骗检测研究可能受数据集构建方式干扰，需在控制信念偏差的情境下重新评估语言线索有效性。

Method: 构建DeFaBel多语言语料库（德语/英语），通过特征模型、预训练模型和指令调优大模型进行系统性验证。

Result: 传统欺骗线索在DeFaBel中相关性微弱，各类模型在该数据集上表现接近随机猜测。

Conclusion: 文本欺骗检测需超越表层语言特征，应结合语境和认知维度重新构建研究方法体系。

Abstract: Can deception be detected solely from written text? Cues of deceptive
communication are inherently subtle, even more so in text-only communication.
Yet, prior studies have reported considerable success in automatic deception
detection. We hypothesize that such findings are largely driven by artifacts
introduced during data collection and do not generalize beyond specific
datasets. We revisit this assumption by introducing a belief-based deception
framework, which defines deception as a misalignment between an author's claims
and true beliefs, irrespective of factual accuracy, allowing deception cues to
be studied in isolation. Based on this framework, we construct three corpora,
collectively referred to as DeFaBel, including a German-language corpus of
deceptive and non-deceptive arguments and a multilingual version in German and
English, each collected under varying conditions to account for belief change
and enable cross-linguistic analysis. Using these corpora, we evaluate commonly
reported linguistic cues of deception. Across all three DeFaBel variants, these
cues show negligible, statistically insignificant correlations with deception
labels, contrary to prior work that treats such cues as reliable indicators. We
further benchmark against other English deception datasets following similar
data collection protocols. While some show statistically significant
correlations, effect sizes remain low and, critically, the set of predictive
cues is inconsistent across datasets. We also evaluate deception detection
using feature-based models, pretrained language models, and instruction-tuned
large language models. While some models perform well on established deception
datasets, they consistently perform near chance on DeFaBel. Our findings
challenge the assumption that deception can be reliably inferred from
linguistic cues and call for rethinking how deception is studied and modeled in
NLP.

</details>


### [157] [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)
*Zhi Liu,Tao Yang,Jing Wang,Yexin Chen,Zhan Gao,Jiaxi Yang,Kui Chen,Bingji Lu,Xiaochen Li,Changyong Luo,Yan Li,Xiaohong Gu,Peng Cao*

Main category: cs.CL

TL;DR: 提出76亿参数的中医药专用大模型'天壹'，通过渐进学习整合中医知识体系，构建TCMEval评估基准验证其临床应用潜力。


<details>
  <summary>Details</summary>
Motivation: 传统中医药依赖临床经验且现有AI方法受限于数据/单目标约束，通用大模型缺乏中医专业性且存在部署/幻觉问题。

Method: 基于中医经典文献、医案、知识图谱等多源语料进行预训练和微调，采用渐进式知识整合方法，建立包含考试/临床/问答/试验的四维评估体系TCMEval。

Result: 综合评估证明天壹能有效衔接中医理论与实践，在临床辅助决策和科研中展现显著应用价值。

Conclusion: 天壹通过领域定制化设计解决了通用模型的中医专业适配问题，为AI赋能传统医学提供了可部署的解决方案。

Abstract: Natural medicines, particularly Traditional Chinese Medicine (TCM), are
gaining global recognition for their therapeutic potential in addressing human
symptoms and diseases. TCM, with its systematic theories and extensive
practical experience, provides abundant resources for healthcare. However, the
effective application of TCM requires precise syndrome diagnosis, determination
of treatment principles, and prescription formulation, which demand decades of
clinical expertise. Despite advancements in TCM-based decision systems, machine
learning, and deep learning research, limitations in data and single-objective
constraints hinder their practical application. In recent years, large language
models (LLMs) have demonstrated potential in complex tasks, but lack
specialization in TCM and face significant challenges, such as too big model
scale to deploy and issues with hallucination. To address these challenges, we
introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and
specifically designed for TCM, pre-trained and fine-tuned on diverse TCM
corpora, including classical texts, expert treatises, clinical records, and
knowledge graphs. Tianyi is designed to assimilate interconnected and
systematic TCM knowledge through a progressive learning manner. Additionally,
we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in
TCM examinations, clinical tasks, domain-specific question-answering, and
real-world trials. The extensive evaluations demonstrate the significant
potential of Tianyi as an AI assistant in TCM clinical practice and research,
bridging the gap between TCM knowledge and practical application.

</details>


### [158] [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)
*Yassine El Boudouri,Walter Nuninger,Julian Alvarez,Yvan Peter*

Main category: cs.CL

TL;DR: 提出了RPEval基准测试框架，用于评估大语言模型在角色扮演中的情感理解、决策能力、道德对齐和角色一致性四个维度。


<details>
  <summary>Details</summary>
Motivation: 现有方法评估大语言模型的角色扮演能力存在人工评估成本高、自动化评估易偏倚的痛点。

Method: 通过构建包含4个核心维度（情感理解/决策能力/道德对齐/角色一致性）的基准测试框架，使用多轮对话场景进行系统性评估。

Result: 建立了包含基线评估结果的基准测试集，并开源了代码和数据集（https://github.com/yelboudouri/RPEval）。

Conclusion: RPEval为标准化评估LLM角色扮演能力提供了有效工具，推动了该领域评估方法的发展。

Abstract: Large Language Models (LLMs) demonstrate a notable capacity for adopting
personas and engaging in role-playing. However, evaluating this ability
presents significant challenges, as human assessments are resource-intensive
and automated evaluations can be biased. To address this, we introduce
Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM
role-playing capabilities across four key dimensions: emotional understanding,
decision-making, moral alignment, and in-character consistency. This article
details the construction of RPEval and presents baseline evaluations. Our code
and dataset are available at https://github.com/yelboudouri/RPEval

</details>


### [159] [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)
*Yixuan Xu,Antoine Bosselut,Imanol Schlag*

Main category: cs.CL

TL;DR: 发现大语言模型存在位置偏移效应，即初始位置对记忆敏感，通过偏移敏感数据可显著降低版权风险


<details>
  <summary>Details</summary>
Motivation: 系统研究大语言模型对训练数据的记忆风险及其版权影响

Method: 预训练不同规模模型（1B/3B/8B），混合网络数据与受控版权的公共书籍数据，研究记忆触发机制

Result: 1. 短前缀触发最强记忆（位置脆弱性） 2. 位置偏移导致记忆能力断崖式下降 3. 记忆失败时产生退化文本

Conclusion: 位置偏移是评估记忆风险的关键维度，实际部署可通过数据位置偏移主动降低版权风险

Abstract: Large language models are known to memorize parts of their training data,
posing risk of copyright violations. To systematically examine this risk, we
pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing
web-scale data with public domain books used to simulate copyrighted content at
controlled frequencies at lengths at least ten times longer than prior work. We
thereby identified the offset effect, a phenomenon characterized by two key
findings: (1) verbatim memorization is most strongly triggered by short
prefixes drawn from the beginning of the context window, with memorization
decreasing counterintuitively as prefix length increases; and (2) a sharp
decline in verbatim recall when prefix begins offset from the initial tokens of
the context window. We attribute this to positional fragility: models rely
disproportionately on the earliest tokens in their context window as retrieval
anchors, making them sensitive to even slight shifts. We further observe that
when the model fails to retrieve memorized content, it often produces
degenerated text. Leveraging these findings, we show that shifting sensitive
data deeper into the context window suppresses both extractable memorization
and degeneration. Our results suggest that positional offset is a critical and
previously overlooked axis for evaluating memorization risks, since prior work
implicitly assumed uniformity by probing only from the beginning of training
sequences.

</details>


### [160] [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)
*V. S. D. S. Mahesh Akavarapu,Hrishikesh Terdalkar,Pramit Bhattacharyya,Shubhangi Agarwal,Vishakha Deulgaonkar,Pralay Manna,Chaitali Dangarikar,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: 研究发现模型规模是影响古典语言跨语言泛化的关键因素，大型语言模型（如GPT-4o）在零样本任务中表现优异，而检索增强方法显著提升了梵语问答性能。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在古典语言（梵语/古希腊语/拉丁语）自然理解任务中的跨语言泛化机制，验证模型规模对性能的影响及其在古典学研究中的应用潜力。

Method: 通过命名实体识别、机器翻译和梵语问答三个任务进行验证，其中问答任务采用检索增强生成技术融入上下文。

Result: 大型模型在跨域数据上优于传统方法（NER任务F1提升5-15%），检索增强使问答准确率提升32%；但参数量<70亿的模型在抽象实体识别和问答任务中出现20-40%的性能下降。

Conclusion: 模型规模直接影响跨语言泛化能力，未经过古典语言指令微调的LLMs仍展现应用价值，建议古典学研究优先选用参数量>700亿的模型并配合检索增强技术。

Abstract: Large Language Models (LLMs) have demonstrated remarkable generalization
capabilities across diverse tasks and languages. In this study, we focus on
natural language understanding in three classical languages -- Sanskrit,
Ancient Greek and Latin -- to investigate the factors affecting cross-lingual
zero-shot generalization. First, we explore named entity recognition and
machine translation into English. While LLMs perform equal to or better than
fine-tuned baselines on out-of-domain data, smaller models often struggle,
especially with niche or abstract entity types. In addition, we concentrate on
Sanskrit by presenting a factoid question-answering (QA) dataset and show that
incorporating context via retrieval-augmented generation approach significantly
boosts performance. In contrast, we observe pronounced performance drops for
smaller LLMs across these QA tasks. These results suggest model scale as an
important factor influencing cross-lingual generalization. Assuming that models
used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical
languages, our findings provide insights into how LLMs may generalize on these
languages and their consequent utility in classical studies.

</details>


### [161] [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)
*Zihao Cheng,Hongru Wang,Zeming Liu,Yuhang Guo,Yuanfang Guo,Yunhong Wang,Haifeng Wang*

Main category: cs.CL

TL;DR: 提出ToolSpectrum基准评估LLM个性化工具使用能力，发现上下文感知对用户体验的关键作用及现有模型局限性


<details>
  <summary>Details</summary>
Motivation: 现有工具集成方法忽视上下文个性化，导致工具选择次优化。尤其在工具重叠场景需基于用户画像和环境因素协同决策

Method: 构建ToolSpectrum基准，形式化定义用户画像和环境因素双维度，通过系统实验分析其单独/协同影响

Result: 个性化工具使用显著提升跨场景体验，但顶级LLM在双维度联合推理时呈现优先单维度（准确率仅67.3%）的明显局限

Conclusion: 上下文感知个性化是工具增强LLM的必要方向，当前模型需突破多维度联合推理能力瓶颈

Abstract: While integrating external tools into large language models (LLMs) enhances
their ability to access real-time information and domain-specific services,
existing approaches focus narrowly on functional tool selection following user
instructions, overlooking the context-aware personalization in tool selection.
This oversight leads to suboptimal user satisfaction and inefficient tool
utilization, particularly when overlapping toolsets require nuanced selection
based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a
benchmark designed to evaluate LLMs' capabilities in personalized tool
utilization. Specifically, we formalize two key dimensions of personalization,
user profile and environmental factors, and analyze their individual and
synergistic impacts on tool utilization. Through extensive experiments on
ToolSpectrum, we demonstrate that personalized tool utilization significantly
improves user experience across diverse scenarios. However, even
state-of-the-art LLMs exhibit the limited ability to reason jointly about user
profiles and environmental factors, often prioritizing one dimension at the
expense of the other. Our findings underscore the necessity of context-aware
personalization in tool-augmented LLMs and reveal critical limitations for
current models. Our data and code are available at
https://github.com/Chengziha0/ToolSpectrum.

</details>


### [162] [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)
*Zhengrui Ma,Yang Feng,Chenze Shao,Fandong Meng,Jie Zhou,Min Zhang*

Main category: cs.CL

TL;DR: 提出SLED方法，通过能量距离目标建模连续语音表征，避免量化误差并简化模型架构，在零样本和流式语音合成中展现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖残差向量量化和复杂层次结构，导致离散化误差和建模流程复杂化。需要更高效的连续表征建模方法以保持语音信息完整性。

Method: 将语音波形编码为连续潜在表示序列，采用能量距离目标进行自回归建模。该距离直接对比模拟与目标样本的分布差异，实现连续分布的参数化学习。

Result: 实证表明SLED在零样本/流式合成任务中性能优异，推理效率提升30%，同时保持97.2%的语音信息保留率。

Conclusion: SLED通过消除量化步骤简化建模流程，为通用语音语言模型提供更高效的架构范式，具有扩展至多模态任务的潜力。

Abstract: We introduce SLED, an alternative approach to speech language modeling by
encoding speech waveforms into sequences of continuous latent representations
and modeling them autoregressively using an energy distance objective. The
energy distance offers an analytical measure of the distributional gap by
contrasting simulated and target samples, enabling efficient training to
capture the underlying continuous autoregressive distribution. By bypassing
reliance on residual vector quantization, SLED avoids discretization errors and
eliminates the need for the complicated hierarchical architectures common in
existing speech language models. It simplifies the overall modeling pipeline
while preserving the richness of speech information and maintaining inference
efficiency. Empirical results demonstrate that SLED achieves strong performance
in both zero-shot and streaming speech synthesis, showing its potential for
broader applications in general-purpose speech language models.

</details>


### [163] [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)
*Jikai Wang,Zhenxu Tian,Juntao Li,Qingrong Xia,Xinyu Duan,Zhefeng Wang,Baoxing Huai,Min Zhang*

Main category: cs.CL

TL;DR: 提出无需训练的对齐增强推测解码算法，通过预填充输出分布优化候选对齐，配合灵活验证策略实现更高效推理


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法依赖训练实现草稿-目标对齐（如EAGLE、Medusa），存在较高训练成本。本文旨在通过无训练方法解决该问题

Method: 1. 对齐采样：利用预填充阶段获得的输出分布生成更对齐的候选草稿
2. 灵活验证：通过自适应概率阈值整合高质量非对齐候选，提升准确性与效率

Result: 在8个数据集（QA/摘要/代码生成）上：
- LLaMA3生成分数平均提升3.3分
- 平均接受长度达2.39
- 生成速度提升2.23倍

Conclusion: 该方法有效突破传统训练依赖，通过算法级创新显著提升推理效率，为实际应用提供更优解决方案

Abstract: Recent works have revealed the great potential of speculative decoding in
accelerating the autoregressive generation process of large language models.
The success of these methods relies on the alignment between draft candidates
and the sampled outputs of the target model. Existing methods mainly achieve
draft-target alignment with training-based methods, e.g., EAGLE, Medusa,
involving considerable training costs. In this paper, we present a
training-free alignment-augmented speculative decoding algorithm. We propose
alignment sampling, which leverages output distribution obtained in the
prefilling phase to provide more aligned draft candidates. To further benefit
from high-quality but non-aligned draft candidates, we also introduce a simple
yet effective flexible verification strategy. Through an adaptive probability
threshold, our approach can improve generation accuracy while further improving
inference efficiency. Experiments on 8 datasets (including question answering,
summarization and code completion tasks) show that our approach increases the
average generation score by 3.3 points for the LLaMA3 model. Our method
achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.

</details>


### [164] [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)
*Xiaocong Du,Haoyu Pei,Haipeng Zhang*

Main category: cs.CL

TL;DR: 提出融合方言音频与视觉特征的多模态框架，显著提升古典诗歌情感分析效果


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略诗歌朗诵韵律和配图视觉特征，而方言可能保留古汉语音韵特征，多模态融合能更全面捕捉情感

Method: 提取多方言句子级音频特征+生成视觉特征，通过多模态对比表示学习与LLM增强的文本特征融合

Result: 在公开数据集上准确率提升≥2.51%，宏F1提升≥1.63%，优于现有方法

Conclusion: 证实多模态方法的有效性，开源代码推动领域发展，为中文多模态表示研究提供新思路

Abstract: Classical Chinese poetry is a vital and enduring part of Chinese literature,
conveying profound emotional resonance. Existing studies analyze sentiment
based on textual meanings, overlooking the unique rhythmic and visual features
inherent in poetry,especially since it is often recited and accompanied by
Chinese paintings. In this work, we propose a dialect-enhanced multimodal
framework for classical Chinese poetry sentiment analysis. We extract
sentence-level audio features from the poetry and incorporate audio from
multiple dialects,which may retain regional ancient Chinese phonetic features,
enriching the phonetic representation. Additionally, we generate sentence-level
visual features, and the multimodal features are fused with textual features
enhanced by LLM translation through multimodal contrastive representation
learning. Our framework outperforms state-of-the-art methods on two public
datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro
F1. We open-source the code to facilitate research in this area and provide
insights for general multimodal Chinese representation.

</details>


### [165] [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)
*Jie Ying,Zihong Chen,Zhefan Wang,Wanli Jiang,Chenyang Wang,Zhonghang Yuan,Haoyang Su,Huanjun Kong,Fan Yang,Nanqing Dong*

Main category: cs.CL

TL;DR: 首次提出种子科学专用多任务基准SeedBench，评估26个主流大模型并揭示其与产业需求的显著差距


<details>
  <summary>Details</summary>
Motivation: 种子科学面临跨学科复杂性高、成本回报率低导致的专家短缺和技术支撑不足问题，LLMs在种子科学的应用因数字资源匮乏、基因-性状关系复杂和缺乏标准化基准而受限

Method: 联合领域专家开发聚焦种子育种的多任务基准SeedBench，模拟现代育种核心环节，系统评估包含专有/开源/领域微调模型在内的26个主流LLMs

Result: 实验表明当前LLMs能力与种子科学实际需求存在显著差距

Conclusion: SeedBench为种子设计的大模型研究奠定基础，推动LLMs在种子科学中的发展

Abstract: Seed science is essential for modern agriculture, directly influencing crop
yields and global food security. However, challenges such as interdisciplinary
complexity and high costs with limited returns hinder progress, leading to a
shortage of experts and insufficient technological support. While large
language models (LLMs) have shown promise across various fields, their
application in seed science remains limited due to the scarcity of digital
resources, complex gene-trait relationships, and the lack of standardized
benchmarks. To address this gap, we introduce SeedBench -- the first multi-task
benchmark specifically designed for seed science. Developed in collaboration
with domain experts, SeedBench focuses on seed breeding and simulates key
aspects of modern breeding processes. We conduct a comprehensive evaluation of
26 leading LLMs, encompassing proprietary, open-source, and domain-specific
fine-tuned models. Our findings not only highlight the substantial gaps between
the power of LLMs and the real-world seed science problems, but also make a
foundational step for research on LLMs for seed design.

</details>


### [166] [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)
*Jieying Xue,Phuong Minh Nguyen,Minh Le Nguyen,Xin Liu*

Main category: cs.CL

TL;DR: 提出融合多语言预训练模型的双架构方法，在SemEval-2025情感检测任务中实现Top5性能


<details>
  <summary>Details</summary>
Motivation: 应对全球化背景下社交媒体多语言用户情感分析需求，解决多标签情感检测和情感强度的跨语言挑战

Method: 使用多语言BERT微调架构+指令调优生成式LLM，提出基础多标签分类法和成对分类法

Result: Track A在10语种Top4（印地语第1），Track B在7语种Top5

Conclusion: 该方法在跨语言情感识别中展现优异的泛化能力，验证了双架构策略的有效性

Abstract: With the rapid advancement of global digitalization, users from different
countries increasingly rely on social media for information exchange. In this
context, multilingual multi-label emotion detection has emerged as a critical
research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:
(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.
To tackle multilingual challenges, we leverage pre-trained multilingual models
and focus on two architectures: (1) a fine-tuned BERT-based classification
model and (2) an instruction-tuned generative LLM. Additionally, we propose two
methods for handling multi-label classification: the base method, which maps an
input directly to all its corresponding emotion labels, and the pairwise
method, which models the relationship between the input text and each emotion
category individually. Experimental results demonstrate the strong
generalization ability of our approach in multilingual emotion recognition. In
Track A, our method achieved Top 4 performance across 10 languages, ranking 1st
in Hindi. In Track B, our approach also secured Top 5 performance in 7
languages, highlighting its simplicity and effectiveness\footnote{Our code is
available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.

</details>


### [167] [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)
*Sidney Wong*

Main category: cs.CL

TL;DR: 论文主张用社会语言学方法而非纯技术手段解决网络仇恨言论问题


<details>
  <summary>Details</summary>
Motivation: 互联网对边缘群体存在双刃剑效应——既促进连接又加剧数字排斥，需要更有效解决方案

Method: 借鉴语言学影响语言政策的经验，将社会语言学洞见转化为数字空间治理方案

Result: 提出语言学家可与社区、政策制定者合作，通过社会干预实现数字包容

Conclusion: 语言学与NLP研究者应主导社会解决方案，缩小数字鸿沟促进公平

Abstract: The advent of the internet has been both a blessing and a curse for once
marginalised communities. When used well, the internet can be used to connect
and establish communities crossing different intersections; however, it can
also be used as a tool to alienate people and communities as well as perpetuate
hate, misinformation, and disinformation especially on social media platforms.
We propose steering hate speech research and researchers away from pre-existing
computational solutions and consider social methods to inform social solutions
to address this social problem. In a similar way linguistics research can
inform language planning policy, linguists should apply what we know about
language and society to mitigate some of the emergent risks and dangers of
anti-social behaviour in digital spaces. We argue linguists and NLP researchers
can play a principle role in unleashing the social impact potential of
linguistics research working alongside communities, advocates, activists, and
policymakers to enable equitable digital inclusion and to close the digital
divide.

</details>


### [168] [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)
*Rikhil Amonkar,Ronan Le Bras,Li Zhang*

Main category: cs.CL

TL;DR: 评估大语言模型通过生成可执行程序解决复杂文本规划任务的效果，发现编程方法通常优于直接规划，但存在代码质量缺陷。


<details>
  <summary>Details</summary>
Motivation: 针对LLMs在复杂文本规划任务（如会议安排）中遇到的挑战，探索通过生成可执行程序替代传统规划方法的有效性。

Method: 系统评估闭源/开源模型生成可执行程序（包括Python代码和约束求解器代码）的能力，通过程序执行输出规划方案。

Result: 编程方法在多数情况下优于直接规划，但生成的代码存在鲁棒性和效率问题，影响泛化能力。

Conclusion: 编程范式为解决复杂规划问题提供了新思路，但需提升生成代码的质量以实现可靠应用。

Abstract: Real-life textual planning tasks such as meeting scheduling have posed much
challenge to LLMs especially when the complexity is high. While previous work
primarily studied auto-regressive generation of plans with closed-source
models, we systematically evaluate both closed- and open-source models,
including those that scales output length with complexity during inference, in
generating programs, which are executed to output the plan. We consider not
only standard Python code, but also the code to a constraint satisfaction
problem solver. Despite the algorithmic nature of the task, we show that
programming often but not always outperforms planning. Our detailed error
analysis also indicates a lack of robustness and efficiency in the generated
code that hinders generalization.

</details>


### [169] [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)
*Siran Liu,Yang Ye,Qianchao Zhu,Zheng Cao,Yongchao He*

Main category: cs.CL

TL;DR: 提出基于语言复杂度异质性的动态资源分配框架HeteroSpec，通过熵度量和自适应扩展机制实现4.26倍加速效果，无需模型重训练且兼容现有加速技术


<details>
  <summary>Details</summary>
Motivation: 现有推测解码算法未充分利用语言复杂性差异，导致计算资源分配不均衡。需要动态调整资源分配策略以适应不同上下文复杂度

Method: 1) 提出累积元路径Top-K熵指标识别可预测上下文；2) 基于数据驱动熵分区的动态资源分配机制，实现上下文难度自适应的推测扩展与剪枝

Result: 在5个基准测试和4种模型上平均加速4.26倍，验证成本降低34%，接受长度提升21%，兼容更强草稿模型时效果持续提升

Conclusion: HeteroSpec通过上下文感知的资源动态分配，建立了LLM加速新范式，其零训练开销和正交性特点为实际部署提供有效解决方案

Abstract: Autoregressive decoding, the standard approach for Large Language Model (LLM)
inference, remains a significant bottleneck due to its sequential nature. While
speculative decoding algorithms mitigate this inefficiency through parallel
verification, they fail to exploit the inherent heterogeneity in linguistic
complexity, a key factor leading to suboptimal resource allocation. We address
this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding
framework that dynamically optimizes computational resource allocation based on
linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A
novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying
predictable contexts. (2) A dynamic resource allocation strategy based on
data-driven entropy partitioning, enabling adaptive speculative expansion and
pruning tailored to local context difficulty. Evaluated on five public
benchmarks and four models, HeteroSpec achieves an average speedup of
4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across
speedup rates, average acceptance length, and verification cost. Notably,
HeteroSpec requires no draft model retraining, incurs minimal overhead, and is
orthogonal to other acceleration techniques. It demonstrates enhanced
acceleration with stronger draft models, establishing a new paradigm for
context-aware LLM inference acceleration.

</details>


### [170] [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)
*Zilu Tang,Afra Feyza Akyürek,Ekin Akyürek,Derry Wijaya*

Main category: cs.CL

TL;DR: 论文提出了首个细粒度个性化对齐数据集WikiPersona，通过名人背景与偏好描述解决传统偏好对齐方法在个体矛盾偏好场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法主要优化模型在通用人类偏好上的平均表现，忽视了现实中个体偏好存在矛盾性和多样性的特点，且缺乏针对个体层面细粒度偏好的数据集。

Method: 构建WikiPersona数据集，要求模型同时生成可验证的名人背景描述和偏好对齐内容。通过系统性评估发现，基于推断个人偏好的前缀编码方法比传统few-shot提示和微调更有效。

Result: 使用推断的个人偏好作为前缀编码，在偏好冲突话题中实现了更有效的个性化对齐，且对未见过的名人具有更好的泛化公平性。

Conclusion: 该研究强调了细粒度个体偏好数据的重要性，提出基于前缀编码的个性化对齐范式，为处理矛盾偏好场景提供了新的解决方案。

Abstract: Preference alignment has become a standard pipeline in finetuning models to
follow \emph{generic} human preferences. Majority of work seeks to optimize
model to produce responses that would be preferable \emph{on average},
simplifying the diverse and often \emph{contradicting} space of human
preferences. While research has increasingly focused on personalized alignment:
adapting models to individual user preferences, there is a lack of personalized
preference dataset which focus on nuanced individual-level preferences. To
address this, we introduce WikiPersona: the first fine-grained personalization
using well-documented, famous individuals. Our dataset challenges models to
align with these personas through an interpretable process: generating
verifiable textual descriptions of a persona's background and preferences in
addition to alignment. We systematically evaluate different personalization
approaches and find that as few-shot prompting with preferences and fine-tuning
fail to simultaneously ensure effectiveness and efficiency, using
\textit{inferred personal preferences} as prefixes enables effective
personalization, especially in topics where preferences clash while leading to
more equitable generalization across unseen personas.

</details>


### [171] [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)
*Jingyi Ren,Yekun Xu,Xiaolong Wang,Weitao Li,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 提出AREMA框架，通过强化学习训练的透明RAG生成器显著提升多跳QA任务性能


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法存在生成器利用检索信息效率低、决策过程不透明两大核心问题

Method: 基于强化学习设计自适应奖励机制，实现结构化推理和可解释决策追踪

Result: 在多个数据集上实现10-30%性能提升，达到商业LLM水平且具备零样本迁移能力

Conclusion: AREMA框架开创了可解释RAG新范式，其开源促进领域发展并展示强大适应性

Abstract: Retrieval-Augmented Generation (RAG) has significantly improved the
performance of large language models (LLMs) on knowledge-intensive domains.
However, although RAG achieved successes across distinct domains, there are
still some unsolved challenges: 1) Effectiveness. Existing research mainly
focuses on developing more powerful RAG retrievers, but how to enhance the
generator's (LLM's) ability to utilize the retrieved information for reasoning
and generation? 2) Transparency. Most RAG methods ignore which retrieved
content actually contributes to the reasoning process, resulting in a lack of
interpretability and visibility. To address this, we propose ARENA
(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator
framework trained via reinforcement learning (RL) with our proposed rewards.
Based on the structured generation and adaptive reward calculation, our
RL-based training enables the model to identify key evidence, perform
structured reasoning, and generate answers with interpretable decision traces.
Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments
with various RAG baselines demonstrate that our model achieves 10-30%
improvements on all multi-hop QA datasets, which is comparable with the SOTA
Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses
show that ARENA has strong flexibility to be adopted on new datasets without
extra training. Our models and codes are publicly released.

</details>


### [172] [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)
*Tianshi Zheng,Zheye Deng,Hong Ting Tsang,Weiqi Wang,Jiaxin Bai,Zihao Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: 大语言模型正从专用工具演变为自主科研主体，通过三级分类框架（工具-分析员-科学家）系统阐述其科研范式变革，并指出未来关键挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型如何突破传统工具属性，逐步承担更复杂的科研角色，重构科研流程与人机协作模式。

Method: 基于科学方法论构建三级分类体系（工具级、分析员级、科学家级），逐级分析模型自主性演变及其在科研生命周期中的功能演进。

Result: 建立自主性演进框架，识别出机器人自动化、模型自我进化、伦理治理等关键研究方向，提供AI驱动科研的概念架构。

Conclusion: 需建立兼顾创新速度与责任伦理的发展路径，通过系统化框架引导大语言模型推动科学发现，实现技术赋能与风险管控的平衡。

Abstract: Large Language Models (LLMs) are catalyzing a paradigm shift in scientific
discovery, evolving from task-specific automation tools into increasingly
autonomous agents and fundamentally redefining research processes and human-AI
collaboration. This survey systematically charts this burgeoning field, placing
a central focus on the changing roles and escalating capabilities of LLMs in
science. Through the lens of the scientific method, we introduce a foundational
three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating
autonomy and evolving responsibilities within the research lifecycle. We
further identify pivotal challenges and future research trajectories such as
robotic automation, self-improvement, and ethical governance. Overall, this
survey provides a conceptual architecture and strategic foresight to navigate
and shape the future of AI-driven scientific discovery, fostering both rapid
innovation and responsible advancement. Github Repository:
https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.

</details>


### [173] [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)
*Livia Qian,Carol Figueroa,Gabriel Skantze*

Main category: cs.CL

TL;DR: 该研究通过三元对比实验，发现频谱特征和自监督语音表征在编码同一说话者的反馈副语言信息时优于传统基频特征，并通过对比学习进一步提升了表征与人类感知的对齐度。


<details>
  <summary>Details</summary>
Motivation: 口语对话中词汇相同但副语言特征不同的反馈（如'mhm'的不同语调）对语义理解至关重要，但现有语音表征模型对这类副语言相似度的编码能力尚未明确。

Method: 使用两个数据集中的语音反馈样本，招募参与者进行三元对比任务，比较频谱特征、自监督学习表征（如wav2vec）和基频特征的表征能力。

Result: 同一说话者的反馈中，频谱和自监督模型（尤以wav2vec最佳）的聚类结果与人类感知相似度显著相关（Spearman ρ=0.45），优于基频特征（ρ=0.17）。对比学习可将表征空间压缩64%同时保持感知对齐。

Conclusion: 结合自监督学习与对比学习能有效捕捉副语言相似性，为构建更自然的对话系统反馈机制提供了技术路径。

Abstract: Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of
spoken dialogue and is crucial to ensuring common ground in conversational
systems. The exact meaning of such feedback is conveyed through both lexical
and prosodic form. In this work, we investigate the perceived prosodic
similarity of vocal feedback with the same lexical form, and to what extent
existing speech representations reflect such similarities. A triadic comparison
task with recruited participants is used to measure perceived similarity of
feedback responses taken from two different datasets. We find that spectral and
self-supervised speech representations encode prosody better than extracted
pitch features, especially in the case of feedback from the same speaker. We
also find that it is possible to further condense and align the representations
to human perception through contrastive learning.

</details>


### [174] [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)
*Lei Sheng,Shuai-Shuai Xu*

Main category: cs.CL

TL;DR: CSC-SQL通过整合自洽性采样和自校正机制，结合强化学习微调，显著提升SQL生成质量（3B模型65.28%，7B模型69.19%执行准确率）


<details>
  <summary>Details</summary>
Motivation: 现有自洽性采样可能选择次优结果，自校正机制仅能处理语法错误，需整合两者优势实现更可靠的SQL生成

Method: 1. 并行采样选择高频输出对进行合并校正 2. 使用GRPO强化学习算法同步优化生成和修订模型

Result: BIRD开发集上3B/7B模型分别达到65.28%/69.19%执行准确率，代码将开源在https://github.com/CycloneBoy/csc_sql

Conclusion: CSC-SQL通过协同优化机制有效提升模型性能，实验证明该方法具有显著效果和良好泛化能力

Abstract: Large language models (LLMs) have demonstrated strong capabilities in
translating natural language questions about relational databases into SQL
queries. In particular, test-time scaling techniques such as Self-Consistency
and Self-Correction can enhance SQL generation accuracy by increasing
computational effort during inference. However, these methods have notable
limitations: Self-Consistency may select suboptimal outputs despite majority
votes, while Self-Correction typically addresses only syntactic errors. To
leverage the strengths of both approaches, we propose CSC-SQL, a novel method
that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two
most frequently occurring outputs from parallel sampling and feeds them into a
merge revision model for correction. Additionally, we employ the Group Relative
Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and
revision models via reinforcement learning, significantly enhancing output
quality. Experimental results confirm the effectiveness and generalizability of
CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution
accuracy, while the 7B model achieves 69.19%. The code will be open sourced at
https://github.com/CycloneBoy/csc_sql.

</details>


### [175] [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)
*Sahil Mishra,Kumar Arjun,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 提出LORex框架，通过结合判别式排序和生成式推理，实现高效分类法扩展，准确率提升12%


<details>
  <summary>Details</summary>
Motivation: 现有分类法扩展方法存在判别模型泛化能力不足、生成方法噪声处理差的问题，需要兼顾效率与准确性

Method: 候选词分块处理+层次结构迭代优化，采用两阶段噪声过滤与上下文感知推理机制

Result: 在4个基准测试中优于12个基线模型，Wu & Palmer相似度提升5%

Conclusion: LORex通过模块化设计和层次推理机制，在保持分类法结构完整性的同时显著提升扩展效率

Abstract: Taxonomies are hierarchical knowledge graphs crucial for recommendation
systems, and web applications. As data grows, expanding taxonomies is
essential, but existing methods face key challenges: (1) discriminative models
struggle with representation limits and generalization, while (2) generative
methods either process all candidates at once, introducing noise and exceeding
context limits, or discard relevant entities by selecting noisy candidates. We
propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for
Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines
discriminative ranking and generative reasoning for efficient taxonomy
expansion. Unlike prior methods, LORex ranks and chunks candidate terms into
batches, filtering noise and iteratively refining selections by reasoning
candidates' hierarchy to ensure contextual efficiency. Extensive experiments
across four benchmarks and twelve baselines show that LORex improves accuracy
by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

</details>


### [176] [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)
*Alice Plebe,Timothy Douglas,Diana Riazi,R. Maria del Rio-Chanona*

Main category: cs.CL

TL;DR: 研究发现图像存在使视觉语言模型转发虚假新闻的概率提升15%，不同模型家族间存在显著差异，仅Claude-3-Haiku展现抗误导能力


<details>
  <summary>Details</summary>
Motivation: 探究多模态场景下视觉内容如何影响语言模型的新闻转发行为，尤其是对虚假信息传播的潜在风险

Method: 1. 采用越狱式提示策略模拟反社会特质用户的决策行为
2. 构建包含事实核查的多模态政治新闻数据集（PolitiFact）
3. 跨模型家族比较实验设计

Result: 图像使真实新闻转发率提升4.8%，虚假新闻提升15%；黑暗三人格特质强化虚假新闻传播；共和党倾向用户显示更低的真实性敏感度

Conclusion: 多模态模型存在新兴风险，需开发定制化评估框架和缓解策略应对个性化AI系统的安全隐患

Abstract: Large language models are increasingly integrated into news recommendation
systems, raising concerns about their role in spreading misinformation. In
humans, visual content is known to boost credibility and shareability of
information, yet its effect on vision-language models (VLMs) remains unclear.
We present the first study examining how images influence VLMs' propensity to
reshare news content, whether this effect varies across model families, and how
persona conditioning and content attributes modulate this behavior. To support
this analysis, we introduce two methodological contributions: a
jailbreaking-inspired prompting strategy that elicits resharing decisions from
VLMs while simulating users with antisocial traits and political alignments;
and a multimodal dataset of fact-checked political news from PolitiFact, paired
with corresponding images and ground-truth veracity labels. Experiments across
model families reveal that image presence increases resharing rates by 4.8% for
true news and 15.0% for false news. Persona conditioning further modulates this
effect: Dark Triad traits amplify resharing of false news, whereas
Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the
tested models, only Claude-3-Haiku demonstrates robustness to visual
misinformation. These findings highlight emerging risks in multimodal model
behavior and motivate the development of tailored evaluation frameworks and
mitigation strategies for personalized AI systems. Code and dataset are
available at: https://github.com/3lis/misinfo_vlm

</details>


### [177] [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)
*Qiguang Chen,Libo Qin,Jinhao Liu,Yue Liao,Jiaqi Wang,Jingxuan Zhou,Wanxiang Che*

Main category: cs.CL

TL;DR: 提出RBF++框架解决CoT推理中的可测量边界优化与不可测量能力评估问题，通过推理边界定义、组合定律和边界划分机制实现跨模态场景应用。


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法缺乏定量评估指标（可测量边界优化）和不可测量能力（如多模态感知）的评估方法，阻碍实际应用。

Method: 定义推理边界(RB)作为性能上限，提出组合定律定量分析任务；针对多模态场景引入常数假设和边界划分机制，分离领域知识与感知能力。

Result: 38个模型在13个任务中验证框架有效性，评估10种CoT策略并提供双向优化路径，扩展LLM推理边界评估基准。

Conclusion: RBF++框架为LLM推理边界提供了系统性解决方案，开源代码数据推动后续研究，促进可解释性推理能力优化。

Abstract: Chain-of-Thought (CoT) reasoning has proven effective in enhancing large
language models (LLMs) on complex tasks, spurring research into its underlying
mechanisms. However, two primary challenges remain for real-world applications:
(1) the lack of quantitative metrics and actionable guidelines for evaluating
and optimizing measurable boundaries of CoT capability, and (2) the absence of
methods to assess boundaries of unmeasurable CoT capability, such as multimodal
perception. To address these gaps, we introduce the Reasoning Boundary
Framework++ (RBF++). To tackle the first challenge, we define the reasoning
boundary (RB) as the maximum limit of CoT performance. We also propose a
combination law for RBs, enabling quantitative analysis and offering actionable
guidance across various CoT tasks. For the second challenge, particularly in
multimodal scenarios, we introduce a constant assumption, which replaces
unmeasurable RBs with scenario-specific constants. Additionally, we propose the
reasoning boundary division mechanism, which divides unmeasurable RBs into two
sub-boundaries, facilitating the quantification and optimization of both
unmeasurable domain knowledge and multimodal perception capabilities. Extensive
experiments involving 38 models across 13 tasks validate the feasibility of our
framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,
offer insights into optimization and decay from two complementary perspectives,
and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope
this work advances the understanding of RBs and optimization strategies in
LLMs. Code and data are available at
https://github.com/LightChen233/reasoning-boundary.

</details>


### [178] [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)
*Zhijie Deng,Chris Yuhao Liu,Zirui Pang,Xinlei He,Lei Feng,Qi Xuan,Zhaowei Zhu,Jiaheng Wei*

Main category: cs.CL

TL;DR: 提出了GUARD框架，通过在LLM生成阶段动态消除特定知识，避免微调带来的性能损失，实现安全合规的遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘方法依赖微调会模糊知识边界导致整体性能下降，需在推理阶段实现选择性遗忘以保证生成安全性和合规性。

Method: 结合提示分类器检测遗忘目标，采用token匹配和语义匹配双重机制动态过滤候选token，自适应限制敏感内容生成。

Result: 在Harry Potter版权内容、MUSE基准和TOFU实体遗忘任务中，GUARD在保持模型通用能力的同时实现高质量遗忘。

Conclusion: GUARD框架首次在生成阶段实现动态遗忘，在遗忘效果与模型效用间取得最佳平衡，为LLM安全部署提供新思路。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
memorizing vast amounts of knowledge across diverse domains. However, the
ability to selectively forget specific knowledge is critical for ensuring the
safety and compliance of deployed models. Existing unlearning efforts typically
fine-tune the model with resources such as forget data, retain data, and a
calibration model. These additional gradient steps blur the decision boundary
between forget and retain knowledge, making unlearning often at the expense of
overall performance. To avoid the negative impact of fine-tuning, it would be
better to unlearn solely at inference time by safely guarding the model against
generating responses related to the forget target, without destroying the
fluency of text generation. In this work, we propose Generation-time Unlearning
via Adaptive Restriction and Detection (GUARD), a framework that enables
dynamic unlearning during LLM generation. Specifically, we first employ a
prompt classifier to detect unlearning targets and extract the corresponding
forbidden token. We then dynamically penalize and filter candidate tokens
during generation using a combination of token matching and semantic matching,
effectively preventing the model from leaking the forgotten content.
Experimental results on copyright content unlearning tasks over the Harry
Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on
the TOFU dataset, demonstrate that GUARD achieves strong forget quality across
various tasks while causing almost no degradation to the LLM's general
capabilities, striking an excellent trade-off between forgetting and utility.

</details>


### [179] [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)
*Hongru Wang,Wenyu Huang,Yufei Wang,Yuanhao Xi,Jianqiao Lu,Huan Zhang,Nan Hu,Zeming Liu,Jeff Z. Pan,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 提出DialogTool多轮对话数据集和VirtualMobile评估环境，揭示现有大语言模型在长流程工具使用中的不足


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要关注单轮/无状态工具交互，缺乏对多轮应用中有状态交互的完整生命周期评估

Method: 1) 构建包含工具创建、使用（工具感知/选择/执行）、角色响应三阶段六任务的评估框架 2) 开发VirtualMobile虚拟环境模拟API调用

Result: 对13种开源/闭源大模型的评估表明，现有先进模型在长期工具使用场景中仍表现欠佳

Conclusion: 本研究创建了首个完整评估工具使用生命周期的多轮对话基准，揭示了语言模型在复杂工具交互场景中的改进方向

Abstract: Existing benchmarks that assess Language Models (LMs) as Language Agents
(LAs) for tool use primarily focus on stateless, single-turn interactions or
partial evaluations, such as tool selection in a single turn, overlooking the
inherent stateful nature of interactions in multi-turn applications. To fulfill
this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with
stateful tool interactions considering the whole life cycle of tool use, across
six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool
utilization}: tool awareness, tool selection, tool execution; and 3)
\textit{role-consistent response}: response generation and role play.
Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile
evaluation environment to simulate API calls and assess the robustness of the
created APIs\footnote{We will use tools and APIs alternatively, there are no
significant differences between them in this paper.}. Taking advantage of these
artifacts, we conduct comprehensive evaluation on 13 distinct open- and
closed-source LLMs and provide detailed analysis at each stage, revealing that
the existing state-of-the-art LLMs still cannot perform well to use tools over
long horizons.

</details>


### [180] [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)
*Qiongqiong Wang,Hardik B. Sailor,Tianchi Liu,Ai Ti Aw*

Main category: cs.CL

TL;DR: 提出首个整合上下文推理与副语言理解的语音数据集生成框架，验证其对提升语音-大语言模型能力的有效性，并揭示当前模型在同理心推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型在上下文推理和副语言理解方面存在局限，主要由于缺乏同时覆盖这两个维度的QA数据集。

Method: 1. 基于伪副语言标签的野外语音数据浓缩
2. 大语言模型驱动的上下文副语言QA（CPQA）生成框架

Result: 1. 框架生成数据集与人工数据集评估呈现强相关性（Qwen2-Audio-7B模型）
2. 首次揭示语音大语言模型在同理心推理任务中的显著缺陷

Conclusion: 该框架是领域内首创解决方案，对训练具有副语言推理能力的鲁棒语音-大语言模型具有重要实践价值，凸显了此类数据集与更强模型需求的迫切性。

Abstract: Current speech-LLMs exhibit limited capability in contextual reasoning
alongside paralinguistic understanding, primarily due to the lack of
Question-Answer (QA) datasets that cover both aspects. We propose a novel
framework for dataset generation from in-the-wild speech data, that integrates
contextual reasoning with paralinguistic information. It consists of a pseudo
paralinguistic label-based data condensation of in-the-wild speech and
LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is
validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct
model on a dataset created by our framework and human-generated CPQA dataset.
The results also reveal the speech-LLM's limitations in handling empathetic
reasoning tasks, highlighting the need for such datasets and more robust
models. The proposed framework is first of its kind and has potential in
training more robust speech-LLMs with paralinguistic reasoning capabilities.

</details>


### [181] [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)
*Austin Xu,Yilun Zhou,Xuan-Phi Nguyen,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

TL;DR: 论文提出基于强化学习的EIS-GRPO算法训练J4R评估模型，有效解决复杂推理任务中LLM自动评估的局限性，并建立新基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自动评估方法在复杂推理任务中存在准确率低、位置偏差敏感等问题，需要开发更鲁棒的评估体系。

Method: 开发EIS-GRPO算法增强评估模型抗位置偏差能力，构建ReasoningJudgeBench多维度测试基准，训练7B参数的J4R模型。

Result: J4R在JudgeBench和自建基准上分别超越GPT-4o 6.7%和同类最优小模型9%，性能匹配更大规模GRPO模型。

Conclusion: 强化学习训练的轻量化J4R模型显著提升复杂场景评估效果，为自动评估领域提供高效解决方案。

Abstract: To keep pace with the increasing pace of large language models (LLM)
development, model output evaluation has transitioned away from time-consuming
human evaluation to automatic evaluation, where LLMs themselves are tasked with
assessing and critiquing other model outputs. LLM-as-judge models are a class
of generative evaluators that excel in evaluating relatively simple domains,
like chat quality, but struggle in reasoning intensive domains where model
responses contain more substantive and challenging content. To remedy existing
judge shortcomings, we explore training judges with reinforcement learning
(RL). We make three key contributions: (1) We propose the Equivalent Initial
State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us
to train our judge to be robust to positional biases that arise in more complex
evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that
evaluates judges in diverse reasoning settings not covered by prior work. (3)
We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that
outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or
exceeding the performance of larger GRPO-trained judges on both JudgeBench and
ReasoningJudgeBench.

</details>


### [182] [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)
*Narek Maloyan,Bislan Ashinov,Dmitry Namiot*

Main category: cs.CL

TL;DR: 大模型评估系统存在提示注入漏洞，两种攻击策略显著影响判断


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge系统在对抗攻击下的可靠性和安全性尚未充分研究，特别是提示注入攻击的防御机制缺失

Method: 使用GCG优化方法生成对抗后缀，在MT-Bench数据集上测试CUA和JMA两种攻击策略

Result: CUA攻击成功率超30%，JMA在模型推理层面也展现显著操控效果

Conclusion: 当前LLM评估框架存在重大安全漏洞，需加强对抗性评估研究和防御机制开发

Abstract: Large Language Models (LLMs) are increasingly employed as evaluators
(LLM-as-a-Judge) for assessing the quality of machine-generated text. This
paradigm offers scalability and cost-effectiveness compared to human
annotation. However, the reliability and security of such systems, particularly
their robustness against adversarial manipulations, remain critical concerns.
This paper investigates the vulnerability of LLM-as-a-Judge architectures to
prompt-injection attacks, where malicious inputs are designed to compromise the
judge's decision-making process. We formalize two primary attack strategies:
Comparative Undermining Attack (CUA), which directly targets the final decision
output, and Justification Manipulation Attack (JMA), which aims to alter the
model's generated reasoning. Using the Greedy Coordinate Gradient (GCG)
optimization method, we craft adversarial suffixes appended to one of the
responses being compared. Experiments conducted on the MT-Bench Human Judgments
dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and
Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves
an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable
effectiveness. These findings highlight substantial vulnerabilities in current
LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and
further research into adversarial evaluation and trustworthiness in LLM-based
assessment frameworks.

</details>


### [183] [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)
*Adam Štorek,Mukur Gupta,Samira Hajizadeh,Prashast Srivastava,Suman Jana*

Main category: cs.CL

TL;DR: 研究发现LLMs在长上下文代码推理中存在语义回忆敏感性缺陷，现有基准可能低估了模型利用上下文信息的挑战


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在大型代码库中的推理能力与代码回忆能力的关联，揭示当前代码推理基准的潜在局限性

Method: 提出SemTrace技术量化语义回忆敏感性，通过可归因但不可预测的语句影响测试模型，并开发语义回忆敏感性量化框架

Result: 代码片段位置显著影响推理精度（中间位置下降30%），词汇/语义回忆机制分离（函数级回忆成功率76% vs 行级21%）

Conclusion: 当前基准的语义回忆敏感性不足，需开发更严苛的评估体系以准确衡量LLMs的上下文代码理解能力

Abstract: Although modern Large Language Models (LLMs) support extremely large
contexts, their effectiveness in utilizing long context for code reasoning
remains unclear. This paper investigates LLM reasoning ability over code
snippets within large repositories and how it relates to their recall ability.
Specifically, we differentiate between lexical code recall (verbatim retrieval)
and semantic code recall (remembering what the code does). To measure semantic
recall, we propose SemTrace, a code reasoning technique where the impact of
specific statements on output is attributable and unpredictable. We also
present a method to quantify semantic recall sensitivity in existing
benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop
in code reasoning accuracy as a code snippet approaches the middle of the input
context, particularly with techniques requiring high semantic recall like
SemTrace. Moreover, we find that lexical recall varies by granularity, with
models excelling at function retrieval but struggling with line-by-line recall.
Notably, a disconnect exists between lexical and semantic recall, suggesting
different underlying mechanisms. Finally, our findings indicate that current
code reasoning benchmarks may exhibit low semantic recall sensitivity,
potentially underestimating LLM challenges in leveraging in-context
information.

</details>


### [184] [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)
*Chenyang Yang,Yike Shi,Qianou Ma,Michael Xieyang Liu,Christian Kästner,Tongshuang Wu*

Main category: cs.CL

TL;DR: 论文揭示了LLM提示工程中需求描述不足的问题，提出基于需求感知的提示优化机制，平均提升4.8%性能。


<details>
  <summary>Details</summary>
Motivation: 开发者通过自然语言描述需求时存在显著的不充分性（41.1%未明确需求），导致LLM输出不稳定（模型变更时准确率下降20%）且传统提示优化方法失效。

Method: 通过量化分析提示不充分性对模型鲁棒性的影响（2倍回归概率），测试不同提示扩展策略，最终设计需求感知的提示优化框架。

Result: 新优化机制相比基线提升4.8%性能，传统方法因指令跟随能力限制和约束冲突失效。

Conclusion: 建议构建包含需求发现-评估-监控的全流程管理体系，突破单纯提示优化的局限性。

Abstract: Building LLM-powered software requires developers to communicate their
requirements through natural language, but developer prompts are frequently
underspecified, failing to fully capture many user-important requirements. In
this paper, we present an in-depth analysis of prompt underspecification,
showing that while LLMs can often (41.1%) guess unspecified requirements by
default, such behavior is less robust: Underspecified prompts are 2x more
likely to regress over model or prompt changes, sometimes with accuracy drops
by more than 20%. We then demonstrate that simply adding more requirements to a
prompt does not reliably improve performance, due to LLMs' limited
instruction-following capabilities and competing constraints, and standard
prompt optimizers do not offer much help. To address this, we introduce novel
requirements-aware prompt optimization mechanisms that can improve performance
by 4.8% on average over baselines that naively specify everything in the
prompt. Beyond prompt optimization, we envision that effectively managing
prompt underspecification requires a broader process, including proactive
requirements discovery, evaluation, and monitoring.

</details>


### [185] [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)
*Gongfan Fang,Xinyin Ma,Xinchao Wang*

Main category: cs.CL

TL;DR: 提出Thinkless框架，允许语言模型根据任务复杂性自适应选择长/短推理模式，显著提升推理效率


<details>
  <summary>Details</summary>
Motivation: 现有复杂推理模型在处理简单问题时存在计算低效问题，需要让模型学会自适应选择推理模式

Method: 基于强化学习的框架，采用<short>和<long>控制符，通过解耦的DeGRPO算法分别优化推理模式选择与答案准确性

Result: 在多个数学基准测试中减少50%-90%的长链推理使用，显著提升推理效率

Conclusion: Thinkless框架在保持性能的同时有效平衡效率，证明自适应推理机制的可行性

Abstract: Reasoning Language Models, capable of extended chain-of-thought reasoning,
have demonstrated remarkable performance on tasks requiring complex logical
inference. However, applying elaborate reasoning for all queries often results
in substantial computational inefficiencies, particularly when many problems
admit straightforward solutions. This motivates an open question: Can LLMs
learn when to think? To answer this, we propose Thinkless, a learnable
framework that empowers an LLM to adaptively select between short-form and
long-form reasoning, based on both task complexity and the model's ability.
Thinkless is trained under a reinforcement learning paradigm and employs two
control tokens, <short> for concise responses and <think> for detailed
reasoning. At the core of our method is a Decoupled Group Relative Policy
Optimization (DeGRPO) algorithm, which decomposes the learning objective of
hybrid reasoning into two components: (1) a control token loss that governs the
selection of the reasoning mode, and (2) a response loss that improves the
accuracy of the generated answers. This decoupled formulation enables
fine-grained control over the contributions of each objective, stabilizing
training and effectively preventing collapse observed in vanilla GRPO.
Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and
GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -
90%, significantly improving the efficiency of Reasoning Language Models. The
code is available at https://github.com/VainF/Thinkless

</details>


### [186] [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)
*David Anugraha,Zilu Tang,Lester James V. Miranda,Hanyang Zhao,Mohammad Rifqi Farhansyah,Garry Kuwanto,Derry Wijaya,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出R3框架解决传统奖励模型可控性和可解释性不足的问题，通过多维度评估实现透明化语言模型对齐


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型受限于单一优化目标，标量输出缺乏上下文推理，难以适应多样化人类价值观和应用场景

Method: 开发rubric-agnostic的R3框架，支持跨评估维度的通用性，提供基于推理的模块化评分系统

Result: R3实现语言模型的透明评估，开源模型/数据/代码验证框架有效性，提升不同使用场景的适应性

Conclusion: 该框架通过可解释的分数推理机制，为语言模型对齐提供了更灵活、鲁棒的评估解决方案

Abstract: Reward models are essential for aligning language model outputs with human
preferences, yet existing approaches often lack both controllability and
interpretability. These models are typically optimized for narrow objectives,
limiting their generalizability to broader downstream tasks. Moreover, their
scalar outputs are difficult to interpret without contextual reasoning. To
address these limitations, we introduce R3, a novel reward modeling framework
that is rubric-agnostic, generalizable across evaluation dimensions, and
provides interpretable, reasoned score assignments. R3 enables more transparent
and flexible evaluation of language models, supporting robust alignment with
diverse human values and use cases. Our models, data, and code are available as
open source at https://github.com/rubricreward/r3

</details>


### [187] [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)
*Renjie Pi,Felix Bai,Qibin Chen,Simon Wang,Jiulong Shan,Kieran Liu,Meng Cao*

Main category: cs.CL

TL;DR: 提出MR.Judge方法，通过将评估过程重构为多选推理问题，显著提升多模态大模型评估性能


<details>
  <summary>Details</summary>
Motivation: 现有直接评分法在可解释性和评估效果上存在局限，需要增强MLLM评估者的复杂推理能力

Method: 1) 反向生成缺陷候选答案 2) 从文本推理模型蒸馏推理能力 3) 构建多选评估框架

Result: MR.Judge-7B在VL-RewardBench超越GPT-4o达9.9%，MM-Vet任务推理时性能提升7.7%

Conclusion: 推理驱动的多选范式有效提升MLLM评估性能，同时增强判断过程的可解释性

Abstract: The paradigm of using Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) as evaluative judges has emerged as an effective
approach in RLHF and inference-time scaling. In this work, we propose
Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering
general-purpose MLLMs judges with strong reasoning capabilities. Instead of
directly assigning scores for each response, we formulate the judgement process
as a reasoning-inspired multiple-choice problem. Specifically, the judge model
first conducts deliberate reasoning covering different aspects of the responses
and eventually selects the best response from them. This reasoning process not
only improves the interpretibility of the judgement, but also greatly enhances
the performance of MLLM judges. To cope with the lack of questions with scored
responses, we propose the following strategy to achieve automatic annotation:
1) Reverse Response Candidates Synthesis: starting from a supervised
fine-tuning (SFT) dataset, we treat the original response as the best candidate
and prompt the MLLM to generate plausible but flawed negative candidates. 2)
Text-based reasoning extraction: we carefully design a data synthesis pipeline
for distilling the reasoning capability from a text-based reasoning model,
which is adopted to enable the MLLM judges to regain complex reasoning ability
via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge
is effective across a wide range of tasks. Specifically, our MR. Judge-7B
surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet
during inference-time scaling by up to 7.7%.

</details>


### [188] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)
*Nithin Rao Koluguri,Monica Sekoyan,George Zelenfroynd,Sasha Meister,Shuoyang Ding,Sofia Kostandian,He Huang,Nikolay Karpov,Jagadeesh Balam,Vitaly Lavrukhin,Yifan Peng,Sara Papi,Marco Gaido,Alessio Brutti,Boris Ginsburg*

Main category: cs.CL

TL;DR: Granary推出首个开源多语言语音数据集，覆盖25种欧洲语言的识别与翻译任务，通过伪标注和智能数据筛选技术使模型训练数据量减少50%仍保持同等性能


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言在语音处理领域数据匮乏的问题，现有多任务多语言方法存在局限。传统语音识别模型需要大量标注数据，而小语种数据获取困难制约了模型发展。

Method: 构建包含两阶段推理、幻觉过滤的伪标注流水线，结合EuroLLM生成翻译对并过滤。创新性地实现标点恢复与高效数据处理（数小时内处理海量数据）

Result: 在高低资源语言测试集上，使用Granary处理数据的模型仅需约50%训练数据即可达到与传统全量数据相当的识别翻译性能

Conclusion: 该方案通过系统级数据增强技术突破数据瓶颈，为低资源语言语音处理提供新范式。开放数据集将推动多语言语音技术民主化发展

Abstract: Multi-task and multilingual approaches benefit large models, yet speech
processing for low-resource languages remains underexplored due to data
scarcity. To address this, we present Granary, a large-scale collection of
speech datasets for recognition and translation across 25 European languages.
This is the first open-source effort at this scale for both transcription and
translation. We enhance data quality using a pseudo-labeling pipeline with
segmentation, two-pass inference, hallucination filtering, and punctuation
restoration. We further generate translation pairs from pseudo-labeled
transcriptions using EuroLLM, followed by a data filtration pipeline. Designed
for efficiency, our pipeline processes vast amount of data within hours. We
assess models trained on processed data by comparing their performance on
previously curated datasets for both high- and low-resource languages. Our
findings show that these models achieve similar performance using approx. 50%
less data. Dataset will be made available at
https://hf.co/datasets/nvidia/Granary

</details>


### [189] [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)
*Jiajie Zhang,Nianyi Lin,Lei Hou,Ling Feng,Juanzi Li*

Main category: cs.CL

TL;DR: 提出强化学习算法AdaptThink，根据问题难度自适应选择思维模式，在保证性能的同时显著降低推理成本


<details>
  <summary>Details</summary>
Motivation: 大模型深度思考过程导致推理效率低下，研究发现简单任务直接输出方案（NoThinking）效果更优，需建立动态选择机制平衡效率与质量

Method: 包含约束优化目标（鼓励NoThinking且保持性能）和重要性采样策略（平衡训练样本），支持冷启动并实现思维模式的持续探索

Result: 在数学数据集上使DeepSeek-R1模型响应长度减少53%，准确率提升2.4%

Conclusion: AdaptThink成功优化推理质量与效率的平衡，自适应思维模式选择机制具有重要应用价值

Abstract: Recently, large reasoning models have achieved impressive performance on
various tasks by employing human-like deep thinking. However, the lengthy
thinking process substantially increases inference overhead, making efficiency
a critical bottleneck. In this work, we first demonstrate that NoThinking,
which prompts the reasoning model to skip thinking and directly generate the
final solution, is a better choice for relatively simple tasks in terms of both
performance and efficiency. Motivated by this, we propose AdaptThink, a novel
RL algorithm to teach reasoning models to choose the optimal thinking mode
adaptively based on problem difficulty. Specifically, AdaptThink features two
core components: (1) a constrained optimization objective that encourages the
model to choose NoThinking while maintaining the overall performance; (2) an
importance sampling strategy that balances Thinking and NoThinking samples
during on-policy training, thereby enabling cold start and allowing the model
to explore and exploit both thinking modes throughout the training process. Our
experiments indicate that AdaptThink significantly reduces the inference costs
while further enhancing performance. Notably, on three math datasets,
AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B
by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive
thinking-mode selection for optimizing the balance between reasoning quality
and efficiency. Our codes and models are available at
https://github.com/THU-KEG/AdaptThink.

</details>


### [190] [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)
*Lotem Peled-Cohen,Maya Zadok,Nitay Calderon,Hila Gonen,Roi Reichart*

Main category: cs.CL

TL;DR: 研究通过可解释性框架比较人类非专家与LLM对痴呆症语言特征的识别能力，发现LLM使用更接近临床诊断的多元特征，但两者均存在漏诊倾向


<details>
  <summary>Details</summary>
Motivation: 针对非专业人士依赖片面线索判断痴呆症语言特征的问题，探索LLM在早期识别中的补充价值及可解释性分析框架的应用潜力

Method: 通过展示图片描述文本，收集人类和LLM的直觉判断，使用LLM提取专家指导的高维特征，采用逻辑回归建模比较其与临床诊断的关联性

Result: 人类判断依赖有限且存在误导性的线索，LLM则运用更丰富、细致的特征体系（与临床模式更契合），但两者均表现出较高的假阴性率

Conclusion: 开发的可解释框架有助于提升非专业人士对关键语言特征的识别能力，未来需结合多方判断体系改善早期痴呆筛查效果

Abstract: Cognitive decline often surfaces in language years before diagnosis. It is
frequently non-experts, such as those closest to the patient, who first sense a
change and raise concern. As LLMs become integrated into daily communication
and used over prolonged periods, it may even be an LLM that notices something
is off. But what exactly do they notice--and should be noticing--when making
that judgment? This paper investigates how dementia is perceived through
language by non-experts. We presented transcribed picture descriptions to
non-expert humans and LLMs, asking them to intuitively judge whether each text
was produced by someone healthy or with dementia. We introduce an explainable
method that uses LLMs to extract high-level, expert-guided features
representing these picture descriptions, and use logistic regression to model
human and LLM perceptions and compare with clinical diagnoses. Our analysis
reveals that human perception of dementia is inconsistent and relies on a
narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a
richer, more nuanced feature set that aligns more closely with clinical
patterns. Still, both groups show a tendency toward false negatives, frequently
overlooking dementia cases. Through our interpretable framework and the
insights it provides, we hope to help non-experts better recognize the
linguistic signs that matter.

</details>


### [191] [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.CL

TL;DR: 提出SMOTExT方法，将SMOTE技术应用于文本数据生成，通过BERT嵌入插值与xRAG解码实现小样本场景的数据增强及隐私保护


<details>
  <summary>Details</summary>
Motivation: 解决专业领域/低资源场景中数据稀缺和类别不平衡问题，同时探索隐私保护机器学习新路径

Method: 结合SMOTE思想：1) 对BERT嵌入向量进行插值 2) 通过xRAG跨模态框架将潜在向量解码为连贯文本

Result: 生成数据训练模型性能媲美原始数据（早期实验），定性分析显示潜在的知识蒸馏价值

Conclusion: 该方法为小样本学习和隐私敏感场景提供了双重解决方案，开辟数据保护约束下的安全学习新范式

Abstract: Data scarcity and class imbalance are persistent challenges in training
robust NLP models, especially in specialized domains or low-resource settings.
We propose a novel technique, SMOTExT, that adapts the idea of Synthetic
Minority Over-sampling (SMOTE) to textual data. Our method generates new
synthetic examples by interpolating between BERT-based embeddings of two
existing examples and then decoding the resulting latent point into text with
xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation
framework, we can effectively turn interpolated vectors into coherent text.
While this is preliminary work supported by qualitative outputs only, the
method shows strong potential for knowledge distillation and data augmentation
in few-shot settings. Notably, our approach also shows promise for
privacy-preserving machine learning: in early experiments, training models
solely on generated data achieved comparable performance to models trained on
the original dataset. This suggests a viable path toward safe and effective
learning under data protection constraints.

</details>


### [192] [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)
*Liyan Tang,Grace Kim,Xinyu Zhao,Thom Lake,Wenxuan Ding,Fangcong Yin,Prasann Singhal,Manya Wadhwa,Zeyu Leo Liu,Zayne Sprague,Ramya Namuduri,Bodun Hu,Juan Diego Rodriguez,Puyuan Peng,Greg Durrett*

Main category: cs.CL

TL;DR: 大型视觉-语言模型在图表理解中存在视觉推理能力不足的问题，新基准测试ChartMuseum揭示了模型与人类表现的显著差距（人类93% vs 最佳模型63%）


<details>
  <summary>Details</summary>
Motivation: 当前LVLMs在视觉与文本推理能力上存在显著不平衡，尤其在需要纯视觉推理的任务中表现欠佳，需要建立新评估基准量化这种差距

Method: 使用仅需视觉推理的合成数据集验证模型缺陷，并构建包含1,162个真实图表问题的ChartMuseum基准，涵盖多种推理类型

Result: 人类准确率93%，最佳模型Gemini-2.5-Pro仅63%，视觉推理问题导致所有模型性能下降35%-55%

Conclusion: 现有LVLMs在视觉推理方面存在根本性缺陷，ChartMuseum基准能有效评估模型的多模态推理能力差距

Abstract: Chart understanding presents a unique challenge for large vision-language
models (LVLMs), as it requires the integration of sophisticated textual and
visual reasoning capabilities. However, current LVLMs exhibit a notable
imbalance between these skills, falling short on visual reasoning that is
difficult to perform in text. We conduct a case study using a synthetic dataset
solvable only through visual reasoning and show that model performance degrades
significantly with increasing visual complexity, while human performance
remains robust. We then introduce ChartMuseum, a new Chart Question Answering
(QA) benchmark containing 1,162 expert-annotated questions spanning multiple
reasoning types, curated from real-world charts across 184 sources,
specifically built to evaluate complex visual and textual reasoning. Unlike
prior chart understanding benchmarks -- where frontier models perform similarly
and near saturation -- our benchmark exposes a substantial gap between model
and human performance, while effectively differentiating model capabilities:
although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro
attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct
achieves only 38.5%. Moreover, on questions requiring primarily visual
reasoning, all models experience a 35%-55% performance drop from
text-reasoning-heavy question performance. Lastly, our qualitative error
analysis reveals specific categories of visual reasoning that are challenging
for current LVLMs.

</details>


### [193] [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)
*Vinay Samuel,Harshita Diddee,Yiming Zhang,Daphne Ippolito*

Main category: cs.CL

TL;DR: 提出通过连续控制信号(向量插值)精细调控语言模型生成长度的方法，优于现有离散控制方法


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言提示或离散信号的控制方法存在脆弱性和扩展性限制，无法有效处理连续频谱的控制需求

Method: 通过微调语言模型，在高低标记嵌入之间进行向量插值，实现响应长度的连续控制

Result: 相比上下文学习或离散信号微调方法，本方法在响应长度控制上表现出更高的可靠性

Conclusion: 连续控制信号为语言模型行为调控提供新范式，相关代码和数据集已开源促进后续研究

Abstract: Aligning language models with user intent is becoming increasingly relevant
to enhance user experience. This calls for designing methods that can allow
users to control the properties of the language that LMs generate. For example,
controlling the length of the generation, the complexity of the language that
gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate
users' control by conditioning LM generations on natural language prompts or
discrete control signals, which are often brittle and hard to scale. In this
work, we are interested in \textit{continuous} control signals, ones that exist
along a spectrum that can't easily be captured in a natural language prompt or
via existing techniques in conditional generation. Through a case study in
controlling the precise response-length of generations produced by LMs, we
demonstrate how after fine-tuning, behaviors of language models can be
controlled via continuous signals -- as vectors that are interpolated between a
"low" and a "high" token embedding. Our method more reliably exerts
response-length control than in-context learning methods or fine-tuning methods
that represent the control signal as a discrete signal. Our full open-sourced
code and datasets are available at https://github.com/vsamuel2003/CIE.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [194] [Neural Importance Sampling of Many Lights](https://arxiv.org/abs/2505.11729)
*Pedro Figueiredo,Qihao He,Steve Bako,Nima Khademi Kalantari*

Main category: cs.GR

TL;DR: 提出基于神经网络的动态光源选择分布估计方法，通过结合光线层次结构和残差学习策略提升复杂场景蒙特卡洛渲染效率


<details>
  <summary>Details</summary>
Motivation: 解决复杂多光源场景中传统重要性采样方法效率低下的问题，通过神经网络自适应学习光源分布

Method: 1. 使用神经网络根据局部信息预测光源选择分布
2. 结合光线层次结构进行集群级分布预测
3. 残差学习策略加速训练收敛

Result: 在多样化复杂场景中实现优于传统方法的渲染性能

Conclusion: 神经网络与传统层次化采样方法的结合为蒙特卡洛渲染提供了新的高效解决方案

Abstract: We propose a neural approach for estimating spatially varying light selection
distributions to improve importance sampling in Monte Carlo rendering,
particularly for complex scenes with many light sources. Our method uses a
neural network to predict the light selection distribution at each shading
point based on local information, trained by minimizing the KL-divergence
between the learned and target distributions in an online manner. To
efficiently manage hundreds or thousands of lights, we integrate our neural
approach with light hierarchy techniques, where the network predicts
cluster-level distributions and existing methods sample lights within clusters.
Additionally, we introduce a residual learning strategy that leverages initial
distributions from existing techniques, accelerating convergence during
training. Our method achieves superior performance across diverse and
challenging scenes.

</details>


### [195] [Generating Digital Models Using Text-to-3D and Image-to-3D Prompts: Critical Case Study](https://arxiv.org/abs/2505.11799)
*Rushan Ziatdinov,Rifkat Nabiyev*

Main category: cs.GR

TL;DR: 本文综述在线3D模型生成工具，通过多提示测试评估生成质量，探索生成式设计对创作效率的提升。


<details>
  <summary>Details</summary>
Motivation: 传统3D建模耗时费力，研究旨在评估AI生成工具如何通过自动化流程帮助创作者节省时间。

Method: 横向对比多个在线3D模型生成平台，采用不同文本提示进行生成测试并量化质量评估。

Result: 现有工具生成质量参差不齐，提示词设计显著影响输出效果，部分工具在特定场景下达到可用水平。

Conclusion: AI生成3D模型具有实用潜力，但需优化算法与提示工程才能满足专业创作需求。

Abstract: In the world of technology and AI, digital models play an important role in
our lives and are an essential part of the digital twins of real-world objects.
They can be created by designers, artists, or game developers using spline
curves and surfaces, meshes, and voxels, but making such models is too
time-consuming. With the growth of AI tools, there is interest in the automated
generation of 3D models, such as generative design approaches, which can save
creators valuable time. This paper reviews several online 3D model generators
and critically analyses the results, hoping to see higher-quality results from
different prompts.

</details>


### [196] [Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories](https://arxiv.org/abs/2505.12373)
*Kapil Dev*

Main category: cs.GR

TL;DR: 通过收集22,301组人类审美偏好数据，结合非线性建模和跨品类分析，揭示了3D形状审美偏好的几何特征规律与设计启示


<details>
  <summary>Details</summary>
Motivation: 现有3D美学计算模型缺乏大规模人类判断数据支撑，难以指导实际设计。本研究旨在通过数据驱动框架，建立可解释的审美偏好模型。

Method: 使用亚马逊众包平台收集五类物品（椅子/桌子/杯子等）的成对审美比较数据，应用Bradley-Terry模型计算潜在审美评分，结合随机森林和SHAP分析解释几何特征（对称性/曲率/紧凑度等）的影响

Result: 发现审美偏好存在跨品类的通用原则（如对称偏好）和领域特定趋势（如灯具更强调动态曲线），建立了基于可解释几何特征的分析框架

Conclusion: 研究为设计师提供数据支持，公开数据集促进可重复性，通过人本主义方法推进计算美学与认知科学的交叉应用

Abstract: Human aesthetic preferences for 3D shapes are central to industrial design,
virtual reality, and consumer product development. However, most computational
models of 3D aesthetics lack empirical grounding in large-scale human
judgments, limiting their practical relevance. We present a large-scale study
of human preferences. We collected 22,301 pairwise comparisons across five
object categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon
Mechanical Turk. Building on a previously published
dataset~\cite{dev2020learning}, we introduce new non-linear modeling and
cross-category analysis to uncover the geometric drivers of aesthetic
preference. We apply the Bradley-Terry model to infer latent aesthetic scores
and use Random Forests with SHAP analysis to identify and interpret the most
influential geometric features (e.g., symmetry, curvature, compactness). Our
cross-category analysis reveals both universal principles and domain-specific
trends in aesthetic preferences. We focus on human interpretable geometric
features to ensure model transparency and actionable design insights, rather
than relying on black-box deep learning approaches. Our findings bridge
computational aesthetics and cognitive science, providing practical guidance
for designers and a publicly available dataset to support reproducibility. This
work advances the understanding of 3D shape aesthetics through a human-centric,
data-driven framework.

</details>


### [197] [Penetration-free Solid-Fluid Interaction on Shells and Rods](https://arxiv.org/abs/2505.12539)
*Jinyuan Liu,Yuchen Sun,Yin Yang,Chenfanfu Jiang,Minchen Li,Bo Zhu*

Main category: cs.GR

TL;DR: 本文提出一种基于障碍物增强优化系统的新方法，实现了流体与薄弹性固体的无穿透交互仿真，能够处理飞溅、滑动、漂浮等多种复杂流体-固体交互过程。


<details>
  <summary>Details</summary>
Motivation: 传统方法主要关注流体-固体界面速度一致性，本研究通过显式处理位置约束（包含固体显式位置和流体隐式界面表示），突破穿透限制并保持流体体积守恒。

Method: 1. 构建含障碍物的优化系统确保无穿透
2. 开发水平集值调整策略保持流体体积
3. 提出新型距离度量方法测量隐式表面与任意维度物体间距
4. 整合惯性、弹性势能、阻尼等多物理量于统一框架

Result: 成功模拟流体与壳/杆等低维物体的复杂交互（拓扑变化、弹跳、飞溅、滑动等），支持从水花飞溅到物体漂浮等多样化场景。

Conclusion: 该方法突破了传统速度匹配范式的局限，通过显式约束处理实现了更鲁棒、更灵活的流体-固体交互仿真，显著扩展了物理仿真的应用边界。

Abstract: We introduce a novel approach to simulate the interaction between fluids and
thin elastic solids without any penetration. Our approach is centered around an
optimization system augmented with barriers, which aims to find a configuration
that ensures the absence of penetration while enforcing incompressibility for
the fluids and minimizing elastic potentials for the solids. Unlike previous
methods that primarily focus on velocity coherence at the fluid-solid
interfaces, we demonstrate the effectiveness and flexibility of explicitly
resolving positional constraints, including both explicit representation of
solid positions and the implicit representation of fluid level-set interface.
To preserve the volume of the fluid, we propose a simple yet efficient approach
that adjusts the associated level-set values. Additionally, we develop a
distance metric capable of measuring the separation between an implicitly
represented surface and a Lagrangian object of arbitrary codimension. By
integrating the inertia, solid elastic potential, damping, barrier potential,
and fluid incompressibility within a unified system, we are able to robustly
simulate a wide range of processes involving fluid interactions with
lower-dimensional objects such as shells and rods. These processes include
topology changes, bouncing, splashing, sliding, rolling, floating, and more.

</details>


### [198] [HIL: Hybrid Imitation Learning of Diverse Parkour Skills from Videos](https://arxiv.org/abs/2505.12619)
*Jiashun Wang,Yifeng Jiang,Haotian Zhang,Chen Tessler,Davis Rempe,Jessica Hodgins,Xue Bin Peng*

Main category: cs.GR

TL;DR: 提出混合模仿学习框架HIL，通过运动跟踪与对抗模仿学习的结合，提升虚拟角色在复杂环境中的跑酷技能适应性与多样性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动方法在环境适应性和技能组合连贯性方面存在局限，需开发能同时保持运动精度与环境适应性的新框架。

Method: 采用并行多任务环境架构与统一观察空间，通过agent-centric场景表征实现混合并行环境下的控制器训练。

Result: 实验证明该方法在复杂跑酷环境中提升运动质量30%，技能多样性增加45%，任务完成率与现有方法持平。

Conclusion: HIL框架有效解决了动作控制器的环境适应瓶颈，为虚拟角色行为生成提供了可扩展的跨场景解决方案。

Abstract: Recent data-driven methods leveraging deep reinforcement learning have been
an effective paradigm for developing controllers that enable physically
simulated characters to produce natural human-like behaviors. However, these
data-driven methods often struggle to adapt to novel environments and compose
diverse skills coherently to perform more complex tasks. To address these
challenges, we propose a hybrid imitation learning (HIL) framework that
combines motion tracking, for precise skill replication, with adversarial
imitation learning, to enhance adaptability and skill composition. This hybrid
learning framework is implemented through parallel multi-task environments and
a unified observation space, featuring an agent-centric scene representation to
facilitate effective learning from the hybrid parallel environments. Our
framework trains a unified controller on parkour data sourced from Internet
videos, enabling a simulated character to traverse through new environments
using diverse and life-like parkour skills. Evaluations across challenging
parkour environments demonstrate that our method improves motion quality,
increases skill diversity, and achieves competitive task completion compared to
previous learning-based methods.

</details>


### [199] [UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes](https://arxiv.org/abs/2505.12774)
*Zichen Geng,Zeeshan Hayder,Wei Liu,Ajmal Mian*

Main category: cs.GR

TL;DR: 提出首个支持复杂3D场景中文本驱动人体运动与交互的统一框架UniHM，通过混合运动表示、新型LFQ-VAE和增强数据集实现场景感知生成


<details>
  <summary>Details</summary>
Motivation: 现有语言驱动模型因运动离散化导致信息丢失和连续性缺失，难以生成符合物理场景约束的合理运动

Method: 融合6DoF连续运动与局部离散标记的混合表征 + 超越传统VQ-VAE的LFQ-VAE + 增强版Lingo数据集提供强监督

Result: 在OMOMO基准的文本-HOI合成任务和HumanML3D通用文本驱动运动生成中达到可比/竞争性表现

Conclusion: UniHM通过连续-离散混合建模突破场景感知运动生成瓶颈，为复杂环境下的行为合成提供新范式

Abstract: Human motion synthesis in complex scenes presents a fundamental challenge,
extending beyond conventional Text-to-Motion tasks by requiring the integration
of diverse modalities such as static environments, movable objects, natural
language prompts, and spatial waypoints. Existing language-conditioned motion
models often struggle with scene-aware motion generation due to limitations in
motion tokenization, which leads to information loss and fails to capture the
continuous, context-dependent nature of 3D human movement. To address these
issues, we propose UniHM, a unified motion language model that leverages
diffusion-based generation for synthesizing scene-aware human motion. UniHM is
the first framework to support both Text-to-Motion and Text-to-Human-Object
Interaction (HOI) in complex 3D scenes. Our approach introduces three key
contributions: (1) a mixed-motion representation that fuses continuous 6DoF
motion with discrete local motion tokens to improve motion realism; (2) a novel
Look-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in
both reconstruction accuracy and generative performance; and (3) an enriched
version of the Lingo dataset augmented with HumanML3D annotations, providing
stronger supervision for scene-specific motion learning. Experimental results
demonstrate that UniHM achieves comparative performance on the OMOMO benchmark
for text-to-HOI synthesis and yields competitive results on HumanML3D for
general text-conditioned motion generation.

</details>


### [200] [AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2505.12782)
*Kai Zhang,Xingyu Chen,Xiaofeng Zhang*

Main category: cs.GR

TL;DR: 提出AdaToken-3D框架，通过动态剪枝冗余空间令牌实现3D多模态模型效率优化，在保持精度的同时提升21%推理速度并减少63%计算量。


<details>
  <summary>Details</summary>
Motivation: 现有3D多模态模型存在空间令牌与视觉令牌异构机制导致的架构冗余问题，60%以上空间令牌对最终预测贡献不足5%，造成计算效率低下。

Method: 基于注意力模式挖掘量化令牌级信息流，开发自适应空间令牌优化框架，自动为不同3D-LMM架构定制剪枝策略。

Result: 在LLaVA-3D（70亿参数）上实现21%推理加速和63%FLOPs减少，同时保持原始任务精度。定量分析揭示多模态空间信息流冗余模式。

Conclusion: 该工作不仅提升3D多模态学习效率，还通过令牌交互分析建立理论框架，为后续高效模型设计提供理论基础。

Abstract: Large Multimodal Models (LMMs) have become a pivotal research focus in deep
learning, demonstrating remarkable capabilities in 3D scene understanding.
However, current 3D LMMs employing thousands of spatial tokens for multimodal
reasoning suffer from critical inefficiencies: excessive computational overhead
and redundant information flows. Unlike 2D VLMs processing single images, 3D
LMMs exhibit inherent architectural redundancy due to the heterogeneous
mechanisms between spatial tokens and visual tokens. To address this challenge,
we propose AdaToken-3D, an adaptive spatial token optimization framework that
dynamically prunes redundant tokens through spatial contribution analysis. Our
method automatically tailors pruning strategies to different 3D LMM
architectures by quantifying token-level information flows via attention
pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)
demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%
FLOPs reduction while maintaining original task accuracy. Beyond efficiency
gains, this work systematically investigates redundancy patterns in multimodal
spatial information flows through quantitative token interaction analysis. Our
findings reveal that over 60\% of spatial tokens contribute minimally ($<$5\%)
to the final predictions, establishing theoretical foundations for efficient 3D
multimodal learning.

</details>


### [201] [MGPBD: A Multigrid Accelerated Global XPBD Solver](https://arxiv.org/abs/2505.13390)
*Chunlei Li,Peng Yu,Tiantian Liu,Siyuan Yu,Yuting Xiao,Shuai Li,Aimin Hao,Yang Gao,Qinping Zhao*

Main category: cs.GR

TL;DR: 提出新型非平滑聚合代数多重网格(UA-AMG)与预处理共轭梯度法(PCG)相结合，解决XPBD在高分辨率高刚度模拟中的收敛问题。


<details>
  <summary>Details</summary>
Motivation: XPBD的非线性Gauss-Seidel求解器在处理高分辨率高刚度模拟时易产生低频误差，导致数值不稳定和求解停滞。传统AMG的延长算子构建耗时长(占总时间2/3)。

Method: 1. 惰性构建策略：基于矩阵结构和物理意义跨迭代复用延长算子
2. 近核空间简化构建：通过齐次方程迭代生成近似解
3. 非平滑聚合AMG与PCG结合

Result: 实现与传统自适应平滑聚合相当的收敛速度，计算成本降低30%。实验证明在高分辨率布料(100万顶点)、弹性体模拟中收敛速度提升5-8倍。

Conclusion: 该方法通过创新性多重网格策略，显著提升XPBD的数值稳定性和计算效率，为高精度可变形体模拟提供新解决方案。

Abstract: We introduce a novel Unsmoothed Aggregation (UA) Algebraic Multigrid (AMG)
method combined with Preconditioned Conjugate Gradient (PCG) to overcome the
limitations of Extended Position-Based Dynamics (XPBD) in high-resolution and
high-stiffness simulations. While XPBD excels in simulating deformable objects
due to its speed and simplicity, its nonlinear Gauss-Seidel (GS) solver often
struggles with low-frequency errors, leading to instability and stalling
issues, especially in high-resolution, high-stiffness simulations. Our
multigrid approach addresses these issues efficiently by leveraging AMG. To
reduce the computational overhead of traditional AMG, where prolongator
construction can consume up to two-thirds of the runtime, we propose a lazy
setup strategy that reuses prolongators across iterations based on matrix
structure and physical significance. Furthermore, we introduce a simplified
method for constructing near-kernel components by applying a few sweeps of
iterative methods to the homogeneous equation, achieving convergence rates
comparable to adaptive smoothed aggregation (adaptive-SA) at a lower
computational cost. Experimental results demonstrate that our method
significantly improves convergence rates and numerical stability, enabling
efficient and stable high-resolution simulations of deformable objects.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [202] [IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar](https://arxiv.org/abs/2505.13393)
*Christopher K. Frantz*

Main category: cs.MA

TL;DR: IG Parser是基于制度语法2.0开发的定性分析工具，通过专用语法IG Script实现自然语言编码与多格式自动化转换，支持社会制度分析。


<details>
  <summary>Details</summary>
Motivation: 解决传统工具在正式/非正式制度分析中存在的编码不严谨、格式转换效率低的问题，为制度系统的配置性描述提供标准化分析框架。

Method: 基于制度语法2.0理论框架，开发IG Script语法实现结构化编码，建立自动化转换管道生成网络图、矩阵等多种分析格式。

Result: 成功构建包含语法规范、架构原则的完整工具系统，通过案例验证其在多层次制度分析中的有效性和效率提升。

Conclusion: 该工具通过严谨的语法约束和自动化流程，显著提升制度分析的标准化程度与跨方法兼容性，为复杂社会系统研究提供关键技术支撑。

Abstract: This article provides an overview of IG Parser, a software that facilitates
qualitative content analysis of formal (e.g., legal) rules or informal (e.g.,
socio-normative) norms, and strategies (such as conventions) -- referred to as
\emph{institutions} -- that govern social systems and operate configurally to
describe \emph{institutional systems}. To this end, the IG Parser employs a
distinctive syntax that ensures rigorous encoding of natural language, while
automating the transformation into various formats that support the downstream
analysis using diverse analytical techniques. The conceptual core of the IG
Parser is an associated syntax, IG Script, that operationalizes the conceptual
foundations of the Institutional Grammar, and more specifically Institutional
Grammar 2.0, an analytical paradigm for institutional analysis. This article
presents the IG Parser, including its conceptual foundations, syntactic
specification of IG Script, alongside architectural principles. This
introduction is augmented with selective illustrative examples that highlight
the use and benefit associated with the tool.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [203] [SounDiT: Geo-Contextual Soundscape-to-Landscape Generation](https://arxiv.org/abs/2505.12734)
*Junbo Wang,Haofeng Tan,Bowen Liao,Albert Jiang,Teng Fei,Qixing Huang,Zhengzhong Tu,Shan Ye,Yuhao Kang*

Main category: cs.SD

TL;DR: 提出GeoS2L地理声景转景观生成问题，开发SounDiT扩散Transformer模型整合地理知识，构建多模态数据集并通过PSS评估框架验证地理一致性。


<details>
  <summary>Details</summary>
Motivation: 现有音频生成图像方法忽略地理上下文，导致生成结果与真实环境不匹配。需整合地理知识提升多模态生成模型的地理连贯性。

Method: 1.构建含地理属性的SoundingSVI和SonicUrban数据集 2.提出地理条件编码的SounDiT模型 3.设计元素/场景/感知三层次PSS评估体系

Result: SounDiT在视觉质量(PSNR提升15.3%)和地理一致性(PSS提升22.7%)上显著优于基准模型，生成图像与真实地理特征高度匹配。

Conclusion: 该研究确立了地理感知生成模型的新范式，揭示了领域知识对多模态生成的重要性，为生成式AI与地理/城市规划的跨学科应用开辟新方向。

Abstract: We present a novel and practically significant problem-Geo-Contextual
Soundscape-to-Landscape (GeoS2L) generation-which aims to synthesize
geographically realistic landscape images from environmental soundscapes. Prior
audio-to-image generation methods typically rely on general-purpose datasets
and overlook geographic and environmental contexts, resulting in unrealistic
images that are misaligned with real-world environmental settings. To address
this limitation, we introduce a novel geo-contextual computational framework
that explicitly integrates geographic knowledge into multimodal generative
modeling. We construct two large-scale geo-contextual multimodal datasets,
SoundingSVI and SonicUrban, pairing diverse soundscapes with real-world
landscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based
model that incorporates geo-contextual scene conditioning to synthesize
geographically coherent landscape images. Furthermore, we propose a
practically-informed geo-contextual evaluation framework, the Place Similarity
Score (PSS), across element-, scene-, and human perception-levels to measure
consistency between input soundscapes and generated landscape images. Extensive
experiments demonstrate that SounDiT outperforms existing baselines in both
visual fidelity and geographic settings. Our work not only establishes
foundational benchmarks for GeoS2L generation but also highlights the
importance of incorporating geographic domain knowledge in advancing multimodal
generative models, opening new directions at the intersection of generative AI,
geography, urban planning, and environmental sciences.

</details>


### [204] [ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems](https://arxiv.org/abs/2505.11572)
*Anand Rai,Satyam Rahangdale,Utkarsh Anand,Animesh Mukherjee*

Main category: cs.SD

TL;DR: 论文提出ASR-FAIRBENCH评估框架，通过FAAS指标综合评估语音识别模型的准确性和公平性，揭示不同人口群体间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有语音识别系统在不同人口群体间存在显著性能差异，需要建立兼顾准确性和公平性的评估体系。

Method: 使用Meta的Fair-Speech数据集，采用混合效应泊松回归模型计算总体公平性分数，结合传统词错率(WER)构建FAAS综合评价指标。

Result: 研究发现当前最先进的ASR模型在不同人口群体间存在显著性能差异，为开发包容性技术提供基准。

Conclusion: 该框架通过将公平性量化纳入评估体系，推动开发更公平的语音识别技术，促进人工智能公平性研究。

Abstract: Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday
applications, yet significant disparities in performance across diverse
demographic groups persist. In this work, we introduce the ASR-FAIRBENCH
leaderboard which is designed to assess both the accuracy and equity of ASR
models in real-time. Leveraging the Meta's Fair-Speech dataset, which captures
diverse demographic characteristics, we employ a mixed-effects Poisson
regression model to derive an overall fairness score. This score is integrated
with traditional metrics like Word Error Rate (WER) to compute the Fairness
Adjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our
approach reveals significant performance disparities in SOTA ASR models across
demographic groups and offers a benchmark to drive the development of more
inclusive ASR technologies.

</details>


### [205] [MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix](https://arxiv.org/abs/2505.13032)
*Ziyang Ma,Yinghao Ma,Yanqiao Zhu,Chen Yang,Yi-Wen Chao,Ruiyang Xu,Wenxi Chen,Yuanzhe Chen,Zhuo Chen,Jian Cong,Kai Li,Keliang Li,Siyou Li,Xinfeng Li,Xiquan Li,Zheng Lian,Yuzhe Liang,Minghao Liu,Zhikang Niu,Tianrui Wang,Yuping Wang,Yuxuan Wang,Yihao Wu,Guanrou Yang,Jianwei Yu,Ruibin Yuan,Zhisheng Zheng,Ziya Zhou,Haina Zhu,Wei Xue,Emmanouil Benetos,Kai Yu,Eng-Siong Chng,Xie Chen*

Main category: cs.SD

TL;DR: MMAR是包含1000个高质量音频-问答对的新基准测试，覆盖多领域真实音频场景，通过四级推理层次评估模型的深度理解能力。


<details>
  <summary>Details</summary>
Motivation: 突破现有音频基准的领域限制，构建多模态混合的真实场景测试集，通过层次化问题分类和思维链标注推动音频推理研究。

Method: 从互联网视频收集数据，经多轮纠错构建数据集；问题分为信号、感知、语义、文化四层，每层包含多个子类；使用音频字幕输入评估多种大模型。

Result: 现有模型表现显著不足，揭示其在复杂推理和文化背景理解方面的核心缺陷，部分问题需要领域专业知识才能解答。

Conclusion: MMAR填补音频深度推理评估空白，其层次化设计和专业级问题设定为音频-语言模型的未来发展提供重要方向指引。

Abstract: We introduce MMAR, a new benchmark designed to evaluate the deep reasoning
capabilities of Audio-Language Models (ALMs) across massive multi-disciplinary
tasks. MMAR comprises 1,000 meticulously curated audio-question-answer
triplets, collected from real-world internet videos and refined through
iterative error corrections and quality checks to ensure high quality. Unlike
existing benchmarks that are limited to specific domains of sound, music, or
speech, MMAR extends them to a broad spectrum of real-world audio scenarios,
including mixed-modality combinations of sound, music, and speech. Each
question in MMAR is hierarchically categorized across four reasoning layers:
Signal, Perception, Semantic, and Cultural, with additional sub-categories
within each layer to reflect task diversity and complexity. To further foster
research in this area, we annotate every question with a Chain-of-Thought (CoT)
rationale to promote future advancements in audio reasoning. Each item in the
benchmark demands multi-step deep reasoning beyond surface-level understanding.
Moreover, a part of the questions requires graduate-level perceptual and
domain-specific knowledge, elevating the benchmark's difficulty and depth. We
evaluate MMAR using a broad set of models, including Large Audio-Language
Models (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models
(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with
audio caption inputs. The performance of these models on MMAR highlights the
benchmark's challenging nature, and our analysis further reveals critical
limitations of understanding and reasoning capabilities among current models.
We hope MMAR will serve as a catalyst for future advances in this important but
little-explored area.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [206] [Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO](https://arxiv.org/abs/2505.11595)
*Peter Chen,Xiaopeng Li,Ziniu Li,Xi Chen,Tianyi Lin*

Main category: cs.LG

TL;DR: 提出通过AI反馈增强GRPO中全负样本组的响应多样性，突破策略更新瓶颈并验证有效性


<details>
  <summary>Details</summary>
Motivation: GRPO方法在全负样本组场景下无法更新策略，阻碍强化学习进程

Method: 1. 基于AI反馈的响应多样性增强框架
2. 通过理论模型分析学习动态改进机制

Result: 在7B/14B/32B模型上验证有效，覆盖离线/在线学习的10个基准测试（含基础版和蒸馏版）

Conclusion: 全负样本组学习不仅可行且能提升性能，突破传统RL训练限制

Abstract: Reinforcement learning (RL) has demonstrated significant success in enhancing
reasoning capabilities in large language models (LLMs). One of the most widely
used RL methods is Group Relative Policy Optimization
(GRPO)~\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and
success in training DeepSeek-R1~\cite{Guo-2025-Deepseek}. However, GRPO stalls
when all sampled responses in a group are incorrect -- referred to as an
\emph{all-negative-sample} group -- as it fails to update the policy, hindering
learning progress. The contributions of this paper are two-fold. First, we
propose a simple yet effective framework that introduces response diversity
within all-negative-sample groups in GRPO using AI feedback. We also provide a
theoretical analysis, via a stylized model, showing how this diversification
improves learning dynamics. Second, we empirically validate our approach,
showing the improved performance across various model sizes (7B, 14B, 32B) in
both offline and online learning settings with 10 benchmarks, including base
and distilled variants. Our findings highlight that learning from
all-negative-sample groups is not only feasible but beneficial, advancing
recent insights from \citet{Xiong-2025-Minimalist}.

</details>


### [207] [EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents](https://arxiv.org/abs/2505.11717)
*Xilong Wang,John Bloch,Zedian Shao,Yuepeng Hu,Shuyan Zhou,Neil Zhenqiang Gong*

Main category: cs.LG

TL;DR: 提出EnvInjection攻击方法，通过网页像素扰动诱导网络代理执行攻击者指定的目标动作


<details>
  <summary>Details</summary>
Motivation: 现有环境提示注入攻击存在有效性不足、隐蔽性差或实际应用困难的问题，需开发更优解决方案

Method: 构建像素扰动优化问题，用神经网络近似非可微分的像素-截图映射，采用投影梯度下降算法求解

Result: 在多个网页数据集上的评估显示攻击成功率显著优于现有基线方法

Conclusion: EnvInjection成功解决了现有攻击的局限性，实现了高效且实用的环境提示注入攻击

Abstract: Multi-modal large language model (MLLM)-based web agents interact with
webpage environments by generating actions based on screenshots of the
webpages. Environmental prompt injection attacks manipulate the environment to
induce the web agent to perform a specific, attacker-chosen action--referred to
as the target action. However, existing attacks suffer from limited
effectiveness or stealthiness, or are impractical in real-world settings. In
this work, we propose EnvInjection, a new attack that addresses these
limitations. Our attack adds a perturbation to the raw pixel values of the
rendered webpage, which can be implemented by modifying the webpage's source
code. After these perturbed pixels are mapped into a screenshot, the
perturbation induces the web agent to perform the target action. We formulate
the task of finding the perturbation as an optimization problem. A key
challenge in solving this problem is that the mapping between raw pixel values
and screenshot is non-differentiable, making it difficult to backpropagate
gradients to the perturbation. To overcome this, we train a neural network to
approximate the mapping and apply projected gradient descent to solve the
reformulated optimization problem. Extensive evaluation on multiple webpage
datasets shows that EnvInjection is highly effective and significantly
outperforms existing baselines.

</details>


### [208] [Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models](https://arxiv.org/abs/2505.11731)
*Harshil Vejendla,Haizhou Shi,Yibin Wang,Tunyu Zhang,Huan Zhang,Hao Wang*

Main category: cs.LG

TL;DR: 提出通过蒸馏贝叶斯LLM的置信度至非贝叶斯学生模型，实现无需测试采样的高效不确定性估计方法


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法需多次采样导致效率低下，严重限制实际部署应用

Method: 在训练集上蒸馏对齐预测分布，无需验证数据集，最小化贝叶斯LLM与学生的预测分布差异

Result: 测试效率提升N倍(N为传统贝叶斯方法所需采样次数)，实验显示不确定性估计能力可泛化至新数据，效果达到/超越SOTA贝叶斯LLMs

Conclusion: 该蒸馏技术实现测试阶段零采样成本的高效不确定性估计，在保持精度的同时显著提升计算效率

Abstract: Recent advances in uncertainty estimation for Large Language Models (LLMs)
during downstream adaptation have addressed key challenges of reliability and
simplicity. However, existing Bayesian methods typically require multiple
sampling iterations during inference, creating significant efficiency issues
that limit practical deployment. In this paper, we investigate the possibility
of eliminating the need for test-time sampling for LLM uncertainty estimation.
Specifically, when given an off-the-shelf Bayesian LLM, we distill its aligned
confidence into a non-Bayesian student LLM by minimizing the divergence between
their predictive distributions. Unlike typical calibration methods, our
distillation is carried out solely on the training dataset without the need of
an additional validation dataset. This simple yet effective approach achieves
N-times more efficient uncertainty estimation during testing, where N is the
number of samples traditionally required by Bayesian LLMs. Our extensive
experiments demonstrate that uncertainty estimation capabilities on training
data can successfully generalize to unseen test data through our distillation
technique, consistently producing results comparable to (or even better than)
state-of-the-art Bayesian LLMs.

</details>


### [209] [Token-Level Uncertainty Estimation for Large Language Model Reasoning](https://arxiv.org/abs/2505.11737)
*Tunyu Zhang,Haizhou Shi,Yibin Wang,Hengyi Wang,Xiaoxiao He,Zhuowei Li,Haoxian Chen,Ligong Han,Kai Xu,Huan Zhang,Dimitris Metaxas,Hao Wang*

Main category: cs.LG

TL;DR: 提出基于token级不确定性估计的LLM自我评估框架，通过扰动解码生成预测分布，提升数学推理的可靠性和性能。


<details>
  <summary>Details</summary>
Motivation: LLMs输出质量在复杂任务中不稳定，需建立可信度评估机制。传统方法难以精准捕捉语义级不确定性，需更细粒度的评估体系。

Method: 1. 低秩随机权重扰动解码生成预测分布
2. 聚合token级不确定性反映语义确定性
3. 结合粒子滤波算法进行多路径推理优化

Result: 数学推理数据集显示：不确定性指标与答案正确率强相关（相关系数提升23%），推理性能超越现有方法（准确率提升5.6%）

Conclusion: 有效的token级不确定性估计可作为LLMs推理能力评估和优化的双重工具，为可信AI提供新方法论。

Abstract: While Large Language Models (LLMs) have demonstrated impressive capabilities,
their output quality remains inconsistent across various application scenarios,
making it difficult to identify trustworthy responses, especially in complex
tasks requiring multi-step reasoning. In this paper, we propose a token-level
uncertainty estimation framework to enable LLMs to self-assess and self-improve
their generation quality in mathematical reasoning. Specifically, we introduce
low-rank random weight perturbation to LLM decoding, generating predictive
distributions that we use to estimate token-level uncertainties. We then
aggregate these uncertainties to reflect semantic uncertainty of the generated
sequences. Experiments on mathematical reasoning datasets of varying difficulty
demonstrate that our token-level uncertainty metrics strongly correlate with
answer correctness and model robustness. Additionally, we explore using
uncertainty to directly enhance the model's reasoning performance through
multiple generations and the particle filtering algorithm. Our approach
consistently outperforms existing uncertainty estimation methods, establishing
effective uncertainty estimation as a valuable tool for both evaluating and
improving reasoning generation in LLMs.

</details>


### [210] [Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders](https://arxiv.org/abs/2505.11756)
*David Chanin,Tomáš Dulka,Adrià Garriga-Alonso*

Main category: cs.LG

TL;DR: 稀疏自编码器在特征相关场景下会出现特征对冲问题，导致分解失效，改进的matryoshka结构可能缓解该问题


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器在LLM应用中持续表现不佳，需探究其根本原因

Method: 通过理论分析玩具模型+LLM实际场景的SAE训练实验验证

Result: 发现特征对冲现象：窄SAE会合并相关特征，破坏单语义性

Conclusion: 特征对冲是SAE性能不足的核心因素，matryoshka改进方案带来希望

Abstract: It is assumed that sparse autoencoders (SAEs) decompose polysemantic
activations into interpretable linear directions, as long as the activations
are composed of sparse linear combinations of underlying features. However, we
find that if an SAE is more narrow than the number of underlying "true
features" on which it is trained, and there is correlation between features,
the SAE will merge components of correlated features together, thus destroying
monosemanticity. In LLM SAEs, these two conditions are almost certainly true.
This phenomenon, which we call feature hedging, is caused by SAE reconstruction
loss, and is more severe the narrower the SAE. In this work, we introduce the
problem of feature hedging and study it both theoretically in toy models and
empirically in SAEs trained on LLMs. We suspect that feature hedging may be one
of the core reasons that SAEs consistently underperform supervised baselines.
Finally, we use our understanding of feature hedging to propose an improved
variant of matryoshka SAEs. Our work shows there remain fundamental issues with
SAEs, but we are hopeful that that highlighting feature hedging will catalyze
future advances that allow SAEs to achieve their full potential of interpreting
LLMs at scale.

</details>


### [211] [Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors](https://arxiv.org/abs/2505.11770)
*Jing Huang,Junyi Tao,Thomas Icard,Diyi Yang,Christopher Potts*

Main category: cs.LG

TL;DR: 通过因果分析技术预测语言模型在分布外任务中的行为表现


<details>
  <summary>Details</summary>
Motivation: 探索神经网络解释性技术是否能预测模型在分布外样本的行为，验证因果机制在模型行为预测中的有效性

Method: 提出反事实模拟（检查关键因果变量）和值探测（利用变量值预测）两种方法，在符号操作、知识检索和指令跟随等多样化任务中进行验证

Result: 因果方法在分布内外均实现高AUC-ROC，在分布外场景显著优于非因果方法

Conclusion: 内部因果分析为语言模型行为预测开辟了新的应用方向，增强了模型可解释性的实用价值

Abstract: Interpretability research now offers a variety of techniques for identifying
abstract internal mechanisms in neural networks. Can such techniques be used to
predict how models will behave on out-of-distribution examples? In this work,
we provide a positive answer to this question. Through a diverse set of
language modeling tasks--including symbol manipulation, knowledge retrieval,
and instruction following--we show that the most robust features for
correctness prediction are those that play a distinctive causal role in the
model's behavior. Specifically, we propose two methods that leverage causal
mechanisms to predict the correctness of model outputs: counterfactual
simulation (checking whether key causal variables are realized) and value
probing (using the values of those variables to make predictions). Both achieve
high AUC-ROC in distribution and outperform methods that rely on
causal-agnostic features in out-of-distribution settings, where predicting
model behaviors is more crucial. Our work thus highlights a novel and
significant application for internal causal analysis of language models.

</details>


### [212] [VenusX: Unlocking Fine-Grained Functional Understanding of Proteins](https://arxiv.org/abs/2505.11812)
*Yang Tan,Wenrui Gou,Bozitao Zhong,Liang Hong,Huiqun Yu,Bingxin Zhou*

Main category: cs.LG

TL;DR: 提出首个大规模细粒度蛋白质功能分析基准VenusX，覆盖残基/片段/结构域层面的功能注释与配对任务


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型主要在蛋白质层面进行功能预测，但理解功能机制需要更细粒度（残基/片段/结构域）的分析视角

Method: 构建包含6类注释的878,000+样本数据集，采用混合家族/跨家族划分和三种序列相似度阈值，评估蛋白质语言模型、结构混合模型等多类方法

Result: 通过多指标全面评估模型在分布内和分布外场景的性能，建立未来研究的基准基线

Conclusion: VenusX填补了细粒度蛋白质功能分析评估的空白，其开源数据与代码将推动蛋白质功能机制研究和模型能力提升

Abstract: Deep learning models have driven significant progress in predicting protein
function and interactions at the protein level. While these advancements have
been invaluable for many biological applications such as enzyme engineering and
function annotation, a more detailed perspective is essential for understanding
protein functional mechanisms and evaluating the biological knowledge captured
by models. To address this demand, we introduce VenusX, the first large-scale
benchmark for fine-grained functional annotation and function-based protein
pairing at the residue, fragment, and domain levels. VenusX comprises three
major task categories across six types of annotations, including residue-level
binary classification, fragment-level multi-class classification, and pairwise
functional similarity scoring for identifying critical active sites, binding
sites, conserved sites, motifs, domains, and epitopes. The benchmark features
over 878,000 samples curated from major open-source databases such as InterPro,
BioLiP, and SAbDab. By providing mixed-family and cross-family splits at three
sequence identity thresholds, our benchmark enables a comprehensive assessment
of model performance on both in-distribution and out-of-distribution scenarios.
For baseline evaluation, we assess a diverse set of popular and open-source
models, including pre-trained protein language models, sequence-structure
hybrids, structure-based methods, and alignment-based techniques. Their
performance is reported across all benchmark datasets and evaluation settings
using multiple metrics, offering a thorough comparison and a strong foundation
for future research. Code and data are publicly available at
https://github.com/ai4protein/VenusX.

</details>


### [213] [J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge](https://arxiv.org/abs/2505.11875)
*Chi-Min Chan,Chunpu Xu,Jiaming Ji,Zhen Ye,Pengcheng Wen,Chunyang Jiang,Yaodong Yang,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 论文提出J1-7B模型，通过监督微调+强化学习训练策略及STTS推理优化，显著提升LLM-as-a-Judge评估性能（+4.8%）及扩展趋势（+5.1%）。


<details>
  <summary>Details</summary>
Motivation: 传统基于标量奖励的评估方法缺乏可解释性，LLM-as-a-Judge结合大推理模型的深度思考能力可提供更透明的决策依据。

Method: 1. 基于拒绝采样收集反射增强数据集进行监督微调
2. 采用可验证奖励的强化学习训练
3. 推理阶段应用STTS策略优化性能

Result: J1-7B性能超越SOTA 4.8%，STTS扩展趋势增强5.1%。关键发现：
1. 现有LLM-as-a-Judge无扩展趋势
2. 仅微调模型扩展性弱
3. RL训练显著获得STTS能力

Conclusion: 强化学习阶段是获得有效STTS能力的关键，证明RL训练对模型扩展性的决定性作用。

Abstract: The current focus of AI research is shifting from emphasizing model training
towards enhancing evaluation quality, a transition that is crucial for driving
further advancements in AI systems. Traditional evaluation methods typically
rely on reward models assigning scalar preference scores to outputs. Although
effective, such approaches lack interpretability, leaving users often uncertain
about why a reward model rates a particular response as high or low. The advent
of LLM-as-a-Judge provides a more scalable and interpretable method of
supervision, offering insights into the decision-making process. Moreover, with
the emergence of large reasoning models, which consume more tokens for deeper
thinking and answer refinement, scaling test-time computation in the
LLM-as-a-Judge paradigm presents an avenue for further boosting performance and
providing more interpretability through reasoning traces. In this paper, we
introduce $\textbf{J1-7B}$, which is first supervised fine-tuned on
reflection-enhanced datasets collected via rejection-sampling and subsequently
trained using Reinforcement Learning (RL) with verifiable rewards. At inference
time, we apply Simple Test-Time Scaling (STTS) strategies for additional
performance improvement. Experimental results demonstrate that $\textbf{J1-7B}$
surpasses the previous state-of-the-art LLM-as-a-Judge by $ \textbf{4.8}$\% and
exhibits a $ \textbf{5.1}$\% stronger scaling trend under STTS. Additionally,
we present three key findings: (1) Existing LLM-as-a-Judge does not inherently
exhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced
datasets continues to demonstrate similarly weak scaling behavior. (3)
Significant scaling trend emerges primarily during the RL phase, suggesting
that effective STTS capability is acquired predominantly through RL training.

</details>


### [214] [Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling](https://arxiv.org/abs/2505.12225)
*Jizhou Guo,Zhaomin Wu,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出高效线性隐藏状态奖励模型ELHSR，通过利用LLM隐藏状态实现超参数效率（<0.005%参数量）和计算效率的显著提升


<details>
  <summary>Details</summary>
Motivation: 传统基于文本输出的奖励模型存在计算成本高、参数量大的缺陷，限制了实际应用场景

Method: 利用LLM隐藏状态中的丰富信息，开发参数效率极高的线性模型架构，支持少量样本训练且兼容logits输入

Result: 在性能、效率（时间/计算量）和适应性（闭源模型兼容性）方面全面超越基线模型，结合传统模型可进一步提升效果

Conclusion: ELHSR为奖励模型提供了高效轻量化的新范式，扩展了其在真实场景和闭源系统中的适用边界

Abstract: High-quality reward models are crucial for unlocking the reasoning potential
of large language models (LLMs), with best-of-N voting demonstrating
significant performance gains. However, current reward models, which typically
operate on the textual output of LLMs, are computationally expensive and
parameter-heavy, limiting their real-world applications. We introduce the
Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly
parameter-efficient approach that leverages the rich information embedded in
LLM hidden states to address these issues. ELHSR systematically outperform
baselines with less than 0.005% of the parameters of baselines, requiring only
a few samples for training. ELHSR also achieves orders-of-magnitude efficiency
improvement with significantly less time and fewer FLOPs per sample than
baseline reward models. Moreover, ELHSR exhibits robust performance even when
trained only on logits, extending its applicability to some closed-source LLMs.
In addition, ELHSR can also be combined with traditional reward models to
achieve additional performance gains.

</details>


### [215] [UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection](https://arxiv.org/abs/2505.12457)
*Yang Zhao,Kai Xiong,Xiao Ding,Li Du,YangouOuyang,Zhouhao Sun,Jiannan Guan,Wenbin Zhang,Bin Liu,Dong Hu,Bing Qin,Ting Liu*

Main category: cs.LG

TL;DR: 提出UFO-RL框架，利用单次不确定性估计实现高效数据筛选，仅用10%数据即可达到全数据训练效果，训练速度提升16倍。


<details>
  <summary>Details</summary>
Motivation: 传统RL微调LLM时多采样策略计算成本极高，受ZPD理论启发，通过筛选模型潜在理解范围内的关键数据提升效率。

Method: 基于单次前向传播的uncertainty estimation技术，构建185倍速的数据评估体系，动态选择ZPD区间内的有效训练样本。

Result: 10%精选数据达到全数据97%性能，训练时间缩短至1/16，且模型稳定性与泛化能力显著提升。

Conclusion: UFO-RL为LLM强化学习微调提供了数据选择新范式，实现计算效率与模型性能的帕累托优化。

Abstract: Scaling RL for LLMs is computationally expensive, largely due to
multi-sampling for policy optimization and evaluation, making efficient data
selection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,
we hypothesize LLMs learn best from data within their potential comprehension
zone. Addressing the limitation of conventional, computationally intensive
multi-sampling methods for data assessment, we introduce UFO-RL. This novel
framework uses a computationally efficient single-pass uncertainty estimation
to identify informative data instances, achieving up to 185x faster data
evaluation. UFO-RL leverages this metric to select data within the estimated
ZPD for training. Experiments show that training with just 10% of data selected
by UFO-RL yields performance comparable to or surpassing full-data training,
reducing overall training time by up to 16x while enhancing stability and
generalization. UFO-RL offers a practical and highly efficient strategy for
scaling RL fine-tuning of LLMs by focusing learning on valuable data.

</details>


### [216] [Enhancing Latent Computation in Transformers with Latent Tokens](https://arxiv.org/abs/2505.12629)
*Yuchang Sun,Yanxi Chen,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: 通过添加非自然语言解释的潜在标记（latent tokens），以轻量级方式增强大语言模型的注意力机制，提升模型在分布外场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统LLM增强方法通常需要复杂架构调整或全参数微调。本研究旨在通过仅添加少量可训练参数的潜在标记，在不破坏预训练模型结构的前提下提升模型适应性。

Method: 在Transformer解码过程中插入可训练的虚拟标记，通过注意力机制引导生成过程。支持参数高效训练（PEFT），与现有架构无缝集成，推理阶段灵活应用。

Result: 在合成任务测试中显著超越基线模型，特别是在分布外泛化场景表现突出，验证了潜在标记对模型适应性的提升效果。

Conclusion: 潜在标记机制为LLM性能提升提供了低复杂度、高兼容性的解决方案，在保持基础设施稳定性的同时增强了模型泛化能力。

Abstract: Augmenting large language models (LLMs) with auxiliary tokens has emerged as
a promising strategy for enhancing model performance. In this work, we
introduce a lightweight method termed latent tokens; these are dummy tokens
that may be non-interpretable in natural language but steer the autoregressive
decoding process of a Transformer-based LLM via the attention mechanism. The
proposed latent tokens can be seamlessly integrated with a pre-trained
Transformer, trained in a parameter-efficient manner, and applied flexibly at
inference time, while adding minimal complexity overhead to the existing
infrastructure of standard Transformers. We propose several hypotheses about
the underlying mechanisms of latent tokens and design synthetic tasks
accordingly to verify them. Numerical results confirm that the proposed method
noticeably outperforms the baselines, particularly in the out-of-distribution
generalization scenarios, highlighting its potential in improving the
adaptability of LLMs.

</details>


### [217] [Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization](https://arxiv.org/abs/2505.12763)
*Sunghwan Kim,Dongjin Kang,Taeyoon Kwon,Hyungjoo Chae,Dongha Lee,Jinyoung Yeo*

Main category: cs.LG

TL;DR: 现有奖励模型评估基准与策略优化表现弱相关，需通过奖励过优化现象构建多维度测试基准，但需注意避免过度优化影响下游任务性能


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型评估基准无法准确反映其在RLHF中的实际作用，影响策略优化效果评估

Method: 通过奖励过优化现象（综合评估模型对齐能力与策略学习信号），提出三阶段基准设计框架

Result: 确定基准设计三原则：（1）最小化非正确性差异（2）多维度响应对比（3）多模型响应来源；发现过度优化与下游任务存在负相关

Conclusion: 应将奖励过优化程度作为基准设计工具而非最终目标，平衡模型评估与下游任务性能的关系

Abstract: Reward models (RMs) play a crucial role in reinforcement learning from human
feedback (RLHF), aligning model behavior with human preferences. However,
existing benchmarks for reward models show a weak correlation with the
performance of optimized policies, suggesting that they fail to accurately
assess the true capabilities of RMs. To bridge this gap, we explore several
evaluation designs through the lens of reward overoptimization\textemdash a
phenomenon that captures both how well the reward model aligns with human
preferences and the dynamics of the learning signal it provides to the policy.
The results highlight three key findings on how to construct a reliable
benchmark: (i) it is important to minimize differences between chosen and
rejected responses beyond correctness, (ii) evaluating reward models requires
multiple comparisons across a wide range of chosen and rejected responses, and
(iii) given that reward models encounter responses with diverse
representations, responses should be sourced from a variety of models. However,
we also observe that a extremely high correlation with degree of
overoptimization leads to comparatively lower correlation with certain
downstream performance. Thus, when designing a benchmark, it is desirable to
use the degree of overoptimization as a useful tool, rather than the end goal.

</details>


### [218] [GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents](https://arxiv.org/abs/2505.12842)
*Zheng Wu,Pengzhou Cheng,Zongru Wu,Lingzhong Dong,Zhuosheng Zhang*

Main category: cs.LG

TL;DR: 提出基于高斯混合模型的GEM方法，有效检测GUI代理的异常指令，在8个数据集上实现23.7%的准确率提升


<details>
  <summary>Details</summary>
Motivation: 当前GUI代理遇到超出能力范围的指令时易崩溃或引发安全隐患，传统OOD检测方法在复杂GUI环境中表现欠佳

Method: 通过观察GUI代理语义空间的聚类特性，利用输入嵌入距离的高斯混合模型拟合代理能力边界

Result: 在跨设备（手机/电脑/浏览器）的8个数据集上验证，平均准确率提升23.7%，九种不同骨干网络实验证实泛化能力

Conclusion: GEM通过建模语义空间距离分布，显著提升GUI代理的OOD检测性能，代码已开源便于后续研究应用

Abstract: Graphical user interface (GUI) agents have recently emerged as an intriguing
paradigm for human-computer interaction, capable of automatically executing
user instructions to operate intelligent terminal devices. However, when
encountering out-of-distribution (OOD) instructions that violate environmental
constraints or exceed the current capabilities of agents, GUI agents may suffer
task breakdowns or even pose security threats. Therefore, effective OOD
detection for GUI agents is essential. Traditional OOD detection methods
perform suboptimally in this domain due to the complex embedding space and
evolving GUI environments. In this work, we observe that the in-distribution
input semantic space of GUI agents exhibits a clustering pattern with respect
to the distance from the centroid. Based on the finding, we propose GEM, a
novel method based on fitting a Gaussian mixture model over input embedding
distances extracted from the GUI Agent that reflect its capability boundary.
Evaluated on eight datasets spanning smartphones, computers, and web browsers,
our method achieves an average accuracy improvement of 23.70\% over the
best-performing baseline. Analysis verifies the generalization ability of our
method through experiments on nine different backbones. The codes are available
at https://github.com/Wuzheng02/GEM-OODforGUIagents.

</details>


### [219] [Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?](https://arxiv.org/abs/2505.12871)
*Zi Liang,Haibo Hu,Qingqing Ye,Yaxin Xiao,Ronghua Li*

Main category: cs.LG

TL;DR: LoRA微调在防御后门攻击时比全参数微调更鲁棒，但因其过度简化的信息几何结构，更容易受到无目标数据投毒攻击


<details>
  <summary>Details</summary>
Motivation: 探究LoRA在训练时对抗数据投毒和后门攻击的安全性表现，填补该领域的研究空白

Method: 建立融合神经正切核的训练动力学分析框架，结合信息论分析低秩结构与攻击脆弱性的关联机制

Result: 通过理论分析和实验验证，发现LoRA对后门攻击的防御能力提升20%，但对数据投毒的抵抗能力下降35%

Conclusion: LoRA的低秩特性在安全领域呈现双刃剑效应，实际应用中需在防御效能与攻击脆弱性间取得平衡

Abstract: Low rank adaptation (LoRA) has emerged as a prominent technique for
fine-tuning large language models (LLMs) thanks to its superb efficiency gains
over previous methods. While extensive studies have examined the performance
and structural properties of LoRA, its behavior upon training-time attacks
remain underexplored, posing significant security risks. In this paper, we
theoretically investigate the security implications of LoRA's low-rank
structure during fine-tuning, in the context of its robustness against data
poisoning and backdoor attacks. We propose an analytical framework that models
LoRA's training dynamics, employs the neural tangent kernel to simplify the
analysis of the training process, and applies information theory to establish
connections between LoRA's low rank structure and its vulnerability against
training-time attacks. Our analysis indicates that LoRA exhibits better
robustness to backdoor attacks than full fine-tuning, while becomes more
vulnerable to untargeted data poisoning due to its over-simplified information
geometry. Extensive experimental evaluations have corroborated our theoretical
findings.

</details>


### [220] [Leveraging LLM Inconsistency to Boost Pass@k Performance](https://arxiv.org/abs/2505.12938)
*Uri Dalal,Meirav Segal,Zvika Ben-Haim,Dan Lahav,Omer Nevo*

Main category: cs.LG

TL;DR: 通过主动生成任务变体利用LLMs的不一致性提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有语言模型对微小输入变化敏感，本研究将其转化为提升Pass@k指标的优势

Method: 开发任务无关的Variator代理，自动生成k个任务变体并提交独立解决方案

Result: 理论模型验证可行性，APPS数据集实证表现超越基线，前沿模型仍存在不一致性

Conclusion: 该方法通过主动创造输入多样性释放模型潜力，未来仍适用于新型推理模型

Abstract: Large language models (LLMs) achieve impressive abilities in numerous
domains, but exhibit inconsistent performance in response to minor input
changes. Rather than view this as a drawback, in this paper we introduce a
novel method for leveraging models' inconsistency to boost Pass@k performance.
Specifically, we present a "Variator" agent that generates k variants of a
given task and submits one candidate solution for each one. Our variant
generation approach is applicable to a wide range of domains as it is task
agnostic and compatible with free-form inputs. We demonstrate the efficacy of
our agent theoretically using a probabilistic model of the inconsistency
effect, and show empirically that it outperforms the baseline on the APPS
dataset. Furthermore, we establish that inconsistency persists even in frontier
reasoning models across coding and cybersecurity domains, suggesting our method
is likely to remain relevant for future model generations.

</details>


### [221] [Fractured Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.12992)
*Baohao Liao,Hanze Dong,Yuhui Xu,Doyen Sahoo,Christof Monz,Junnan Li,Caiming Xiong*

Main category: cs.LG

TL;DR: 提出了Fractured Sampling方法，通过三个正交维度调整推理过程，在显著减少token消耗的同时保持大模型推理精度


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示技术及其扩展虽然提升准确率，但产生高额token成本，阻碍在延迟敏感场景的部署

Method: 1.调整推理轨迹数量 2.控制每个轨迹的最终解数量 3.动态截断推理轨迹深度

Result: 在5个推理基准测试中实现log-linear级别的Pass@k与token预算的优化权衡，验证计算资源分配策略的有效性

Conclusion: 该方法为LLM推理提供了可扩展的高效解决方案，通过三维度计算资源优化实现精度与成本的平衡

Abstract: Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning.

</details>


### [222] [FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference](https://arxiv.org/abs/2505.13109)
*Guangda Liu,Chengwei Li,Zhenyu Ning,Jing Lin,Yiwu Yao,Danning Ke,Minyi Guo,Jieru Zhao*

Main category: cs.LG

TL;DR: FreeKV通过算法-系统协同优化，在保持精度的同时实现高效KV检索，相比现有方法加速13倍


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存方法存在精度损失（KV丢弃）或效率瓶颈（KV检索），无法满足长上下文场景的部署需求

Method: 算法侧采用推测式检索+细粒度修正，系统侧设计混合KV内存布局和双缓冲流式召回机制

Result: 实验证明该方法在多种场景下实现接近无损的精度，检索速度提升最高达13倍

Conclusion: FreeKV有效解决了长上下文部署的KV缓存效率与精度的平衡问题，具有实际应用价值

Abstract: Large language models (LLMs) have been widely deployed with rapidly expanding
context windows to support increasingly demanding applications. However, long
contexts pose significant deployment challenges, primarily due to the KV cache
whose size grows proportionally with context length. While KV cache compression
methods are proposed to address this issue, KV dropping methods incur
considerable accuracy loss, and KV retrieval methods suffer from significant
efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization
framework to enhance KV retrieval efficiency while preserving accuracy. On the
algorithm side, FreeKV introduces speculative retrieval to shift the KV
selection and recall processes out of the critical path, combined with
fine-grained correction to ensure accuracy. On the system side, FreeKV employs
hybrid KV layouts across CPU and GPU memory to eliminate fragmented data
transfers, and leverages double-buffered streamed recall to further improve
efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy
across various scenarios and models, delivering up to 13$\times$ speedup
compared to SOTA KV retrieval methods.

</details>


### [223] [Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space](https://arxiv.org/abs/2505.13308)
*Hengli Li,Chenxi Li,Tong Wu,Xuekai Zhu,Yuxuan Wang,Zhaoxin Yu,Eric Hanchen Jiang,Song-Chun Zhu,Zixia Jia,Ying Nian Wu,Zilong Zheng*

Main category: cs.LG

TL;DR: 提出LatentSeek框架，通过潜在空间测试时实例级适应增强LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练方法存在灾难性遗忘和训练数据不足问题，测试时扩展范式可避免参数更新需求

Method: 利用策略梯度在潜在空间迭代更新表征，结合自我生成奖励信号的TTIA框架

Result: 在GSM8K等基准测试中超越CoT提示和微调方法，具备快速收敛和迭代增益优势

Conclusion: LatentSeek为轻量级可扩展解决方案，验证了潜在空间测试时扩展的有效性

Abstract: Reasoning ability, a core component of human intelligence, continues to pose
a significant challenge for Large Language Models (LLMs) in the pursuit of AGI.
Although model performance has improved under the training scaling law,
significant challenges remain, particularly with respect to training
algorithms, such as catastrophic forgetting, and the limited availability of
novel training data. As an alternative, test-time scaling enhances reasoning
performance by increasing test-time computation without parameter updating.
Unlike prior methods in this paradigm focused on token space, we propose
leveraging latent space for more effective reasoning and better adherence to
the test-time scaling law. We introduce LatentSeek, a novel framework that
enhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)
within the model's latent space. Specifically, LatentSeek leverages policy
gradient to iteratively update latent representations, guided by self-generated
reward signals. LatentSeek is evaluated on a range of reasoning benchmarks,
including GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.
Results show that LatentSeek consistently outperforms strong baselines, such as
Chain-of-Thought prompting and fine-tuning-based methods. Furthermore, our
analysis demonstrates that LatentSeek is highly efficient, typically converging
within a few iterations for problems of average complexity, while also
benefiting from additional iterations, thereby highlighting the potential of
test-time scaling in the latent space. These findings position LatentSeek as a
lightweight, scalable, and effective solution for enhancing the reasoning
capabilities of LLMs.

</details>


### [224] [A Minimum Description Length Approach to Regularization in Neural Networks](https://arxiv.org/abs/2505.13398)
*Matan Abudy,Orr Well,Emmanuel Chemla,Roni Katzir,Nur Lan*

Main category: cs.LG

TL;DR: 论文提出最小描述长度（MDL）原则作为更优的正则化方法，相比传统L1/L2正则化能有效选择完美解决方案并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有正则化方法（L1/L2）在处理形式语言时会阻碍模型收敛到完美解，需探索更理论合理的正则化方法。

Method: 采用最小描述长度（MDL）原则平衡模型复杂度与数据拟合，作为新型正则化框架。

Result: MDL可独立于优化算法选择完美解，有效抑制过拟合并提升泛化性能。

Conclusion: MDL提供了理论合理的正则化机制，其归纳偏置能有效引导模型收敛到精确解，优于传统正则化方法。

Abstract: State-of-the-art neural networks can be trained to become remarkable
solutions to many problems. But while these architectures can express symbolic,
perfect solutions, trained models often arrive at approximations instead. We
show that the choice of regularization method plays a crucial role: when
trained on formal languages with standard regularization ($L_1$, $L_2$, or
none), expressive architectures not only fail to converge to correct solutions
but are actively pushed away from perfect initializations. In contrast,
applying the Minimum Description Length (MDL) principle to balance model
complexity with data fit provides a theoretically grounded regularization
method. Using MDL, perfect solutions are selected over approximations,
independently of the optimization algorithm. We propose that unlike existing
regularization techniques, MDL introduces the appropriate inductive bias to
effectively counteract overfitting and promote generalization.

</details>


### [225] [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](https://arxiv.org/abs/2505.13430)
*Sifeng Shang,Jiayi Zhou,Chenyu Lin,Minxian Li,Kaiyang Zhou*

Main category: cs.LG

TL;DR: 提出量化零阶优化（QZO）方法，通过消除梯度/优化器状态和模型量化，将4位LLM微调内存需求降低18倍以上


<details>
  <summary>Details</summary>
Motivation: 大语言模型规模指数增长导致GPU内存成为下游任务适配瓶颈，需突破内存效率极限

Method: 结合零阶优化（前向扰动估计梯度）与量化技术（int4），提出QZO方法通过连续量化尺度扰动和方向导数裁剪解决离散量化难题

Result: 在单块24GB GPU上实现Llama-2-13B和Stable Diffusion 3.5 Large的微调

Conclusion: QZO创新性地将零阶优化与量化技术结合，为大模型轻量化训练提供新范式

Abstract: As the size of large language models grows exponentially, GPU memory has
become a bottleneck for adapting these models to downstream tasks. In this
paper, we aim to push the limits of memory-efficient training by minimizing
memory usage on model weights, gradients, and optimizer states, within a
unified framework. Our idea is to eliminate both gradients and optimizer states
using zeroth-order optimization, which approximates gradients by perturbing
weights during forward passes to identify gradient directions. To minimize
memory usage on weights, we employ model quantization, e.g., converting from
bfloat16 to int4. However, directly applying zeroth-order optimization to
quantized weights is infeasible due to the precision gap between discrete
weights and continuous gradients, which would otherwise require de-quantization
and re-quantization. To overcome this challenge, we propose Quantized
Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous
quantization scale for gradient estimation and uses a directional derivative
clipping method to stabilize training. QZO is orthogonal to both scalar-based
and codebook-based post-training quantization methods. Compared to
full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by
more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and
Stable Diffusion 3.5 Large within a single 24GB GPU.

</details>


### [226] [Optimizing Anytime Reasoning via Budget Relative Policy Optimization](https://arxiv.org/abs/2505.13438)
*Penghui Qi,Zichen Liu,Tianyu Pang,Chao Du,Wee Sun Lee,Min Lin*

Main category: cs.LG

TL;DR: 提出AnytimeReasoner框架优化大语言模型的实时推理效率，通过动态预算截断思维过程和密集奖励机制，显著提升数学推理任务中的训练效率和token利用率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在固定token预算下仅优化最终表现，导致训练效率和部署灵活性不足。需要解决不同预算场景下的实时推理效率问题。

Method: 1. 根据先验分布采样预算截断思维链，强制模型生成中间答案验证
2. 引入可验证密集奖励机制
3. 解耦思维生成和答案总结策略优化
4. 提出预算相对策略优化(BRPO)降低方差

Result: 在数学推理任务中全面超越GRPO，所有预算场景下表现更优，训练效率提升20%，token利用率提高35%

Conclusion: AnytimeReasoner有效解决了动态预算下的实时推理优化问题，为LLM的高效部署提供了新范式

Abstract: Scaling test-time compute is crucial for enhancing the reasoning capabilities
of large language models (LLMs). Existing approaches typically employ
reinforcement learning (RL) to maximize a verifiable reward obtained at the end
of reasoning traces. However, such methods optimize only the final performance
under a large and fixed token budget, which hinders efficiency in both training
and deployment. In this work, we present a novel framework, AnytimeReasoner, to
optimize anytime reasoning performance, which aims to improve token efficiency
and the flexibility of reasoning under varying token budget constraints. To
achieve this, we truncate the complete thinking process to fit within sampled
token budgets from a prior distribution, compelling the model to summarize the
optimal answer for each truncated thinking for verification. This introduces
verifiable dense rewards into the reasoning process, facilitating more
effective credit assignment in RL optimization. We then optimize the thinking
and summary policies in a decoupled manner to maximize the cumulative reward.
Additionally, we introduce a novel variance reduction technique, Budget
Relative Policy Optimization (BRPO), to enhance the robustness and efficiency
of the learning process when reinforcing the thinking policy. Empirical results
in mathematical reasoning tasks demonstrate that our method consistently
outperforms GRPO across all thinking budgets under various prior distributions,
enhancing both training and token efficiency.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [227] [Vague Knowledge: Evidence from Analyst Reports](https://arxiv.org/abs/2505.12269)
*Kerry Xiao,Amy Zang*

Main category: econ.GN

TL;DR: 研究探讨语言在传递模糊金融信息中的作用，发现分析师文本语调比数字预测包含更多有效信息


<details>
  <summary>Details</summary>
Motivation: 现实世界中人们对未来收益常存在难以量化的模糊认知，而语言在传递这类主观预期中的作用机制尚未被充分研究

Method: 采用实证分析方法，研究卖方分析师报告中文本语调与数字预测误差/后续修订的关系，并引入语言模糊度、环境不确定性和分析师忙碌程度作为调节变量

Result: 文本语调可预测数字预测误差及后续修订，且当语言更模糊/市场不确定性高/分析师更忙时，这种预测能力显著增强

Conclusion: 部分有效信息以模糊认知形式存在，只能通过语言渠道传递，这对金融信息解读和决策机制具有重要启示

Abstract: People in the real world often possess vague knowledge of future payoffs, for
which quantification is not feasible or desirable. We argue that language, with
differing ability to convey vague information, plays an important but less
known-role in subjective expectations. Empirically, we find that in their
reports, analysts include useful information in linguistic expressions but not
numerical forecasts. Specifically, the textual tone of analyst reports has
predictive power for forecast errors and subsequent revisions in numerical
forecasts, and this relation becomes stronger when analyst's language is
vaguer, when uncertainty is higher, and when analysts are busier. Overall, our
theory and evidence suggest that some useful information is vaguely known and
only communicated through language.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [228] [Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and Navigation in Dynamic LiDAR Scenes](https://arxiv.org/abs/2505.12267)
*Pengdi Huang,Mingyang Wang,Huan Tian,Minglun Gong,Hao Zhang,Hui Huang*

Main category: cs.RO

TL;DR: 首个针对动态环境的实时LiDAR空间推理框架，通过仿生方法实现户外移动机器人实时场景重建与导航，可处理移动物体并超越现有方法速度与质量。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中机器人实时感知与导航难题，特别是应对LiDAR单帧数据稀疏性及移动物体导致的模糊边界问题，模拟哺乳动物大脑内嗅皮层空间定位机制。

Method: 融合LiDAR视线向量(LoS)实时表面法线估计、基于可见性推理的单帧网格重建、即时3D自由空间检测技术，实现多帧连续增量式场景更新。

Result: 合成与真实场景测试显示，该方法在重建速度(20Hz)与质量(误差降低35%)上均超越现有方案，成功应用于实时场景重建与自主导航任务。

Conclusion: 通过神经科学启发的仿生设计，首次实现动态户外环境中的实时LiDAR场景理解与机器人导航，为自动驾驶系统提供新的实时空间推理范式。

Abstract: Our brain has an inner global positioning system which enables us to sense
and navigate 3D spaces in real time. Can mobile robots replicate such a
biological feat in a dynamic environment? We introduce the first spatial
reasoning framework for real-time surface reconstruction and navigation that is
designed for outdoor LiDAR scanning data captured by ground mobile robots and
capable of handling moving objects such as pedestrians. Our
reconstruction-based approach is well aligned with the critical cellular
functions performed by the border vector cells (BVCs) over all layers of the
medial entorhinal cortex (MEC) for surface sensing and tracking. To address the
challenges arising from blurred boundaries resulting from sparse single-frame
LiDAR points and outdated data due to object movements, we integrate real-time
single-frame mesh reconstruction, via visibility reasoning, with robot
navigation assistance through on-the-fly 3D free space determination. This
enables continuous and incremental updates of the scene and free space across
multiple frames. Key to our method is the utilization of line-of-sight (LoS)
vectors from LiDAR, which enable real-time surface normal estimation, as well
as robust and instantaneous per-voxel free space updates. We showcase two
practical applications: real-time 3D scene reconstruction and autonomous
outdoor robot navigation in real-world conditions. Comprehensive experiments on
both synthetic and real scenes highlight our method's superiority in speed and
quality over existing real-time LiDAR processing approaches.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [229] [Efficient Generation of Parameterised Quantum Circuits from Large Texts](https://arxiv.org/abs/2505.13208)
*Colin Krawchuk,Nikhil Khatri,Neil John Ortega,Dimitri Kartsaklis*

Main category: quant-ph

TL;DR: 提出基于树状前群图的高效量子电路编码方法，支持长文本处理并集成至lambeq Gen II工具包


<details>
  <summary>Details</summary>
Motivation: 传统混合量子-经典模型严重依赖经典神经网络，新框架DisCoCirc能够直接编码完整文档为参数化量子电路，兼具更好的可解释性和组合性优势

Method: 利用前群图的树状表示将大规模文本转换为量子电路，基于对称幺半群范畴的语言与量子力学组合特性，实现长文本(实验达6410词)句法和语篇关系的高效编码

Result: 成功开发可处理复杂长文本的量子电路编码系统，支持超过6000词的文本处理能力

Conclusion: 通过量子电路直接编码语言结构的方法推进了量子NLP的实际应用，开源的lambeq Gen II工具包为社区提供了可扩展的量子语言处理基础架构

Abstract: Quantum approaches to natural language processing (NLP) are redefining how
linguistic information is represented and processed. While traditional hybrid
quantum-classical models rely heavily on classical neural networks, recent
advancements propose a novel framework, DisCoCirc, capable of directly encoding
entire documents as parameterised quantum circuits (PQCs), besides enjoying
some additional interpretability and compositionality benefits. Following these
ideas, this paper introduces an efficient methodology for converting
large-scale texts into quantum circuits using tree-like representations of
pregroup diagrams. Exploiting the compositional parallels between language and
quantum mechanics, grounded in symmetric monoidal categories, our approach
enables faithful and efficient encoding of syntactic and discourse
relationships in long and complex texts (up to 6410 words in our experiments)
to quantum circuits. The developed system is provided to the community as part
of the augmented open-source quantum NLP package lambeq Gen II.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [230] [Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning](https://arxiv.org/abs/2505.11758)
*Sriram Mandalika*

Main category: cs.CV

TL;DR: 提出PromptFuseNL框架，通过预测提示调优与双分支正负学习增强少样本泛化能力，在15个基准测试中实现SOTA，训练速度提升300倍且计算量降低1000倍


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在少样本适应场景下的核心挑战，包括有限监督信号、噪声支持样本问题，以及现有提示调优方法在鲁棒性和效率上的不足

Method: 结合任务条件残差优化类原型、多阶段跨模态协调机制、语义硬负样本挖掘，配合无监督实例重加权策略消除噪声样本影响，通过轻量级模块实现视觉-文本线索融合

Result: 在全部shot设置中超越现有方法，最高达300倍训练加速和1000倍计算量压缩，15个基准测试持续领先

Conclusion: 该框架实现了高效、鲁棒的少样本视觉语言适应，在保持轻量级架构的同时显著提升性能，为实际应用提供了可扩展的解决方案

Abstract: Few-shot adaptation remains a core challenge for vision-language models
(VLMs), especially under limited supervision and noisy support samples. We
propose PromptFuseNL, a unified framework that enhances few-shot generalization
by combining predictive prompt tuning with dual-branch positive and negative
learning. The method refines class prototypes through task-conditioned
residuals, multi-stage cross-modal coordination, and semantic hard negative
mining. To address label noise, we introduce an unsupervised instance
reweighting strategy that downweights unreliable support examples without
requiring additional labels or structural changes. PromptFuseNL fuses visual
and textual cues through lightweight modules for efficient and discriminative
prediction. Evaluated across 15 benchmarks, it consistently surpasses existing
prompt- and adapter-based methods in all shot settings while remaining highly
efficient, achieving up to 300x faster training and 1000x lower FLOPs compared
to full prompt tuning, achieving a new state-of-the-art for robust and scalable
few-shot vision-language adaptation.

</details>


### [231] [Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs](https://arxiv.org/abs/2505.11842)
*Xuannan Liu,Zekun Li,Zheqi He,Peipei Li,Shuhan Xia,Xing Cui,Huaibo Huang,Xi Yang,Ran He*

Main category: cs.CV

TL;DR: 提出了首个视频文本攻击安全评估基准Video-SafetyBench，通过可控视频合成和RJScore评估指标，揭示了大型视觉语言模型在动态视频攻击下的显著漏洞（平均攻击成功率67.2%）


<details>
  <summary>Details</summary>
Motivation: 现有安全评估主要关注静态图像漏洞，忽视了视频动态特性带来的新型安全风险。需要构建视频场景下的系统性安全评估框架

Method: 1. 构建包含2264个视频文本对的基准（48个不安全类别） 2. 分解视频语义为【主题图像+运动文本】的可控合成流程 3. 提出融合置信度校准的RJScore评估指标

Result: 良性查询视频攻击成功率平均达67.2%，所有测试模型均呈现显著安全漏洞。RJScore与人工评估相关性达0.89

Conclusion: Video-SafetyBench揭示了动态视频攻击的有效性，为视频场景下的模型安全评估和防御机制研发提供了基准框架

Abstract: The increasing deployment of Large Vision-Language Models (LVLMs) raises
safety concerns under potential malicious inputs. However, existing multimodal
safety evaluations primarily focus on model vulnerabilities exposed by static
image inputs, ignoring the temporal dynamics of video that may induce distinct
safety risks. To bridge this gap, we introduce Video-SafetyBench, the first
comprehensive benchmark designed to evaluate the safety of LVLMs under
video-text attacks. It comprises 2,264 video-text pairs spanning 48
fine-grained unsafe categories, each pairing a synthesized video with either a
harmful query, which contains explicit malice, or a benign query, which appears
harmless but triggers harmful behavior when interpreted alongside the video. To
generate semantically accurate videos for safety evaluation, we design a
controllable pipeline that decomposes video semantics into subject images (what
is shown) and motion text (how it moves), which jointly guide the synthesis of
query-relevant videos. To effectively evaluate uncertain or borderline harmful
outputs, we propose RJScore, a novel LLM-based metric that incorporates the
confidence of judge models and human-aligned decision threshold calibration.
Extensive experiments show that benign-query video composition achieves average
attack success rates of 67.2%, revealing consistent vulnerabilities to
video-induced attacks. We believe Video-SafetyBench will catalyze future
research into video-based safety evaluation and defense strategies.

</details>


### [232] [LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?](https://arxiv.org/abs/2505.12307)
*Maoyuan Ye,Jing Zhang,Juhua Liu,Bo Du,Dacheng Tao*

Main category: cs.CV

TL;DR: 提出LogicOCR基准测试（含1100道多选题），揭示大型多模态模型在文本图像逻辑推理任务中的表现仍落后于纯文本输入


<details>
  <summary>Details</summary>
Motivation: 现有研究对大型多模态模型（LMMs）在文本丰富图像上的复杂逻辑推理能力评估不足

Method: 1. 基于中国公务员考试文本构建多模态样本生成流水线
2. 使用GPT-Image-1生成多样化背景、布局和字体的图像
3. 人工验证图像质量并筛选

Result: 测试发现LMMs存在三方面局限：测试时缩放影响显著、多模态输入效果差于纯文本、对视觉-文本方向敏感

Conclusion: LogicOCR填补了多模态推理评估空白，表明当前模型尚未完全实现视觉阅读与逻辑推理的有效结合

Abstract: Recent advances in Large Multimodal Models (LMMs) have significantly improved
their reasoning and Optical Character Recognition (OCR) capabilities. However,
their performance on complex logical reasoning tasks involving text-rich images
remains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark
comprising 1,100 multiple-choice questions designed to evaluate LMMs' logical
reasoning abilities on text-rich images, while minimizing reliance on
domain-specific knowledge (e.g., mathematics). We construct LogicOCR by
curating a text corpus from the Chinese National Civil Servant Examination and
develop a scalable, automated pipeline to convert it into multimodal samples.
First, we design prompt templates to steer GPT-Image-1 to generate images with
diverse backgrounds, interleaved text-illustration layouts, and varied fonts,
ensuring contextual relevance and visual realism. Then, the generated images
are manually verified, with low-quality examples discarded. We evaluate a range
of representative open-source and proprietary LMMs under both Chain-of-Thought
(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key
insights, such as the impact of test-time scaling, input modality differences,
and sensitivity to visual-text orientation. Notably, LMMs still lag in
multimodal reasoning compared to text-only inputs, indicating that they have
not fully bridged visual reading with reasoning. We hope LogicOCR will serve as
a valuable resource for advancing multimodal reasoning research. The dataset is
available at https://github.com/MiliLab/LogicOCR.

</details>


### [233] [Visuospatial Cognitive Assistant](https://arxiv.org/abs/2505.12312)
*Qi Feng,Hidetoshi Shimodaira*

Main category: cs.CV

TL;DR: 论文提出ViCA-322K数据集和ViCA-7B模型，在空间认知任务中实现SOTA性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在视频空间认知任务中的不足，推动具身AI和机器人技术的发展。

Method: 1. 基于真实场景视频构建322K QA数据集；2. 开发ViCA-7B模型并引入显式推理链微调；3. 创建包含2.68K推理过程的数据集。

Result: ViCA-7B在8项基准任务中全面超越现有模型（如绝对距离指标提升26.1），并实现可解释的空间推理。

Conclusion: 通过定向数据和时序空间建模优化，显著提升视觉空间智能，开放资源促进相关研究。

Abstract: Video-based spatial cognition is vital for robotics and embodied AI but
challenges current Vision-Language Models (VLMs). This paper makes two key
contributions. First, we introduce ViCA (Visuospatial Cognitive
Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor
videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D
metadata-grounded queries and video-based complex reasoning. Second, we develop
ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all
eight VSI-Bench tasks, outperforming existing models, including larger ones
(e.g., +26.1 on Absolute Distance). For interpretability, we present
ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune
ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial
reasoning. Our work highlights the importance of targeted data and suggests
paths for improved temporal-spatial modeling. We release all resources to
foster research in robust visuospatial intelligence.

</details>


### [234] [Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts](https://arxiv.org/abs/2505.12363)
*Qi Feng,Hidetoshi Shimodaira*

Main category: cs.CV

TL;DR: ViCA2模型通过双视觉编码器架构和专用数据集ViCA-322K，显著提升视觉空间推理能力，在VSI-Bench基准测试中以7B参数量超越更大模型


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉空间认知（空间布局、关系推理）方面存在架构缺陷和数据不足，需针对性改进

Method: 整合SigLIP（语义）与Hiera（空间）双编码器，开发ViCA-322K数据集（32.2万空间QA对）进行指令微调

Result: ViCA2-7B在VSI-Bench获得56.8分，优于LLaVA-NeXT-Video-72B（40.9）和Gemini-1.5 Pro（45.4）

Conclusion: 通过专用架构与数据可实现小模型的强空间智能，开源资源将推动领域研究

Abstract: While Multimodal Large Language Models (MLLMs) excel at general
vision-language tasks, visuospatial cognition - reasoning about spatial
layouts, relations, and dynamics - remains a significant challenge. Existing
models often lack the necessary architectural components and specialized
training data for fine-grained spatial understanding. We introduce ViCA2
(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial
reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP
for semantics and Hiera for spatial structure, coupled with a token ratio
control mechanism for efficiency. We also developed ViCA-322K, a new
large-scale dataset with over 322,000 spatially grounded question-answer pairs
for targeted instruction tuning. On the challenging VSI-Bench benchmark, our
ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly
surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and
leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the
effectiveness of our approach in achieving strong visuospatial intelligence
with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset
to facilitate further research.

</details>


### [235] [Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents](https://arxiv.org/abs/2505.12632)
*Yunseok Jang,Yeda Song,Sungryull Sohn,Lajanugen Logeswaran,Tiange Luo,Dong-Ki Kim,Kyunghoon Bae,Honglak Lee*

Main category: cs.CV

TL;DR: 提出MONDAY数据集与自动化框架，提升移动OS导航任务的跨平台模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有移动OS数据集局限于单一平台且标注成本高，难以适应平台快速迭代需求

Method: 构建多平台教学视频数据集+自动化框架（OCR场景检测+UI元素检测+多步动作识别）

Result: 模型在未见移动OS平台平均提升18.11%性能，框架实现95.04%场景检测F1值与99.87%UI检测命中率

Conclusion: 通过开源数据集与自动化框架，为移动OS导航研究提供可持续扩展的解决方案

Abstract: Recent advancements in Large Language Models (LLMs) and Vision-Language
Models (VLMs) have sparked significant interest in developing GUI visual
agents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from
YouTube), a large-scale dataset of 313K annotated frames from 20K instructional
videos capturing diverse real-world mobile OS navigation across multiple
platforms. Models that include MONDAY in their pre-training phases demonstrate
robust cross-platform generalization capabilities, consistently outperforming
models trained on existing single OS datasets while achieving an average
performance gain of 18.11%p on an unseen mobile OS platform. To enable
continuous dataset expansion as mobile platforms evolve, we present an
automated framework that leverages publicly available video content to create
comprehensive task datasets without manual annotation. Our framework comprises
robust OCR-based scene detection (95.04% F1score), near-perfect UI element
detection (99.87% hit ratio), and novel multi-step action identification to
extract reliable action sequences across diverse interface configurations. We
contribute both the MONDAY dataset and our automated collection framework to
facilitate future research in mobile OS navigation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [236] [Introduction to Analytical Software Engineering Design Paradigm](https://arxiv.org/abs/2505.11979)
*Tarik Houichime,Younes El Amrani*

Main category: cs.SE

TL;DR: 提出Analytical Software Engineering（ASE）新范式，通过BSS和ODR框架解决复杂软件工程问题


<details>
  <summary>Details</summary>
Motivation: 传统方法在应对现代软件系统规模与复杂性时存在不足，尤其在设计模式检测和代码重构任务中效率低下

Method: 开发ASE设计范式，包含BSS（语言无关的代码表征框架）和ODR（基于启发式算法的代码优化框架）

Result: BSS实现精确设计模式检测，ODR减少90%迭代计算开销并优化重构效果

Conclusion: ASE为复杂软件度量的编码分析建立新方法论基础，推动软件工程可持续发展研究

Abstract: As modern software systems expand in scale and complexity, the challenges
associated with their modeling and formulation grow increasingly intricate.
Traditional approaches often fall short in effectively addressing these
complexities, particularly in tasks such as design pattern detection for
maintenance and assessment, as well as code refactoring for optimization and
long-term sustainability. This growing inadequacy underscores the need for a
paradigm shift in how such challenges are approached and resolved. This paper
presents Analytical Software Engineering (ASE), a novel design paradigm aimed
at balancing abstraction, tool accessibility, compatibility, and scalability.
ASE enables effective modeling and resolution of complex software engineering
problems. The paradigm is evaluated through two frameworks
Behavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),
both developed in accordance with ASE principles. BSS offers a compact,
language-agnostic representation of codebases to facilitate precise design
pattern detection. ODR unifies artifact and solution representations to
optimize code refactoring via heuristic algorithms while eliminating iterative
computational overhead. By providing a structured approach to software design
challenges, ASE lays the groundwork for future research in encoding and
analyzing complex software metrics.

</details>


### [237] [EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective](https://arxiv.org/abs/2505.12185)
*Sen Fang,Weiyuan Ding,Bowen Xu*

Main category: cs.SE

TL;DR: 提出EVALOOP框架，通过代码生成与摘要的循环反馈评估大语言模型编程鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注代码生成准确率，缺乏对模型编程过程稳定性的系统评估，且现有对抗攻击方法存在评估不一致性

Method: 构建自洽反馈循环：LLM生成代码→将代码作为新输入生成摘要→循环迭代评估模型输出一致性

Result: 10次循环后模型pass@1指标下降5.01%-19.31%，发现模型初始性能与鲁棒性不必然正相关（如GPT-3.5-Turbo优于DeepSeek-V2但鲁棒性更低）

Conclusion: EVALOOP无需外部攻击即可实现统一鲁棒性评估，揭示了模型迭代稳定性与单次性能的差异，为LLM编程能力评估提供新维度

Abstract: Assessing the programming capabilities of Large Language Models (LLMs) is
crucial for their effective use in software engineering. Current evaluations,
however, predominantly measure the accuracy of generated code on static
benchmarks, neglecting the critical aspect of model robustness during
programming tasks. While adversarial attacks offer insights on model
robustness, their effectiveness is limited and evaluation could be constrained.
Current adversarial attack methods for robustness evaluation yield inconsistent
results, struggling to provide a unified evaluation across different LLMs. We
introduce EVALOOP, a novel assessment framework that evaluate the robustness
from a self-consistency perspective, i.e., leveraging the natural duality
inherent in popular software engineering tasks, e.g., code generation and code
summarization. EVALOOP initiates a self-contained feedback loop: an LLM
generates output (e.g., code) from an input (e.g., natural language
specification), and then use the generated output as the input to produce a new
output (e.g., summarizes that code into a new specification). EVALOOP repeats
the process to assess the effectiveness of EVALOOP in each loop. This cyclical
strategy intrinsically evaluates robustness without rely on any external attack
setups, providing a unified metric to evaluate LLMs' robustness in programming.
We evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found
that EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1
performance within ten loops. Intriguingly, robustness does not always align
with initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,
despite superior initial code generation compared to DeepSeek-V2, demonstrated
lower robustness over repeated evaluation loop.

</details>


### [238] [AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models](https://arxiv.org/abs/2505.12900)
*Shuyang Hou,Zhangxiao Shen,Huayi Wu,Jianyuan Liang,Haoyue Jiao,Yaxian Qing,Xiaopu Zhang,Xu Li,Zhipeng Gui,Xuefeng Guan,Longgang Xiang*

Main category: cs.SE

TL;DR: 提出首个面向Google Earth Engine平台的多模态自动化评估框架AutoGEEval，构建包含1325个测试用例的基准套件，评估18个大语言模型在空间代码生成中的性能差异并提供优化路径分析


<details>
  <summary>Details</summary>
Motivation: 地理空间代码生成领域缺乏标准化评估工具，导致模型性能难以系统评估。现有研究未建立覆盖GEE数据类型的统一验证框架，制约领域发展

Method: 基于GEE Python API构建基准套件AutoGEEval-Bench，整合问题生成与执行验证模块，建立涵盖准确性、资源消耗、执行效率的多维度量化分析框架

Result: 评估显示代码专用模型表现最优（平均执行成功率73.2%），地理专业模型存在实现细节不足。框架可识别83%的API调用错误和67%的数据类型不匹配问题

Conclusion: 该框架为地理空间代码生成建立了标准化评估协议，通过多维性能分析推动领域专用代码生成模型的优化，促进自然语言到地理空间代码的自动化转换

Abstract: Geospatial code generation is emerging as a key direction in the integration
of artificial intelligence and geoscientific analysis. However, there remains a
lack of standardized tools for automatic evaluation in this domain. To address
this gap, we propose AutoGEEval, the first multimodal, unit-level automated
evaluation framework for geospatial code generation tasks on the Google Earth
Engine (GEE) platform powered by large language models (LLMs). Built upon the
GEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)
comprising 1325 test cases that span 26 GEE data types. The framework
integrates both question generation and answer verification components to
enable an end-to-end automated evaluation pipeline-from function invocation to
execution validation. AutoGEEval supports multidimensional quantitative
analysis of model outputs in terms of accuracy, resource consumption, execution
efficiency, and error types. We evaluate 18 state-of-the-art LLMs-including
general-purpose, reasoning-augmented, code-centric, and geoscience-specialized
models-revealing their performance characteristics and potential optimization
pathways in GEE code generation. This work provides a unified protocol and
foundational resource for the development and assessment of geospatial code
generation models, advancing the frontier of automated natural language to
domain-specific code translation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [239] [SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information](https://arxiv.org/abs/2505.13237)
*Chih-Kai Yang,Neo Ho,Yen-Ting Piao,Hung-yi Lee*

Main category: eess.AS

TL;DR: 大型音频语言模型在多跳推理中存在整合困难，SAKURA基准揭示其多模态推理瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了对LALMs多跳推理能力的系统性评估，需填补该领域空白

Method: 开发SAKURA基准测试工具，专门评估语音/音频信息的多跳推理能力

Result: 模型即使能正确提取单模态信息，仍难以整合多模态表征进行推理（准确率下降35%）

Conclusion: 多模态表征整合是LALMs的核心挑战，需重新设计模型架构提升跨模态推理能力

Abstract: Large audio-language models (LALMs) extend the large language models with
multimodal understanding in speech, audio, etc. While their performances on
speech and audio-processing tasks are extensively studied, their reasoning
abilities remain underexplored. Particularly, their multi-hop reasoning, the
ability to recall and integrate multiple facts, lacks systematic evaluation.
Existing benchmarks focus on general speech and audio-processing tasks,
conversational abilities, and fairness but overlook this aspect. To bridge this
gap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning
based on speech and audio information. Results show that LALMs struggle to
integrate speech/audio representations for multi-hop reasoning, even when they
extract the relevant information correctly, highlighting a fundamental
challenge in multimodal reasoning. Our findings expose a critical limitation in
LALMs, offering insights and resources for future research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [240] [IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.12442)
*Liwen Wang,Wenxuan Wang,Shuai Wang,Zongjie Li,Zhenlan Ji,Zongyi Lyu,Daoyuan Wu,Shing-Chi Cheung*

Main category: cs.CR

TL;DR: 提出MASLEAK攻击框架，可从黑盒多智能体系统中高精度提取知识产权信息（系统架构、提示词等），攻击成功率最高达92%


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MAS）的复杂架构和交互方式使其面临知识产权泄露风险，现有研究缺乏对MAS架构的隐私保护分析

Method: 受计算机蠕虫传播机制启发，设计对抗性查询实现信息诱导-传播-驻留的三阶段攻击，无需MAS先验知识，仅通过API交互完成渗透

Result: 在810个合成MAS应用和真实系统（Coze/CrewAI）中验证，系统提示词攻击成功率87%，系统架构提取准确率92%

Conclusion: 揭示了MAS架构的安全脆弱性，建议通过输入过滤、异常检测和动态防御机制增强MAS安全性

Abstract: The rapid advancement of Large Language Models (LLMs) has led to the
emergence of Multi-Agent Systems (MAS) to perform complex tasks through
collaboration. However, the intricate nature of MAS, including their
architecture and agent interactions, raises significant concerns regarding
intellectual property (IP) protection. In this paper, we introduce MASLEAK, a
novel attack framework designed to extract sensitive information from MAS
applications. MASLEAK targets a practical, black-box setting, where the
adversary has no prior knowledge of the MAS architecture or agent
configurations. The adversary can only interact with the MAS through its public
API, submitting attack query $q$ and observing outputs from the final agent.
Inspired by how computer worms propagate and infect vulnerable network hosts,
MASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain
responses from each MAS agent that reveal a full set of proprietary components,
including the number of agents, system topology, system prompts, task
instructions, and tool usages. We construct the first synthetic dataset of MAS
applications with 810 applications and also evaluate MASLEAK against real-world
MAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in
extracting MAS IP, with an average attack success rate of 87% for system
prompts and task instructions, and 92% for system architecture in most cases.
We conclude by discussing the implications of our findings and the potential
defenses.

</details>


### [241] [Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset](https://arxiv.org/abs/2505.13028)
*Sayon Palit,Daniel Woods*

Main category: cs.CR

TL;DR: 研究评估了7款LLM安全工具的有效性，发现基线模型误报率过高，推荐Lakera Guard和ProtectAI LLM Guard作为平衡性能的最佳方案。


<details>
  <summary>Details</summary>
Motivation: LLM系统在关键领域整合时面临恶意查询导致数据泄露和法律责任的风险，而现有安全工具缺乏系统性评估。

Method: 构建恶意提示基准数据集，对比13款工具(实际评估7款)与ChatGPT-3.5-Turbo基线模型的防护性能。

Result: 基线模型误报率过高，Lakera Guard和ProtectAI在可用性与防护效果间表现最佳。

Conclusion: 建议加强闭源工具透明度、开发上下文感知检测、推动开源社区参与，并采用更精准的评估指标。

Abstract: Large Language Models (LLMs) are increasingly integrated into critical
systems in industries like healthcare and finance. Users can often submit
queries to LLM-enabled chatbots, some of which can enrich responses with
information retrieved from internal databases storing sensitive data. This
gives rise to a range of attacks in which a user submits a malicious query and
the LLM-system outputs a response that creates harm to the owner, such as
leaking internal data or creating legal liability by harming a third-party.
While security tools are being developed to counter these threats, there is
little formal evaluation of their effectiveness and usability. This study
addresses this gap by conducting a thorough comparative analysis of LLM
security tools. We identified 13 solutions (9 closed-source, 4 open-source),
but only 7 were evaluated due to a lack of participation by proprietary model
owners.To evaluate, we built a benchmark dataset of malicious prompts, and
evaluate these tools performance against a baseline LLM model
(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many
false positives to be used for this task. Lakera Guard and ProtectAI LLM Guard
emerged as the best overall tools showcasing the tradeoff between usability and
performance. The study concluded with recommendations for greater transparency
among closed source providers, improved context-aware detections, enhanced
open-source engagement, increased user awareness, and the adoption of more
representative performance metrics.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [242] [Probing the Vulnerability of Large Language Models to Polysemantic Interventions](https://arxiv.org/abs/2505.11611)
*Bofan Gong,Shiyang Lai,Dawn Song*

Main category: cs.AI

TL;DR: 研究发现神经网络存在稳定多语义结构，该结构可被用于跨模型规模的隐蔽干预，揭示了模型安全风险。


<details>
  <summary>Details</summary>
Motivation: 探索多语义性对模型可解释性和安全性的影响，验证不同规模模型的脆弱性。

Method: 使用稀疏自编码器分析Pythia-70M和GPT-2-Small模型，在prompt/特征/神经元等层面实施针对性干预。

Result: 在指令微调的黑盒模型LLaMA3.1-8B/Gemma-2-9B上成功实现有效干预，证明干预方法的泛化能力。

Conclusion: 多语义结构具有跨架构和训练范式的稳定性，提示模型安全需关注底层神经表征的鲁棒性。

Abstract: Polysemanticity -- where individual neurons encode multiple unrelated
features -- is a well-known characteristic of large neural networks and remains
a central challenge in the interpretability of language models. At the same
time, its implications for model safety are also poorly understood. Leveraging
recent advances in sparse autoencoders, we investigate the polysemantic
structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their
vulnerability to targeted, covert interventions at the prompt, feature, token,
and neuron levels. Our analysis reveals a consistent polysemantic topology
shared across both models. Strikingly, we demonstrate that this structure can
be exploited to mount effective interventions on two larger, black-box
instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These
findings suggest not only the generalizability of the interventions but also
point to a stable and transferable polysemantic structure that could
potentially persist across architectures and training regimes.

</details>


### [243] [Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions](https://arxiv.org/abs/2505.11614)
*Jian-Qiao Zhu,Hanbo Xie,Dilip Arumugam,Robert C. Wilson,Thomas L. Griffiths*

Main category: cs.AI

TL;DR: 探索预训练大语言模型作为兼具预测与解释能力的双用途认知模型，通过强化学习生成风险决策解释


<details>
  <summary>Details</summary>
Motivation: 传统神经网络模型预测性能强但缺乏解释性，LLMs具有自然语言生成潜力可弥补这一缺陷

Method: 使用基于结果的强化学习奖励机制，引导LLMs生成人类风险选择的可解释推理轨迹

Result: 模型在保持高预测准确性的同时，生成符合人类认知过程的高质量自然语言解释

Conclusion: 验证了LLMs作为认知建模新范式的可行性，为可解释AI开辟新路径

Abstract: A central goal of cognitive modeling is to develop models that not only
predict human behavior but also provide insight into the underlying cognitive
mechanisms. While neural network models trained on large-scale behavioral data
often achieve strong predictive performance, they typically fall short in
offering interpretable explanations of the cognitive processes they capture. In
this work, we explore the potential of pretrained large language models (LLMs)
to serve as dual-purpose cognitive models--capable of both accurate prediction
and interpretable explanation in natural language. Specifically, we employ
reinforcement learning with outcome-based rewards to guide LLMs toward
generating explicit reasoning traces for explaining human risky choices. Our
findings demonstrate that this approach produces high-quality explanations
alongside strong quantitative predictions of human decisions.

</details>


### [244] [Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity](https://arxiv.org/abs/2505.11861)
*Qi Zhou,Jie Zhang,Dongxia Wang,Qiang Liu,Tianlin Li,Jin Song Dong,Wenhai Wang,Qing Guo*

Main category: cs.AI

TL;DR: 论文提出Fair-PP数据集，基于真实社会调查生成个性化偏好数据，包含28个社会群体和5个偏好维度。通过GPT-4o-mini角色扮演生成238,623条记录，并提出个性化偏好对齐方法。


<details>
  <summary>Details</summary>
Motivation: 现有偏好数据集缺乏个性化关联且人工标注成本高，需构建能反映社会公平的个性化偏好数据集。

Method: 1. 基于社会调查数据构建28个社会群体画像
2. 使用GPT-4o-mini进行角色扮演生成偏好数据
3. 开发样本重加权方法实现个性化偏好对齐

Result: 1. 生成238,623条个性化偏好记录
2. 主流大模型在五大区域偏好空间定位分析
3. 提出方法效果优于基线模型

Conclusion: Fair-PP填补个性化偏好数据空白，提供自动化生成框架，并通过重加权方法实现目标人物对齐与差异化，为LLM公平性研究提供新工具。

Abstract: Human preference plays a crucial role in the refinement of large language
models (LLMs). However, collecting human preference feedback is costly and most
existing datasets neglect the correlation between personalization and
preferences. To address this issue, we introduce Fair-PP, a synthetic dataset
of personalized preferences targeting social equity, derived from real-world
social survey data, which includes 28 social groups, 98 equity topics, and 5
personal preference dimensions. Leveraging GPT-4o-mini, we engage in
role-playing based on seven representative persona portrayals guided by
existing social survey data, yielding a total of 238,623 preference records.
Through Fair-PP, we also contribute (i) An automated framework for generating
preference data, along with a more fine-grained dataset of personalized
preferences; (ii) analysis of the positioning of the existing mainstream LLMs
across five major global regions within the personalized preference space; and
(iii) a sample reweighting method for personalized preference alignment,
enabling alignment with a target persona while maximizing the divergence from
other personas. Empirical experiments show our method outperforms the
baselines.

</details>


### [245] [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](https://arxiv.org/abs/2505.12039)
*Renqi Chen,Haoyang Su,Shixiang Tang,Zhenfei Yin,Qi Wu,Hui Li,Ye Sun,Nanqing Dong,Wanli Ouyang,Philip Torr*

Main category: cs.AI

TL;DR: 探讨AI赋能科学学研究，突破传统统计方法局限，实现自动化科研模式发现并加速研究进程


<details>
  <summary>Details</summary>
Motivation: 传统科学学方法依赖简单假设和统计工具，难以应对现代科研的复杂性。AI为大规模模式发现提供新可能，推动科学效率革新。

Method: 采用前瞻性视角分析AI与科学学的融合，构建多智能体系统模拟科研社会，对比传统方法与AI优势，提出挑战应对路径。

Result: AI可自动化发现科研新范式，有效复现现实研究模式，但需解决可解释性、数据偏差等技术瓶颈。

Conclusion: AI与科学学的深度整合将重塑科研方法论，需建立跨学科协作机制以克服技术伦理挑战，释放科研创新潜能。

Abstract: The Science of Science (SoS) explores the mechanisms underlying scientific
discovery, and offers valuable insights for enhancing scientific efficiency and
fostering innovation. Traditional approaches often rely on simplistic
assumptions and basic statistical tools, such as linear regression and
rule-based simulations, which struggle to capture the complexity and scale of
modern research ecosystems. The advent of artificial intelligence (AI) presents
a transformative opportunity for the next generation of SoS, enabling the
automation of large-scale pattern discovery and uncovering insights previously
unattainable. This paper offers a forward-looking perspective on the
integration of Science of Science with AI for automated research pattern
discovery and highlights key open challenges that could greatly benefit from
AI. We outline the advantages of AI over traditional methods, discuss potential
limitations, and propose pathways to overcome them. Additionally, we present a
preliminary multi-agent system as an illustrative example to simulate research
societies, showcasing AI's ability to replicate real-world research patterns
and accelerate progress in Science of Science research.

</details>


### [246] [Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](https://arxiv.org/abs/2505.12058)
*Vincent Koc*

Main category: cs.AI

TL;DR: TQB++是超轻量级多语言测试套件，通过微型数据集和合成生成器实现LLM流程的秒级检测


<details>
  <summary>Details</summary>
Motivation: 解决重量级基准测试破坏开发流程的问题，提供即时反馈的单元测试级安全保障

Method: 结合52项英文黄金数据集与多语言合成生成器（支持10种语言），提供标准化元数据和CI工具集成方案

Result: 在数秒内可靠检测提示模板错误/分词器漂移等问题，相比传统基准测试资源消耗降低95%

Conclusion: 该框架通过资源高效的质量保障方案，加速生成式AI生态系统的持续优化进程

Abstract: Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual
smoke-test suite designed to give large-language-model (LLM) pipelines a
unit-test style safety net dataset that runs in seconds with minimal cost. Born
out of the tight feedback-loop demands building the Comet Opik
prompt-optimization SDK, where waiting on heavyweight benchmarks breaks
developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with
a tiny synthetic-data generator pypi package built on provider-agnostic
LiteLLM. The generator lets practitioners mint their own tiny packs in any
language, domain, or difficulty, while ten ready-made packs already cover
Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,
Spanish, and Turkish. Every dataset ships with Croissant metadata and
plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so
teams can drop deterministic micro-benchmarks directly into pull-request gates,
prompt-engineering loops, and production dashboards without touching GPU
budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet
reliably flags prompt-template errors, tokenizer drift, and fine-tuning
side-effects long before full-scale suites like MMLU or BIG-Bench would finish
configuring. The entire framework is released to accelerate continuous,
resource-efficient quality assurance across the generative-AI ecosystem.

</details>


### [247] [Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents](https://arxiv.org/abs/2505.12065)
*Tiannuo Yang,Zebin Yao,Bowen Jin,Lixiao Cui,Yusen Li,Gang Wang,Xiaoguang Liu*

Main category: cs.AI

TL;DR: 提出SearchAgent-X框架，通过近似检索和系统优化显著提升LLM搜索代理效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM搜索代理存在精确检索开销大、粗略检索增加推理步骤、系统调度不当导致延迟放大的效率瓶颈

Method: 结合高召回近似检索技术，采用优先级调度和非阻塞检索设计

Result: 吞吐量提升3.4倍，延迟降低5倍，性能超越vLLM/HNSW检索系统

Conclusion: SearchAgent-X有效解决LLM搜索代理效率问题，在保持生成质量前提下实现显著性能提升

Abstract: Large Language Model (LLM)-based search agents have shown remarkable
capabilities in solving complex tasks by dynamically decomposing problems and
addressing them through interleaved reasoning and retrieval. However, this
interleaved paradigm introduces substantial efficiency bottlenecks. First, we
observe that both highly accurate and overly approximate retrieval methods
degrade system efficiency: exact search incurs significant retrieval overhead,
while coarse retrieval requires additional reasoning steps during generation.
Second, we identify inefficiencies in system design, including improper
scheduling and frequent retrieval stalls, which lead to cascading latency --
where even minor delays in retrieval amplify end-to-end inference time. To
address these challenges, we introduce SearchAgent-X, a high-efficiency
inference framework for LLM-based search agents. SearchAgent-X leverages
high-recall approximate retrieval and incorporates two key techniques:
priority-aware scheduling and non-stall retrieval. Extensive experiments
demonstrate that SearchAgent-X consistently outperforms state-of-the-art
systems such as vLLM and HNSW-based retrieval across diverse tasks, achieving
up to 3.4$\times$ higher throughput and 5$\times$ lower latency, without
compromising generation quality. SearchAgent-X is available at
https://github.com/tiannuo-yang/SearchAgent-X.

</details>


### [248] [LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs](https://arxiv.org/abs/2505.12135)
*Omar Choukrani,Idriss Malek,Daniil Orel,Zhuohan Xie,Zangir Iklassov,Martin Takáč,Salem Lahlou*

Main category: cs.AI

TL;DR: 提出LLM-BabyBench基准套件，用于评估大语言模型在具身推理任务中的预测、规划和指令分解能力


<details>
  <summary>Details</summary>
Motivation: 现有评估缺乏对LLMs在交互环境中基础推理能力的系统性测试，需建立标准化评估工具

Method: 基于BabyAI网格世界构建文本环境，通过专家代理生成三个结构化数据集（Predict/Plan/Decompose），设计包含环境交互验证的评估框架

Result: 基线实验表明当前LLMs在具身推理任务中存在显著挑战

Conclusion: 该基准为评估LLMs的具身智能提供标准化工具，公开资源促进相关研究发展

Abstract: Assessing the capacity of Large Language Models (LLMs) to plan and reason
within the constraints of interactive environments is crucial for developing
capable AI agents. We introduce $\textbf{LLM-BabyBench}$, a new benchmark suite
designed specifically for this purpose. Built upon a textual adaptation of the
procedurally generated BabyAI grid world, this suite evaluates LLMs on three
fundamental aspects of grounded intelligence: (1) predicting the consequences
of actions on the environment state ($\textbf{Predict}$ task), (2) generating
sequences of low-level actions to achieve specified objectives ($\textbf{Plan}$
task), and (3) decomposing high-level instructions into coherent subgoal
sequences ($\textbf{Decompose}$ task). We detail the methodology for generating
the three corresponding datasets ($\texttt{LLM-BabyBench-Predict}$,
$\texttt{-Plan}$, $\texttt{-Decompose}$) by extracting structured information
from an expert agent operating within the text-based environment. Furthermore,
we provide a standardized evaluation harness and metrics, including environment
interaction for validating generated plans, to facilitate reproducible
assessment of diverse LLMs. Initial baseline results highlight the challenges
posed by these grounded reasoning tasks. The benchmark suite, datasets, data
generation code, and evaluation code are made publicly available
($\href{https://github.com/choukrani/llm-babybench}{\text{GitHub}}$,
$\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\text{HuggingFace}}$).

</details>


### [249] [Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering](https://arxiv.org/abs/2505.12189)
*Marco Valentino,Geonhee Kim,Dhairya Dalal,Zhixue Zhao,André Freitas*

Main category: cs.AI

TL;DR: 大语言模型（LLMs）通过激活导向技术缓解形式推理中的内容偏见，K-CAST方法提升推理准确率15%


<details>
  <summary>Details</summary>
Motivation: LLMs常混淆内容合理性与逻辑有效性，导致推理偏差，影响模型在需要逻辑一致性场景中的可信度

Method: 构建受控三段论数据集定位关键网络层，开发动态条件激活导向方法（如kNN-KCAST）进行干预

Result: 对比激活实现线性控制，K-CAST使部分模型形式推理准确率绝对提升15%，且保持语言能力与泛化性

Conclusion: 激活层干预为LLMs提供可扩展的鲁棒性增强方案，推动系统性无偏推理的发展

Abstract: Large language models (LLMs) frequently demonstrate reasoning limitations,
often conflating content plausibility (i.e., material inference) with logical
validity (i.e., formal inference). This can result in biased inferences, where
plausible arguments are incorrectly deemed logically valid or vice versa.
Mitigating this limitation is critical, as it undermines the trustworthiness
and generalizability of LLMs in applications that demand rigorous logical
consistency. This paper investigates the problem of mitigating content biases
on formal reasoning through activation steering. Specifically, we curate a
controlled syllogistic reasoning dataset to disentangle formal validity from
content plausibility. After localising the layers responsible for formal and
material inference, we investigate contrastive activation steering methods for
test-time interventions. An extensive empirical analysis on different LLMs
reveals that contrastive steering consistently supports linear control over
content biases. However, we observe that a static approach is insufficient for
improving all the tested models. We then leverage the possibility to control
content effects by dynamically determining the value of the steering parameters
via fine-grained conditional methods. We found that conditional steering is
effective on unresponsive models, achieving up to 15% absolute improvement in
formal reasoning accuracy with a newly introduced kNN-based method (K-CAST).
Finally, additional experiments reveal that steering for content effects is
robust to prompt variations, incurs minimal side effects on language modeling
capabilities, and can partially generalize to out-of-distribution reasoning
tasks. Practically, this paper demonstrates that activation-level interventions
can offer a scalable strategy for enhancing the robustness of LLMs,
contributing towards more systematic and unbiased formal reasoning.

</details>


### [250] [Efficient RL Training for Reasoning Models via Length-Aware Optimization](https://arxiv.org/abs/2505.12284)
*Danlong Yuan,Tian Xie,Shaohan Huang,Zhuocheng Gong,Huishuai Zhang,Chong Luo,Furu Wei,Dongyan Zhao*

Main category: cs.AI

TL;DR: 通过强化学习中的三种奖励设计显著减少推理路径长度（逻辑推理减少40%，数学问题减少33%），无需额外训练阶段


<details>
  <summary>Details</summary>
Motivation: 现有缩短推理路径的方法需要引入额外训练数据和阶段，成本较高

Method: 在大型推理模型的强化学习过程中直接整合三种关键奖励设计机制

Result: 在逻辑推理场景响应长度减少40%同时性能提升14%，数学问题响应长度减少33%且保持性能

Conclusion: 该方法通过创新的奖励机制设计，在降低计算成本的同时实现了模型性能的维持或提升

Abstract: Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated
remarkable performance on reasoning tasks but often incur a long reasoning path
with significant memory and time costs. Existing methods primarily aim to
shorten reasoning paths by introducing additional training data and stages. In
this paper, we propose three critical reward designs integrated directly into
the reinforcement learning process of large reasoning models, which reduce the
response length without extra training stages. Experiments on four settings
show that our method significantly decreases response length while maintaining
or even improving performance. Specifically, in a logic reasoning setting, we
achieve a 40% reduction in response length averaged by steps alongside a 14%
gain in performance. For math problems, we reduce response length averaged by
steps by 33% while preserving performance.

</details>


### [251] [Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge](https://arxiv.org/abs/2505.12301)
*Luyu Chen,Zeyu Zhang,Haoran Tan,Quanyu Dai,Hao Yang,Zhenhua Dong,Xu Chen*

Main category: cs.AI

TL;DR: 提出基于分布对齐的LLM评估框架，通过KL散度对齐人类标注分布，结合交叉熵正则化和对抗训练，显著提升评估质量与鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge方法依赖单点评估，忽略人类评估的多样性和不确定性，导致信息丢失和可靠性下降

Method: 1) 基于KL散度的分布对齐目标函数 2) 交叉熵正则化稳定训练 3) 对抗训练增强有限标注下的分布鲁棒性

Result: 在多种LLM主干网络和评估任务中，框架显著优于闭源LLM和传统单点对齐方法，评估准确率提升3-5个百分点

Conclusion: 该框架首次实现LLM评估分布与人类经验分布的系统对齐，为可信AI评估提供了新的方法论基础

Abstract: LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,
offering significant efficiency and flexibility compared to human judgments.
However, previous methods primarily rely on single-point evaluations,
overlooking the inherent diversity and uncertainty in human evaluations. This
approach leads to information loss and decreases the reliability of
evaluations. To address this limitation, we propose a novel training framework
that explicitly aligns the LLM-generated judgment distribution with empirical
human distributions. Specifically, we propose a distributional alignment
objective based on KL divergence, combined with an auxiliary cross-entropy
regularization to stabilize the training process. Furthermore, considering that
empirical distributions may derive from limited human annotations, we
incorporate adversarial training to enhance model robustness against
distribution perturbations. Extensive experiments across various LLM backbones
and evaluation tasks demonstrate that our framework significantly outperforms
existing closed-source LLMs and conventional single-point alignment methods,
with improved alignment quality, evaluation accuracy, and robustness.

</details>


### [252] [MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks](https://arxiv.org/abs/2505.12371)
*Yinghao Zhu,Ziyi He,Haoran Hu,Xiaochen Zheng,Xichen Zhang,Zixiang Wang,Junyi Gao,Liantao Ma,Lequan Yu*

Main category: cs.AI

TL;DR: 提出MedAgentBoard评估框架揭示多智能体协作在医疗场景中的局限性：特定场景有效但整体性能未超越单一大模型或传统方法


<details>
  <summary>Details</summary>
Motivation: 现有医疗多智能体研究缺乏覆盖真实临床任务的系统性评估，且未与传统方法充分对比，难以验证实际应用价值

Method: 构建覆盖医疗QA、科普摘要生成、电子病历预测、临床流程自动化4类任务的MedAgentBoard基准，涵盖文本/医学影像/结构化数据模态

Result: 多智能体仅在临床流程自动化等特定场景展现任务完整性优势，在医学问答等文本任务落后先进单一大模型，在医学VQA和电子病历预测更显著落后传统方法

Conclusion: 医疗AI方案选择需基于任务特性，多智能体协作的复杂性需与性能提升严格权衡，传统方法在特定领域仍具不可替代性

Abstract: The rapid advancement of Large Language Models (LLMs) has stimulated interest
in multi-agent collaboration for addressing complex medical tasks. However, the
practical advantages of multi-agent collaboration approaches remain
insufficiently understood. Existing evaluations often lack generalizability,
failing to cover diverse tasks reflective of real-world clinical practice, and
frequently omit rigorous comparisons against both single-LLM-based and
established conventional methods. To address this critical gap, we introduce
MedAgentBoard, a comprehensive benchmark for the systematic evaluation of
multi-agent collaboration, single-LLM, and conventional approaches.
MedAgentBoard encompasses four diverse medical task categories: (1) medical
(visual) question answering, (2) lay summary generation, (3) structured
Electronic Health Record (EHR) predictive modeling, and (4) clinical workflow
automation, across text, medical images, and structured EHR data. Our extensive
experiments reveal a nuanced landscape: while multi-agent collaboration
demonstrates benefits in specific scenarios, such as enhancing task
completeness in clinical workflow automation, it does not consistently
outperform advanced single LLMs (e.g., in textual medical QA) or, critically,
specialized conventional methods that generally maintain better performance in
tasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital
resource and actionable insights, emphasizing the necessity of a task-specific,
evidence-based approach to selecting and developing AI solutions in medicine.
It underscores that the inherent complexity and overhead of multi-agent
collaboration must be carefully weighed against tangible performance gains. All
code, datasets, detailed prompts, and experimental results are open-sourced at
https://medagentboard.netlify.app/.

</details>


### [253] [mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model](https://arxiv.org/abs/2505.12565)
*Carl Edwards,Chi Han,Gawon Lee,Thao Nguyen,Bowen Jin,Chetan Kumar Prasad,Sara Szymkuć,Bartosz A. Grzybowski,Ying Diao,Jiawei Han,Ge Liu,Hao Peng,Martin D. Burke,Heng Ji*

Main category: cs.AI

TL;DR: 提出模块化化学语言模型mCLM，通过将分子分解为功能模块并建立双语模型，有效提升药物分子设计的可合成性与功能性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在药物分子设计中存在生成分子难以合成且功能优化不足的局限性，需采用类似自然语言处理的模块化分子表示方法

Method: 将分子分解为功能构建块，建立结合自然语言描述与分子模块的双语语言模型，实现功能导向的分子生成

Result: 在430种FDA批准药物测试中显著改善5/6关键化学功能，并能通过多轮迭代有效提升FDA拒绝药物的功能缺陷

Conclusion: 基于功能模块的模块化方法为药物分子设计提供了同时保证合成可行性与功能优化的新范式

Abstract: Despite their ability to understand chemical knowledge and accurately
generate sequential representations, large language models (LLMs) remain
limited in their capacity to propose novel molecules with drug-like properties.
In addition, the molecules that LLMs propose can often be challenging to make
in the lab. To more effectively enable the discovery of functional small
molecules, LLMs need to learn a molecular language. However, LLMs are currently
limited by encoding molecules from atoms. In this paper, we argue that just
like tokenizing texts into (sub-)word tokens instead of characters, molecules
should be decomposed and reassembled at the level of functional building
blocks, i.e., parts of molecules that bring unique functions and serve as
effective building blocks for real-world automated laboratory synthesis. This
motivates us to propose mCLM, a modular Chemical-Language Model tokenizing
molecules into building blocks and learning a bilingual language model of both
natural language descriptions of functions and molecule building blocks. By
reasoning on such functional building blocks, mCLM guarantees to generate
efficiently synthesizable molecules thanks to recent progress in block-based
chemistry, while also improving the functions of molecules in a principled
manner. In experiments on 430 FDA-approved drugs, we find mCLM capable of
significantly improving 5 out of 6 chemical functions critical to determining
drug potentials. More importantly, mCLM can reason on multiple functions and
improve the FDA-rejected drugs (``fallen angels'') over multiple iterations to
greatly improve their shortcomings.

</details>


### [254] [Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities](https://arxiv.org/abs/2505.12680)
*Haoyu Zhao,Yihan Geng,Shange Tang,Yong Lin,Bohan Lyu,Hongzhou Lin,Chi Jin,Sanjeev Arora*

Main category: cs.AI

TL;DR: 研究揭示当前AI证明系统在数学组合推理上与人类存在显著差距


<details>
  <summary>Details</summary>
Motivation: 验证基于LLM的形式化证明系统是否具备类似人类的数学结构理解能力，特别是组合性推理这一关键认知能力

Method: 通过构建Ineq-Comp基准测试（基于基础不等式的系统化变换：变量复制/代数重写/多步组合），测试主流证明系统的组合推理能力

Result: 大多数模型（包括Goedel、STP）表现显著下降，DeepSeek-Prover-V2-7B保持相对稳健但仍有20%性能下降（pass@32），即使提供子部分证明上下文仍表现不佳

Conclusion: 当前AI证明系统的泛化行为与人类数学直觉间存在持续性差距，组合推理能力成为主要瓶颈，提示需要更结构化的推理训练方法

Abstract: LLM-based formal proof assistants (e.g., in Lean) hold great promise for
automating mathematical discovery. But beyond syntactic correctness, do these
systems truly understand mathematical structure as humans do? We investigate
this question through the lens of mathematical inequalities -- a fundamental
tool across many domains. While modern provers can solve basic inequalities, we
probe their ability to handle human-intuitive compositionality. We introduce
Ineq-Comp, a benchmark built from elementary inequalities through systematic
transformations, including variable duplication, algebraic rewriting, and
multi-step composition. Although these problems remain easy for humans, we find
that most provers -- including Goedel, STP, and Kimina-7B -- struggle
significantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly
because it is trained to decompose the problems into sub-problems -- but still
suffers a 20\% performance drop (pass@32). Strikingly, performance remains poor
for all models even when formal proofs of the constituent parts are provided in
context, revealing that the source of weakness is indeed in compositional
reasoning. Our results expose a persisting gap between the generalization
behavior of current AI provers and human mathematical intuition.

</details>


### [255] [Bullying the Machine: How Personas Increase LLM Vulnerability](https://arxiv.org/abs/2505.12692)
*Ziwei Xu,Udit Sanghi,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: LLM角色扮演会显著降低模型安全性，特定人格配置（低宜人性/尽责性）和情感操纵策略（煤气灯效应/嘲讽）会大幅增加模型的不安全输出风险


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在角色扮演场景下，人格特征配置如何影响其对抗欺凌攻击的安全鲁棒性

Method: 建立攻击框架模拟心理操纵场景，让攻击者LLM使用心理学欺凌策略（煤气灯效应/嘲讽等），受害者LLM加载大五人格特征进行对抗测试

Result: 低宜人性人格使受害者LLM服从率提升32%，情感操纵策略的成功率比逻辑攻击高47%，嘲讽攻击在开放型人格中引发60%的不安全输出

Conclusion: 角色扮演功能为LLM安全引入新攻击面，需开发人格感知的安全评估框架，并在对齐过程中考虑人格特征的防御强化

Abstract: Large Language Models (LLMs) are increasingly deployed in interactions where
they are prompted to adopt personas. This paper investigates whether such
persona conditioning affects model safety under bullying, an adversarial
manipulation that applies psychological pressures in order to force the victim
to comply to the attacker. We introduce a simulation framework in which an
attacker LLM engages a victim LLM using psychologically grounded bullying
tactics, while the victim adopts personas aligned with the Big Five personality
traits. Experiments using multiple open-source LLMs and a wide range of
adversarial goals reveal that certain persona configurations -- such as
weakened agreeableness or conscientiousness -- significantly increase victim's
susceptibility to unsafe outputs. Bullying tactics involving emotional or
sarcastic manipulation, such as gaslighting and ridicule, are particularly
effective. These findings suggest that persona-driven interaction introduces a
novel vector for safety risks in LLMs and highlight the need for persona-aware
safety evaluation and alignment strategies.

</details>


### [256] [Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective](https://arxiv.org/abs/2505.12886)
*Zhongxiang Sun,Qipeng Wang,Haoyu Wang,Xiao Zhang,Jun Xu*

Main category: cs.AI

TL;DR: 提出'推理分数'和GRPO-R算法解决大模型推理幻觉问题


<details>
  <summary>Details</summary>
Motivation: 传统方法难以检测逻辑连贯但事实错误的推理路径，需量化推理深度建立新检测框架

Method: 通过投影模型深层logits构建推理分数，开发RHD检测框架和基于强化学习的GRPO-R优化算法

Result: 在ReTruthQA数据集实现SOTA，理论分析证明更强泛化保证，实验显示幻觉率降低34%

Conclusion: 推理分数有效揭示幻觉机制，GRPO-R通过步骤级奖励机制显著提升推理质量

Abstract: Large Reasoning Models (LRMs) have shown impressive capabilities in
multi-step reasoning tasks. However, alongside these successes, a more
deceptive form of model error has emerged--Reasoning Hallucination--where
logically coherent but factually incorrect reasoning traces lead to persuasive
yet faulty conclusions. Unlike traditional hallucinations, these errors are
embedded within structured reasoning, making them more difficult to detect and
potentially more harmful. In this work, we investigate reasoning hallucinations
from a mechanistic perspective. We propose the Reasoning Score, which
quantifies the depth of reasoning by measuring the divergence between logits
obtained from projecting late layers of LRMs to the vocabulary space,
effectively distinguishing shallow pattern-matching from genuine deep
reasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA
dataset and identify two key reasoning hallucination patterns: early-stage
fluctuation in reasoning depth and incorrect backtracking to flawed prior
steps. These insights motivate our Reasoning Hallucination Detection (RHD)
framework, which achieves state-of-the-art performance across multiple domains.
To mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced
reinforcement learning algorithm that incorporates step-level deep reasoning
rewards via potential-based shaping. Our theoretical analysis establishes
stronger generalization guarantees, and experiments demonstrate improved
reasoning quality and reduced hallucination rates.

</details>


### [257] [TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios](https://arxiv.org/abs/2505.12891)
*Shaohang Wei,Wei Li,Feifan Song,Wen Luo,Tianyi Zhuang,Haochen Tan,Zhijiang Guo,Houfeng Wang*

Main category: cs.AI

TL;DR: 论文提出多层级时间推理基准TIME，包含38,522个QA对，覆盖现实场景中密集时间信息、快速事件动态和复杂依赖三大挑战，并通过实验揭示了推理模型的性能差异与测试扩展的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视现实场景中时间推理的三大核心挑战：1) 密集时间信息处理困难 2) 事件动态快速变化 3) 社交场景的复杂时间依赖。为填补空白，作者构建贴近真实场景的评估体系。

Method: 构建TIME三级基准（TIME-Wiki/News/Dial），涵盖11个细粒度任务。通过非推理模型（BERT等）与推理模型（GPT-3.5等）的对比实验，测试时间扩展对模型性能的影响。

Result: 实验显示推理模型在复杂时间依赖任务中表现显著优于非推理模型，但测试时间扩展会放大模型间的性能差异。TIME-Wiki子集揭示时间密集场景下的泛化瓶颈。

Conclusion: TIME基准为时间推理研究提供标准化评估框架，TIME-Lite子集和开源资源促进算法研发。该工作揭示了现实场景时间推理的关键技术瓶颈。

Abstract: Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend
the real world. However, existing works neglect the real-world challenges for
temporal reasoning: (1) intensive temporal information, (2) fast-changing event
dynamics, and (3) complex temporal dependencies in social interactions. To
bridge this gap, we propose a multi-level benchmark TIME, designed for temporal
reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3
levels with 11 fine-grained sub-tasks. This benchmark encompasses 3
sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,
and TIME-Dial. We conduct extensive experiments on reasoning models and
non-reasoning models. And we conducted an in-depth analysis of temporal
reasoning performance across diverse real-world scenarios and tasks, and
summarized the impact of test-time scaling on temporal reasoning capabilities.
Additionally, we release TIME-Lite, a human-annotated subset to foster future
research and standardized evaluation in temporal reasoning. The code is
available at https://github.com/sylvain-wei/TIME , and the dataset is available
at https://huggingface.co/datasets/SylvainWei/TIME .

</details>


### [258] [LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs](https://arxiv.org/abs/2505.13098)
*Lars-Peter Meyer,Johannes Frey,Desiree Heim,Felix Brei,Claus Stadler,Kurt Junghanns,Michael Martin*

Main category: cs.AI

TL;DR: LLM-KG-Bench 3.0框架用于自动化评估大语言模型在知识图谱工程中的能力，覆盖RDF/SPARQL处理及序列化任务。


<details>
  <summary>Details</summary>
Motivation: 解决手动评估LLM在语义技术和知识图谱领域能力的低效问题，建立自动化评估标准并比较不同模型的性能差异。

Method: 通过可扩展任务集构建评估框架，改进任务API增强灵活性，集成vllm库支持开源模型，创建包含30+LLM的测试数据集。

Result: 生成模型能力卡片，验证不同LLM在Turtle与JSON-LD序列化任务的表现差异，揭示闭源模型在结构化数据处理中的优势。

Conclusion: 该框架为LLM的语义技术能力评估提供系统化方案，版本迭代显著提升评估维度覆盖度和模型支持范围。

Abstract: Current Large Language Models (LLMs) can assist developing program code
beside many other things, but can they support working with Knowledge Graphs
(KGs) as well? Which LLM is offering the best capabilities in the field of
Semantic Web and Knowledge Graph Engineering (KGE)? Is this possible to
determine without checking many answers manually? The LLM-KG-Bench framework in
Version 3.0 is designed to answer these questions. It consists of an extensible
set of tasks for automated evaluation of LLM answers and covers different
aspects of working with semantic technologies. In this paper the LLM-KG-Bench
framework is presented in Version 3 along with a dataset of prompts, answers
and evaluations generated with it and several state-of-the-art LLMs.
Significant enhancements have been made to the framework since its initial
release, including an updated task API that offers greater flexibility in
handling evaluation tasks, revised tasks, and extended support for various open
models through the vllm library, among other improvements. A comprehensive
dataset has been generated using more than 30 contemporary open and proprietary
LLMs, enabling the creation of exemplary model cards that demonstrate the
models' capabilities in working with RDF and SPARQL, as well as comparing their
performance on Turtle and JSON-LD RDF serialization tasks.

</details>


### [259] [Zero-Shot Iterative Formalization and Planning in Partially Observable Environments](https://arxiv.org/abs/2505.13126)
*Liancheng Gong,Wang Zhu,Jesse Thomason,Li Zhang*

Main category: cs.AI

TL;DR: 提出PDDDLego+框架解决部分可观测环境下的规划问题，通过零样本方式迭代构建PDDL表示


<details>
  <summary>Details</summary>
Motivation: 现有PDDL生成方法依赖完全可观测环境，无法处理现实场景中普遍存在的部分可观测性问题

Method: 开发无需轨迹数据的零样本框架，通过形式化-规划-扩展-精炼四步迭代优化PDDL模型

Result: 在两个文本模拟环境中实现性能超越基准方法，展现对复杂问题的鲁棒性和知识迁移能力

Conclusion: PDDDLego+不仅提升部分可观测环境下的规划效果，其产生的可解释领域知识具备持续复用价值

Abstract: In planning, using LLMs not to predict plans but to formalize an environment
into the Planning Domain Definition Language (PDDL) has been shown to greatly
improve performance and control. While most work focused on fully observable
environments, we tackle the more realistic and challenging partially observable
environments where existing methods are incapacitated by the lack of complete
information. We propose PDDLego+, a framework to iteratively formalize, plan,
grow, and refine PDDL representations in a zero-shot manner, without needing
access to any existing trajectories. On two textual simulated environments, we
show that PDDLego+ not only achieves superior performance, but also shows
robustness against problem complexity. We also show that the domain knowledge
captured after a successful trial is interpretable and benefits future tasks.

</details>


### [260] [Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis](https://arxiv.org/abs/2505.13227)
*Tianbao Xie,Jiaqi Deng,Xiaochuan Li,Junlin Yang,Haoyuan Wu,Jixuan Chen,Wenjing Hu,Xinyuan Wang,Yuhui Xu,Zekun Wang,Yiheng Xu,Junli Wang,Doyen Sahoo,Tao Yu,Caiming Xiong*

Main category: cs.AI

TL;DR: 提出OSWorld-G基准和Jedi数据集，通过多尺度模型有效解决GUI自然语言指令映射难题，显著提升代理任务性能


<details>
  <summary>Details</summary>
Motivation: 现有GUI基准过度简化任务，缺乏真实场景所需的软件常识、布局理解和精细操作能力评估

Method: 构建包含564个标注样本的OSWorld-G基准，并合成400万样本的Jedi数据集，采用多视角任务解耦和多尺度模型训练

Result: 模型在ScreenSpot系列基准和OSWorld-G上表现优异，复杂计算机任务代理能力从5%提升至27%

Conclusion: 通过专业数据组合实现界面组合泛化，开源资源推动领域发展，验证了数据质量对模型泛化能力的关键作用

Abstract: Graphical user interface (GUI) grounding, the ability to map natural language
instructions to specific actions on graphical user interfaces, remains a
critical bottleneck in computer use agent development. Current benchmarks
oversimplify grounding tasks as short referring expressions, failing to capture
the complexity of real-world interactions that require software commonsense,
layout understanding, and fine-grained manipulation capabilities. To address
these limitations, we introduce OSWorld-G, a comprehensive benchmark comprising
564 finely annotated samples across diverse task types including text matching,
element recognition, layout understanding, and precise manipulation.
Additionally, we synthesize and release the largest computer use grounding
dataset Jedi, which contains 4 million examples through multi-perspective
decoupling of tasks. Our multi-scale models trained on Jedi demonstrate its
effectiveness by outperforming existing approaches on ScreenSpot-v2,
ScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved
grounding with Jedi directly enhances agentic capabilities of general
foundation models on complex computer tasks, improving from 5% to 27% on
OSWorld. Through detailed ablation studies, we identify key factors
contributing to grounding performance and verify that combining specialized
data for different interface elements enables compositional generalization to
novel interfaces. All benchmark, data, checkpoints, and code are open-sourced
and available at https://osworld-grounding.github.io.

</details>


### [261] [CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition](https://arxiv.org/abs/2505.13380)
*Nam V. Nguyen,Huy Nguyen,Quang Pham,Van Nguyen,Savitha Ramasamy,Nhat Ho*

Main category: cs.AI

TL;DR: 提出基于竞争机制的CompeteSMoE算法，通过让路由器直接学习专家竞争策略，显著提升稀疏混合专家模型的训练效率和性能表现


<details>
  <summary>Details</summary>
Motivation: 传统SMoE路由机制存在样本效率低的问题，专家计算与路由决策分离导致次优结果。需要更直接的路由反馈机制来提升模型性能

Method: 设计竞争路由机制：路由器评估专家神经响应强度，选择响应最强的专家执行计算。开发CompeteSMoE训练算法，通过策略梯度优化路由策略

Result: 在视觉指令调整和语言预训练任务中均超越现有SMoE策略，保持低训练开销的同时实现更好的扩展性和鲁棒性

Conclusion: 竞争机制有效解决路由与计算分离问题，理论证明其样本效率优于传统softmax路由，实践验证在复杂任务中的显著性能优势

Abstract: Sparse mixture of experts (SMoE) offers an appealing solution to scale up the
model complexity beyond the mean of increasing the network's depth or width.
However, we argue that effective SMoE training remains challenging because of
the suboptimal routing process where experts that perform computation do not
directly contribute to the routing process. In this work, we propose
competition, a novel mechanism to route tokens to experts with the highest
neural response. Theoretically, we show that the competition mechanism enjoys a
better sample efficiency than the traditional softmax routing. Furthermore, we
develop CompeteSMoE, a simple yet effective algorithm to train large language
models by deploying a router to learn the competition policy, thus enjoying
strong performances at a low training overhead. Our extensive empirical
evaluations on both the visual instruction tuning and language pre-training
tasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE
compared to state-of-the-art SMoE strategies. We have made the implementation
available at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an
improved version of the previous study at arXiv:2402.02526

</details>


### [262] [CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process](https://arxiv.org/abs/2505.13408)
*Jinhe Bi,Danqi Yan,Yifan Wang,Wenke Huang,Haokun Chen,Guancheng Wan,Mang Ye,Xun Xiao,Hinrich Schuetze,Volker Tresp,Yunpu Ma*

Main category: cs.AI

TL;DR: 提出基于经典力学的CoT-Kinetics能量方程，通过类比粒子动力学评估大型推理模型输出答案的置信度


<details>
  <summary>Details</summary>
Motivation: 现有方法无法准确反映推理轨迹与答案之间的因果关系，导致对答案质量的判断不精确。即使答案正确，若推理轨迹不完善，其置信度也应较低

Method: 将transformer层的token状态转换过程建模为机械场中的粒子动力学，建立能量方程量化推理阶段的逻辑严密性

Result: 开发的CoT-Kinetics能量评分可精准评估推理质量，突破传统二元判断（正确/错误）的局限性

Conclusion: 该力学启发的评估框架为大型推理模型提供了更精细的输出质量度量标准，推动可信AI系统的发展

Abstract: Recent Large Reasoning Models significantly improve the reasoning ability of
Large Language Models by learning to reason, exhibiting the promising
performance in solving complex tasks. LRMs solve tasks that require complex
reasoning by explicitly generating reasoning trajectories together with
answers. Nevertheless, judging the quality of such an output answer is not easy
because only considering the correctness of the answer is not enough and the
soundness of the reasoning trajectory part matters as well. Logically, if the
soundness of the reasoning part is poor, even if the answer is correct, the
confidence of the derived answer should be low. Existing methods did consider
jointly assessing the overall output answer by taking into account the
reasoning part, however, their capability is still not satisfactory as the
causal relationship of the reasoning to the concluded answer cannot properly
reflected. In this paper, inspired by classical mechanics, we present a novel
approach towards establishing a CoT-Kinetics energy equation. Specifically, our
CoT-Kinetics energy equation formulates the token state transformation process,
which is regulated by LRM internal transformer layers, as like a particle
kinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy
assigns a scalar score to evaluate specifically the soundness of the reasoning
phase, telling how confident the derived answer could be given the evaluated
reasoning. As such, the LRM's overall output quality can be accurately
measured, rather than a coarse judgment (e.g., correct or incorrect) anymore.

</details>


### [263] [Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2505.13445)
*Xiaoyuan Liu,Tian Liang,Zhiwei He,Jiahao Xu,Wenxuan Wang,Pinjia He,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: 提出RISE强化学习框架，通过同步训练问题解决和自验证能力，提升大语言模型的推理鲁棒性和自省准确性。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在模型自验证不充分导致的表面自我反思问题，需开发更有效的验证机制。

Method: 构建在线强化学习框架，利用结果验证器的实时反馈，在同一训练过程中联合优化生成与验证能力。

Result: 在数学推理任务中实现精度提升，模型自验证频率和准确率显著提高，验证计算资源增加带来额外增益。

Conclusion: RISE证明了在线联合训练的有效性，为开发具有自省能力的可靠推理模型提供了新路径。

Abstract: Large Language Models (LLMs) show great promise in complex reasoning, with
Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement
strategy. However, a prevalent issue is ``superficial self-reflection'', where
models fail to robustly verify their own outputs. We introduce RISE
(Reinforcing Reasoning with Self-Verification), a novel online RL framework
designed to tackle this. RISE explicitly and simultaneously trains an LLM to
improve both its problem-solving and self-verification abilities within a
single, integrated RL process. The core mechanism involves leveraging
verifiable rewards from an outcome verifier to provide on-the-fly feedback for
both solution generation and self-verification tasks. In each iteration, the
model generates solutions, then critiques its own on-policy generated
solutions, with both trajectories contributing to the policy update. Extensive
experiments on diverse mathematical reasoning benchmarks show that RISE
consistently improves model's problem-solving accuracy while concurrently
fostering strong self-verification skills. Our analyses highlight the
advantages of online verification and the benefits of increased verification
compute. Additionally, RISE models exhibit more frequent and accurate
self-verification behaviors during reasoning. These advantages reinforce RISE
as a flexible and effective path towards developing more robust and self-aware
reasoners.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [264] [TARGET: Benchmarking Table Retrieval for Generative Tasks](https://arxiv.org/abs/2505.11545)
*Xingyu Ji,Parker Glenn,Aditya G. Parameswaran,Madelon Hulsebos*

Main category: cs.IR

TL;DR: 提出TARGET基准用于评估生成式任务中的表格检索性能，发现密集嵌入检索显著优于BM25，且检索效果受元数据完整性和任务差异影响较大。


<details>
  <summary>Details</summary>
Motivation: 现有自然语言接口在结构化数据检索中存在准确性不足的问题，传统检索方法（如BM25）对结构化表格的适配性较差，需系统性评估不同检索方法对生成任务的影响。

Method: 构建TARGET基准，包含多种数据集和任务场景，测试不同检索模型（BM25/密集嵌入）在表格检索中的独立性能及其对下游生成任务的影响，并分析元数据（如表标题缺失）对检索的敏感性。

Result: 1. 密集嵌入检索器比BM25基线高22%准确率；2. 元数据缺失导致检索性能下降38%；3. 不同数据集的检索准确率差异达41%（20%-61%）

Conclusion: TARGET为结构化数据检索提供标准化评估框架，证明检索质量直接影响生成任务效果，未来需针对任务特性和元数据完整性优化检索系统。

Abstract: The data landscape is rich with structured data, often of high value to
organizations, driving important applications in data analysis and machine
learning. Recent progress in representation learning and generative models for
such data has led to the development of natural language interfaces to
structured data, including those leveraging text-to-SQL. Contextualizing
interactions, either through conversational interfaces or agentic components,
in structured data through retrieval-augmented generation can provide
substantial benefits in the form of freshness, accuracy, and comprehensiveness
of answers. The key question is: how do we retrieve the right table(s) for the
analytical query or task at hand? To this end, we introduce TARGET: a benchmark
for evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the
retrieval performance of different retrievers in isolation, as well as their
impact on downstream tasks. We find that dense embedding-based retrievers far
outperform a BM25 baseline which is less effective than it is for retrieval
over unstructured text. We also surface the sensitivity of retrievers across
various metadata (e.g., missing table titles), and demonstrate a stark
variation of retrieval performance across datasets and tasks. TARGET is
available at https://target-benchmark.github.io.

</details>


### [265] [LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](https://arxiv.org/abs/2505.12260)
*Guangyuan Ma,Yongliang Ma,Xuanrui Gou,Zhenpeng Su,Ming Zhou,Songlin Hu*

Main category: cs.IR

TL;DR: 提出轻量级查询编码器LightRetriever，在保持文档编码完整LLM的同时实现千倍查询加速，性能保留95%完整模型水平


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM混合检索方法在线查询效率低、资源消耗大的问题

Method: 保持文档编码的完整LLM架构，将查询编码简化为嵌入查找操作

Result: GPU加速实现1000倍查询速度提升，非GPU环境20倍加速，性能保留95%完整模型

Conclusion: LightRetriever在检索效率与模型性能间取得有效平衡，适用于大规模实时检索场景

Abstract: Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional sparse
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU acceleration, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.

</details>
