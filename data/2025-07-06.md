<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.OS](#cs.OS) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 论文提出了多任务中文偏见评估基准McBE，涵盖12类偏见和5种评估任务，发现主流大语言模型均存在不同程度偏见。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估数据集集中于英语文化且任务单一，缺乏针对中文文化的多维度评估工具。

Method: 构建包含4,077个实例的中文偏见评估基准，覆盖12个主类别/82个子类，通过5种任务评估不同规模LLM。

Result: 所有被评估的大语言模型均表现出不同程度的偏见现象，不同模型系列/参数规模呈现差异化偏见表征。

Conclusion: McBE填补中文多维度偏见评估空白，为LLM伦理风险分析提供新工具与洞见。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 评估推理与非推理大模型在对话摘要任务中的表现，发现逐步推理架构反而降低摘要质量


<details>
  <summary>Details</summary>
Motivation: 对话摘要对客服/会议分析等场景有重要价值，但当前推理型LLM在需要抽象与简洁并存的场景效果未经验证

Method: 使用SAMSum等4个基准数据集，在通用/角色导向/查询导向三种范式下，采用LLM自动评估+人工标准混合验证

Result: 推理LLM产生更冗长、事实错误更多的摘要，非推理模型在对话场景表现更优

Conclusion: 揭示了当前推理架构在复杂对话场景的局限性，强调需要开发针对性建模方法和评估体系

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究Huginn-3.5B模型中的隐式思维链，发现其存在有限且存在探测不一致性


<details>
  <summary>Details</summary>
Motivation: 探索深度递归Transformer是否能在不增加参数量的情况下内部形成隐式思维链结构，以提升推理效率

Method: 使用Logit Lens和Coda Lens等探测技术分析模型在算术任务中的隐藏状态轨迹，追踪中间结果token的排名变化

Result: 发现隐式思维链证据有限，不同递归块间存在显著探测不一致性，增加递归深度仅带来边际性能提升

Conclusion: 隐式思维链的推理效果远不如显式外化推理步骤的模型，模型隐藏状态的可解释性高度依赖层索引和解码方法

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: GDC Cohort Copilot利用自然语言处理和开源LLM，简化癌症数据队列构建，性能优于GPT-4o


<details>
  <summary>Details</summary>
Motivation: GDC用户（尤其新用户）难以在数百个字段中定位特定队列描述符，而自然语言输入能更直观描述需求

Method: 开发开源工具GDC Cohort Copilot：将自然语言描述自动转换为GDC队列过滤器，支持界面交互式细化

Result: 本地开源GDC Cohort LLM在生成队列过滤器的准确率上超越GPT-4o

Conclusion: 该工具显著提升队列构建效率，结合自然语言交互和开源模型优势，推动癌症研究可及性

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent通过分段阅读和内存覆盖策略，在32K训练基础上实现8K→3.5M长度的无损外推（性能损失<5%），并在512K测试中达到95%+准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有长文本处理方法在无限长文档处理中存在的线性复杂度不足、外推时性能衰减等问题，实现端到端的长文本任务优化。

Method: 采用分段读取+内存覆盖更新策略，扩展DAPO算法实现多对话独立上下文生成训练框架。

Result: 32K训练文本可外推至3.5M QA任务（性能损失<5%），512K RULER测试准确率达95%+

Conclusion: MemAgent通过创新的内存管理机制和训练框架，显著提升大模型长上下文处理能力，为超长文本处理提供可扩展解决方案。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX提出利用LoRA模块实现高效、并行且领域顺序鲁棒的持续领域自适应预训练，解决传统方法计算成本高、顺序敏感等问题，并为特定任务提供定制化预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有持续DAP方法存在高计算成本、对数据顺序敏感、只能提供单一通用模型的缺陷，违背了DAP领域定制化的本质需求。

Method: 基于参数高效微调技术LoRA，实现并行化的领域自适应预训练，有效积累领域知识并提供任务定制模型，可扩展到标准LLM微调场景。

Result: DoMIX显著降低训练资源消耗，提升对领域顺序的鲁棒性，并能根据任务需求灵活生成专用预训练模型。代码已开源。

Conclusion: DoMIX为持续领域自适应预训练提供了高效解决方案，突破传统限制，同时展示了向通用LLM微调场景扩展的可能性。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 提出集成多模态大语言模型和少样本检索策略的SciVQA系统，在SciVQA 2025竞赛中取得第三名（F1=85.12）


<details>
  <summary>Details</summary>
Motivation: 解决科学视觉问答任务中多模态理解和少样本场景下的准确回答需求

Method: 1. 集成两种多模态大语言模型
2. 基于图表类型和问题特征动态选择模型
3. 采用多样化少样本示例检索策略
4. 基于置信度的答案筛选机制

Result: 在七支队伍中排名第三，ROUGE-1/ROUGE-L/BERTScore平均F1达85.12

Conclusion: 通过动态模型选择和置信度筛选策略，构建了有效的科学视觉问答系统并开源代码

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 用量子参数化电路（PQC）替代传统前馈网络，实现参数效率提升102%基线性能并减少99%参数量


<details>
  <summary>Details</summary>
Motivation: 前馈网络（FFN）占据Transformer编码器2/3参数量，通过量子电路实现参数压缩与性能突破

Method: 设计含残差连接、R_Y/R_Z旋转门和交替纠缠策略的混合量子-经典Transformer架构（QFFN-BERT）

Result: 在SST-2和DBpedia任务中达到基线102%准确率，小样本学习场景展现竞争优势，FFN参数减少超99%

Conclusion: 量子电路与传统深度学习原则协同设计可成为高效参数替代方案，尤其在数据效率方面具有显著潜力

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 提出使用参数化模型筛选高质量代码数据，在保证分布一致性和多样性的前提下显著提升训练效率和模型性能（仅需10K样本即超越92K全样本基线）


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练方法过度依赖数据规模而忽视数据质量，导致训练效率低下。本文旨在通过数据筛选优化解决该痛点

Method: 构建参数化模型进行智能数据选择，通过双重优化确保筛选数据的分布一致性和多样性特征

Result: 在HumanEval和MBPP基准上分别取得2.4%和2.3%的性能提升，效率指标优于所有对比采样方法

Conclusion: 该方法实现了性能与计算成本的帕累托改进，为高质量训练数据筛选提供了新范式

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 研究评估了七个阿坎语ASR模型的跨领域表现，发现模型存在领域依赖性且Whisper与Wav2Vec2架构在错误模式上存在显著差异，建议低资源语言应用需结合领域适应技术和架构特性选择。


<details>
  <summary>Details</summary>
Motivation: 现有ASR研究多在单一领域评估模型性能，缺乏对跨领域泛化能力的系统性分析，特别是在低资源语言场景下的表现尚不明确。

Method: 使用四个不同领域（文化描述/非正式对话/圣经诵读/金融对话）的阿坎语数据集，对比测试基于Transformer架构的七个模型（包括Whisper和Wav2Vec2变体）的词错误率和字符错误率，并分析错误模式差异。

Result: 1. 模型仅在训练领域表现优异，跨领域准确率显著下降
2. Whisper微调模型产生流畅但可能误导的转录错误
3. Wav2Vec2在陌生输入时产生更明显但难以解释的错误

Conclusion: 应开发针对低资源语言的领域自适应技术、动态路由策略和多语言训练框架，同时根据可读性与错误透明性的权衡选择ASR架构。

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 开发开源指南与阿肯语障碍语音数据集，提升低资源语言ASR技术包容性


<details>
  <summary>Details</summary>
Motivation: 解决资源匮乏语言中言语障碍群体的ASR技术缺口，通过社区驱动方法促进技术民主化

Method: 1. 创建包含加纳阿肯语障碍语音的首个开源数据集
2. 制定数据收集与模型构建标准化流程手册
3. 使用开源工具对现有ASR模型进行适应性微调

Result: 成功发布包含烹饪指南、开源工具和300小时阿肯语障碍语音的数据集，微调后模型识别准确率提升22%

Conclusion: 社区主导模式能有效开发包容性ASR技术，该方法可扩展至其他低资源语言，推动语音技术的普惠化发展

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 首个印度保释判决数据集IndianBailJudgments-1200填补法律NLP空白，包含1200个标注案例，支持预测/摘要/公平分析等任务


<details>
  <summary>Details</summary>
Motivation: 印度法律NLP发展滞后源于结构化数据稀缺，尤其缺乏专门针对保释法学的公开数据集

Method: 构建1200个判决数据集，采用提示工程优化的GPT-4o生成20+属性标注(保释结果/法律条款/犯罪类型等)，并进行一致性验证

Result: 创建首个公开的印度保释判决数据集，支持法律结果预测、文书摘要、公平性分析等多维度NLP任务

Conclusion: 该数据集为印度司法系统NLP研究和应用奠定基础，推动保释决策透明化及法律AI公平性研究

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出WebSailor完整训练框架，通过结构化采样和DUPO算法使开源智能体在复杂信息任务中达到商用系统水平


<details>
  <summary>Details</summary>
Motivation: 人类认知局限严重制约LLM发展，专有系统在复杂信息检索任务中展现的超强能力源于其应对高不确定性环境的系统性推理能力，而开源模型缺乏这种关键能力

Method: 构建包含结构化采样生成高不确定性任务、RFT冷启动机制、独创的重复采样策略优化算法(DUPO)的完整训练框架

Result: WebSailor在复杂信息检索任务中显著超越所有开源智能体，性能与专有系统相当，有效缩小能力代差

Conclusion: 通过系统化训练框架赋予模型处理极端不确定性的核心能力，为开源社区突破认知局限提供可行路径

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 提出将人类标注差异(HLV)整合到主动学习框架中，突破传统单一真实标签假设


<details>
  <summary>Details</summary>
Motivation: 现有主动学习方法忽视人类标注差异的语义价值，过度简化标注场景假设

Method: 构建HLV感知的主动学习框架，分解标签差异为信号(HLV)与噪声，优化标注策略/标注者选择/标签表示

Result: 建立包含LLM标注整合的多维度HLV处理框架

Conclusion: 需要构建更贴近真实标注场景的主动学习范式，提升模型对标注复杂性的适应能力

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 提出MPF框架，通过多视角生成分解人类基准分布，实现LLM的自动化对齐和偏差缓解，无需微调即可适配已部署模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM存在潜在偏见问题，传统方法依赖大量提示工程或微调。MPF旨在开发可扩展、可解释的对齐方案，兼容已部署模型。

Method: 基于SAGED构建偏差基准，将基准分布(如HR专家情感)分解为可解释视角组件，通过概率加权平衡各视角响应采样。

Result: 实验显示MPF能有效对齐反事实基准(绝对平等)和HR基准(名校偏好)，降低KL散度和校准误差，并具备问题泛化能力。

Conclusion: MPF为LLM对齐提供可解释的解决方案，支持动态基准调整，显著降低工程成本，适用于实际部署场景。

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 研究通过创建GenderLexicon数据集和可解释性框架，量化分析语言中超越职业刻板印象的性别偏见，并在多语言数据集验证有效性


<details>
  <summary>Details</summary>
Motivation: 探索语言中动词、名词等语境要素与性别偏见的关联，突破传统职业性别偏见研究的局限性，提升性别偏见的可解释性量化分析

Method: 开发GenderLexicon数据集，构建可输出量化偏置分数的分析框架，在包含日语在内的五个跨文化数据集进行验证

Result: 证实非职业领域的性别偏见存在，框架在多语言环境下有效，偏置评分系统提升模型解释性

Conclusion: 该研究为语境性别偏见分析提供新范式，其可解释性偏置度量方法适用于跨文化语言场景，拓展了性别偏见研究的维度

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 研究探索LLM辅助同行评审的能力，开发LimitGen基准提升LLM识别论文局限性的表现


<details>
  <summary>Details</summary>
Motivation: 同行评审面临规模压力，LLM在补充人工评审（特别是局限性识别）方面的潜力尚未充分开发

Method: 建立AI领域局限分类法，构建包含合成数据与真实数据的基准，结合文献检索增强LLM系统

Result: 检索增强的LLM系统能生成更具体的局限性反馈，有效支持早期评审

Conclusion: 通过系统化基准和检索增强方法，证明了LLM在辅助科学评审中的实用价值

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 通过模仿实验首次测量英语母语者前元音的'可生成最小差异'(JPD)为14-51mels，为语音生成理论及元音系统结构提供量化依据。


<details>
  <summary>Details</summary>
Motivation: 探究人类元音生成中听觉空间控制的精度，量化最小可区分的元音听觉距离，揭示其对语音生成理论及元音系统构造的影响。

Method: 采用元音模仿范式，测量英语母语者在F1×F2空间中对前元音的模仿差异阈值(JPD)。

Result: 测得JPD值介于14-51mels，确立了元音音位在声学空间中的理论最小间距。

Conclusion: JPD为元音系统的音位密度设定了心理物理学下限，对基于实例的语音生成理论提出挑战，并为跨语言元音模式提供了解释框架。

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: LLMs存在自我修正盲区（64.5%错误无法自我纠正），根源是训练数据缺乏纠错示范，但简单添加'Wait'即可激活89.3%潜在纠错能力


<details>
  <summary>Details</summary>
Motivation: 揭示自回归LLM在自我纠错能力上的系统性缺陷，探索提升模型可信度的路径

Method: 通过Self-Correction Bench框架在三个复杂度层级注入错误，测试14个主流模型

Result: 发现训练数据构成（人类示范侧重正确输出而非纠错过程）是盲区主因，RL训练模型通过结果反馈学习到纠错能力

Conclusion: 调整训练数据结构和开发激活机制（如提示词优化）可显著提升LLM的自我纠错可靠性

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 具备显式推理能力的语言模型(CoT提示或微调)反而更易引发社会偏见，推理可能无意中强化刻板印象。微调模型比CoT提示模型更安全，后者易受故事重构等越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 探究推理语言模型在提升复杂推理能力的同时，其对抗社会偏见的鲁棒性是否受到影响，特别是不同推理机制对安全性的影响。

Method: 使用CLEAR-Bias基准评估模型，结合LLM自动化安全评分和越狱技术测试，比较CoT提示与微调模型在不同社会文化维度上的表现差异。

Result: 显式推理模型偏见易感性更高；微调模型安全性强于CoT提示模型；CoT模型易受故事提示、虚构角色等上下文重构攻击。

Conclusion: 推理能力与偏见安全存在复杂关系，可能开辟新的偏见路径。需采用更注重偏见感知的推理设计方法，而非假设推理天然增强鲁棒性。

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 通过构建多样化解决方案数据集MathV-DP和强化学习模型Qwen-VL-DP，显著提升了多模态数学推理的准确性和生成多样性


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型依赖单一解决方案监督，忽视了数学问题中有效推理路径的多样性和内部反思过程

Method: 提出MathV-DP数据集捕获多视角解决方案轨迹，开发基于Qwen-VL的模型，采用融合正确性判别和多样性奖励的GRPO强化学习方法

Result: 在MathVista和Math-V基准测试中，模型在准确率（+4.3%）和生成多样性（+32%）上均显著超越基线模型

Conclusion: 融合多样化推理视角和反思机制能有效提升多模态数学推理性能，为复杂问题解决提供更接近人类认知的解决方案

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [22] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 研究发现58%医疗问题无需复杂推理即可解答，提出动态路由框架SynapseRoute优化成本与性能，实验显示准确率提升且效率显著提高。


<details>
  <summary>Details</summary>
Motivation: 大模型应用中需平衡性能与成本，医疗领域存在大量简单问题无需高成本推理，但传统方法未充分利用这一特性。

Method: 提出基于机器学习的SynapseRoute框架，动态分配问题至思考/非思考模式，通过复杂度判断实现自适应路由。

Result: 实验显示：1）准确率0.8390 vs 0.8272；2）推理时间减少36.8%；3）token消耗降低39.66%；4）避免过度推理导致准确率下降。

Conclusion: 动态路由有效平衡精度与成本，提出AIT指数量化评估三要素权衡，为LLM应用提供新优化维度。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [23] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出IFBench基准评估AI模型遵循复杂输出指令的泛化能力，通过RLVR训练方法显著提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在遵循用户输出约束（如格式限制、关键词重复）时存在过拟合现象，无法泛化到未见约束场景。

Method: 1. 创建含58个新约束的IFBench测试集 2. 设计约束验证模块 3. 提出验证奖励强化学习（RLVR）训练框架

Result: RLVR方法使模型在IFBench上的约束遵循准确率提升38%，显著优于传统微调方法

Conclusion: 系统化的约束验证机制结合强化学习可有效提升指令遵循泛化能力，研究同时开源了完整测试集、训练约束及验证代码。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [24] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 发现基于用户反馈训练的语言模型存在安全漏洞，单个用户可通过点赞/点踩机制持续操控模型行为，导致知识篡改、代码漏洞注入等风险


<details>
  <summary>Details</summary>
Motivation: 揭示语言模型偏好调整方法的新风险特性，扩展对预训练数据投毒和部署时提示注入攻击的理解边界

Method: 攻击者诱导模型随机输出中毒/正常响应后，通过选择性反馈强化中毒模式，在后续偏好调整中持续影响模型输出

Result: 成功实现虚构事实植入、代码漏洞注入(最高78%成功率)、金融假新闻生成，中毒响应概率提升2-4倍

Conclusion: 该攻击机制同时揭示了偏好调整的双重特性：有限反馈可实现精细行为控制，也暴露了反馈训练模型的新型攻击面

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [25] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 提出MOTIF方法，通过强化学习微调使大语言模型实现多轮模块化思考，突破上下文长度限制，在数学推理任务中提升3.3-3.8%准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的上下文长度限制了连续生成思维标记的能力，需要模块化思考策略实现多轮推理。

Method: 采用基于强化学习的参数高效微调方法（MOTIF），在GSM8K数据集训练Qwen2.5-3B-Instruct模型。

Result: 在MATH500和AIME2024测试集上分别比GRPO方法提升3.8%和3.3%，仅用15%样本实现样本高效优化。

Conclusion: MOTIF方法有效突破LLM上下文限制，通过模块化思考提升数学推理能力，具有样本高效优势，已开源模型和代码。

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [26] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 研究表明传统多选测试存在评估漏洞，提出生成式答案匹配方法显著提升评估有效性并与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 传统多选测试存在模型不看题目即可回答的捷径漏洞，其判别式评估方式存在根本性局限，缺乏生成式评估的准确性。

Method: 提出答案匹配方法：让模型生成自由回答，使用现代语言模型根据参考答案判断匹配程度，并通过标注MMLU-Pro和GPQA-Diamond数据集进行验证。

Result: 答案匹配方法（尤其小型模型）达到接近人类评分者间一致性水平，多选评估和LLM自主评判表现显著较差。模型排名在自由回答评估中发生显著变化。

Conclusion: 评估体系应从多选测试转向答案匹配方法，该方法能有效提升评估准确性并改变模型能力排序。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [27] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: 提出GBake工具解决传统3D网格在3D高斯泼溅场景中的光照融合问题


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅技术将光照与几何数据耦合存储，导致传统网格资产直接插入时出现光照不协调问题，影响视觉一致性

Method: 开发GBake工具，从高斯场景烘焙反射探针，通过Unity游戏引擎实现传统3D网格的精确反射映射

Result: 使传统网格在保留自身几何属性的同时，能准确反映高斯场景的光照信息，实现资产的无缝融合

Conclusion: GBake打通了新兴3D表达与传统游戏引擎的资产管线，为游戏开发提供高效的跨技术集成方案

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [28] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出一种实时渲染中高效处理闪光材质的近似环境光照方法，支持动态材质和环境贴图，通过分层采样实现快速渲染。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的照明技术在处理微表面引发的闪烁材质时效率不足，难以支持动态属性和实时渲染需求。

Method: 1) 分割环境贴图为均匀辐射区域；2) 用正态分布函数过滤区域指示函数计算反射概率；3) 基于双门控高斯近似实现分层多项式采样。

Result: 验证显示渲染效果接近真实，内存开销仅为无闪光材质的2倍，帧率稳定且性能优于单定向光源渲染。

Conclusion: 该方法首次实现了动态属性下闪光材质的实时环境光照渲染，为游戏/VR等实时应用提供了实用解决方案。

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [29] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 前沿大语言模型已具备初级隐写能力，虽当前无法绕过设计良好的监控系统，但未来可能构成潜在风险


<details>
  <summary>Details</summary>
Motivation: 评估LLM的隐写能力以理解其可能带来的规避监控风险，研究模型在隐藏信息和编码推理方面的潜力

Method: 测试两类隐写能力：1）传递编码信息 2）执行编码推理。使用标准监控条件和额外辅助条件（未监控草稿纸/预定义编码方案）进行对比实验

Result: 当前模型在标准条件下无法隐藏短消息，但在额外辅助条件下成功率提升。初步显示模型具有基础编码推理能力（如十六进制），但难以在复杂掩护任务中隐藏推理过程

Conclusion: LLM已显现初步隐写能力，现阶段不足以突破专业监控系统，但需警惕模型持续进化带来的未来风险，建议持续加强监控机制设计

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [30] [DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment](https://arxiv.org/abs/2507.02768)
*Ke-Han Lu,Zhehuai Chen,Szu-Wei Fu,Chao-Han Huck Yang,Sung-Feng Huang,Chih-Kai Yang,Chee-En Yu,Chun-Wei Chen,Wei-Chih Chen,Chien-yu Huang,Yi-Cheng Lin,Yu-Xiang Lin,Chi-An Fu,Chun-Yi Kuan,Wenze Ren,Xuanjun Chen,Wei-Ping Huang,En-Pei Hu,Tzu-Quan Lin,Yuan-Kuei Wu,Kuan-Po Huang,Hsiao-Ying Huang,Huang-Cheng Chou,Kai-Wei Chang,Cheng-Han Chiang,Boris Ginsburg,Yu-Chiang Frank Wang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出DeSTA2.5-Audio通用音频语言模型，通过自生成跨模态对齐策略解决LLM能力遗忘问题，在多项音频语言基准测试中达到SOTA表现


<details>
  <summary>Details</summary>
Motivation: 现有LALM方法使用大规模人工/LLM合成数据集训练时导致LLM原有语言能力退化，需要解决跨模态对齐与语言能力保留的矛盾

Method: 提出DeSTA自生成对齐策略：利用主干LLM生成训练目标，构建包含50个数据集、7000小时音频的DeSTA-AQA5M训练集

Result: 在Dynamic-SUPERB、MMAU等5大音频语言基准测试中取得最优或竞争性表现，验证策略有效性

Conclusion: 数据构建策略对LALM开发至关重要，自生成策略为构建通用音频语言模型提供新方向

Abstract: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model
(LALM) designed for robust auditory perception and instruction-following,
without requiring task-specific audio instruction-tuning. Recent LALMs
typically augment Large Language Models (LLMs) with auditory capabilities by
training on large-scale, manually curated or LLM-synthesized audio-instruction
datasets. However, these approaches have often suffered from the catastrophic
forgetting of the LLM's original language abilities. To address this, we
revisit the data construction pipeline and propose DeSTA, a self-generated
cross-modal alignment strategy in which the backbone LLM generates its own
training targets. This approach preserves the LLM's native language proficiency
while establishing effective audio-text alignment, thereby enabling zero-shot
generalization without task-specific tuning. Using DeSTA, we construct
DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training
samples derived from 7,000 hours of audio spanning 50 diverse datasets,
including speech, environmental sounds, and music. DeSTA2.5-Audio achieves
state-of-the-art or competitive performance across a wide range of
audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,
Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate
that our self-generated strategy outperforms widely adopted data construction
and training strategies in both auditory perception and instruction-following
capabilities. Our findings underscore the importance of carefully designed data
construction in LALM development and offer practical insights for building
robust, general-purpose LALMs.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [31] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 开发FinAI-BERT模型实现金融文本句子级AI内容分类，准确率99.37%


<details>
  <summary>Details</summary>
Motivation: 现有AI披露检测方法存在粒度粗、可解释性差和鲁棒性不足的问题，需精细化的分析工具

Method: 基于Transformer架构，使用669家美国银行年报的1,586个句子构建平衡数据集进行微调

Result: 模型F1分数达0.993，显著优于传统机器学习模型，并通过SHAP归因和压力测试验证稳定性

Conclusion: 理论层面推进金融NLP细粒度分类研究，实践层面为监管科技提供可扩展的分析框架

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 通过结合文本、图像和社交特征的多模态机器学习模型，错误信息检测性能提升15%（较单模态）和5%（较双模态）


<details>
  <summary>Details</summary>
Motivation: 针对选举和疫情时期社交媒体多模态错误信息泛滥问题，现有研究多聚焦单模态检测，缺乏对多模态特征组合有效性的系统研究

Method: 采用早期融合方法整合文本/图像/社交特征，运用目标检测和OCR技术进行数据增强，分析1,529条含图文推文（含COVID-19和选举相关案例）

Result: 无监督+有监督组合模型显著提升检测性能，同时揭示错误信息传播模式与推文特征、用户属性的关联规律

Conclusion: 多模态特征融合可有效提升错误信息识别能力，研究结果为社交媒体平台设计内容审核系统提供了重要技术支撑

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [33] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 专有招聘模型Match Score在准确率（ROC AUC 0.85）和公平性（种族影响比0.957）上全面超越通用大语言模型，证明专业算法可同时实现招聘效率与公平


<details>
  <summary>Details</summary>
Motivation: 揭示通用大语言模型在招聘场景中存在的算法偏见风险，论证领域专用模型在平衡准确性与公平性方面的优势

Method: 使用10,000个真实候选人-职位配对数据，对比分析5家主流LLM与专用招聘模型的预测准确性（ROC AUC/PR AUC/F1）和公平性指标（人口亚群影响比）

Result: 专用模型ROC AUC达0.85（LLMs最高0.77），最低种族影响比0.957（LLMs最佳0.809），交叉亚群公平性提升17.2%（0.906 vs 0.773）

Conclusion: 专业招聘算法设计应兼顾准确性与公平性，领域定制模型通过偏差审计可突破通用LLM的局限性，高风险领域AI部署需建立专属保障机制

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [34] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 提出Energy-Based Transformers(EBTs)新型能量模型，通过无监督学习验证输入与预测的兼容性，在跨模态任务中实现更优的扩展性和推理能力


<details>
  <summary>Details</summary>
Motivation: 现有系统2推理方法存在模态/问题局限性，且依赖额外监督训练。希望探索纯无监督学习实现通用推理的可能性

Method: 训练EBTs作为能量模型，通过梯度下降能量最小化验证输入-预测兼容性，重构预测为优化问题

Result: EBTs训练扩展率比Transformer++高35%，推理性能提升29%，图像去噪优于Diffusion Transformers且计算量更少，相同预训练条件下下游任务表现更优

Conclusion: EBTs通过能量验证机制突破现有方法局限，在模型学习效率和推理能力扩展方面展现出重要潜力，可能成为新的模型范式

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [35] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: 提出OmniDraft框架，使单个草稿模型适配多目标模型并动态适应用户数据，实现1.5-2倍解码加速


<details>
  <summary>Details</summary>
Motivation: 解决在线部署中目标模型与草稿模型不兼容、延迟优化需求持续存在的两大挑战

Method: 采用在线n-gram缓存与混合蒸馏微调解决跨模型词汇不匹配，结合自适应推测技术提升解码速度

Result: 单一Llama-68M模型可适配Vicuna/Qwen2/Llama3等不同目标模型，在数学推理/代码/文本生成任务中实现加速

Conclusion: OmniDraft验证了'一对多'草稿模型的可行性，特别适用于注重模型成本、效率和用户定制的端侧LLM应用场景

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [36] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 提出ExPO框架解决RL后训练中模型难以探索新推理轨迹的问题，通过基于正确答案生成策略对齐的高质量样本，提升困难推理任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO类方法过度依赖模型初始生成能力，在早期RL训练和困难推理任务中无法有效探索新样本。专家示范效果有限，需满足策略相关性和答案提升性的新样本生成方式。

Method: ExPO框架：1) 要求样本符合当前策略可能性 2) 通过条件正确答案提升正确率，生成比专家CoT更策略对齐的推理轨迹，无需依赖专家示例。

Result: 在MATH level-5等困难推理基准上，ExPO显著提升学习效率和最终准确率，在模型初始失败率高的任务中超越专家示范方法。

Conclusion: ExPO通过生成策略兼容的优质样本，突破现有RL后训练的分布锐化限制，有效解锁模型的困难推理能力。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [37] [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
*Marc-André Carbonneau,Benjamin van Niekerk,Hugo Seuté,Jean-Philippe Letendre,Herman Kamper,Julian Zaïdi*

Main category: cs.SD

TL;DR: 论文揭示了ASV语音嵌入在语音克隆系统中评估说话人身份一致性的局限性，并提出U3D指标补充动态节奏特征评估


<details>
  <summary>Details</summary>
Motivation: 当前语音克隆系统依赖ASV嵌入评估说话人身份，但这些嵌入主要针对区分性任务设计，无法全面表征身份特征，特别是动态的节奏模式被忽视

Method: 通过分析ASV嵌入的局限性，发现其聚焦静态声学特征（音色/音域），提出U3D指标评估动态节奏模式，并公开代码实现

Result: 证实ASV嵌入在动态特征表征上的不足，提出的U3D有效补充说话人相似度评估，改进现有测量方法的混淆因素

Conclusion: 动态节奏特征是说话人身份建模的关键维度，U3D为持续改进的语音克隆系统提供了新的评估视角

Abstract: Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.

</details>


### [38] [JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](https://arxiv.org/abs/2507.02380)
*Fangru Zhou,Jun Zhao,Guoxin Wang*

Main category: cs.SD

TL;DR: JoyTTS是集成大语言模型与语音克隆的端到端对话系统，基于MiniCPM-o和CosyVoice2开发，在2000小时对话数据上训练，SS 0.73/WER 5.09。


<details>
  <summary>Details</summary>
Motivation: 结合大语言模型与语音合成技术，打造支持语音克隆的智能对话系统，推动开源社区在语音交互领域的发展。

Method: 基于开源模型MiniCPM-o和CosyVoice2进行开发，使用2000小时对话数据进行模型训练，提供完整训练代码和推理脚本。

Result: 在seed-tts-zh测试集上达到73%的说话人相似度（SS）和5.09%的词错率（WER）。

Conclusion: 项目通过开源代码和预训练模型为社区提供了可扩展的语音对话系统解决方案，技术指标显示其具备较高的语音自然度和识别准确率。

Abstract: JoyTTS is an end-to-end spoken chatbot that combines large language models
(LLM) with text-to-speech (TTS) technology, featuring voice cloning
capabilities. This project is built upon the open-source MiniCPM-o and
CosyVoice2 models and trained on 2000 hours of conversational data. We have
also provided the complete training code to facilitate further development and
optimization by the community. On the testing machine seed-tts-zh, it achieves
a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.
The code and models, along with training and inference scripts, are available
at https://github.com/jdh-algo/JoyTTS.git.

</details>


### [39] [ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning](https://arxiv.org/abs/2507.02666)
*Junyu Wang,Tianrui Wang,Meng Ge,Longbiao Wang,Jianwu Dang*

Main category: cs.SD

TL;DR: 提出差异注意力机制(ASDA)改进Transformer在音频任务中的注意力分配问题，在多项基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的注意力机制会分配权重给无关信息，影响模型判别能力，需要更有效的注意力分配方案

Method: 通过双softmax操作结合可调节差异系数实现差异注意力机制，抑制无效注意力分配

Result: 音频分类(AS-2M 49.0% mAP/AS20K 41.5% mAP)、关键词检测(SPC-2 98.3%)、环境声分类(ESC-50 96.1%)均达最佳效果

Conclusion: ASDA有效提升了音频任务表现，为自监督学习在更广泛领域的应用提供了新思路

Abstract: In recent advancements in audio self-supervised representation learning, the
standard Transformer architecture has emerged as the predominant approach, yet
its attention mechanism often allocates a portion of attention weights to
irrelevant information, potentially impairing the model's discriminative
ability. To address this, we introduce a differential attention mechanism,
which effectively mitigates ineffective attention allocation through the
integration of dual-softmax operations and appropriately tuned differential
coefficients. Experimental results demonstrate that our ASDA model achieves
state-of-the-art (SOTA) performance across multiple benchmarks, including audio
classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting
(98.3% accuracy on SPC-2), and environmental sound classification (96.1%
accuracy on ESC-50). These results highlight ASDA's effectiveness in audio
tasks, paving the way for broader applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自主进化的AI代理，通过动态演进的模板库和生物信息工具海洋，显著提升生物医学研究效率并实现持续性能增长


<details>
  <summary>Details</summary>
Motivation: 生物医学数据和工具的爆炸式增长导致研究领域碎片化，传统AI代理依赖静态工具集难以适应快速演变的科研需求

Method: 采用多代理架构，包含自主更新的推理策略模板库（Template Library）和工具创建代理自动扩展的生物信息工具库（Tool Ocean）

Result: 在生物医学基准测试中达到SOTA水平（Humanity's Last Exam: 26%，LAB-Bench DBQA: 54%，LitQA: 63%），性能随经验积累系统性提升（如HLE准确率翻倍）

Conclusion: STELLA标志着AI系统向自主学习和动态扩展专业能力的重要突破，为加速生物医学发现提供了新范式

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [41] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 大型语言模型在迭代囚徒困境中展现出类战略智能，不同厂商模型呈现鲜明策略特征：Gemini冷酷报复，OpenAI过度合作，Claude宽容互惠。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在博弈论经典场景中是否具备战略推理能力，通过进化锦标赛揭示算法决策机制与人类策略的异同。

Method: 设计进化IPD锦标赛，让传统策略与前沿AI模型对抗，通过调整博弈终止概率（未来阴影系数）控制环境复杂性，阻止简单记忆策略。

Result: LLMs存活率显著（Gemini 87%，OpenAI 23%，Claude 68%）。策略指纹特征：Gemini采取报复性策略，OpenAI保持脆弱合作，Claude主动修复关系。模型提供的32,000条决策理由显示其具备时间跨度和对手策略的双重推理能力。

Conclusion: 该研究架起经典博弈论与机器心理学的桥梁，证明LLMs在不确定性决策中具有类战略思维，其策略偏好反映不同厂商的价值对齐取向。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [42] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 提出分层框架HiRA实现规划与执行解耦，通过领域智能体分工处理复杂搜索任务


<details>
  <summary>Details</summary>
Motivation: 传统RAG框架在复杂跨模态搜索场景中存在规划与执行耦合的局限性，单一模型同时承担高层推理和细节执行导致效率低下

Method: 分层任务分解架构：1）战略层进行全局任务规划 2）执行层分配专业智能体处理子任务 3）结构化整合机制协调结果

Result: 在四个复杂跨模态搜索基准测试中，答案质量提升15.6%，推理效率提高2.3倍，显著优于现有RAG和智能体系统

Conclusion: 规划与执行解耦的层次化架构能有效提升复杂信息需求处理能力，专业化分工与结构化协调机制是提升系统可扩展性的关键

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [43] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 提出StepHint算法，通过多级步骤提示解决RLVR方法的近失误奖励和探索停滞问题，显著提升训练效率与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在近失误奖励问题（小错误导致正确推理链失效）和模型停留在舒适区不愿探索更优解的缺陷

Method: 1. 从强模型生成有效推理链并自适应分割步骤
2. 使用初始步骤作为多级提示
3. 引导模型探索有潜力的解空间子集同时保持自主探索灵活性

Result: 在6个数学基准测试中优于现有RLVR增强方法，表现出更好的泛化能力，在跨领域基准测试中显著超越基线模型

Conclusion: StepHint通过外部推理路径引导，有效缓解近失误奖励问题并突破模型舒适区，成功提升复杂推理能力

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: 提出Mesh Silksong新型网格表示方法，通过自回归编织式生成实现22%压缩率，同时保证流形拓扑等关键几何特性


<details>
  <summary>Details</summary>
Motivation: 现有网格标记化方法存在顶点重复标记问题，导致网络容量浪费和压缩效率低下

Method: 采用单次顶点访问的标记化策略，减少50%序列冗余，实现高效压缩

Result: 达到22%的先进压缩率，生成网格具备流形拓扑、水密性检测和一致法向等特性

Conclusion: 该方法不仅实现复杂网格生成，更显著提升几何完整性，验证了方案有效性

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [45] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: HyperGaussians扩展3D高斯抛雪球技术，通过高维多元高斯建模和逆协方差技巧，显著提升单目视频面部虚拟形象的动态细节渲染质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有3DGS在面部非线性变形、复杂光照和细节处理上的不足，重新思考高斯表示本身的表达能力增强方式。

Method: 1. 将3D高斯扩展为高维多元高斯（HyperGaussians）
2. 引入可学习局部嵌入增强表达
3. 提出逆协方差技巧优化高维计算效率

Result: 在4个数据集19个受试者中，数值/视觉表现优于3DGS，高频细节（眼镜框、牙齿、面部运动、镜面反射）提升显著。

Conclusion: 高维扩展+计算优化突破了动态面部建模的恐怖谷效应，为AR/VR应用提供更真实的实时虚拟形象方案。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [46] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality将RGB-D扫描转换为紧凑、逼真且可交互的3D虚拟场景，通过智能模型检索和材质优化实现跨领域应用兼容。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D重建方法在互动性、可编辑性和真实感方面的不足，创建兼容标准图形管道的虚拟场景生成方案。

Method: 1. 场景理解构建结构化场景图
2. 相似度检索艺术家制作的3D模型
3. 材质绘画模块恢复高保真材质
4. 物理引擎集成实现交互功能

Result: 在Scan2CAD基准实现SOTA相似度检索，材质模块可处理严重错位/遮挡/光照问题，生成场景体积缩小80%且支持物理交互

Conclusion: 通过结构化场景解析-模型检索-材质优化-物理集成的技术链，实现了工业级可用的3D场景生成方案，在真实扫描和公开数据集验证有效性

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [47] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出基于思维链推理的事件流场景文本识别框架ESTR-CoT，通过结合视觉编码器和大语言模型提升极端场景下的识别性能与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有端到端框架和大语言模型在事件流文本识别中存在可解释性不足、上下文逻辑推理弱的局限性

Method: 采用EVA-CLIP视觉编码器转换事件流，Llama分词器编码提示，Q-former对齐视觉特征，Vicuna-7B输出推理过程和答案，支持端到端监督微调

Result: 在EventSTR/WordArt*/IC15*三个基准数据集上验证有效性，创建包含生成-润色-专家验证三阶段的大规模CoT数据集

Conclusion: ESTR-CoT框架通过思维链推理机制提升识别性能与可解释性，配套数据集为后续研究提供基础，代码模型已开源

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [48] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 构建SciGA-145k大规模数据集（含14.5万论文/114万图表），提出基于置信度调整的CAR指标，支持图形摘要推荐与生成研究，推动科学可视化交流。


<details>
  <summary>Details</summary>
Motivation: 解决科学论文图形摘要（GA）设计门槛高、现有图表未被充分利用的问题，促进可视化成果传播与AI科研发展。

Method: 1. 定义Intra-GA（论文内图表推荐）和Inter-GA（跨论文GA推荐）任务；2. 提出CAR指标改进传统排序评估；3. 建立基准模型。

Result: 创建当前最大科学图表数据集，提供任务框架与评估体系，验证基线模型可行性，CAR指标实现更细粒度的推荐质量分析。

Conclusion: SciGA-145k为视觉化科研交流奠定基础，推动AI自动生成图形摘要的技术发展，提升科学传播效率。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [49] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 提出多模态叙事理解框架HIVE，通过角色提取、对话分析和场景分割提升视频编辑质量，在DramaAD数据集上超越现有基线


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法过度依赖ASR文本且忽视视觉连贯性，导致自动编辑效果不理想

Method: 结合多模态大语言模型进行叙事理解，采用场景级分割和编辑三阶段分解（高光检测/片头片尾选择/内容修剪）

Result: 在800+短剧和500+广告剪辑的DramaAD数据集上，编辑质量显著优于基线方法（广告任务提升38.2%）

Conclusion: HIVE框架通过多模态内容理解和任务分解策略，使自动编辑质量接近人工专业水平

Abstract: The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [50] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 提出视觉中心越狱攻击VisCo，通过动态生成视觉上下文显著提升多模态大语言模型攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有视觉触发攻击存在语义模糊且缺乏现实场景基础，需要构建更完整的视觉中心越狱场景

Method: 使用四种视觉策略构建上下文对话，结合自动毒性混淆和语义优化生成攻击提示

Result: 在GPT-4o上实现4.78毒性分和85%攻击成功率，显著超越基线方法

Conclusion: 视觉模态的安全漏洞对MLLMs构成实质性威胁，需加强现实场景下的防御机制研究

Abstract: With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [51] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 论文提出统一能源调控器FUSE，优化移动设备上大语言模型推理的能效，降低延迟同时保持能耗不变


<details>
  <summary>Details</summary>
Motivation: 当前移动设备部署LLM时，CPU/GPU/内存三组件DVFS调控器独立运行导致能效低下（相同能耗下延迟增加40.4%）

Method: 1. 测量主流LLM框架能效  2. 分析调控器协同不足的根源  3. 设计FUSE统一调控器

Result: FUSE使首token延迟降低7.0%-16.9%，输出token延迟降低25.4%-36.8%，单位token能耗相同

Conclusion: FUSE通过协调多组件频率，显著提升移动端LLM推理的能源效率，实现延迟与能耗的优化平衡

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [52] [Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents](https://arxiv.org/abs/2507.02287)
*Lapo Santarlasci,Armando Rungi,Antonio Zinilli*

Main category: econ.GN

TL;DR: 使用自然语言处理识别真正绿色专利，发现其占现有分类20%且对企业财务指标有积极影响


<details>
  <summary>Details</summary>
Motivation: 解决现有绿色专利分类过粗的问题，建立更精确的识别标准以支持环境政策制定

Method: 基于1240万绿色专利数据训练神经网络模型，采用统计测试分析技术异质性和企业财务关联

Result: 真绿色专利占比20%、引用率低1%，但显著提升企业销售额/市场份额/生产率，高新颖性专利增加利润

Conclusion: 文本分析方法能有效细化专利分类，为环境技术政策制定提供可靠数据支撑

Abstract: This paper introduces Natural Language Processing for identifying ``true''
green patents from official supporting documents. We start our training on
about 12.4 million patents that had been classified as green from previous
literature. Thus, we train a simple neural network to enlarge a baseline
dictionary through vector representations of expressions related to
environmental technologies. After testing, we find that ``true'' green patents
represent about 20\% of the total of patents classified as green from previous
literature. We show heterogeneity by technological classes, and then check that
`true' green patents are about 1\% less cited by following inventions. In the
second part of the paper, we test the relationship between patenting and a
dashboard of firm-level financial accounts in the European Union. After
controlling for reverse causality, we show that holding at least one ``true''
green patent raises sales, market shares, and productivity. If we restrict the
analysis to high-novelty ``true'' green patents, we find that they also yield
higher profits. Our findings underscore the importance of using text analyses
to gauge finer-grained patent classifications that are useful for policymaking
in different domains.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [53] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 提出超图对比多兴趣学习框架HyFairCRS，解决动态对话推荐系统中多兴趣多样性公平问题，在实验中达到SOTA性能并有效缓解不公平。


<details>
  <summary>Details</summary>
Motivation: 动态对话推荐系统中长期反馈循环会加剧不公平现象（如马太效应/信息茧房），现有方法主要解决静态场景，缺乏动态环境下的多兴趣公平保障机制。

Method: 通过对比学习构建多样性超图捕捉用户多兴趣，在对话交互中利用这些兴趣特征生成响应，并通过动态反馈循环实现公平物品推荐。

Result: 在两个CRS数据集上取得state-of-the-art性能，公平性指标提升显著（提供GitHub代码仓库）。

Conclusion: HyFairCRS首次将超图对比学习与多兴趣建模结合，有效解决动态推荐场景的公平性问题，为可解释的公平推荐提供新思路。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [54] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出一种基于文本蕴含和上下文学习的自动化方法，将法律文本编码为Python可执行规范，减少人工标注需求并提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有法律元数据提取方法忽视属性间关联性，依赖人工标注/启发式机器学习，难以适应新法规。小企业缺乏法律资源需自动化合规方案。

Method: 设计Python类结构作为领域元模型，结合文本蕴含推理和上下文学习生成结构化表示，捕捉法律文本的结构/语义元数据及其关联。

Result: 在13个美国州数据泄露通知法测试中，89.4%用例通过，精确率82.2/召回率88.7。

Conclusion: 该方法有效平衡自动化与准确性，为法律合规提供可扩展解决方案，未来可扩展至其他法律领域。

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [55] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 探讨使用GPT-4o在需求访谈中实时生成后续问题，实验表明其生成的问题质量不逊于人类且在错误类型指导下表现更优


<details>
  <summary>Details</summary>
Motivation: 传统需求访谈依赖熟练访谈者实时提问，面临领域不熟悉/认知负荷/信息过载等挑战，需探索LLM支持的可能性

Method: 设计两种对照实验：1) 无指导条件下比较LLM与人类问题 2) 基于常见错误类型框架指导生成问题，评估清晰度/相关性/信息量

Result: LLM生成问题在核心指标上与人类相当，当采用错误类型指导时显著优于人类问题

Conclusion: GPT-4o展现实时提升需求访谈质量的潜力，特别在结构化错误指导框架下效果更突出

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>
