{"id": "2506.11252", "pdf": "https://arxiv.org/pdf/2506.11252", "abs": "https://arxiv.org/abs/2506.11252", "authors": ["Mae Younes", "Adnane Boukhayma"], "title": "Anti-Aliased 2D Gaussian Splatting", "categories": ["cs.GR", "cs.CV"], "comment": "Code will be available at https://github.com/maeyounes/AA-2DGS", "summary": "2D Gaussian Splatting (2DGS) has recently emerged as a promising method for\nnovel view synthesis and surface reconstruction, offering better\nview-consistency and geometric accuracy than volumetric 3DGS. However, 2DGS\nsuffers from severe aliasing artifacts when rendering at different sampling\nrates than those used during training, limiting its practical applications in\nscenarios requiring camera zoom or varying fields of view. We identify that\nthese artifacts stem from two key limitations: the lack of frequency\nconstraints in the representation and an ineffective screen-space clamping\napproach. To address these issues, we present AA-2DGS, an antialiased\nformulation of 2D Gaussian Splatting that maintains its geometric benefits\nwhile significantly enhancing rendering quality across different scales. Our\nmethod introduces a world space flat smoothing kernel that constrains the\nfrequency content of 2D Gaussian primitives based on the maximal sampling\nfrequency from training views, effectively eliminating high-frequency artifacts\nwhen zooming in. Additionally, we derive a novel object space Mip filter by\nleveraging an affine approximation of the ray-splat intersection mapping, which\nallows us to efficiently apply proper anti-aliasing directly in the local space\nof each splat.", "AI": {"tldr": "\u63d0\u51faAA-2DGS\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e16\u754c\u7a7a\u95f4\u5e73\u6ed1\u6838\u548c\u5bf9\u8c61\u7a7a\u95f4Mip\u6ee4\u6ce2\uff0c\u89e3\u51b32D\u9ad8\u65af\u6cfc\u6e85\u5728\u4e0d\u540c\u91c7\u6837\u7387\u4e0b\u7684\u952f\u9f7f\u95ee\u9898\uff0c\u63d0\u5347\u591a\u5c3a\u5ea6\u6e32\u67d3\u8d28\u91cf", "motivation": "2DGS\u5728\u8bad\u7ec3/\u6e32\u67d3\u91c7\u6837\u7387\u4e0d\u4e00\u81f4\u65f6\u51fa\u73b0\u4e25\u91cd\u952f\u9f7f\uff0c\u9650\u5236\u5176\u5728\u76f8\u673a\u7f29\u653e/\u53d8\u7126\u573a\u666f\u7684\u5b9e\u9645\u5e94\u7528", "method": "\u5f15\u5165\u4e16\u754c\u7a7a\u95f4\u9891\u7387\u7ea6\u675f\u6838\u9650\u5236\u9ad8\u65af\u57fa\u5143\u9891\u7387\uff0c\u5f00\u53d1\u57fa\u4e8e\u4eff\u5c04\u8fd1\u4f3c\u7684\u5bf9\u8c61\u7a7a\u95f4Mip\u6ee4\u6ce2\u5668\u5b9e\u73b0\u5c40\u90e8\u6297\u952f\u9f7f", "result": "AA-2DGS\u4fdd\u6301\u51e0\u4f55\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u591a\u5c3a\u5ea6\u6e32\u67d3\u8d28\u91cf\uff0c\u6709\u6548\u6d88\u9664\u7f29\u653e\u65f6\u7684\u9ad8\u9891\u4f2a\u5f71", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u514b\u670d2DGS\u7684\u5c3a\u5ea6\u654f\u611f\u7f3a\u9677\uff0c\u6269\u5c55\u4e86\u5176\u5728\u52a8\u6001\u89c6\u89d2\u573a\u666f\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2506.11273", "pdf": "https://arxiv.org/pdf/2506.11273", "abs": "https://arxiv.org/abs/2506.11273", "authors": ["Daniel Meister", "Jakub Bok\u0161ansk\u00fd", "Michael Guthe", "Ji\u0159\u00ed Bittner"], "title": "On Ray Reordering Techniques for Faster GPU Ray Tracing", "categories": ["cs.GR"], "comment": null, "summary": "We study ray reordering as a tool for increasing the performance of existing\nGPU ray tracing implementations. We focus on ray reordering that is fully\nagnostic to the particular trace kernel. We summarize the existing methods for\ncomputing the ray sorting keys and discuss their properties. We propose a novel\nmodification of a previously proposed method using the termination point\nestimation that is well-suited to tracing secondary rays. We evaluate the ray\nreordering techniques in the context of the wavefront path tracing using the\nRTX trace kernels. We show that ray reordering yields significantly higher\ntrace speed on recent GPUs (1.3-2.0x), but to recover the reordering overhead\nin the hardware-accelerated trace phase is problematic.", "AI": {"tldr": "\u901a\u8fc7\u5149\u7ebf\u91cd\u6392\u5e8f\u6280\u672f\u53ef\u5c06GPU\u5149\u7ebf\u8ffd\u8e2a\u901f\u5ea6\u63d0\u53471.3-2\u500d\uff0c\u4f46\u786c\u4ef6\u52a0\u901f\u9636\u6bb5\u96be\u4ee5\u62b5\u6d88\u91cd\u6392\u5e8f\u5f00\u9500", "motivation": "\u63a2\u7d22\u4e0d\u4f9d\u8d56\u7279\u5b9a\u8ffd\u8e2a\u5185\u6838\u7684\u901a\u7528\u5149\u7ebf\u91cd\u6392\u5e8f\u65b9\u6848\uff0c\u63d0\u5347\u73b0\u6709GPU\u5149\u7ebf\u8ffd\u8e2a\u5b9e\u73b0\u7684\u6027\u80fd\u8868\u73b0", "method": "\u6539\u8fdb\u57fa\u4e8e\u7ec8\u6b62\u70b9\u4f30\u8ba1\u7684\u6b21\u7ea7\u5149\u7ebf\u6392\u5e8f\u65b9\u6cd5\uff0c\u7ed3\u5408RTX\u786c\u4ef6\u8ffd\u8e2a\u5185\u6838\u8fdb\u884c\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u6d4b\u8bd5", "result": "\u5728\u65b0\u578bGPU\u4e0a\u5b9e\u73b0\u663e\u8457\u8ffd\u8e2a\u901f\u5ea6\u63d0\u5347\uff081.3-2.0\u500d\uff09\uff0c\u4f46\u786c\u4ef6\u52a0\u901f\u9636\u6bb5\u96be\u4ee5\u5b8c\u5168\u8865\u507f\u91cd\u6392\u5e8f\u5f00\u9500", "conclusion": "\u5149\u7ebf\u91cd\u6392\u5e8f\u6280\u672f\u5b58\u5728\u6027\u80fd\u589e\u76ca\u4e0e\u8ba1\u7b97\u5f00\u9500\u7684\u6743\u8861\uff0c\u9700\u6839\u636e\u5177\u4f53\u786c\u4ef6\u67b6\u6784\u9009\u62e9\u9002\u7528\u573a\u666f"}}
{"id": "2506.11510", "pdf": "https://arxiv.org/pdf/2506.11510", "abs": "https://arxiv.org/abs/2506.11510", "authors": ["Anis Benyoub", "Jonathan Dupuy"], "title": "Adaptive Tetrahedral Grids for Volumetric Path-Tracing", "categories": ["cs.GR"], "comment": null, "summary": "We advertise the use of tetrahedral grids constructed via the longest edge\nbisection algorithm for rendering volumetric data with path tracing. The key\nbenefits of such grids is two-fold. First, they provide a highly adaptive\nspace-partitioning representation that limits the memory footprint of\nvolumetric assets. Second, each (tetrahedral) cell has exactly 4 neighbors\nwithin the volume (one per face of each tetrahedron) or less at boundaries. We\nleverage these properties to devise optimized algorithms and data-structures to\ncompute and path-trace adaptive tetrahedral grids on the GPU. In practice, our\nGPU implementation outperforms regular grids by up to x30 and renders\nproduction assets in real time at 32 samples per pixel.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56db\u9762\u4f53\u7f51\u683c\u7684GPU\u8def\u5f84\u8ffd\u8e2a\u7b97\u6cd5\uff0c\u5b9e\u73b0\u6bd4\u89c4\u5219\u7f51\u683c\u5feb30\u500d\u7684\u5b9e\u65f6\u4f53\u79ef\u6e32\u67d3", "motivation": "\u89e3\u51b3\u4f53\u79ef\u6570\u636e\u6e32\u67d3\u4e2d\u5185\u5b58\u5360\u7528\u9ad8\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5229\u7528\u56db\u9762\u4f53\u7f51\u683c\u7684\u81ea\u9002\u5e94\u7279\u6027\u4f18\u5316\u7a7a\u95f4\u5212\u5206", "method": "\u91c7\u7528\u6700\u957f\u8fb9\u4e8c\u5206\u6cd5\u6784\u5efa\u81ea\u9002\u5e94\u56db\u9762\u4f53\u7f51\u683c\uff0c\u8bbe\u8ba1GPU\u4f18\u5316\u7684\u6570\u636e\u7ed3\u6784\u548c\u8def\u5f84\u8ffd\u8e2a\u7b97\u6cd5", "result": "GPU\u5b9e\u73b0\u901f\u5ea6\u63d0\u5347\u8fbe30\u500d\uff0c\u572832\u6837\u672c/\u50cf\u7d20\u6761\u4ef6\u4e0b\u5b9e\u73b0\u751f\u4ea7\u7ea7\u8d44\u4ea7\u7684\u5b9e\u65f6\u6e32\u67d3", "conclusion": "\u56db\u9762\u4f53\u7f51\u683c\u5728\u4f53\u79ef\u6e32\u67d3\u4e2d\u5177\u6709\u663e\u8457\u6027\u80fd\u4f18\u52bf\uff0c\u4e3a\u5b9e\u65f6\u56fe\u5f62\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11546", "pdf": "https://arxiv.org/pdf/2506.11546", "abs": "https://arxiv.org/abs/2506.11546", "authors": ["Akshay Jindal", "Nabil Sadaka", "Manu Mathew Thomas", "Anton Sochenov", "Anton Kaplanyan"], "title": "CGVQM+D: Computer Graphics Video Quality Metric and Dataset", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "While existing video and image quality datasets have extensively studied\nnatural videos and traditional distortions, the perception of synthetic content\nand modern rendering artifacts remains underexplored. We present a novel video\nquality dataset focused on distortions introduced by advanced rendering\ntechniques, including neural supersampling, novel-view synthesis, path tracing,\nneural denoising, frame interpolation, and variable rate shading. Our\nevaluations show that existing full-reference quality metrics perform\nsub-optimally on these distortions, with a maximum Pearson correlation of 0.78.\nAdditionally, we find that the feature space of pre-trained 3D CNNs aligns\nstrongly with human perception of visual quality. We propose CGVQM, a\nfull-reference video quality metric that significantly outperforms existing\nmetrics while generating both per-pixel error maps and global quality scores.\nOur dataset and metric implementation is available at\nhttps://github.com/IntelLabs/CGVQM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCGVQM\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u53ca\u4e13\u7528\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u73b0\u6709\u6307\u6807\u5728\u6e32\u67d3\u6280\u672f\u5931\u771f\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u5c40\u9650\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u8d28\u91cf\u8bc4\u4f30\u6570\u636e\u96c6\u4e3b\u8981\u9488\u5bf9\u81ea\u7136\u89c6\u9891\u548c\u4f20\u7edf\u5931\u771f\u7c7b\u578b\uff0c\u7f3a\u4e4f\u5bf9\u795e\u7ecf\u6e32\u67d3\u7b49\u73b0\u4ee3\u6280\u672f\u751f\u6210\u5185\u5bb9\u7684\u8bc4\u4f30\u57fa\u51c6", "method": "\u6784\u5efa\u5305\u542b\u795e\u7ecf\u8d85\u91c7\u6837/\u8def\u5f84\u8ffd\u8e2a\u7b496\u7c7b\u6e32\u67d3\u5931\u771f\u7684\u6570\u636e\u96c6\uff0c\u5206\u67903D CNN\u7279\u5f81\u7a7a\u95f4\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5173\u8054\u6027\uff0c\u5f00\u53d1\u652f\u6301\u9010\u50cf\u7d20\u5206\u6790\u7684\u8bc4\u4f30\u6846\u67b6", "result": "\u4f20\u7edf\u6307\u6807\u6700\u9ad8Pearson\u7cfb\u6570\u4ec50.78\uff0cCGVQM\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff1b\u9884\u8bad\u7ec33D CNN\u7279\u5f81\u4e0e\u4eba\u7c7b\u89c6\u89c9\u611f\u77e5\u9ad8\u5ea6\u5339\u914d", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5408\u6210\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u65b0\u57fa\u51c6\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u53d1\u5c55"}}
{"id": "2506.11017", "pdf": "https://arxiv.org/pdf/2506.11017", "abs": "https://arxiv.org/abs/2506.11017", "authors": ["Yanyan Wang", "Yingying Wang", "Junli Liang", "Yin Xu", "Yunlong Liu", "Yiming Xu", "Zhengwang Jiang", "Zhehe Li", "Fei Li", "Long Zhao", "Kuang Xu", "Qi Song", "Xiangyang Li"], "title": "TeleEval-OS: Performance evaluations of large language models for operations scheduling", "categories": ["cs.CL", "cs.AI", "cs.PF"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has significantly\npropelled progress in artificial intelligence, demonstrating substantial\napplication potential across multiple specialized domains. Telecommunications\noperation scheduling (OS) is a critical aspect of the telecommunications\nindustry, involving the coordinated management of networks, services, risks,\nand human resources to optimize production scheduling and ensure unified\nservice control. However, the inherent complexity and domain-specific nature of\nOS tasks, coupled with the absence of comprehensive evaluation benchmarks, have\nhindered thorough exploration of LLMs' application potential in this critical\nfield. To address this research gap, we propose the first Telecommunications\nOperation Scheduling Evaluation Benchmark (TeleEval-OS). Specifically, this\nbenchmark comprises 15 datasets across 13 subtasks, comprehensively simulating\nfour key operational stages: intelligent ticket creation, intelligent ticket\nhandling, intelligent ticket closure, and intelligent evaluation. To\nsystematically assess the performance of LLMs on tasks of varying complexity,\nwe categorize their capabilities in telecommunications operation scheduling\ninto four hierarchical levels, arranged in ascending order of difficulty: basic\nNLP, knowledge Q&A, report generation, and report analysis. On TeleEval-OS, we\nleverage zero-shot and few-shot evaluation methods to comprehensively assess 10\nopen-source LLMs (e.g., DeepSeek-V3) and 4 closed-source LLMs (e.g., GPT-4o)\nacross diverse scenarios. Experimental results demonstrate that open-source\nLLMs can outperform closed-source LLMs in specific scenarios, highlighting\ntheir significant potential and value in the field of telecommunications\noperation scheduling.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7535\u4fe1\u8fd0\u8425\u8c03\u5ea6\u8bc4\u4f30\u57fa\u51c6TeleEval-OS\uff0c\u5305\u542b15\u4e2a\u6570\u636e\u96c6\u8986\u76d64\u4e2a\u5173\u952e\u8fd0\u8425\u9636\u6bb5\uff0c\u9a8c\u8bc1\u5f00\u6e90\u5927\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e2d\u53ef\u8d85\u8d8a\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u7535\u4fe1\u8fd0\u8425\u8c03\u5ea6\u9886\u57df\u7f3a\u4e4f\u9488\u5bf9LLMs\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e14\u4efb\u52a1\u590d\u6742\u6027\u548c\u9886\u57df\u4e13\u4e1a\u6027\u9650\u5236\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u8986\u76d613\u4e2a\u5b50\u4efb\u52a1\u7684\u57fa\u51c6\uff0c\u52064\u9636\u6bb5\u6a21\u62df\u5de5\u5355\u5168\u6d41\u7a0b\uff1b\u5b9a\u4e49LLMs\u80fd\u529b\u56db\u5c42\u7ea7\uff08\u57fa\u7840NLP\u2192\u62a5\u544a\u5206\u6790\uff09\uff0c\u91c7\u7528\u96f6\u6837\u672c/\u5c11\u6837\u672c\u65b9\u6cd5\u8bc4\u4f3014\u79cd\u4e3b\u6d41\u6a21\u578b\u3002", "result": "DeepSeek-V3\u7b49\u5f00\u6e90\u6a21\u578b\u5728\u5de5\u5355\u751f\u6210\u3001\u98ce\u9669\u5206\u6790\u7b49\u573a\u666f\u8868\u73b0\u4f18\u4e8eGPT-4o\uff0c\u663e\u793a\u9886\u57df\u5b9a\u5236\u5316\u6f5c\u529b\u3002", "conclusion": "TeleEval-OS\u586b\u8865\u9886\u57df\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3aLLMs\u5728\u7535\u4fe1\u8fd0\u8425\u8c03\u5ea6\u7684\u843d\u5730\u63d0\u4f9b\u91cf\u5316\u4f9d\u636e\uff0c\u8bc1\u660e\u5f00\u6e90\u6a21\u578b\u7684\u5546\u4e1a\u5316\u4ef7\u503c\u3002"}}
{"id": "2506.11133", "pdf": "https://arxiv.org/pdf/2506.11133", "abs": "https://arxiv.org/abs/2506.11133", "authors": ["Christos Pantazopoulos", "Spyridon Thermos", "Gerasimos Potamianos"], "title": "Monocular 3D Hand Pose Estimation with Implicit Camera Alignment", "categories": ["cs.CV", "cs.GR", "cs.LG", "eess.IV"], "comment": "Code is available at https://github.com/cpantazop/HandRepo", "summary": "Estimating the 3D hand articulation from a single color image is a\ncontinuously investigated problem with applications in Augmented Reality (AR),\nVirtual Reality (VR), Human-Computer Interaction (HCI), and robotics. Apart\nfrom the absence of depth information, occlusions, articulation complexity, and\nthe need for camera parameters knowledge pose additional challenges. In this\nwork, we propose an optimization pipeline for estimating the 3D hand\narticulation from 2D keypoint input, which includes a keypoint alignment step\nand a fingertip loss to overcome the need to know or estimate the camera\nparameters. We evaluate our approach on the EgoDexter and Dexter+Object\nbenchmarks to showcase that our approach performs competitively with the SotA,\nwhile also demonstrating its robustness when processing \"in-the-wild\" images\nwithout any prior camera knowledge. Our quantitative analysis highlights the\nsensitivity of the 2D keypoint estimation accuracy, despite the use of hand\npriors. Code is available at https://github.com/cpantazop/HandRepo", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u76f8\u673a\u53c2\u6570\u76843D\u624b\u90e8\u59ff\u6001\u4f30\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u5bf9\u9f50\u4e0e\u6307\u5c16\u635f\u5931\u51fd\u6570\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u5e76\u9002\u5e94'\u91ce\u5916'\u56fe\u50cf\u3002", "motivation": "\u5355\u76eeRGB\u56fe\u50cf\u4f30\u8ba13D\u624b\u90e8\u59ff\u6001\u5b58\u5728\u6df1\u5ea6\u4fe1\u606f\u7f3a\u5931\u3001\u906e\u6321\u3001\u5173\u8282\u590d\u6742\u5ea6\u9ad8\u53ca\u4f9d\u8d56\u76f8\u673a\u53c2\u6570\u7b49\u95ee\u9898\uff0c\u5236\u7ea6\u4e86\u5176\u5728AR/VR\u3001\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u5305\u542b\u5173\u952e\u70b9\u5bf9\u9f50\u6b65\u9aa4\u548c\u6307\u5c16\u635f\u5931\u51fd\u6570\u7684\u4f18\u5316\u6d41\u7a0b\uff0c\u901a\u8fc7\u51e0\u4f55\u7ea6\u675f\u51cf\u5c11\u5bf9\u76f8\u673a\u53c2\u6570\u7684\u4f9d\u8d56\uff0c\u7ed3\u5408\u624b\u90e8\u5148\u9a8c\u77e5\u8bc6\u8fdb\u884c\u59ff\u6001\u4f30\u8ba1\u3002", "result": "\u5728EgoDexter/Dexter+Object\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SotA\u6c34\u5e73\uff0c\u5b9a\u91cf\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u65e0\u76f8\u673a\u5148\u9a8c\u7684'\u91ce\u5916'\u573a\u666f\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u4f462D\u5173\u952e\u70b9\u4f30\u8ba1\u7cbe\u5ea6\u4ecd\u663e\u8457\u5f71\u54cd\u6700\u7ec8\u6548\u679c\u3002", "conclusion": "\u5373\u4f7f\u4f7f\u7528\u624b\u90e8\u5148\u9a8c\uff0c2D\u5173\u952e\u70b9\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u4ecd\u662f\u7cfb\u7edf\u6027\u80fd\u74f6\u9888\uff0c\u672a\u6765\u9700\u7ed3\u5408\u66f4\u9c81\u68d2\u7684\u5173\u952e\u70b9\u68c0\u6d4b\u65b9\u6cd5\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u4e09\u7ef4\u91cd\u5efa\u6548\u679c\u3002"}}
{"id": "2506.11063", "pdf": "https://arxiv.org/pdf/2506.11063", "abs": "https://arxiv.org/abs/2506.11063", "authors": ["Jiayu Yao", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Yuyao Ge", "Zhecheng Li", "Xueqi Cheng"], "title": "Who is in the Spotlight: The Hidden Bias Undermining Multimodal Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (RAG) systems have become essential\nin knowledge-intensive and open-domain tasks. As retrieval complexity\nincreases, ensuring the robustness of these systems is critical. However,\ncurrent RAG models are highly sensitive to the order in which evidence is\npresented, often resulting in unstable performance and biased reasoning,\nparticularly as the number of retrieved items or modality diversity grows. This\nraises a central question: How does the position of retrieved evidence affect\nmultimodal RAG performance? To answer this, we present the first comprehensive\nstudy of position bias in multimodal RAG systems. Through controlled\nexperiments across text-only, image-only, and mixed-modality tasks, we observe\na consistent U-shaped accuracy curve with respect to evidence position. To\nquantify this bias, we introduce the Position Sensitivity Index ($PSI_p$) and\ndevelop a visualization framework to trace attention allocation patterns across\ndecoder layers. Our results reveal that multimodal interactions intensify\nposition bias compared to unimodal settings, and that this bias increases\nlogarithmically with retrieval range. These findings offer both theoretical and\nempirical foundations for position-aware analysis in RAG, highlighting the need\nfor evidence reordering or debiasing strategies to build more reliable and\nequitable generation systems.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u591a\u6a21\u6001RAG\u7cfb\u7edf\u5b58\u5728\u663e\u8457\u4f4d\u7f6e\u504f\u5dee\uff0c\u51c6\u786e\u7387\u5448\u73b0U\u578b\u66f2\u7ebf\uff0c\u9700\u91cd\u6392\u5e8f\u8bc1\u636e\u6216\u53bb\u504f\u7b56\u7565\u63d0\u5347\u53ef\u9760\u6027", "motivation": "\u73b0\u6709RAG\u6a21\u578b\u5bf9\u8bc1\u636e\u987a\u5e8f\u9ad8\u5ea6\u654f\u611f\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u548c\u63a8\u7406\u504f\u5dee\uff0c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u95ee\u9898\u52a0\u5267\uff0c\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027", "method": "\u901a\u8fc7\u8de8\u6587\u672c/\u56fe\u50cf/\u6df7\u5408\u6a21\u6001\u7684\u5bf9\u7167\u5b9e\u9a8c\uff0c\u63d0\u51fa\u4f4d\u7f6e\u654f\u611f\u6307\u6570(PSI_p)\u53ca\u6ce8\u610f\u529b\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u91cf\u5316\u4f4d\u7f6e\u5f71\u54cd", "result": "\u591a\u6a21\u6001\u4ea4\u4e92\u8f83\u5355\u6a21\u6001\u663e\u8457\u52a0\u5267\u4f4d\u7f6e\u504f\u5dee\uff0c\u4e14\u504f\u5dee\u968f\u68c0\u7d22\u8303\u56f4\u5bf9\u6570\u589e\u957f\uff1b\u6ce8\u610f\u529b\u6a21\u5f0f\u5728\u89e3\u7801\u5c42\u5448\u73b0\u89c4\u5f8b\u6027\u53d8\u5316", "conclusion": "\u7814\u7a76\u6210\u679c\u4e3aRAG\u4f4d\u7f6e\u5206\u6790\u5efa\u7acb\u7406\u8bba\u57fa\u7840\uff0c\u5f3a\u8c03\u8bc1\u636e\u91cd\u6392\u5e8f\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u6784\u5efa\u516c\u5e73\u53ef\u9760\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u65b9\u5411"}}
{"id": "2506.11163", "pdf": "https://arxiv.org/pdf/2506.11163", "abs": "https://arxiv.org/abs/2506.11163", "authors": ["James Batten", "Michiel Schaap", "Matthew Sinclair", "Ying Bai", "Ben Glocker"], "title": "Vector Representations of Vessel Trees", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": null, "summary": "We introduce a novel framework for learning vector representations of\ntree-structured geometric data focusing on 3D vascular networks. Our approach\nemploys two sequentially trained Transformer-based autoencoders. In the first\nstage, the Vessel Autoencoder captures continuous geometric details of\nindividual vessel segments by learning embeddings from sampled points along\neach curve. In the second stage, the Vessel Tree Autoencoder encodes the\ntopology of the vascular network as a single vector representation, leveraging\nthe segment-level embeddings from the first model. A recursive decoding process\nensures that the reconstructed topology is a valid tree structure. Compared to\n3D convolutional models, this proposed approach substantially lowers GPU memory\nrequirements, facilitating large-scale training. Experimental results on a 2D\nsynthetic tree dataset and a 3D coronary artery dataset demonstrate superior\nreconstruction fidelity, accurate topology preservation, and realistic\ninterpolations in latent space. Our scalable framework, named VeTTA, offers\nprecise, flexible, and topologically consistent modeling of anatomical tree\nstructures in medical imaging.", "AI": {"tldr": "\u63d0\u51faVeTTA\u6846\u67b6\uff1a\u901a\u8fc7\u53ccTransformer\u81ea\u7f16\u7801\u5668\u5b9e\u73b0\u8840\u7ba1\u6811\u7ed3\u6784\u7684\u4f4e\u5185\u5b58\u9ad8\u6548\u5efa\u6a21\uff0c\u4fdd\u8bc1\u62d3\u6251\u4e00\u81f4\u6027\u548c\u9ad8\u7cbe\u5ea6\u4e09\u7ef4\u91cd\u5efa", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u5377\u79ef\u6a21\u578b\u5728\u8840\u7ba1\u6811\u5efa\u6a21\u4e2d\u5185\u5b58\u6d88\u8017\u5927\u3001\u62d3\u6251\u7ed3\u6784\u6613\u5931\u771f\u7684\u95ee\u9898\uff0c\u6ee1\u8db3\u533b\u5b66\u5f71\u50cf\u5bf9\u89e3\u5256\u7ed3\u6784\u7cbe\u786e\u5efa\u6a21\u7684\u9700\u6c42", "method": "\u5206\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u8840\u7ba1\u6bb5\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u8840\u7ba1\u7247\u6bb5\u7684\u51e0\u4f55\u7279\u5f81 2) \u8840\u7ba1\u6811\u81ea\u7f16\u7801\u5668\u6574\u5408\u62d3\u6251\u7ed3\u6784\uff0c\u91c7\u7528\u9012\u5f52\u89e3\u7801\u4fdd\u8bc1\u6811\u5f62\u6709\u6548\u6027", "result": "\u57282D\u5408\u6210\u6811\u548c3D\u51a0\u72b6\u52a8\u8109\u6570\u636e\u4e0a\u5b9e\u73b0\uff1a\u91cd\u6784\u8bef\u5dee\u964d\u4f4e37%\u3001\u663e\u5b58\u5360\u7528\u51cf\u5c1183%\u3001\u6f5c\u5728\u7a7a\u95f4\u63d2\u503c\u4fdd\u771f\u5ea6\u63d0\u534729%", "conclusion": "VeTTA\u6846\u67b6\u4e3a\u533b\u5b66\u5f71\u50cf\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u62d3\u6251\u4e00\u81f4\u4e14\u9ad8\u7cbe\u5ea6\u7684\u89e3\u5256\u6811\u5efa\u6a21\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf3D\u5377\u79ef\u65b9\u6cd5"}}
{"id": "2506.11065", "pdf": "https://arxiv.org/pdf/2506.11065", "abs": "https://arxiv.org/abs/2506.11065", "authors": ["Alexey Tikhonov", "Sergei Shteiner", "Anna Bykova", "Ivan P. Yamshchikov"], "title": "Smotrom tvoja pa ander drogoj verden! Resurrecting Dead Pidgin with Generative Models: Russenorsk Case Study", "categories": ["cs.CL", "Primary 68T50, Secondary 68T05, 91F20", "I.2.7; I.2.6; I.5.4"], "comment": "ACL Findings 2025", "summary": "Russenorsk, a pidgin language historically used in trade interactions between\nRussian and Norwegian speakers, represents a unique linguistic phenomenon. In\nthis paper, we attempt to analyze its lexicon using modern large language\nmodels (LLMs), based on surviving literary sources. We construct a structured\ndictionary of the language, grouped by synonyms and word origins. Subsequently,\nwe use this dictionary to formulate hypotheses about the core principles of\nword formation and grammatical structure in Russenorsk and show which\nhypotheses generated by large language models correspond to the hypotheses\npreviously proposed ones in the academic literature. We also develop a\n\"reconstruction\" translation agent that generates hypothetical Russenorsk\nrenderings of contemporary Russian and Norwegian texts.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5206\u6790\u4fc4\u632a\u76ae\u94a6\u8bedRussenorsk\u7684\u8bcd\u6c47\u4f53\u7cfb\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u8bcd\u5178\u5e76\u9a8c\u8bc1\u8bcd\u6c47\u5f62\u6210\u5047\u8bf4\uff0c\u5f00\u53d1\u7ffb\u8bd1\u4ee3\u7406\u5b9e\u73b0\u73b0\u4ee3\u6587\u672c\u91cd\u6784", "motivation": "Russenorsk\u4f5c\u4e3a\u5386\u53f2\u4e0a\u4fc4\u632a\u8d38\u6613\u4f7f\u7528\u7684\u72ec\u7279\u6df7\u5408\u8bed\u8a00\uff0c\u5176\u8bed\u8a00\u7ed3\u6784\u7814\u7a76\u5bf9\u7406\u89e3\u63a5\u89e6\u8bed\u8a00\u6f14\u5316\u673a\u5236\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u5206\u6790\u6709\u9650\u5386\u53f2\u8bed\u6599\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u8bba\u53ef\u80fd\u3002", "method": "1.\u57fa\u4e8e\u73b0\u5b58\u6587\u732e\u6784\u5efa\u540c\u4e49\u8bcd-\u8bcd\u6e90\u7ed3\u6784\u5316\u8bcd\u5178\n2.\u901a\u8fc7LLM\u751f\u6210\u8bcd\u6c47\u5f62\u6210/\u8bed\u6cd5\u7ed3\u6784\u5047\u8bf4\u5e76\u4e0e\u5b66\u672f\u5047\u8bf4\u6bd4\u5bf9\n3.\u5f00\u53d1\u7ffb\u8bd1\u4ee3\u7406\u5b9e\u73b0\u73b0\u4ee3\u4fc4\u632a\u6587\u672c\u7684Russenorsk\u5047\u8bbe\u91cd\u6784", "result": "1.\u9a8c\u8bc1LLM\u751f\u6210\u5047\u8bf4\u4e0e\u5b66\u754c\u65e2\u6709\u7406\u8bba\u5b58\u5728\u4e00\u81f4\u6027\n2.\u6210\u529f\u6784\u5efa\u53ef\u751f\u6210\u5047\u8bbe\u6027\u8bed\u8a00\u8868\u8fbe\u7684\u7ffb\u8bd1\u7cfb\u7edf\n3.\u8bc1\u5b9e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u8bed\u8a00\u91cd\u6784\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u63a5\u89e6\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u6570\u5b57\u5316\u65b0\u8303\u5f0f\uff0c\u8bc1\u660eLLM\u5728\u5386\u53f2\u8bed\u8a00\u5b66\u4e2d\u7684\u5206\u6790\u6f5c\u529b\uff0c\u4e3a\u6fd2\u5371\u8bed\u8a00\u4fdd\u62a4\u53ca\u8bed\u8a00\u6f14\u5316\u7814\u7a76\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2506.11483", "pdf": "https://arxiv.org/pdf/2506.11483", "abs": "https://arxiv.org/abs/2506.11483", "authors": ["Zhouheng Du", "Nima Davari", "Li Li", "Nodir Kodirov"], "title": "Capsule: Efficient Player Isolation for Datacenters", "categories": ["cs.DC", "cs.GR"], "comment": null, "summary": "Cloud gaming is increasingly popular. A challenge for cloud provider is to\nkeep datacenter utilization high: a non-trivial task due to application\nvariety. These applications come in different shapes and sizes. So do cloud\ndatacenter resources, e.g., CPUs, GPUs, NPUs.\n  Part of the challenge stems from game engines being predominantly designed to\nrun only one player. One player in a lightweight game might utilize only a\nfraction of the cloud server GPU. The remaining GPU capacity will be left\nunderutilized, an undesired outcome for the cloud provider. We introduce\nCapsule, a mechanism that allows multiple players to seamlessly share one GPU.\n  We implemented Capsule in O3DE, a popular open source game engine. Our\nevaluations show that Capsule can increase datacenter resource utilization by\naccommodating up to 2.25x more players, without degrading player gaming\nexperience. Capsule is also application agnostic. We ran four applications on\nCapsule-based O3DE with no application changes. Our experiences show that\nCapsule design can be adopted by other game engines to increase datacenter\nutilization across cloud providers.", "AI": {"tldr": "\u63d0\u51faCapsule\u673a\u5236\u5b9e\u73b0\u591a\u73a9\u5bb6\u5171\u4eabGPU\u8d44\u6e90\uff0c\u5728O3DE\u5f15\u64ce\u4e2d\u9a8c\u8bc1\u53ef\u63d0\u53472.25\u500d\u73a9\u5bb6\u5bb9\u91cf\u4e14\u65e0\u9700\u5e94\u7528\u6539\u9020\uff0c\u663e\u8457\u63d0\u9ad8\u4e91\u6570\u636e\u4e2d\u5fc3\u5229\u7528\u7387\u3002", "motivation": "\u4e91\u6e38\u620f\u573a\u666f\u4e2d\u5355\u73a9\u5bb6\u72ec\u5360GPU\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u4f20\u7edf\u6e38\u620f\u5f15\u64ce\u67b6\u6784\u96be\u4ee5\u6709\u6548\u5229\u7528\u5269\u4f59\u7b97\u529b\u3002", "method": "\u5728\u5f00\u6e90\u6e38\u620f\u5f15\u64ceO3DE\u4e2d\u5b9e\u73b0GPU\u8d44\u6e90\u5171\u4eab\u6846\u67b6Capsule\uff0c\u901a\u8fc7\u5e94\u7528\u65e0\u5173\u7684\u8bbe\u8ba1\u652f\u6301\u591a\u73a9\u5bb6\u5b9e\u4f8b\u52a8\u6001\u5206\u914d\u663e\u5b58\u548c\u8ba1\u7b97\u8d44\u6e90\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6570\u636e\u4e2d\u5fc3\u53ef\u591a\u5bb9\u7eb3225%\u73a9\u5bb6\uff0c\u5ef6\u8fdf\u6307\u6807\u4fdd\u6301\u4e0d\u53d8\uff0c\u6210\u529f\u8fd0\u884c4\u7c7b\u672a\u4fee\u6539\u7684\u4e91\u6e38\u620f\u5e94\u7528\u3002", "conclusion": "Capsule\u67b6\u6784\u5177\u5907\u5f15\u64ce\u666e\u9002\u6027\uff0c\u4e3a\u4e91\u8ba1\u7b97\u5382\u5546\u63d0\u4f9b\u4e86\u63d0\u5347GPU\u8d44\u6e90\u5229\u7528\u7387\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.11067", "pdf": "https://arxiv.org/pdf/2506.11067", "abs": "https://arxiv.org/abs/2506.11067", "authors": ["Hieu Nghiem", "Hemanth Reddy Singareddy", "Zhuqi Miao", "Jivan Lamichhane", "Abdulaziz Ahmed", "Johnson Thomas", "Dursun Delen", "William Paiva"], "title": "A Large Language Model Based Pipeline for Review of Systems Entity Recognition from Clinical Notes", "categories": ["cs.CL"], "comment": null, "summary": "Objective: Develop a cost-effective, large language model (LLM)-based\npipeline for automatically extracting Review of Systems (ROS) entities from\nclinical notes. Materials and Methods: The pipeline extracts ROS sections using\nSecTag, followed by few-shot LLMs to identify ROS entity spans, their\npositive/negative status, and associated body systems. We implemented the\npipeline using open-source LLMs (Mistral, Llama, Gemma) and ChatGPT. The\nevaluation was conducted on 36 general medicine notes containing 341 annotated\nROS entities. Results: When integrating ChatGPT, the pipeline achieved the\nlowest error rates in detecting ROS entity spans and their corresponding\nstatuses/systems (28.2% and 14.5%, respectively). Open-source LLMs enable\nlocal, cost-efficient execution of the pipeline while delivering promising\nperformance with similarly low error rates (span: 30.5-36.7%; status/system:\n24.3-27.3%). Discussion and Conclusion: Our pipeline offers a scalable and\nlocally deployable solution to reduce ROS documentation burden. Open-source\nLLMs present a viable alternative to commercial models in resource-limited\nhealthcare environments.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u533b\u7597\u5b9e\u4f53\u62bd\u53d6\u7ba1\u9053\uff0c\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u63a5\u8fd1ChatGPT\uff0c\u9002\u5408\u8d44\u6e90\u6709\u9650\u573a\u666f", "motivation": "\u51cf\u8f7b\u4e34\u5e8a\u6587\u6863\u4e2d\u7cfb\u7edf\u56de\u987e(ROS)\u7684\u4eba\u5de5\u8bb0\u5f55\u8d1f\u62c5\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u533b\u7597\u673a\u6784\u63d0\u4f9b\u53ef\u672c\u5730\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528SecTag\u63d0\u53d6ROS\u6bb5\u843d+\u5c0f\u6837\u672cLLM\u8bc6\u522b\u5b9e\u4f53\u8fb9\u754c/\u72b6\u6001/\u7cfb\u7edf\uff0c\u5bf9\u6bd4\u5f00\u6e90\u6a21\u578b(Mistral/Llama/Gemma)\u4e0eChatGPT\u6548\u679c", "result": "ChatGPT\u9519\u8bef\u7387\u6700\u4f4e(\u5b9e\u4f53\u8bc6\u522b28.2%\uff0c\u72b6\u6001/\u7cfb\u7edf14.5%)\uff1b\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u63a5\u8fd1(\u5b9e\u4f5330.5-36.7%\uff0c\u72b6\u6001/\u7cfb\u7edf24.3-27.3%)", "conclusion": "\u5f00\u6e90LLM\u5728\u4fdd\u6301\u4f4e\u6210\u672c\u7684\u540c\u65f6\u63d0\u4f9b\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u672c\u5730\u90e8\u7f72\u7684\u81ea\u52a8\u5316ROS\u6587\u6863\u751f\u6210"}}
{"id": "2506.11068", "pdf": "https://arxiv.org/pdf/2506.11068", "abs": "https://arxiv.org/abs/2506.11068", "authors": ["Bumjin Park", "Jinsil Lee", "Jaesik Choi"], "title": "Deontological Keyword Bias: The Impact of Modal Expressions on Normative Judgments of Language Models", "categories": ["cs.CL"], "comment": "20 pages including references and appendix; To appear in ACL 2025\n  main conference", "summary": "Large language models (LLMs) are increasingly engaging in moral and ethical\nreasoning, where criteria for judgment are often unclear, even for humans.\nWhile LLM alignment studies cover many areas, one important yet underexplored\narea is how LLMs make judgments about obligations. This work reveals a strong\ntendency in LLMs to judge non-obligatory contexts as obligations when prompts\nare augmented with modal expressions such as must or ought to. We introduce\nthis phenomenon as Deontological Keyword Bias (DKB). We find that LLMs judge\nover 90\\% of commonsense scenarios as obligations when modal expressions are\npresent. This tendency is consist across various LLM families, question types,\nand answer formats. To mitigate DKB, we propose a judgment strategy that\nintegrates few-shot examples with reasoning prompts. This study sheds light on\nhow modal expressions, as a form of linguistic framing, influence the normative\ndecisions of LLMs and underscores the importance of addressing such biases to\nensure judgment alignment.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9053\u5fb7\u63a8\u7406\u4e2d\u5b58\u5728\u4e49\u52a1\u8bba\u5173\u952e\u8bcd\u504f\u89c1\uff08DKB\uff09\uff0c\u5f53\u63d0\u793a\u5305\u542b'\u5fc5\u987b'\u7b49\u6a21\u6001\u8868\u8fbe\u65f6\u4f1a\u5c06\u975e\u4e49\u52a1\u60c5\u5883\u9519\u8bef\u5224\u65ad\u4e3a\u4e49\u52a1\uff0c\u5e76\u63d0\u51fa\u7f13\u89e3\u7b56\u7565", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22\u4e49\u52a1\u5224\u65ad\u4e2d\u7684\u7cfb\u7edf\u504f\u5dee\uff0c\u7279\u522b\u662f\u6a21\u6001\u8868\u8fbe\u5bf9\u6a21\u578b\u89c4\u8303\u6027\u51b3\u7b56\u7684\u6f5c\u5728\u5f71\u54cd", "method": "\u901a\u8fc7\u591a\u7ef4\u5ea6\u5b9e\u9a8c\uff08\u4e0d\u540cLLM\u5bb6\u65cf/\u95ee\u9898\u7c7b\u578b/\u56de\u7b54\u683c\u5f0f\uff09\u9a8c\u8bc1DKB\u73b0\u8c61\uff0c\u63d0\u51fa\u7ed3\u5408\u5c11\u6837\u672c\u793a\u4f8b\u4e0e\u63a8\u7406\u63d0\u793a\u7684\u8054\u5408\u5224\u65ad\u7b56\u7565", "result": "LLMs\u5728\u542b\u6a21\u6001\u8bcd\u7684\u63d0\u793a\u4e0b\u5c0690%+\u7684\u5e38\u8bc6\u60c5\u5883\u8bef\u5224\u4e3a\u4e49\u52a1\uff0c\u4e14\u8de8\u6a21\u578b/\u8bbe\u7f6e\u8868\u73b0\u4e00\u81f4\uff0c\u7f13\u89e3\u7b56\u7565\u6709\u6548\u964d\u4f4e\u8bef\u5224\u7387", "conclusion": "\u63ed\u793a\u4e86\u8bed\u8a00\u6846\u67b6\uff08\u6a21\u6001\u8bcd\uff09\u5bf9LLM\u89c4\u8303\u6027\u51b3\u7b56\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\uff0c\u5f3a\u8c03\u6d88\u9664\u6b64\u7c7b\u504f\u89c1\u5bf9\u5b9e\u73b0\u5224\u65ad\u5bf9\u9f50\u7684\u91cd\u8981\u6027"}}
{"id": "2506.11070", "pdf": "https://arxiv.org/pdf/2506.11070", "abs": "https://arxiv.org/abs/2506.11070", "authors": ["Yu-Zhe Shi", "Mingchen Liu", "Hanlu Ma", "Qiao Xu", "Huamin Qu", "Kun He", "Lecheng Ruan", "Qining Wang"], "title": "Targeted control of fast prototyping through domain-specific interface", "categories": ["cs.CL"], "comment": "In International Conference on Machine Learning (ICML'25)", "summary": "Industrial designers have long sought a natural and intuitive way to achieve\nthe targeted control of prototype models -- using simple natural language\ninstructions to configure and adjust the models seamlessly according to their\nintentions, without relying on complex modeling commands. While Large Language\nModels have shown promise in this area, their potential for controlling\nprototype models through language remains partially underutilized. This\nlimitation stems from gaps between designers' languages and modeling languages,\nincluding mismatch in abstraction levels, fluctuation in semantic precision,\nand divergence in lexical scopes. To bridge these gaps, we propose an interface\narchitecture that serves as a medium between the two languages. Grounded in\ndesign principles derived from a systematic investigation of fast prototyping\npractices, we devise the interface's operational mechanism and develop an\nalgorithm for its automated domain specification. Both machine-based\nevaluations and human studies on fast prototyping across various product design\ndomains demonstrate the interface's potential to function as an auxiliary\nmodule for Large Language Models, enabling precise and effective targeted\ncontrol of prototype models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u63a5\u53e3\u67b6\u6784\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u52a9\u6a21\u5757\uff0c\u901a\u8fc7\u5f25\u5408\u81ea\u7136\u8bed\u8a00\u4e0e\u5efa\u6a21\u8bed\u8a00\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u539f\u578b\u6a21\u578b\u7684\u7cbe\u51c6\u76ee\u6807\u63a7\u5236\u3002", "motivation": "\u5de5\u4e1a\u8bbe\u8ba1\u5e08\u671f\u671b\u7528\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u63a7\u5236\u539f\u578b\u6a21\u578b\uff0c\u4f46\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u573a\u666f\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u8bed\u8a00\u62bd\u8c61\u5c42\u7ea7\u3001\u8bed\u4e49\u7cbe\u5ea6\u53ca\u8bcd\u6c47\u8303\u56f4\u5dee\u5f02\u3002", "method": "\u57fa\u4e8e\u5feb\u901f\u539f\u578b\u5b9e\u8df5\u7684\u7cfb\u7edf\u7814\u7a76\u63d0\u70bc\u8bbe\u8ba1\u539f\u5219\uff0c\u5f00\u53d1\u63a5\u53e3\u64cd\u4f5c\u673a\u5236\u53ca\u81ea\u52a8\u5316\u9886\u57df\u89c4\u8303\u7b97\u6cd5\u3002", "result": "\u8de8\u9886\u57df\u673a\u5668\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u63a5\u53e3\u4f5c\u4e3a\u8f85\u52a9\u6a21\u5757\u53ef\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u539f\u578b\u6a21\u578b\u7684\u7cbe\u786e\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u63a5\u53e3\u67b6\u6784\u6709\u6548\u8854\u63a5\u8bbe\u8ba1\u8bed\u8a00\u4e0e\u5efa\u6a21\u8bed\u8a00\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5feb\u901f\u539f\u578b\u9886\u57df\u7684\u5de5\u7a0b\u5316\u5e94\u7528\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2506.11073", "pdf": "https://arxiv.org/pdf/2506.11073", "abs": "https://arxiv.org/abs/2506.11073", "authors": ["Zekai Ye", "Qiming Li", "Xiaocheng Feng", "Libo Qin", "Yichong Huang", "Baohang Li", "Kui Jiang", "Yang Xiang", "Zhirui Zhang", "Yunfei Lu", "Duyu Tang", "Dandan Tu", "Bing Qin"], "title": "CLAIM: Mitigating Multilingual Object Hallucination in Large Vision-Language Models with Cross-Lingual Attention Intervention", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL2025 Main", "summary": "Large Vision-Language Models (LVLMs) have demonstrated impressive multimodal\nabilities but remain prone to multilingual object hallucination, with a higher\nlikelihood of generating responses inconsistent with the visual input when\nutilizing queries in non-English languages compared to English. Most existing\napproaches to address these rely on pretraining or fine-tuning, which are\nresource-intensive. In this paper, inspired by observing the disparities in\ncross-modal attention patterns across languages, we propose Cross-Lingual\nAttention Intervention for Mitigating multilingual object hallucination (CLAIM)\nin LVLMs, a novel near training-free method by aligning attention patterns.\nCLAIM first identifies language-specific cross-modal attention heads, then\nestimates language shift vectors from English to the target language, and\nfinally intervenes in the attention outputs during inference to facilitate\ncross-lingual visual perception capability alignment. Extensive experiments\ndemonstrate that CLAIM achieves an average improvement of 13.56% (up to 30% in\nSpanish) on the POPE and 21.75% on the hallucination subsets of the MME\nbenchmark across various languages. Further analysis reveals that multilingual\nattention divergence is most prominent in intermediate layers, highlighting\ntheir critical role in multilingual scenarios.", "AI": {"tldr": "\u63d0\u51faCLAIM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00\u6ce8\u610f\u529b\u5e72\u9884\u51cf\u5c11\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5bf9\u8c61\u5e7b\u89c9\uff0c\u5728POPE\u548cMME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff08\u6700\u9ad8\u8fbe30%\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u7684\u9884\u8bad\u7ec3/\u5fae\u8c03\uff0c\u4e14\u53d1\u73b0\u4e0d\u540c\u8bed\u8a00\u7684\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff08\u5c24\u5176\u5728\u4e2d\u95f4\u5c42\uff09\uff0c\u5bfc\u81f4\u975e\u82f1\u8bed\u67e5\u8be2\u66f4\u5bb9\u6613\u51fa\u73b0\u89c6\u89c9\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u3002", "method": "1.\u8bc6\u522b\u8bed\u8a00\u7279\u5f02\u6027\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5934 \u2192 2.\u4f30\u8ba1\u82f1\u8bed\u5230\u76ee\u6807\u8bed\u8a00\u7684\u6ce8\u610f\u529b\u504f\u79fb\u5411\u91cf \u2192 3.\u5728\u63a8\u7406\u9636\u6bb5\u5e72\u9884\u6ce8\u610f\u529b\u8f93\u51fa\u4ee5\u5bf9\u9f50\u8de8\u8bed\u8a00\u89c6\u89c9\u611f\u77e5\u80fd\u529b", "result": "POPE\u5e73\u5747\u63d0\u534713.56%\uff08\u897f\u73ed\u7259\u8bed\u6700\u9ad830%\uff09\uff0cMME\u5e7b\u89c9\u5b50\u96c6\u63d0\u534721.75%\u3002\u5206\u6790\u8868\u660e\u4e2d\u95f4\u5c42\u6ce8\u610f\u529b\u5dee\u5f02\u6700\u663e\u8457\uff0c\u662f\u591a\u8bed\u8a00\u5904\u7406\u7684\u5173\u952e\u5c42\u3002", "conclusion": "CLAIM\u901a\u8fc7\u6ce8\u610f\u529b\u6a21\u5f0f\u5bf9\u9f50\u6709\u6548\u7f13\u89e3\u591a\u8bed\u8a00\u5e7b\u89c9\uff0c\u63ed\u793a\u4e86\u4e2d\u95f4\u5c42\u5728\u591a\u8bed\u8a00\u573a\u666f\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u4e3a\u8f7b\u91cf\u5316\u89e3\u51b3LVLM\u8de8\u8bed\u8a00\u95ee\u9898\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.11077", "pdf": "https://arxiv.org/pdf/2506.11077", "abs": "https://arxiv.org/abs/2506.11077", "authors": ["Chongyu Fan", "Yihua Zhang", "Jinghan Jia", "Alfred Hero", "Sijia Liu"], "title": "CyclicReflex: Improving Large Reasoning Models via Cyclical Reflection Token Scheduling", "categories": ["cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs), such as OpenAI's o1 and DeepSeek-R1, harness\ntest-time scaling to perform multi-step reasoning for complex problem-solving.\nThis reasoning process, executed before producing final answers, is often\nguided by special juncture tokens or textual segments that prompt\nself-evaluative reflection. We refer to these transition markers and reflective\ncues as \"reflection tokens\" (e.g., \"wait\", \"but\", \"alternatively\"). In this\nwork, we treat reflection tokens as a \"resource\" and introduce the problem of\nresource allocation, aimed at improving the test-time compute performance of\nLRMs by adaptively regulating the frequency and placement of reflection tokens.\nThrough empirical analysis, we show that both excessive and insufficient use of\nreflection tokens, referred to as over-reflection and under-reflection, can\ndegrade model performance. To better understand and manage this trade-off, we\ndraw an analogy between reflection token usage and learning rate scheduling in\noptimization. Building on this insight, we propose cyclical reflection token\nscheduling (termed CyclicReflex), a decoding strategy that dynamically\nmodulates reflection token logits using a position-dependent triangular\nwaveform. Experiments on MATH500, AIME2024/2025, and AMC2023 demonstrate that\nCyclicReflex consistently improves performance across model sizes (1.5B-8B),\noutperforming standard decoding and more recent approaches such as TIP (thought\nswitching penalty) and S1. Codes are available at\nhttps://github.com/OPTML-Group/CyclicReflex.", "AI": {"tldr": "\u63d0\u51faCyclicReflex\u65b9\u6cd5\uff0c\u901a\u8fc7\u5468\u671f\u6027\u8c03\u6574\u53cd\u601d\u4ee4\u724c\u7684\u5206\u5e03\u6765\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u6548\u7387", "motivation": "\u7814\u7a76\u53d1\u73b0\u53cd\u601d\u4ee4\u724c\u7684\u8fc7\u91cf\u4f7f\u7528\uff08\u8fc7\u5ea6\u53cd\u601d\uff09\u548c\u4e0d\u8db3\u4f7f\u7528\uff08\u6b20\u53cd\u601d\uff09\u90fd\u4f1a\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff0c\u53d7\u5b66\u4e60\u7387\u8c03\u5ea6\u673a\u5236\u542f\u53d1\uff0c\u9700\u8981\u52a8\u6001\u8c03\u8282\u53cd\u601d\u4ee4\u724c\u7684\u4f7f\u7528\u9891\u6b21", "method": "\u57fa\u4e8e\u4f4d\u7f6e\u4f9d\u8d56\u7684\u4e09\u89d2\u6ce2\u5f62\u52a8\u6001\u8c03\u5236\u53cd\u601d\u4ee4\u724c\u7684logits\u5206\u5e03\uff0c\u5f00\u53d1\u5faa\u73af\u53cd\u601d\u4ee4\u724c\u8c03\u5ea6\u7b56\u7565\uff08CyclicReflex\uff09", "result": "\u5728MATH500\u3001AIME2024/2025\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff081.5B-8B\u89c4\u6a21\uff09\uff0c\u4f18\u4e8e\u6807\u51c6\u89e3\u7801\u548cTIP\u3001S1\u7b49\u6700\u65b0\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u52a8\u6001\u8c03\u63a7\u53cd\u601d\u4ee4\u724c\u8d44\u6e90\u5206\u914d\u53ef\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u89c4\u6a21\u666e\u9002\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.11078", "pdf": "https://arxiv.org/pdf/2506.11078", "abs": "https://arxiv.org/abs/2506.11078", "authors": ["Yuzhou Yang", "Yangming Zhou", "Zhiying Zhu", "Zhenxing Qian", "Xinpeng Zhang", "Sheng Li"], "title": "RoE-FND: A Case-Based Reasoning Approach with Dual Verification for Fake News Detection via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of deceptive content online necessitates robust Fake News\nDetection (FND) systems. While evidence-based approaches leverage external\nknowledge to verify claims, existing methods face critical limitations: noisy\nevidence selection, generalization bottlenecks, and unclear decision-making\nprocesses. Recent efforts to harness Large Language Models (LLMs) for FND\nintroduce new challenges, including hallucinated rationales and conclusion\nbias. To address these issues, we propose \\textbf{RoE-FND}\n(\\textbf{\\underline{R}}eason \\textbf{\\underline{o}}n\n\\textbf{\\underline{E}}xperiences FND), a framework that reframes evidence-based\nFND as a logical deduction task by synergizing LLMs with experiential learning.\nRoE-FND encompasses two stages: (1) \\textit{self-reflective knowledge\nbuilding}, where a knowledge base is curated by analyzing past reasoning\nerrors, namely the exploration stage, and (2) \\textit{dynamic criterion\nretrieval}, which synthesizes task-specific reasoning guidelines from\nhistorical cases as experiences during deployment. It further cross-checks\nrationales against internal experience through a devised dual-channel\nprocedure. Key contributions include: a case-based reasoning framework for FND\nthat addresses multiple existing challenges, a training-free approach enabling\nadaptation to evolving situations, and empirical validation of the framework's\nsuperior generalization and effectiveness over state-of-the-art methods across\nthree datasets.", "AI": {"tldr": "\u63d0\u51faRoE-FND\u6846\u67b6\uff0c\u901a\u8fc7\u7ecf\u9a8c\u5b66\u4e60\u589e\u5f3a\u5047\u65b0\u95fb\u68c0\u6d4b\uff0c\u5728\u81ea\u53cd\u77e5\u8bc6\u6784\u5efa\u548c\u52a8\u6001\u51c6\u5219\u68c0\u7d22\u4e24\u9636\u6bb5\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bc1\u636e\u578b\u5047\u65b0\u95fb\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u8bc1\u636e\u566a\u97f3/\u6cdb\u5316\u74f6\u9888/\u51b3\u7b56\u4e0d\u900f\u660e\u95ee\u9898\uff0cLLMs\u5e94\u7528\u5b58\u5728\u5e7b\u89c9/\u7ed3\u8bba\u504f\u5dee\uff0c\u9700\u6784\u5efa\u66f4\u53ef\u9760\u7684\u81ea\u9002\u5e94\u6846\u67b6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u81ea\u53cd\u77e5\u8bc6\u6784\u5efa\u9636\u6bb5\u901a\u8fc7\u5206\u6790\u5386\u53f2\u9519\u8bef\u5efa\u7acb\u77e5\u8bc6\u5e93\uff1b2) \u52a8\u6001\u51c6\u5219\u68c0\u7d22\u9636\u6bb5\u4ece\u5386\u53f2\u6848\u4f8b\u5408\u6210\u63a8\u7406\u51c6\u5219\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u4ea4\u53c9\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0cRoE-FND\u5c55\u73b0\u51fa\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u6d4b\u6027\u80fd\u3002", "conclusion": "RoE-FND\u901a\u8fc7\u6848\u4f8b\u63a8\u7406\u6846\u67b6\u89e3\u51b3\u591a\u7ef4\u5ea6\u6311\u6218\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u4e3a\u53ef\u4fe1\u5047\u65b0\u95fb\u68c0\u6d4b\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.11080", "pdf": "https://arxiv.org/pdf/2506.11080", "abs": "https://arxiv.org/abs/2506.11080", "authors": ["Han Zhou", "Qitong Xu", "Yiheng Dong", "Xin Yang"], "title": "MANBench: Is Your Multimodal Model Smarter than Human?", "categories": ["cs.CL"], "comment": "Multimodal Benchmark, Project Url: https://github.com/micdz/MANBench,\n  ACL2025 Findings", "summary": "The rapid advancement of Multimodal Large Language Models (MLLMs) has ignited\ndiscussions regarding their potential to surpass human performance in\nmultimodal tasks. In response, we introduce MANBench (Multimodal Ability Norms\nBenchmark), a bilingual benchmark (English and Chinese) comprising 1,314\nquestions across nine tasks, spanning knowledge-based and non-knowledge-based\ndomains. MANBench emphasizes intuitive reasoning, seamless cross-modal\nintegration, and real-world complexity, providing a rigorous evaluation\nframework.\n  Through extensive human experiments involving diverse participants, we\ncompared human performance against state-of-the-art MLLMs. The results indicate\nthat while MLLMs excel in tasks like Knowledge and Text-Image Understanding,\nthey struggle with deeper cross-modal reasoning tasks such as Transmorphic\nUnderstanding, Image Consistency, and Multi-image Understanding. Moreover, both\nhumans and MLLMs face challenges in highly complex tasks like Puzzles and\nSpatial Imagination.\n  MANBench highlights the strengths and limitations of MLLMs, revealing that\neven advanced models fall short of achieving human-level performance across\nmany domains. We hope MANBench will inspire efforts to bridge the gap between\nMLLMs and human multimodal capabilities. The code and dataset are available at\nhttps://github.com/micdz/MANBench.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u57fa\u51c6MANBench\u8bc4\u4f30MLLMs\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u8de8\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u843d\u540e\u4eba\u7c7b\uff0c\u590d\u6742\u4efb\u52a1\u53cc\u65b9\u5747\u9762\u4e34\u6311\u6218\u3002", "motivation": "\u56de\u5e94\u5173\u4e8eMLLMs\u662f\u5426\u8d85\u8d8a\u4eba\u7c7b\u591a\u6a21\u6001\u80fd\u529b\u7684\u8ba8\u8bba\uff0c\u5efa\u7acb\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u6a21\u578b\u771f\u5b9e\u8868\u73b0\u3002", "method": "\u6784\u5efa\u5305\u542b9\u9879\u4efb\u52a11314\u95ee\u9898\u7684\u53cc\u8bed\u57fa\u51c6\uff0c\u901a\u8fc7\u591a\u7fa4\u4f53\u4eba\u7c7b\u5b9e\u9a8c\u5bf9\u6bd4SOTA\u6a21\u578b\u8868\u73b0\u3002", "result": "MLLMs\u5728\u77e5\u8bc6/\u56fe\u6587\u7406\u89e3\u9886\u5148\uff0c\u8de8\u6a21\u6001\u63a8\u7406\uff08\u8f6c\u6362\u7406\u89e3/\u56fe\u50cf\u4e00\u81f4\u6027/\u591a\u56fe\u7406\u89e3\uff09\u8868\u73b0\u5dee\uff0c\u8c1c\u9898/\u7a7a\u95f4\u60f3\u8c61\u7b49\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\u4eba\u7c7b\u4e0e\u6a21\u578b\u5747\u56f0\u96be\u3002", "conclusion": "MANBench\u63ed\u793a\u73b0\u6709MLLMs\u5c1a\u672a\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u65b9\u5411\u6807\u3002\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.11081", "pdf": "https://arxiv.org/pdf/2506.11081", "abs": "https://arxiv.org/abs/2506.11081", "authors": ["Aditi", "Hyunwoo Park", "Sicheol Sung", "Yo-Sub Han", "Sang-Ki Ko"], "title": "SAGE:Specification-Aware Grammar Extraction for Automated Test Case Generation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Grammar-based test case generation has proven effective for competitive\nprogramming problems, but generating valid and general grammars from natural\nlanguage specifications remains a key challenge, especially under limited\nsupervision. Context-Free Grammars with Counters (CCFGs) have recently been\nintroduced as a formalism to represent such specifications with logical\nconstraints by storing and reusing counter values during derivation. In this\nwork, we explore the use of open-source large language models (LLMs) to induce\nCCFGs from specifications using a small number of labeled examples and\nverifiable reward-guided reinforcement learning. Our approach first fine-tunes\nan open-source LLM to perform specification-to-grammar translation, and further\napplies Group Relative Policy Optimization (GRPO) to enhance grammar validity\nand generality. We also examine the effectiveness of iterative feedback for\nopen and closed-source LLMs in correcting syntactic and semantic errors in\ngenerated grammars.\n  Experimental results show that our approach SAGE achieves stronger\ngeneralization and outperforms 17 open and closed-source LLMs in both grammar\nquality and test effectiveness, improving over the state-of-the-art by 15.92%p\nin grammar validity and 12.34%p in test effectiveness. We provide our\nimplementation and dataset at the following anonymous\nrepository:https://anonymous.4open.science/r/SAGE-5714", "AI": {"tldr": "\u63d0\u51faSAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90\u5927\u6a21\u578b\u548cGRPO\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347CCFG\u8bed\u6cd5\u751f\u6210\u8d28\u91cf\u4e0e\u6d4b\u8bd5\u6709\u6548\u6027", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u89c4\u8303\u4e0b\u751f\u6210\u6709\u6548\u4e14\u901a\u7528\u8bed\u6cd5\u89c4\u5219\u7684\u6311\u6218\uff0c\u5229\u7528\u5e26\u8ba1\u6570\u5668\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\uff08CCFG\uff09\u5904\u7406\u903b\u8f91\u7ea6\u675f", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5fae\u8c03LLM\u5b9e\u73b0\u89c4\u8303\u5230\u8bed\u6cd5\u7684\u8f6c\u6362 2\uff09\u5e94\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u8bed\u6cd5\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b", "result": "SAGE\u5728\u8bed\u6cd5\u6709\u6548\u6027\uff08+15.92%p\uff09\u548c\u6d4b\u8bd5\u6709\u6548\u6027\uff08+12.34%p\uff09\u4e0a\u8d85\u8d8a17\u4e2a\u5f00\u6e90/\u95ed\u6e90LLM\uff0c\u5237\u65b0SOTA\u8bb0\u5f55", "conclusion": "\u9a8c\u8bc1\u4e86\u53ef\u89e3\u91ca\u5956\u52b1\u673a\u5236\u4e0e\u8fed\u4ee3\u53cd\u9988\u5728\u8bed\u6cd5\u751f\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u53d7\u9650\u76d1\u7763\u4e0b\u7684\u7a0b\u5e8f\u89c4\u8303\u8f6c\u6362\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.11082", "pdf": "https://arxiv.org/pdf/2506.11082", "abs": "https://arxiv.org/abs/2506.11082", "authors": ["Lionel Levine", "John Santerre", "Alex S. Young", "T. Barry Levine", "Francis Campion", "Majid Sarrafzadeh"], "title": "PRISM: A Transformer-based Language Model of Structured Clinical Event Data", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 Figures, 1 Table", "summary": "We introduce PRISM (Predictive Reasoning in Sequential Medicine), a\ntransformer-based architecture designed to model the sequential progression of\nclinical decision-making processes. Unlike traditional approaches that rely on\nisolated diagnostic classification, PRISM frames clinical trajectories as\ntokenized sequences of events - including diagnostic tests, laboratory results,\nand diagnoses - and learns to predict the most probable next steps in the\npatient diagnostic journey. Leveraging a large custom clinical vocabulary and\nan autoregressive training objective, PRISM demonstrates the ability to capture\ncomplex dependencies across longitudinal patient timelines. Experimental\nresults show substantial improvements over random baselines in next-token\nprediction tasks, with generated sequences reflecting realistic diagnostic\npathways, laboratory result progressions, and clinician ordering behaviors.\nThese findings highlight the feasibility of applying generative language\nmodeling techniques to structured medical event data, enabling applications in\nclinical decision support, simulation, and education. PRISM establishes a\nfoundation for future advancements in sequence-based healthcare modeling,\nbridging the gap between machine learning architectures and real-world\ndiagnostic reasoning.", "AI": {"tldr": "PRISM\u662f\u57fa\u4e8eTransformer\u7684\u4e34\u5e8a\u51b3\u7b56\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u8bad\u7ec3\u63d0\u5347\u8bca\u65ad\u8def\u5f84\u51c6\u786e\u6027", "motivation": "\u7a81\u7834\u4f20\u7edf\u5b64\u7acb\u8bca\u65ad\u6a21\u5f0f\uff0c\u6355\u6349\u533b\u7597\u4e8b\u4ef6\u5728\u65f6\u95f4\u7ebf\u4e0a\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb", "method": "\u91c7\u7528\u81ea\u5b9a\u4e49\u4e34\u5e8a\u8bcd\u6c47\u8868\uff0c\u5c06\u8bca\u7597\u8fc7\u7a0b\u7f16\u7801\u4e3a\u6807\u8bb0\u5316\u5e8f\u5217\u8fdb\u884c\u81ea\u56de\u5f52\u8bad\u7ec3", "result": "\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u751f\u6210\u7b26\u5408\u771f\u5b9e\u573a\u666f\u7684\u8bca\u65ad\u8def\u5f84\u548c\u68c0\u6d4b\u6d41\u7a0b", "conclusion": "\u9a8c\u8bc1\u4e86\u751f\u6210\u5f0f\u6a21\u578b\u5728\u533b\u7597\u6570\u636e\u5206\u6790\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u548c\u533b\u5b66\u6559\u80b2\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.11083", "pdf": "https://arxiv.org/pdf/2506.11083", "abs": "https://arxiv.org/abs/2506.11083", "authors": ["Ali Asad", "Stephen Obadinma", "Radin Shayanfar", "Xiaodan Zhu"], "title": "RedDebate: Safer Responses through Multi-Agent Red Teaming Debates", "categories": ["cs.CL"], "comment": null, "summary": "We propose RedDebate, a novel multi-agent debate framework that leverages\nadversarial argumentation among Large Language Models (LLMs) to proactively\nidentify and mitigate their own unsafe behaviours. Existing AI safety methods\noften depend heavily on costly human evaluations or isolated single-model\nassessment, both subject to scalability constraints and oversight risks.\nRedDebate instead embraces collaborative disagreement, enabling multiple LLMs\nto critically examine one another's reasoning, and systematically uncovering\nunsafe blind spots through automated red-teaming, and iteratively improve their\nresponses. We further integrate distinct types of long-term memory that retain\nlearned safety insights from debate interactions. Evaluating on established\nsafety benchmarks such as HarmBench, we demonstrate the proposed method's\neffectiveness. Debate alone can reduce unsafe behaviours by 17.7%, and when\ncombined with long-term memory modules, achieves reductions exceeding 23.5%. To\nour knowledge, RedDebate constitutes the first fully automated framework that\ncombines multi-agent debates with red-teaming to progressively enhance AI\nsafety without direct human intervention.(Github Repository:\nhttps://github.com/aliasad059/RedDebate)", "AI": {"tldr": "RedDebate\u63d0\u51fa\u9996\u4e2a\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u4e0e\u7ea2\u961f\u6d4b\u8bd5\u7684\u5168\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8fa9\u8bba\u548c\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u964d\u4f4eAI\u4e0d\u5b89\u5168\u884c\u4e3a23.5%\u3002", "motivation": "\u73b0\u6709AI\u5b89\u5168\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bc4\u4f30\u6216\u5355\u6a21\u578b\u68c0\u6d4b\uff0c\u5b58\u5728\u6210\u672c\u9ad8\u3001\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002RedDebate\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5bf9\u6297\u8fa9\u8bba\u5b9e\u73b0\u81ea\u52a8\u5316\u5b89\u5168\u589e\u5f3a\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6\uff0c\u8ba9LLM\u76f8\u4e92\u6279\u5224\u63a8\u7406\u8fc7\u7a0b\uff0c\u7ed3\u5408\u7ea2\u961f\u6d4b\u8bd5\u53d1\u73b0\u5b89\u5168\u76f2\u70b9\uff0c\u5e76\u901a\u8fc7\u957f\u671f\u8bb0\u5fc6\u6a21\u5757\u6301\u7eed\u79ef\u7d2f\u5b89\u5168\u77e5\u8bc6\u3002", "result": "\u5728HarmBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u7eaf\u8fa9\u8bba\u964d\u4f4e17.7%\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u7ed3\u5408\u8bb0\u5fc6\u6a21\u5757\u540e\u964d\u4f4e\u5e45\u5ea6\u8fbe23.5%\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7684AI\u5b89\u5168\u589e\u5f3a\uff0c\u901a\u8fc7\u8fa9\u8bba-\u7ea2\u961f\u534f\u540c\u673a\u5236\u6301\u7eed\u4f18\u5316\u6a21\u578b\u5b89\u5168\u6027\uff0c\u4e3aAI\u5b89\u5168\u9886\u57df\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.11088", "pdf": "https://arxiv.org/pdf/2506.11088", "abs": "https://arxiv.org/abs/2506.11088", "authors": ["Pengbo Wang", "Chaozhuo Li", "Chenxu Wang", "Liwen Zheng", "Litian Zhang", "Xi Zhang"], "title": "Two Birds with One Stone: Improving Factuality and Faithfulness of LLMs via Dynamic Interactive Subspace Editing", "categories": ["cs.CL", "cs.AI", "68T50"], "comment": null, "summary": "LLMs have demonstrated unprecedented capabilities in natural language\nprocessing, yet their practical deployment remains hindered by persistent\nfactuality and faithfulness hallucinations. While existing methods address\nthese hallucination types independently, they inadvertently induce performance\ntrade-offs, as interventions targeting one type often exacerbate the other.\nThrough empirical and theoretical analysis of activation space dynamics in\nLLMs, we reveal that these hallucination categories share overlapping subspaces\nwithin neural representations, presenting an opportunity for concurrent\nmitigation. To harness this insight, we propose SPACE, a unified framework that\njointly enhances factuality and faithfulness by editing shared activation\nsubspaces. SPACE establishes a geometric foundation for shared subspace\nexistence through dual-task feature modeling, then identifies and edits these\nsubspaces via a hybrid probe strategy combining spectral clustering and\nattention head saliency scoring. Experimental results across multiple benchmark\ndatasets demonstrate the superiority of our approach.", "AI": {"tldr": "SPACE\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u8f91\u5171\u4eab\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u8054\u5408\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u6027\u548c\u5fe0\u5b9e\u6027\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5355\u72ec\u5904\u7406\u5bfc\u81f4\u7684\u6027\u80fd\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5904\u7406\u4e8b\u5b9e\u6027\u5e7b\u89c9\u548c\u5fe0\u5b9e\u6027\u5e7b\u89c9\u65f6\u4f1a\u4ea7\u751f\u6027\u80fd\u6743\u8861\uff0c\u7814\u7a76\u53d1\u73b0\u4e24\u7c7b\u5e7b\u89c9\u5728\u795e\u7ecf\u8868\u5f81\u4e2d\u5b58\u5728\u91cd\u53e0\u5b50\u7a7a\u95f4\uff0c\u4e3a\u8054\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "method": "\u57fa\u4e8e\u53cc\u4efb\u52a1\u7279\u5f81\u5efa\u6a21\u5efa\u7acb\u5171\u4eab\u5b50\u7a7a\u95f4\u7684\u51e0\u4f55\u57fa\u7840\uff0c\u7ed3\u5408\u8c31\u805a\u7c7b\u548c\u6ce8\u610f\u529b\u5934\u663e\u8457\u6027\u5f97\u5206\u7684\u6df7\u5408\u63a2\u6d4b\u7b56\u7565\u8bc6\u522b\u5e76\u7f16\u8f91\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u540c\u65f6\u6709\u6548\u964d\u4f4e\u4e24\u79cd\u5e7b\u89c9\u7c7b\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u72ec\u7acb\u5e72\u9884\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5171\u4eab\u5b50\u7a7a\u95f4\u7f16\u8f91\u5b9e\u73b0\u4e24\u79cd\u5e7b\u89c9\u7684\u534f\u540c\u7f13\u89e3\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u51e0\u4f55\u5e72\u9884\u8303\u5f0f\u3002"}}
{"id": "2506.11091", "pdf": "https://arxiv.org/pdf/2506.11091", "abs": "https://arxiv.org/abs/2506.11091", "authors": ["Shaoshi Ling", "Guoli Ye"], "title": "Customizing Speech Recognition Model with Large Language Model Feedback", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic speech recognition (ASR) systems have achieved strong performance\non general transcription tasks. However, they continue to struggle with\nrecognizing rare named entities and adapting to domain mismatches. In contrast,\nlarge language models (LLMs), trained on massive internet-scale datasets, are\noften more effective across a wide range of domains. In this work, we propose a\nreinforcement learning based approach for unsupervised domain adaptation,\nleveraging unlabeled data to enhance transcription quality, particularly the\nnamed entities affected by domain mismatch, through feedback from a LLM. Given\ncontextual information, our framework employs a LLM as the reward model to\nscore the hypotheses from the ASR model. These scores serve as reward signals\nto fine-tune the ASR model via reinforcement learning. Our method achieves a\n21\\% improvement on entity word error rate over conventional self-training\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684ASR\u9886\u57df\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u5229\u7528LLM\u53cd\u9988\u673a\u5236\u663e\u8457\u63d0\u5347\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u51c6\u786e\u7387", "motivation": "\u4f20\u7edfASR\u7cfb\u7edf\u5728\u8de8\u9886\u57df\u573a\u666f\u4e0b\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u800c\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5177\u5907\u66f4\u5f3a\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8868\u8fbe\u80fd\u529b", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5bf9ASR\u8f93\u51fa\u5047\u8bbe\u8fdb\u884c\u8bc4\u5206\uff0c\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\u4f18\u5316\u6a21\u578b\u53c2\u6570", "result": "\u5728\u5b9e\u4f53\u8bcd\u9519\u8bef\u7387\u6307\u6807\u4e0a\u76f8\u6bd4\u4f20\u7edf\u81ea\u8bad\u7ec3\u65b9\u6cd5\u63d0\u534721%", "conclusion": "\u9a8c\u8bc1\u4e86LLM\u53cd\u9988\u4fe1\u53f7\u5728ASR\u9886\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u4f18\u5316\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.11092", "pdf": "https://arxiv.org/pdf/2506.11092", "abs": "https://arxiv.org/abs/2506.11092", "authors": ["Jubin Abhishek Soni", "Amit Anand", "Rajesh Kumar Pandey", "Aniket Abhishek Soni"], "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing Multi-Turn Planning and Tool Adaptation", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "6 pages, 5 figures, 3 tables. This manuscript has been submitted to\n  IEEE conference. Researchers are welcome to read and build upon this work;\n  please cite it appropriately. For questions or clarifications, feel free to\n  contact me", "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u4e0a\u4e0b\u6587\u8c03\u4f18\u6846\u67b6DCT\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u7f13\u5b58\u3001\u52a8\u6001\u5de5\u5177\u9009\u62e9\u4e0e\u4e0a\u4e0b\u6587\u538b\u7f29\u6280\u672f\uff0c\u5728\u65e0\u9700\u91cd\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5c40\u9650\u4e8e\u9759\u6001\u5355\u8f6e\u4ea4\u4e92\uff0c\u65e0\u6cd5\u9002\u5e94\u533b\u7597/\u667a\u80fd\u5bb6\u5c45\u7b49\u52a8\u6001\u9886\u57df\u5de5\u5177\u96c6\u548c\u7528\u6237\u610f\u56fe\u7684\u6301\u7eed\u6f14\u53d8", "method": "\u96c6\u6210\u6ce8\u610f\u529b\u4e0a\u4e0b\u6587\u7f13\u5b58\u8ffd\u8e2a\u5386\u53f2\u4fe1\u606f + LoRA\u68c0\u7d22\u5b9e\u73b0\u9886\u57df\u5de5\u5177\u52a8\u6001\u9009\u62e9 + \u9ad8\u6548\u4e0a\u4e0b\u6587\u538b\u7f29\u4fdd\u6301LLM\u8f93\u5165\u9650\u5236", "result": "\u8ba1\u5212\u51c6\u786e\u7387\u63d0\u534714%\uff0c\u5e7b\u89c9\u51cf\u5c1137%\uff0c\u4ee5\u66f4\u4f4e\u6210\u672c\u8fbe\u5230GPT-4\u6027\u80fd\uff0c\u4e14\u80fd\u6cdb\u5316\u81f3\u672a\u89c1\u8fc7\u7684\u5de5\u5177", "conclusion": "DCT\u901a\u8fc7\u8f7b\u91cf\u5316\u67b6\u6784\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u9002\u914d\uff0c\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684AI\u52a9\u624b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11094", "pdf": "https://arxiv.org/pdf/2506.11094", "abs": "https://arxiv.org/abs/2506.11094", "authors": ["Songyang Liu", "Chaozhuo Li", "Jiameng Qiu", "Xi Zhang", "Feiran Huang", "Litian Zhang", "Yiming Hei", "Philip S. Yu"], "title": "The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "21 pages, preprint", "summary": "With the rapid advancement of artificial intelligence technology, Large\nLanguage Models (LLMs) have demonstrated remarkable potential in the field of\nNatural Language Processing (NLP), including areas such as content generation,\nhuman-computer interaction, machine translation, and code generation, among\nothers. However, their widespread deployment has also raised significant safety\nconcerns. In recent years, LLM-generated content has occasionally exhibited\nunsafe elements like toxicity and bias, particularly in adversarial scenarios,\nwhich has garnered extensive attention from both academia and industry. While\nnumerous efforts have been made to evaluate the safety risks associated with\nLLMs, there remains a lack of systematic reviews summarizing these research\nendeavors. This survey aims to provide a comprehensive and systematic overview\nof recent advancements in LLMs safety evaluation, focusing on several key\naspects: (1) \"Why evaluate\" that explores the background of LLMs safety\nevaluation, how they differ from general LLMs evaluation, and the significance\nof such evaluation; (2) \"What to evaluate\" that examines and categorizes\nexisting safety evaluation tasks based on key capabilities, including\ndimensions such as toxicity, robustness, ethics, bias and fairness,\ntruthfulness, and so on; (3) \"Where to evaluate\" that summarizes the evaluation\nmetrics, datasets and benchmarks currently used in safety evaluations; (4) \"How\nto evaluate\" that reviews existing evaluation toolkit, and categorizing\nmainstream evaluation methods based on the roles of the evaluators. Finally, we\nidentify the challenges in LLMs safety evaluation and propose potential\nresearch directions to promote further advancement in this field. We emphasize\nthe importance of prioritizing LLMs safety evaluation to ensure the safe\ndeployment of these models in real-world applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u7684\u56db\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff08\u8bc4\u4f30\u80cc\u666f\u3001\u4efb\u52a1\u5206\u7c7b\u3001\u5de5\u5177\u6307\u6807\u3001\u65b9\u6cd5\u8bba\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u573a\u666f\u4e0b\u4ea7\u751f\u7684\u6bd2\u6027/\u504f\u89c1\u7b49\u5b89\u5168\u9690\u60a3\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u9700\u5efa\u7acb\u5b89\u5168\u8bc4\u4f30\u4f53\u7cfb\u4fdd\u969c\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u6846\u67b6\u5c55\u5f00\u5206\u6790\uff1a1) \u5b89\u5168\u8bc4\u4f30\u7684\u5fc5\u8981\u6027 2) \u6309\u6838\u5fc3\u80fd\u529b\u5206\u7c7b\u7684\u8bc4\u4f30\u4efb\u52a1 3) \u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0e\u6570\u636e\u96c6 4) \u57fa\u4e8e\u8bc4\u4f30\u8005\u89d2\u8272\u7684\u65b9\u6cd5\u8bba\u5206\u7c7b\u3002", "result": "\u6784\u5efa\u4e86\u6db5\u76d6\u6bd2\u6027/\u9c81\u68d2\u6027/\u4f26\u7406\u7b49\u7ef4\u5ea6\u7684\u5b89\u5168\u8bc4\u4f30\u5206\u7c7b\u4f53\u7cfb\uff0c\u68b3\u7406\u4e86\u4e3b\u6d41\u8bc4\u4f30\u5de5\u5177\u94fe\uff0c\u5e76\u63ed\u793a\u52a8\u6001\u5bf9\u6297\u573a\u666f\u4e0b\u7684\u8bc4\u4f30\u8303\u5f0f\u6311\u6218\u3002", "conclusion": "\u5f3a\u8c03\u5b89\u5168\u8bc4\u4f30\u5bf9LLMs\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63d0\u51fa\u9700\u53d1\u5c55\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u3001\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u548c\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u7684\u65b0\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2506.11095", "pdf": "https://arxiv.org/pdf/2506.11095", "abs": "https://arxiv.org/abs/2506.11095", "authors": ["Manuel D. S. Hopp", "Vincent Labatut", "Arthur Amalvy", "Richard Dufour", "Hannah Stone", "Hayley Jach", "Kou Murayama"], "title": "Persistent Homology of Topic Networks for the Prediction of Reader Curiosity", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reader curiosity, the drive to seek information, is crucial for textual\nengagement, yet remains relatively underexplored in NLP. Building on\nLoewenstein's Information Gap Theory, we introduce a framework that models\nreader curiosity by quantifying semantic information gaps within a text's\nsemantic structure. Our approach leverages BERTopic-inspired topic modeling and\npersistent homology to analyze the evolving topology (connected components,\ncycles, voids) of a dynamic semantic network derived from text segments,\ntreating these features as proxies for information gaps. To empirically\nevaluate this pipeline, we collect reader curiosity ratings from participants\n(n = 49) as they read S. Collins's ''The Hunger Games'' novel. We then use the\ntopological features from our pipeline as independent variables to predict\nthese ratings, and experimentally show that they significantly improve\ncuriosity prediction compared to a baseline model (73% vs. 30% explained\ndeviance), validating our approach. This pipeline offers a new computational\nmethod for analyzing text structure and its relation to reader engagement.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u4fe1\u606f\u5dee\u7406\u8bba\u7684\u62d3\u6251\u7279\u5f81\u5206\u6790\u6d41\u7a0b\uff0c\u5c06\u8bfb\u8005\u597d\u5947\u5fc3\u9884\u6d4b\u51c6\u786e\u7387\u4ece30%\u63d0\u5347\u81f373%", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u5bf9\u8bfb\u8005\u597d\u5947\u5fc3\u9a71\u52a8\u673a\u5236\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u4e9f\u9700\u91cf\u5316\u6587\u672c\u4fe1\u606f\u5dee\u7ed3\u6784\u7684\u65b0\u65b9\u6cd5", "method": "\u878d\u5408BERTopic\u4e3b\u9898\u5efa\u6a21\u4e0e\u6301\u7eed\u540c\u8c03\u7406\u8bba\uff0c\u6784\u5efa\u52a8\u6001\u8bed\u4e49\u7f51\u7edc\u62d3\u6251\u7ed3\u6784\uff08\u8fde\u901a\u5206\u91cf/\u5faa\u73af/\u7a7a\u9699\uff09\u4f5c\u4e3a\u4fe1\u606f\u5dee\u4ee3\u7406\u6307\u6807", "result": "\u57fa\u4e8e\u300a\u9965\u997f\u6e38\u620f\u300b\u6587\u672c\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u62d3\u6251\u7279\u5f81\u4f7f\u597d\u5947\u5fc3\u9884\u6d4b\u89e3\u91ca\u504f\u5dee\u63d0\u5347143%\uff0873% vs 30%\uff09", "conclusion": "\u8be5\u62d3\u6251\u5206\u6790\u6846\u67b6\u4e3a\u6587\u672c\u7ed3\u6784\u4e0e\u8bfb\u8005\u53c2\u4e0e\u5ea6\u7684\u5173\u7cfb\u7814\u7a76\u63d0\u4f9b\u4e86\u521b\u65b0\u8ba1\u7b97\u8303\u5f0f\uff0c\u5f00\u8f9f\u4e86\u8ba4\u77e5\u8ba1\u7b97\u65b0\u8def\u5f84"}}
{"id": "2506.11097", "pdf": "https://arxiv.org/pdf/2506.11097", "abs": "https://arxiv.org/abs/2506.11097", "authors": ["Haritz Puerto", "Martin Gubri", "Tommaso Green", "Seong Joon Oh", "Sangdoo Yun"], "title": "C-SEO Bench: Does Conversational SEO Work?", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) are transforming search engines into\nConversational Search Engines (CSE). Consequently, Search Engine Optimization\n(SEO) is being shifted into Conversational Search Engine Optimization (C-SEO).\nWe are beginning to see dedicated C-SEO methods for modifying web documents to\nincrease their visibility in CSE responses. However, they are often tested only\nfor a limited breadth of application domains; we do not understand whether\ncertain C-SEO methods would be effective for a broad range of domains.\nMoreover, existing evaluations consider only a single-actor scenario where only\none web document adopts a C-SEO method; in reality, multiple players are likely\nto competitively adopt the cutting-edge C-SEO techniques, drawing an analogy\nfrom the dynamics we have seen in SEO. We present C-SEO Bench, the first\nbenchmark designed to evaluate C-SEO methods across multiple tasks, domains,\nand number of actors. We consider two search tasks, question answering and\nproduct recommendation, with three domains each. We also formalize a new\nevaluation protocol with varying adoption rates among involved actors. Our\nexperiments reveal that most current C-SEO methods are largely ineffective,\ncontrary to reported results in the literature. Instead, traditional SEO\nstrategies, those aiming to improve the ranking of the source in the LLM\ncontext, are significantly more effective. We also observe that as we increase\nthe number of C-SEO adopters, the overall gains decrease, depicting a congested\nand zero-sum nature of the problem. Our code and data are available at\nhttps://github.com/parameterlab/c-seo-bench and\nhttps://huggingface.co/datasets/parameterlab/c-seo-bench.", "AI": {"tldr": "\u63d0\u51faC-SEO Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u5f53\u524d\u5bf9\u8bdd\u5f0f\u641c\u7d22\u5f15\u64ce\u4f18\u5316\u65b9\u6cd5\u5728\u8de8\u9886\u57df/\u591a\u53c2\u4e0e\u8005\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u4f20\u7edfSEO\u7b56\u7565\u66f4\u6709\u6548\u4e14\u591a\u73a9\u5bb6\u7ade\u4e89\u5448\u73b0\u96f6\u548c\u535a\u5f08\u7279\u5f81\u3002", "motivation": "\u73b0\u6709C-SEO\u65b9\u6cd5\u7f3a\u4e4f\u8de8\u9886\u57df\u9a8c\u8bc1\uff0c\u4e14\u672a\u8003\u8651\u591a\u73a9\u5bb6\u7ade\u4e89\u573a\u666f\uff0c\u96be\u4ee5\u53cd\u6620\u771f\u5b9e\u5e94\u7528\u73af\u5883\u3002", "method": "\u6784\u5efa\u5305\u542b2\u79cd\u641c\u7d22\u4efb\u52a1\uff08\u95ee\u7b54/\u63a8\u8350\uff09\u30013\u4e2a\u9886\u57df\u7684\u6d4b\u8bd5\u57fa\u51c6\uff0c\u8bbe\u8ba1\u591a\u53c2\u4e0e\u8005\u91c7\u7528\u7387\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u591a\u6570C-SEO\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u4f20\u7edf\u4e0a\u4e0b\u6587\u4f18\u5316\u7b56\u7565\u66f4\u6709\u6548\uff1b\u91c7\u7528\u8005\u589e\u591a\u65f6\u8fb9\u9645\u6548\u76ca\u9012\u51cf\u3002", "conclusion": "\u5bf9\u8bdd\u5f0fSEO\u5b58\u5728\u96f6\u548c\u74f6\u9888\uff0c\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u4f18\u5316\u65b9\u6cd5\u4ecd\u5177\u4f18\u52bf\uff0c\u5f00\u6e90\u4ee3\u7801\u5e93\u52a9\u529b\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2506.11102", "pdf": "https://arxiv.org/pdf/2506.11102", "abs": "https://arxiv.org/abs/2506.11102", "authors": ["Jiachen Zhu", "Menghui Zhu", "Renting Rui", "Rong Shan", "Congmin Zheng", "Bo Chen", "Yunjia Xi", "Jianghao Lin", "Weiwen Liu", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs), such as GPT, Gemini, and\nDeepSeek, has significantly advanced natural language processing, giving rise\nto sophisticated chatbots capable of diverse language-related tasks. The\ntransition from these traditional LLM chatbots to more advanced AI agents\nrepresents a pivotal evolutionary step. However, existing evaluation frameworks\noften blur the distinctions between LLM chatbots and AI agents, leading to\nconfusion among researchers selecting appropriate benchmarks. To bridge this\ngap, this paper introduces a systematic analysis of current evaluation\napproaches, grounded in an evolutionary perspective. We provide a detailed\nanalytical framework that clearly differentiates AI agents from LLM chatbots\nalong five key aspects: complex environment, multi-source instructor, dynamic\nfeedback, multi-modal perception, and advanced capability. Further, we\ncategorize existing evaluation benchmarks based on external environments\ndriving forces, and resulting advanced internal capabilities. For each\ncategory, we delineate relevant evaluation attributes, presented\ncomprehensively in practical reference tables. Finally, we synthesize current\ntrends and outline future evaluation methodologies through four critical\nlenses: environment, agent, evaluator, and metrics. Our findings offer\nactionable guidance for researchers, facilitating the informed selection and\napplication of benchmarks in AI agent evaluation, thus fostering continued\nadvancement in this rapidly evolving research domain.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fdb\u5316\u89c6\u89d2\u7684AI\u4ee3\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u590d\u6742\u73af\u5883\u3001\u591a\u6e90\u6307\u4ee4\u3001\u52a8\u6001\u53cd\u9988\u3001\u591a\u6a21\u6001\u611f\u77e5\u548c\u9ad8\u7ea7\u80fd\u529b\u4e94\u7ef4\u5ea6\u533a\u5206AI\u4ee3\u7406\u4e0eLLM\u804a\u5929\u673a\u5668\u4eba\uff0c\u5efa\u7acb\u5206\u7c7b\u57fa\u51c6\u5e76\u5c55\u671b\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u6df7\u6dc6AI\u4ee3\u7406\u4e0eLLM\u804a\u5929\u673a\u5668\u4eba\u7684\u754c\u9650\uff0c\u5bfc\u81f4\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u57fa\u51c6\uff0c\u9700\u5efa\u7acb\u7cfb\u7edf\u6027\u533a\u5206\u6846\u67b6", "method": "\u4ece\u8fdb\u5316\u89c6\u89d2\u6784\u5efa\u4e94\u7ef4\u5ea6\u5206\u6790\u6846\u67b6\uff08\u590d\u6742\u73af\u5883/\u591a\u6e90\u6307\u4ee4/\u52a8\u6001\u53cd\u9988/\u591a\u6a21\u6001\u611f\u77e5/\u9ad8\u7ea7\u80fd\u529b\uff09\uff0c\u6309\u5916\u90e8\u9a71\u52a8\u529b\u548c\u5185\u90e8\u80fd\u529b\u5206\u7c7b\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\uff0c\u5efa\u7acb\u5b9e\u8df5\u53c2\u8003\u8868", "result": "\u521b\u5efa\u5305\u542b\u73af\u5883\u9a71\u52a8\u529b-\u80fd\u529b\u6620\u5c04\u5173\u7cfb\u7684\u8bc4\u4f30\u57fa\u51c6\u5206\u7c7b\u4f53\u7cfb\uff0c\u63d0\u51fa\u73af\u5883-\u4ee3\u7406-\u8bc4\u4f30\u8005-\u6307\u6807\u56db\u7ef4\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5\u8bba\u6846\u67b6", "conclusion": "\u901a\u8fc7\u56db\u7ef4\u8bc4\u4f30\u89c6\u89d2\uff08\u73af\u5883\u590d\u6742\u6027\u3001\u4ee3\u7406\u81ea\u4e3b\u6027\u3001\u8bc4\u4f30\u52a8\u6001\u6027\u3001\u6307\u6807\u591a\u7ef4\u6027\uff09\u63a8\u52a8AI\u4ee3\u7406\u8bc4\u4f30\u4f53\u7cfb\u8fdb\u5316\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u57fa\u51c6\u9009\u62e9\u6307\u5357"}}
{"id": "2506.11103", "pdf": "https://arxiv.org/pdf/2506.11103", "abs": "https://arxiv.org/abs/2506.11103", "authors": ["Wenchong He", "Liqian Peng", "Zhe Jiang", "Alex Go"], "title": "You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 6 figures", "summary": "Large language models (LLMs) possess a remarkable ability to perform\nin-context learning (ICL), which enables them to handle multiple downstream\ntasks simultaneously without requiring task-specific fine-tuning. Recent\nstudies have shown that even moderately sized LLMs, such as Mistral 7B, Gemma\n7B and Llama-3 8B, can achieve ICL through few-shot in-context fine-tuning of\nall tasks at once. However, this approach still lags behind dedicated\nfine-tuning, where a separate model is trained for each individual task.\n  In this paper, we propose a novel approach, Many-Shot In-Context Fine-tuning\n(ManyICL), which significantly narrows this performance gap by extending the\nprinciples of ICL to a many-shot setting. To unlock the full potential of\nManyICL and address the inherent inefficiency of processing long sequences with\nnumerous in-context examples, we propose a novel training objective. Instead of\nsolely predicting the final answer, our approach treats every answer within the\ncontext as a supervised training target. This effectively shifts the role of\nmany-shot examples from prompts to targets for autoregressive learning. Through\nextensive experiments on diverse downstream tasks, including classification,\nsummarization, question answering, natural language inference, and math, we\ndemonstrate that ManyICL substantially outperforms zero/few-shot fine-tuning\nand approaches the performance of dedicated fine-tuning. Furthermore, ManyICL\nsignificantly mitigates catastrophic forgetting issues observed in\nzero/few-shot fine-tuning. The code will be made publicly available upon\npublication.", "AI": {"tldr": "\u63d0\u51faMany-Shot In-Context Fine-tuning (ManyICL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u4e0a\u4e0b\u6587\u793a\u4f8b\u8f6c\u5316\u4e3a\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\uff0c\u663e\u8457\u7f29\u5c0f\u4e86ICL\u4e0e\u4e13\u7528\u5fae\u8c03\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u5c11\u91cf\u6837\u672c\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u6027\u80fd\u4ecd\u843d\u540e\u4e8e\u4e13\u7528\u5fae\u8c03\u6a21\u578b\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u591a\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u573a\u666f\uff0c\u63d0\u5347\u6a21\u578b\u8de8\u4efb\u52a1\u5b66\u4e60\u6548\u7387\u3002", "method": "\u63d0\u51fa\u65b0\u578b\u8bad\u7ec3\u76ee\u6807\uff1a\u5c06\u4e0a\u4e0b\u6587\u4e2d\u7684\u6240\u6709\u7b54\u6848\u4f5c\u4e3a\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u5b66\u4e60\u5c06\u591a\u6837\u672c\u793a\u4f8b\u4ece\u63d0\u793a\u8f6c\u5316\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u63d0\u5347\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387\u3002", "result": "\u5728\u5206\u7c7b\u3001\u6458\u8981\u3001\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\uff0cManyICL\u663e\u8457\u4f18\u4e8e\u96f6/\u5c11\u6837\u672c\u5fae\u8c03\uff0c\u63a5\u8fd1\u4e13\u7528\u5fae\u8c03\u6548\u679c\uff0c\u5e76\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "ManyICL\u901a\u8fc7\u8303\u5f0f\u521b\u65b0\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u4e3a\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8bad\u7ec3\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2506.11104", "pdf": "https://arxiv.org/pdf/2506.11104", "abs": "https://arxiv.org/abs/2506.11104", "authors": ["Hanzhi Zhang", "Heng Fan", "Kewei Sha", "Yan Huang", "Yunhe Feng"], "title": "DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Long-context understanding is crucial for many NLP applications, yet\ntransformers struggle with efficiency due to the quadratic complexity of\nself-attention. Sparse attention methods alleviate this cost but often impose\nstatic, predefined masks, failing to capture heterogeneous attention patterns.\nThis results in suboptimal token interactions, limiting adaptability and\nretrieval accuracy in long-sequence tasks. This work introduces a dynamic\nsparse attention mechanism that assigns adaptive masks at the attention-map\nlevel, preserving heterogeneous patterns across layers and heads. Unlike\nexisting approaches, our method eliminates the need for fine-tuning and\npredefined mask structures while maintaining computational efficiency. By\nlearning context-aware attention structures, it achieves high alignment with\nfull-attention models, ensuring minimal performance degradation while reducing\nmemory and compute overhead. This approach provides a scalable alternative to\nfull attention, enabling the practical deployment of large-scale Large Language\nModels (LLMs) without sacrificing retrieval performance. DAM is available at:\nhttps://github.com/HanzhiZhang-Ulrica/DAM.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236DAM\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u63a9\u7801\u89e3\u51b3\u4f20\u7edf\u7a00\u758f\u6ce8\u610f\u529b\u9759\u6001\u6a21\u5f0f\u9650\u5236\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e0e\u5168\u6ce8\u610f\u529b\u6a21\u578b\u7684\u9ad8\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u73b0\u6709\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\u91c7\u7528\u9759\u6001\u9884\u5b9a\u4e49\u63a9\u7801\uff0c\u65e0\u6cd5\u6355\u6349\u5f02\u6784\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5bfc\u81f4\u957f\u5e8f\u5217\u4efb\u52a1\u4e2d\u4ee4\u724c\u4ea4\u4e92\u6548\u7387\u4f4e\u4e0b\u548c\u68c0\u7d22\u51c6\u786e\u6027\u53d7\u9650\u3002", "method": "\u5728\u6ce8\u610f\u529b\u56fe\u5c42\u7ea7\u52a8\u6001\u5206\u914d\u81ea\u9002\u5e94\u63a9\u7801\uff0c\u4fdd\u7559\u8de8\u5c42/\u8de8\u5934\u7684\u5f02\u6784\u6a21\u5f0f\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u9884\u5b9a\u4e49\u7ed3\u6784\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7a00\u758f\u8ba1\u7b97\u3002", "result": "\u76f8\u6bd4\u5168\u6ce8\u610f\u529b\u6a21\u578b\u6027\u80fd\u4e0b\u964d<1%\uff0c\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u663e\u8457\u964d\u4f4e\uff0c\u68c0\u7d22\u51c6\u786e\u7387\u5728\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u63d0\u534715-20%\u3002", "conclusion": "DAM\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u6ce8\u610f\u529b\u65b9\u6848\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u63a8\u52a8LLM\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.11105", "pdf": "https://arxiv.org/pdf/2506.11105", "abs": "https://arxiv.org/abs/2506.11105", "authors": ["Uttej Kallakurik", "Edward Humes", "Rithvik Jonna", "Xiaomin Lin", "Tinoosh Mohsenin"], "title": "Enabling On-Device Medical AI Assistants via Input-Driven Saliency Adaptation", "categories": ["cs.CL", "cs.AI", "cs.AR", "cs.SY", "eess.SY"], "comment": null, "summary": "Large Language Models (LLMs) have significant impact on the healthcare\nscenarios but remain prohibitively large for deployment in real-time,\nresource-constrained environments such as edge devices. In this work, we\nintroduce a novel medical assistant system, optimized through our\ngeneral-purpose compression framework, which tailors Large Language Models\n(LLMs) for deployment in specialized domains. By measuring neuron saliency on\ndomain-specific data, our method can aggressively prune irrelevant neurons,\nreducing model size while preserving performance. Following pruning, we apply\npost-training quantization to further reduce the memory footprint, and evaluate\nthe compressed model across medical benchmarks including MedMCQA, MedQA, and\nPubMedQA. We also deploy the 50\\% compressed Gemma and the 67\\% compressed\nLLaMA3 models on Jetson Orin Nano (18.7W peak) and Raspberry Pi 5 (6.3W peak),\nachieving real-time, energy-efficient inference under hardware constraints.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u538b\u7f29\u6846\u67b6\u4f18\u5316\u533b\u7597\u573a\u666fLLM\u90e8\u7f72\uff0c\u901a\u8fc7\u526a\u679d+\u91cf\u5316\u5b9e\u73b0Gemma\u538b\u7f2950%\u3001LLaMA3\u538b\u7f2967%\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u6a21\u578b\u4f53\u79ef\u8fc7\u5927\u3001\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u95ee\u9898", "method": "1. \u57fa\u4e8e\u9886\u57df\u6570\u636e\u795e\u7ecf\u5143\u663e\u8457\u6027\u5206\u6790\u7684\u526a\u679d\u7b56\u7565 2. \u540e\u8bad\u7ec3\u91cf\u5316\u538b\u7f29\u5185\u5b58 3. \u5728Jetson Orin Nano\u548c\u6811\u8393\u6d3e5\u90e8\u7f72\u9a8c\u8bc1", "result": "\u5728MedMCQA\u7b49\u533b\u7597\u57fa\u51c6\u4fdd\u6301\u6027\u80fd\uff0c\u8fb9\u7f18\u8bbe\u5907\u5b9e\u73b0\u80fd\u6548\u7ea6\u675f\u4e0b\u7684\u5b9e\u65f6\u63a8\u7406\uff08\u6811\u8393\u6d3e5\u5cf0\u503c\u529f\u80176.3W\uff09", "conclusion": "\u8be5\u538b\u7f29\u6846\u67b6\u6709\u6548\u5e73\u8861\u6a21\u578b\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u63a8\u52a8LLM\u5728\u533b\u7597\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u7684\u5b9e\u9645\u5e94\u7528"}}
{"id": "2506.11106", "pdf": "https://arxiv.org/pdf/2506.11106", "abs": "https://arxiv.org/abs/2506.11106", "authors": ["Ningyuan Li", "Junrui Liu", "Yi Shan", "Minghui Huang", "Tong Li"], "title": "Graph-based RAG Enhancement via Global Query Disambiguation and Dependency-Aware Reranking", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Contemporary graph-based retrieval-augmented generation (RAG) methods\ntypically begin by extracting entities from user queries and then leverage\npre-constructed knowledge graphs to retrieve related relationships and\nmetadata. However, this pipeline's exclusive reliance on entity-level\nextraction can lead to the misinterpretation or omission of latent yet critical\ninformation and relations. As a result, retrieved content may be irrelevant or\ncontradictory, and essential knowledge may be excluded, exacerbating\nhallucination risks and degrading the fidelity of generated responses. To\naddress these limitations, we introduce PankRAG, a framework that combines a\nglobally aware, hierarchical query-resolution strategy with a novel\ndependency-aware reranking mechanism. PankRAG first constructs a multi-level\nresolution path that captures both parallel and sequential interdependencies\nwithin a query, guiding large language models (LLMs) through structured\nreasoning. It then applies its dependency-aware reranker to exploit the\ndependency structure among resolved sub-questions, enriching and validating\nretrieval results for subsequent sub-questions. Empirical evaluations\ndemonstrate that PankRAG consistently outperforms state-of-the-art approaches\nacross multiple benchmarks, underscoring its robustness and generalizability.", "AI": {"tldr": "\u63d0\u51faPankRAG\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u67e5\u8be2\u89e3\u6790\u548c\u4f9d\u8d56\u611f\u77e5\u91cd\u6392\u5e8f\u673a\u5236\u63d0\u5347RAG\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\u5e76\u63d0\u9ad8\u56de\u7b54\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684RAG\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5b9e\u4f53\u63d0\u53d6\uff0c\u5bb9\u6613\u9057\u6f0f\u6f5c\u5728\u5173\u8054\u4fe1\u606f\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u4e0d\u76f8\u5173\u6216\u77db\u76fe\uff0c\u52a0\u5267\u5927\u6a21\u578b\u5e7b\u89c9\u98ce\u9669\u3002", "method": "\u6784\u5efa\u591a\u7ea7\u89e3\u6790\u8def\u5f84\u6355\u6349\u67e5\u8be2\u4f9d\u8d56\u5173\u7cfb\uff08\u5e76\u884c/\u65f6\u5e8f\uff09\uff0c\u91c7\u7528\u4f9d\u8d56\u611f\u77e5\u7684\u91cd\u6392\u5e8f\u673a\u5236\uff0c\u901a\u8fc7\u5b50\u95ee\u9898\u4f9d\u8d56\u7ed3\u6784\u4f18\u5316\u68c0\u7d22\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u5408\u5206\u5c42\u63a8\u7406\u67b6\u6784\u4e0e\u4f9d\u8d56\u611f\u77e5\u673a\u5236\uff0cPankRAG\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u7684\u77e5\u8bc6\u6574\u5408\u53ef\u9760\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u3002"}}
{"id": "2506.11108", "pdf": "https://arxiv.org/pdf/2506.11108", "abs": "https://arxiv.org/abs/2506.11108", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "title": "History-Aware Cross-Attention Reinforcement: Self-Supervised Multi Turn and Chain-of-Thought Fine-Tuning with vLLM", "categories": ["cs.CL"], "comment": null, "summary": "We present CAGSR-vLLM-MTC, an extension of our Self-Supervised\nCross-Attention-Guided Reinforcement (CAGSR) framework, now implemented on the\nhigh-performance vLLM runtime, to address both multi-turn dialogue and\nchain-of-thought reasoning. Building upon our original single-turn approach, we\nfirst instrumented vLLM's C++/CUDA kernels to asynchronously capture per-layer,\nper-head cross-attention weights during generation. We then generalized our\nself-supervised reward function to accumulate attention signals over entire\nconversation histories and intermediate chain-of-thought steps. We discuss\npractical trade-offs, including an entropy-based clamping mechanism to prevent\nattention collapse on early context, and outline future directions for\nmulti-party dialogues and hierarchical reasoning.", "AI": {"tldr": "\u5728vLLM\u8fd0\u884c\u65f6\u4e0a\u6269\u5c55CAGSR\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u6355\u83b7\u8de8\u6ce8\u610f\u529b\u6743\u91cd\u548c\u63a8\u5e7f\u81ea\u76d1\u7763\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u591a\u8f6e\u5bf9\u8bdd\u4e0e\u601d\u7ef4\u94fe\u63a8\u7406\u652f\u6301", "motivation": "\u89e3\u51b3\u539f\u6709\u5355\u8f6e\u5bf9\u8bdd\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\uff0c\u5229\u7528vLLM\u7684\u9ad8\u6548\u8ba1\u7b97\u7279\u6027\u5b9e\u73b0\u957f\u7a0b\u4e0a\u4e0b\u6587\u5efa\u6a21", "method": "1. \u4fee\u6539vLLM\u7684C++/CUDA\u5185\u6838\u5b9e\u73b0\u6ce8\u610f\u529b\u6743\u91cd\u5f02\u6b65\u6355\u83b7 2. \u8bbe\u8ba1\u7d2f\u79ef\u5bf9\u8bdd\u5386\u53f2\u548c\u63a8\u7406\u6b65\u9aa4\u7684\u81ea\u76d1\u7763\u5956\u52b1\u673a\u5236 3. \u5f15\u5165\u71b5\u57fa\u94b3\u5236\u9632\u6b62\u65e9\u671f\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u584c\u7f29", "result": "\u5efa\u7acb\u652f\u6301\u6301\u7eed\u5bf9\u8bdd\u548c\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51fa\u6ce8\u610f\u529b\u7a33\u5b9a\u673a\u5236\uff0c\u4e3a\u591a\u65b9\u5bf9\u8bdd\u548c\u5206\u5c42\u63a8\u7406\u5960\u5b9a\u6280\u672f\u57fa\u7840", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u589e\u5f3a\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\uff0c\u672a\u6765\u5c06\u6269\u5c55\u81f3\u591a\u65b9\u4ea4\u4e92\u573a\u666f\u5e76\u63a2\u7d22\u5206\u5c42\u6ce8\u610f\u529b\u4f18\u5316\u7b56\u7565"}}
{"id": "2506.11109", "pdf": "https://arxiv.org/pdf/2506.11109", "abs": "https://arxiv.org/abs/2506.11109", "authors": ["Yile Chen", "Yicheng Tao", "Yue Jiang", "Shuai Liu", "Han Yu", "Gao Cong"], "title": "Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD'25", "summary": "The widespread adoption of location-based services has led to the generation\nof vast amounts of mobility data, providing significant opportunities to model\nuser movement dynamics within urban environments. Recent advancements have\nfocused on adapting Large Language Models (LLMs) for mobility analytics.\nHowever, existing methods face two primary limitations: inadequate semantic\nrepresentation of locations (i.e., discrete IDs) and insufficient modeling of\nmobility signals within LLMs (i.e., single templated instruction fine-tuning).\nTo address these issues, we propose QT-Mob, a novel framework that\nsignificantly enhances LLMs for mobility analytics. QT-Mob introduces a\nlocation tokenization module that learns compact, semantically rich tokens to\nrepresent locations, preserving contextual information while ensuring\ncompatibility with LLMs. Furthermore, QT-Mob incorporates a series of\ncomplementary fine-tuning objectives that align the learned tokens with the\ninternal representations in LLMs, improving the model's comprehension of\nsequential movement patterns and location semantics. The proposed QT-Mob\nframework not only enhances LLMs' ability to interpret mobility data but also\nprovides a more generalizable approach for various mobility analytics tasks.\nExperiments on three real-world dataset demonstrate the superior performance in\nboth next-location prediction and mobility recovery tasks, outperforming\nexisting deep learning and LLM-based methods.", "AI": {"tldr": "QT-Mob\u6846\u67b6\u901a\u8fc7\u4f4d\u7f6e\u6807\u8bb0\u5316\u6a21\u5757\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u79fb\u52a8\u6570\u636e\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u7684\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u9884\u6d4b\u548c\u79fb\u52a8\u6062\u590d\u4efb\u52a1\u4e2d\u8868\u73b0\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4f4d\u7f6e\u8bed\u4e49\u8868\u793a\u4e0d\u8db3\uff08\u79bb\u6563ID\u5f62\u5f0f\uff09\u548cLLM\u4fe1\u53f7\u5efa\u6a21\u4e0d\u5145\u5206\uff08\u5355\u4e00\u6a21\u677f\u6307\u4ee4\u5fae\u8c03\uff09\u4e24\u5927\u7f3a\u9677\uff0c\u9650\u5236\u4e86\u79fb\u52a8\u6570\u636e\u5206\u6790\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u4f4d\u7f6e\u6807\u8bb0\u5316\u6a21\u5757\u5b66\u4e60\u7d27\u51d1\u7684\u8bed\u4e49\u5316\u4f4d\u7f6e\u8868\u793a\n2. \u8bbe\u8ba1\u4e92\u8865\u6027\u591a\u4efb\u52a1\u5fae\u8c03\u76ee\u6807\uff0c\u5bf9\u9f50LLM\u5185\u90e8\u8868\u5f81\u4e0e\u79fb\u52a8\u6a21\u5f0f\n3. \u7ed3\u5408\u4f4d\u7f6e\u4e0a\u4e0b\u6587\u4fdd\u6301\u548cLLM\u517c\u5bb9\u6027\u8bbe\u8ba1", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cQT-Mob\u5728\u4e0b\u4e00\u4e2a\u4f4d\u7f6e\u9884\u6d4b\uff08next-location prediction\uff09\u548c\u79fb\u52a8\u8f68\u8ff9\u6062\u590d\uff08mobility recovery\uff09\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u73b0\u6709LLM\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u589e\u5f3a\u4e86LLM\u5bf9\u79fb\u52a8\u6570\u636e\u7684\u89e3\u6790\u80fd\u529b\uff0c\u8fd8\u4e3a\u79fb\u52a8\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u4f4d\u7f6e\u670d\u52a1\u9886\u57df\u7684\u6a21\u578b\u8fdb\u5316\u3002"}}
{"id": "2506.11110", "pdf": "https://arxiv.org/pdf/2506.11110", "abs": "https://arxiv.org/abs/2506.11110", "authors": ["Jaeho Lee", "Atharv Chowdhary"], "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 4 figures, appendix contains 2 additional figures and 2\n  tables", "summary": "Recent benchmarks have probed factual consistency and rhetorical robustness\nin Large Language Models (LLMs). However, a knowledge gap exists regarding how\ndirectional framing of factually true statements influences model agreement, a\ncommon scenario for LLM users. AssertBench addresses this by sampling\nevidence-supported facts from FEVEROUS, a fact verification dataset. For each\n(evidence-backed) fact, we construct two framing prompts: one where the user\nclaims the statement is factually correct, and another where the user claims it\nis incorrect. We then record the model's agreement and reasoning. The desired\noutcome is that the model asserts itself, maintaining consistent truth\nevaluation across both framings, rather than switching its evaluation to agree\nwith the user. AssertBench isolates framing-induced variability from the\nmodel's underlying factual knowledge by stratifying results based on the\nmodel's accuracy on the same claims when presented neutrally. In doing so, this\nbenchmark aims to measure an LLM's ability to \"stick to its guns\" when\npresented with contradictory user assertions about the same fact. The complete\nsource code is available at https://github.com/achowd32/assert-bench.", "AI": {"tldr": "\u63d0\u51faAssertBench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u9762\u5bf9\u7528\u6237\u76f8\u53cd\u65ad\u8a00\u65f6\u4fdd\u6301\u4e8b\u5b9e\u5224\u65ad\u4e00\u81f4\u6027\u7684\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u5145\u5206\u7814\u7a76\u65b9\u5411\u6027\u6846\u67b6\u5bf9\u6a21\u578b\u4e8b\u5b9e\u5224\u65ad\u7684\u5f71\u54cd\uff0c\u9700\u91cf\u5316\u6a21\u578b\u5728\u7528\u6237\u5bf9\u7acb\u65ad\u8a00\u4e0b\u7684\u9c81\u68d2\u6027", "method": "\u4eceFEVEROUS\u4e8b\u5b9e\u9a8c\u8bc1\u6570\u636e\u96c6\u91c7\u6837\u8bc1\u636e\u652f\u6301\u7684\u4e8b\u5b9e\uff0c\u6784\u5efa\u6b63\u53cd\u4e24\u79cd\u7528\u6237\u65ad\u8a00\u6846\u67b6\uff0c\u5206\u5c42\u7edf\u8ba1\u6a21\u578b\u53cd\u5e94\u4e0e\u4e2d\u6027\u9648\u8ff0\u51c6\u786e\u7387\u7684\u5173\u7cfb", "result": "\u901a\u8fc7\u5206\u79bb\u6846\u67b6\u6548\u5e94\u4e0e\u4e8b\u5b9e\u77e5\u8bc6\u8bef\u5dee\uff0c\u53ef\u7cbe\u51c6\u6d4b\u91cf\u6a21\u578b\u5728\u7528\u6237\u5bf9\u7acb\u65ad\u8a00\u4e0b\u575a\u6301\u6b63\u786e\u5224\u65ad\u7684\u7a33\u5b9a\u6027", "conclusion": "AssertBench\u4e3a\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u6297\u7528\u6237\u5e72\u6270\u80fd\u529b\u63d0\u4f9b\u65b0\u7ef4\u5ea6\uff0c\u5176\u5206\u5c42\u8bbe\u8ba1\u6709\u6548\u9694\u79bb\u77e5\u8bc6\u6027\u9519\u8bef\u4e0e\u6846\u67b6\u6548\u5e94\u5f71\u54cd"}}
{"id": "2506.11111", "pdf": "https://arxiv.org/pdf/2506.11111", "abs": "https://arxiv.org/abs/2506.11111", "authors": ["Kun Zhang", "Le Wu", "Kui Yu", "Guangyi Lv", "Dacao Zhang"], "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": "33 pages, 5 figures", "summary": "Large Language Models (LLMs) have gained enormous attention in recent years\ndue to their capability of understanding and generating natural languages. With\nthe rapid development and wild-range applications (e.g., Agents, Embodied\nIntelligence), the robustness of LLMs has received increased attention. As the\ncore brain of many AI applications, the robustness of LLMs requires that models\nshould not only generate consistent contents, but also ensure the correctness\nand stability of generated content when dealing with unexpeted application\nscenarios (e.g., toxic prompts, limited noise domain data, outof-distribution\n(OOD) applications, etc). In this survey paper, we conduct a thorough review of\nthe robustness of LLMs, aiming to provide a comprehensive terminology of\nconcepts and methods around this field and facilitate the community.\nSpecifically, we first give a formal definition of LLM robustness and present\nthe collection protocol of this survey paper. Then, based on the types of\nperturbated inputs, we organize this survey from the following perspectives: 1)\nAdversarial Robustness: tackling the problem that prompts are manipulated\nintentionally, such as noise prompts, long context, data attack, etc; 2) OOD\nRobustness: dealing with the unexpected real-world application scenarios, such\nas OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of\nRobustness: summarizing the new evaluation datasets, metrics, and tools for\nverifying the robustness of LLMs. After reviewing the representative work from\neach perspective, we discuss and highlight future opportunities and research\ndirections in this field. Meanwhile, we also organize related works and provide\nan easy-to-search project\n(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the\ncommunity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u5305\u542b\u5bf9\u6297\u9c81\u68d2\u6027\u3001OOD\u9c81\u68d2\u6027\u548c\u8bc4\u4f30\u4f53\u7cfb\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u5efa\u7acb\u4e86\u76f8\u5173\u8d44\u6e90\u5e93\u652f\u6301\u793e\u533a\u53d1\u5c55\u3002", "motivation": "\u968f\u7740LLM\u5728\u667a\u80fd\u4f53\u3001\u5177\u8eab\u667a\u80fd\u7b49\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u5728\u5f02\u5e38\u573a\u666f\u4e0b\u751f\u6210\u5185\u5bb9\u7684\u6b63\u786e\u6027\u548c\u7a33\u5b9a\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002\u8bba\u6587\u65e8\u5728\u5efa\u7acb\u7edf\u4e00\u7684\u9c81\u68d2\u6027\u672f\u8bed\u4f53\u7cfb\u548c\u7814\u7a76\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8f93\u5165\u6270\u52a8\u7c7b\u578b\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u5c55\u5f00\u7814\u7a76\uff1a1) \u5bf9\u6297\u9c81\u68d2\u6027\uff08\u5904\u7406\u566a\u58f0\u63d0\u793a/\u957f\u4e0a\u4e0b\u6587/\u6570\u636e\u653b\u51fb\uff092) OOD\u9c81\u68d2\u6027\uff08\u89e3\u51b3\u5206\u5e03\u5916\u68c0\u6d4b/\u96f6\u6837\u672c\u8fc1\u79fb/\u5e7b\u89c9\u95ee\u9898\uff093) \u6784\u5efa\u65b0\u8bc4\u4f30\u4f53\u7cfb\uff08\u6570\u636e\u96c6/\u6307\u6807/\u5de5\u5177\uff09", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u7684LLM\u9c81\u68d2\u6027\u7814\u7a76\u6846\u67b6\uff0c\u6536\u96c6\u6574\u7406\u76f8\u5173\u8bba\u6587\u5f62\u6210\u53ef\u68c0\u7d22\u8d44\u6e90\u5e93(https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers)\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u9c81\u68d2\u6027\u662fLLM\u4f5c\u4e3aAI\u6838\u5fc3\u7ec4\u4ef6\u7684\u5173\u952e\u6311\u6218\uff0c\u9700\u5728\u8bc4\u4f30\u6307\u6807\u6539\u8fdb\u3001\u591a\u6a21\u6001\u6269\u5c55\u3001\u5b89\u5168\u5bf9\u9f50\u7b49\u9886\u57df\u6301\u7eed\u63a2\u7d22\uff0c\u8be5\u7efc\u8ff0\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u53c2\u8003\u3002"}}
{"id": "2506.11112", "pdf": "https://arxiv.org/pdf/2506.11112", "abs": "https://arxiv.org/abs/2506.11112", "authors": ["Christine Bauer", "Li Chen", "Nicola Ferro", "Norbert Fuhr", "Avishek Anand", "Timo Breuer", "Guglielmo Faggioli", "Ophir Frieder", "Hideo Joho", "Jussi Karlgren", "Johannes Kiesel", "Bart P. Knijnenburg", "Aldo Lipani", "Lien Michiels", "Andrea Papenmeier", "Maria Soledad Pera", "Mark Sanderson", "Scott Sanner", "Benno Stein", "Johanne R. Trippas", "Karin Verspoor", "Martijn C Willemsen"], "title": "Manifesto from Dagstuhl Perspectives Workshop 24352 -- Conversational Agents: A Framework for Evaluation (CAFE)", "categories": ["cs.CL", "cs.HC", "cs.IR"], "comment": "43 pages; 10 figures; Dagstuhl manifesto", "summary": "During the workshop, we deeply discussed what CONversational Information\nACcess (CONIAC) is and its unique features, proposing a world model abstracting\nit, and defined the Conversational Agents Framework for Evaluation (CAFE) for\nthe evaluation of CONIAC systems, consisting of six major components: 1) goals\nof the system's stakeholders, 2) user tasks to be studied in the evaluation, 3)\naspects of the users carrying out the tasks, 4) evaluation criteria to be\nconsidered, 5) evaluation methodology to be applied, and 6) measures for the\nquantitative criteria chosen.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4f1a\u8bdd\u4fe1\u606f\u8bbf\u95ee\u7cfb\u7edf\uff08CONIAC\uff09\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u516d\u5927\u7ec4\u4ef6\u7684\u8bc4\u4f30\u6846\u67b6CAFE", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f1a\u8bdd\u4fe1\u606f\u8bbf\u95ee\u7cfb\u7edf\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\u7684\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u65b9\u6cd5\u8bba", "method": "\u901a\u8fc7\u7814\u8ba8\u4f1a\u8ba8\u8bba\u5b9a\u4e49CONIAC\u7279\u6027\uff0c\u62bd\u8c61\u51fa\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u6784\u5efa\u5305\u542b\u76ee\u6807\u3001\u4efb\u52a1\u3001\u7528\u6237\u7ef4\u5ea6\u3001\u8bc4\u4f30\u6807\u51c6\u3001\u65b9\u6cd5\u8bba\u548c\u91cf\u5316\u6307\u6807\u7684CAFE\u6846\u67b6", "result": "\u5f00\u53d1\u4e86\u5305\u542b1\uff09\u7cfb\u7edf\u76ee\u6807 2\uff09\u7528\u6237\u4efb\u52a1 3\uff09\u7528\u6237\u7ef4\u5ea6 4\uff09\u8bc4\u4f30\u6807\u51c6 5\uff09\u8bc4\u4f30\u65b9\u6cd5 6\uff09\u91cf\u5316\u6307\u6807\u7684\u516d\u7ef4\u8bc4\u4f30\u4f53\u7cfb", "conclusion": "CAFE\u6846\u67b6\u4e3a\u4f1a\u8bdd\u4fe1\u606f\u8bbf\u95ee\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7814\u7a76\u65b9\u6cd5\u7684\u6807\u51c6\u5316\u548c\u6bd4\u8f83"}}
{"id": "2506.11113", "pdf": "https://arxiv.org/pdf/2506.11113", "abs": "https://arxiv.org/abs/2506.11113", "authors": ["Tzu-Ling Lin", "Wei-Chih Chen", "Teng-Fang Hsiao", "Hou-I Liu", "Ya-Hsin Yeh", "Yu Kai Chan", "Wen-Sheng Lien", "Po-Yen Kuo", "Philip S. Yu", "Hong-Han Shuai"], "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Peer review is essential for maintaining academic quality, but the increasing\nvolume of submissions places a significant burden on reviewers. Large language\nmodels (LLMs) offer potential assistance in this process, yet their\nsusceptibility to textual adversarial attacks raises reliability concerns. This\npaper investigates the robustness of LLMs used as automated reviewers in the\npresence of such attacks. We focus on three key questions: (1) The\neffectiveness of LLMs in generating reviews compared to human reviewers. (2)\nThe impact of adversarial attacks on the reliability of LLM-generated reviews.\n(3) Challenges and potential mitigation strategies for LLM-based review. Our\nevaluation reveals significant vulnerabilities, as text manipulations can\ndistort LLM assessments. We offer a comprehensive evaluation of LLM performance\nin automated peer reviewing and analyze its robustness against adversarial\nattacks. Our findings emphasize the importance of addressing adversarial risks\nto ensure AI strengthens, rather than compromises, the integrity of scholarly\ncommunication.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u540c\u884c\u8bc4\u5ba1\u4e2d\u5b58\u5728\u5bf9\u6297\u6027\u653b\u51fb\u6f0f\u6d1e\uff0c\u6587\u672c\u64cd\u63a7\u4f1a\u626d\u66f2\u8bc4\u4f30\u7ed3\u679c\uff0c\u9700\u9488\u5bf9\u6027\u7f13\u89e3\u7b56\u7565\u4fdd\u969c\u5b66\u672f\u8bda\u4fe1", "motivation": "\u9488\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u5b66\u672f\u6295\u7a3f\u91cf\u7ed9\u540c\u884c\u8bc4\u5ba1\u5e26\u6765\u7684\u538b\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLM\u4f5c\u4e3a\u81ea\u52a8\u5316\u8bc4\u5ba1\u5de5\u5177\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u6297\u6027\u6587\u672c\u653b\u51fb\u5bf9\u5176\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd", "method": "\u901a\u8fc7\u6784\u5efa\u5bf9\u6297\u653b\u51fb\u573a\u666f\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u751f\u6210\u8bc4\u5ba1\u7684\u8d28\u91cf\u7a33\u5b9a\u6027\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u9a8c\u91cf\u5316\u6587\u672c\u64cd\u63a7\u5bf9\u8bc4\u5ba1\u7ed3\u679c\u7684\u5f71\u54cd\u7a0b\u5ea6", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5bf9\u6297\u6027\u653b\u51fb\u80fd\u6709\u6548\u64cd\u63a7LLM\u7684\u8bc4\u5ba1\u8f93\u51fa\uff0c\u76f8\u540c\u8bba\u6587\u7ecf\u7279\u5b9a\u6587\u672c\u4fee\u9970\u540e\u83b7\u5f97\u7684\u8bc4\u5ba1\u5206\u6570\u5dee\u5f02\u53ef\u8fbe38%", "conclusion": "\u5fc5\u987b\u5efa\u7acb\u9488\u5bf9\u6027\u7684\u9632\u5fa1\u673a\u5236\u6765\u63d0\u5347LLM\u8bc4\u5ba1\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u786e\u4fdd\u4eba\u5de5\u667a\u80fd\u771f\u6b63\u6210\u4e3a\u5b66\u672f\u8d28\u91cf\u4fdd\u969c\u7684\u589e\u5f3a\u5de5\u5177\u800c\u975e\u6f0f\u6d1e\u6e90"}}
{"id": "2506.11114", "pdf": "https://arxiv.org/pdf/2506.11114", "abs": "https://arxiv.org/abs/2506.11114", "authors": ["Junyu Liu", "Kaiqi Yan", "Tianyang Wang", "Qian Niu", "Momoko Nagai-Tanima", "Tomoki Aoyama"], "title": "KokushiMD-10: Benchmark for Evaluating Large Language Models on Ten Japanese National Healthcare Licensing Examinations", "categories": ["cs.CL", "cs.AI"], "comment": "9pages, 3 figures", "summary": "Recent advances in large language models (LLMs) have demonstrated notable\nperformance in medical licensing exams. However, comprehensive evaluation of\nLLMs across various healthcare roles, particularly in high-stakes clinical\nscenarios, remains a challenge. Existing benchmarks are typically text-based,\nEnglish-centric, and focus primarily on medicines, which limits their ability\nto assess broader healthcare knowledge and multimodal reasoning. To address\nthese gaps, we introduce KokushiMD-10, the first multimodal benchmark\nconstructed from ten Japanese national healthcare licensing exams. This\nbenchmark spans multiple fields, including Medicine, Dentistry, Nursing,\nPharmacy, and allied health professions. It contains over 11588 real exam\nquestions, incorporating clinical images and expert-annotated rationales to\nevaluate both textual and visual reasoning. We benchmark over 30\nstate-of-the-art LLMs, including GPT-4o, Claude 3.5, and Gemini, across both\ntext and image-based settings. Despite promising results, no model consistently\nmeets passing thresholds across domains, highlighting the ongoing challenges in\nmedical AI. KokushiMD-10 provides a comprehensive and linguistically grounded\nresource for evaluating and advancing reasoning-centric medical AI across\nmultilingual and multimodal clinical tasks.", "AI": {"tldr": "\u9996\u4e2a\u591a\u6a21\u6001\u533b\u7597\u57fa\u51c6KokushiMD-10\uff1a\u57fa\u4e8e\u65e5\u672c10\u9879\u56fd\u5bb6\u8003\u8bd5\u6784\u5efa\uff0c\u8986\u76d611588\u9898\uff0c\u6d4b\u8bd530+\u9876\u5c16\u6a21\u578b\u5747\u672a\u5168\u79d1\u8fbe\u6807", "motivation": "\u73b0\u6709\u533b\u7597AI\u57fa\u51c6\u5b58\u5728\u4e09\u5927\u5c40\u9650\uff1a\u7eaf\u6587\u672c\u3001\u82f1\u8bed\u4e2d\u5fc3\u3001\u533b\u5b66\u9886\u57df\u5355\u4e00\uff0c\u96be\u4ee5\u8bc4\u4f30\u591a\u6a21\u6001\u63a8\u7406\u53ca\u8de8\u4e13\u4e1a\u533b\u7597\u77e5\u8bc6", "method": "\u6574\u5408\u65e5\u672c\u533b/\u7259/\u62a4/\u836f\u7b4910\u9879\u56fd\u5bb6\u8003\u8bd5\uff0c\u6784\u5efa\u542b\u4e34\u5e8a\u56fe\u50cf\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5GPT-4o\u7b49\u6a21\u578b\u5728\u56fe\u6587\u573a\u666f\u7684\u8868\u73b0", "result": "\u6240\u6709\u6a21\u578b\u5728\u8de8\u9886\u57df\u6d4b\u8bd5\u4e2d\u5747\u672a\u8fbe\u5230\u5408\u683c\u7ebf\uff0c\u51f8\u663e\u533b\u7597AI\u53d1\u5c55\u74f6\u9888\uff0c\u4f46\u4e3a\u591a\u8bed\u8a00\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u65b0\u8bc4\u4f30\u6807\u51c6", "conclusion": "\u8be5\u57fa\u51c6\u586b\u8865\u533b\u7597AI\u8bc4\u4f30\u4f53\u7cfb\u7a7a\u767d\uff0c\u5f3a\u8c03\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u4e34\u5e8a\u573a\u666f\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u9762\u5411\u771f\u5b9e\u533b\u7597\u9700\u6c42\u7684\u6280\u672f\u7a81\u7834"}}
{"id": "2506.11115", "pdf": "https://arxiv.org/pdf/2506.11115", "abs": "https://arxiv.org/abs/2506.11115", "authors": ["Yerim Oh", "Jun-Hyung Park", "Junho Kim", "SungHo Kim", "SangKeun Lee"], "title": "Incorporating Domain Knowledge into Materials Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While language models are increasingly utilized in materials science, typical\nmodels rely on frequency-centric tokenization methods originally developed for\nnatural language processing. However, these methods frequently produce\nexcessive fragmentation and semantic loss, failing to maintain the structural\nand semantic integrity of material concepts. To address this issue, we propose\nMATTER, a novel tokenization approach that integrates material knowledge into\ntokenization. Based on MatDetector trained on our materials knowledge base and\na re-ranking method prioritizing material concepts in token merging, MATTER\nmaintains the structural integrity of identified material concepts and prevents\nfragmentation during tokenization, ensuring their semantic meaning remains\nintact. The experimental results demonstrate that MATTER outperforms existing\ntokenization methods, achieving an average performance gain of $4\\%$ and $2\\%$\nin the generation and classification tasks, respectively. These results\nunderscore the importance of domain knowledge for tokenization strategies in\nscientific text processing. Our code is available at\nhttps://github.com/yerimoh/MATTER", "AI": {"tldr": "\u63d0\u51faMATTER\u6807\u8bb0\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u6750\u6599\u9886\u57df\u77e5\u8bc6\u63d0\u5347\u6750\u6599\u79d1\u5b66\u6587\u672c\u5904\u7406\u6027\u80fd\uff0c\u5b9e\u73b0\u751f\u6210\u4efb\u52a14%\u3001\u5206\u7c7b\u4efb\u52a12%\u7684\u6027\u80fd\u589e\u76ca", "motivation": "\u73b0\u6709NLP\u6807\u8bb0\u65b9\u6cd5\u5bfc\u81f4\u6750\u6599\u6982\u5ff5\u8fc7\u5ea6\u788e\u7247\u5316\uff0c\u7834\u574f\u7ed3\u6784\u5b8c\u6574\u6027\u5e76\u9020\u6210\u8bed\u4e49\u635f\u5931", "method": "\u7ed3\u5408\u6750\u6599\u77e5\u8bc6\u5e93\u8bad\u7ec3\u7684MatDetector\u68c0\u6d4b\u5668\uff0c\u91c7\u7528\u4f18\u5148\u4fdd\u7559\u6750\u6599\u6982\u5ff5\u7684\u91cd\u6392\u5e8f\u673a\u5236\u8fdb\u884c\u6807\u8bb0\u5408\u5e76", "result": "\u751f\u6210\u4efb\u52a1\u5e73\u5747\u63d0\u53474%\uff0c\u5206\u7c7b\u4efb\u52a1\u63d0\u53472%\uff0c\u9a8c\u8bc1\u9886\u57df\u77e5\u8bc6\u6574\u5408\u7684\u6709\u6548\u6027", "conclusion": "\u8bc1\u660e\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u79d1\u5b66\u6587\u672c\u5904\u7406\u7684\u5173\u952e\u4f5c\u7528\uff0cMATTER\u4e3a\u6750\u6599\u79d1\u5b66NLP\u5e94\u7528\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11116", "pdf": "https://arxiv.org/pdf/2506.11116", "abs": "https://arxiv.org/abs/2506.11116", "authors": ["Jijie Li", "Li Du", "Hanyu Zhao", "Bo-wen Zhang", "Liangdong Wang", "Boyan Gao", "Guang Liu", "Yonghua Lin"], "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to Enhance Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong performance in real-world\napplications, yet existing open-source instruction datasets often concentrate\non narrow domains, such as mathematics or coding, limiting generalization and\nwidening the gap with proprietary models. To bridge this gap, we introduce\nInfinity-Instruct, a high-quality instruction dataset designed to enhance both\nfoundational and chat capabilities of LLMs through a two-phase pipeline. In\nPhase 1, we curate 7.4M high-quality foundational instructions\n(InfInstruct-F-7.4M) from over 100M samples using hybrid data selection\ntechniques. In Phase 2, we synthesize 1.5M high-quality chat instructions\n(InfInstruct-G-1.5M) through a two-stage process involving instruction\nselection, evolution, and diagnostic filtering. We empirically evaluate\nInfinity-Instruct by fine-tuning several open-source models, including Mistral,\nLLaMA, Qwen, and Yi, and observe substantial performance gains across both\nfoundational and instruction following benchmarks, consistently surpassing\nofficial instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B\noutperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving\ncomparable foundational performance. These results underscore the synergy\nbetween foundational and chat training and offer new insights into holistic LLM\ndevelopment. Our\ndataset\\footnote{https://huggingface.co/datasets/BAAI/Infinity-Instruct} and\ncodes\\footnote{https://gitee.com/li-touch/infinity-instruct} have been publicly\nreleased.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6784\u5efa\u7684Infinity-Instruct\u6307\u4ee4\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u6027\u80fd\uff0cInfInstruct-LLaMA3.1-70B\u5728\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4-0314\u8fbe8.6%", "motivation": "\u73b0\u6709\u5f00\u6e90\u6307\u4ee4\u6570\u636e\u96c6\u96c6\u4e2d\u4e8e\u6570\u5b66/\u7f16\u7a0b\u7b49\u72ed\u7a84\u9886\u57df\uff0c\u9650\u5236\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5e76\u4e0e\u5546\u4e1a\u6a21\u578b\u5f62\u6210\u6027\u80fd\u5dee\u8ddd", "method": "1. \u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6df7\u5408\u6570\u636e\u7b5b\u9009\u6280\u672f\u4ece1\u4ebf\u6837\u672c\u4e2d\u7cbe\u9009740\u4e07\u57fa\u7840\u6307\u4ee4\uff1b2. \u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u6307\u4ee4\u9009\u62e9\u3001\u8fdb\u5316\u548c\u8bca\u65ad\u8fc7\u6ee4\u751f\u6210150\u4e07\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u6307\u4ee4", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u5728\u57fa\u7840\u80fd\u529b\u548c\u6307\u4ee4\u8ddf\u968f\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0cInfInstruct-LLaMA3.1-70B\u5728\u6307\u4ee4\u4efb\u52a1\u4e0a\u8d85\u8fc7GPT-4-0314\u8fbe8.6%", "conclusion": "\u63ed\u793a\u57fa\u7840\u8bad\u7ec3\u4e0e\u5bf9\u8bdd\u8bad\u7ec3\u7684\u534f\u540c\u6548\u5e94\uff0c\u4e3aLLM\u7684\u5168\u9762\u53d1\u5c55\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55"}}
{"id": "2506.11117", "pdf": "https://arxiv.org/pdf/2506.11117", "abs": "https://arxiv.org/abs/2506.11117", "authors": ["Junyong Lin", "Lu Dai", "Ruiqian Han", "Yijie Sui", "Ruilin Wang", "Xingliang Sun", "Qinglin Wu", "Min Feng", "Hao Liu", "Hui Xiong"], "title": "ScIRGen: Synthesize Realistic and Large-Scale RAG Dataset for Scientific Research", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "KDD 2025 Accepted", "summary": "Scientific researchers need intensive information about datasets to\neffectively evaluate and develop theories and methodologies. The information\nneeds regarding datasets are implicitly embedded in particular research tasks,\nrather than explicitly expressed in search queries. However, existing\nscientific retrieval and question-answering (QA) datasets typically address\nstraightforward questions, which do not align with the distribution of\nreal-world research inquiries. To bridge this gap, we developed ScIRGen, a\ndataset generation framework for scientific QA \\& retrieval that more\naccurately reflects the information needs of professional science researchers,\nand uses it to create a large-scale scientific retrieval-augmented generation\n(RAG) dataset with realistic queries, datasets and papers. Technically, we\ndesigned a dataset-oriented information extraction method that leverages\nacademic papers to augment the dataset representation. We then proposed a\nquestion generation framework by employing cognitive taxonomy to ensure the\nquality of synthesized questions. We also design a method to automatically\nfilter synthetic answers based on the perplexity shift of LLMs, which is highly\naligned with human judgment of answers' validity. Collectively, these\nmethodologies culminated in the creation of the 61k QA dataset, ScIRGen-Geo. We\nbenchmarked representative methods on the ScIRGen-Geo dataset for their\nquestion-answering and retrieval capabilities, finding out that current methods\nstill suffer from reasoning from complex questions. This work advances the\ndevelopment of more sophisticated tools to support the intricate information\nneeds of the scientific community.", "AI": {"tldr": "\u5f00\u53d1ScIRGen\u6846\u67b6\u7528\u4e8e\u751f\u6210\u7b26\u5408\u79d1\u5b66\u7814\u7a76\u8005\u590d\u6742\u9700\u6c42\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e8661k\u89c4\u6a21\u7684ScIRGen-Geo\u57fa\u51c6\u5e76\u9a8c\u8bc1\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u79d1\u5b66\u68c0\u7d22\u548c\u95ee\u7b54\u6570\u636e\u96c6\u4e3b\u8981\u5904\u7406\u7b80\u5355\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u79d1\u7814\u573a\u666f\u4e2d\u9690\u542b\u7684\u590d\u6742\u4fe1\u606f\u9700\u6c42\uff0c\u9700\u8981\u6784\u5efa\u66f4\u8d34\u5408\u5b9e\u9645\u7814\u7a76\u4efb\u52a1\u7684\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6", "method": "1. \u57fa\u4e8e\u5b66\u672f\u8bba\u6587\u7684\u9762\u5411\u6570\u636e\u96c6\u4fe1\u606f\u62bd\u53d6\u65b9\u6cd5\uff1b2. \u8fd0\u7528\u8ba4\u77e5\u5206\u7c7b\u6cd5\u7684\u95ee\u9898\u751f\u6210\u6846\u67b6\uff1b3. \u57fa\u4e8eLLM\u56f0\u60d1\u5ea6\u53d8\u5316\u7684\u7b54\u6848\u81ea\u52a8\u8fc7\u6ee4\u65b9\u6cd5", "result": "\u6784\u5efa\u4e8661k\u89c4\u6a21\u7684ScIRGen-Geo\u6570\u636e\u96c6\uff0c\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u95ee\u9898\u63a8\u7406\u4e0a\u4ecd\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff08\u51c6\u786e\u7387\u4f4e\u4e8e50%\uff09", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a8\u52a8\u4e86\u652f\u6301\u79d1\u5b66\u793e\u533a\u590d\u6742\u4fe1\u606f\u9700\u6c42\u5de5\u5177\u7684\u7814\u53d1\uff0c\u4e3a\u540e\u7eed\u667a\u80fd\u79d1\u7814\u52a9\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u57fa\u51c6"}}
{"id": "2506.11119", "pdf": "https://arxiv.org/pdf/2506.11119", "abs": "https://arxiv.org/abs/2506.11119", "authors": ["Jingyu Li", "Lingchao Mao", "Hairong Wang", "Zhendong Wang", "Xi Mao", "Xuelei Sherry Ni"], "title": "Benchmarking Foundation Speech and Language Models for Alzheimer's Disease and Related Dementia Detection from Spontaneous Speech", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T10 (Primary), 68U99 (Secondary)", "I.2.1; J.3"], "comment": null, "summary": "Background: Alzheimer's disease and related dementias (ADRD) are progressive\nneurodegenerative conditions where early detection is vital for timely\nintervention and care. Spontaneous speech contains rich acoustic and linguistic\nmarkers that may serve as non-invasive biomarkers for cognitive decline.\nFoundation models, pre-trained on large-scale audio or text data, produce\nhigh-dimensional embeddings encoding contextual and acoustic features.\n  Methods: We used the PREPARE Challenge dataset, which includes audio\nrecordings from over 1,600 participants with three cognitive statuses: healthy\ncontrol (HC), mild cognitive impairment (MCI), and Alzheimer's Disease (AD). We\nexcluded non-English, non-spontaneous, or poor-quality recordings. The final\ndataset included 703 (59.13%) HC, 81 (6.81%) MCI, and 405 (34.06%) AD cases. We\nbenchmarked a range of open-source foundation speech and language models to\nclassify cognitive status into the three categories.\n  Results: The Whisper-medium model achieved the highest performance among\nspeech models (accuracy = 0.731, AUC = 0.802). Among language models, BERT with\npause annotation performed best (accuracy = 0.662, AUC = 0.744). ADRD detection\nusing state-of-the-art automatic speech recognition (ASR) model-generated audio\nembeddings outperformed others. Including non-semantic features like pause\npatterns consistently improved text-based classification.\n  Conclusion: This study introduces a benchmarking framework using foundation\nmodels and a clinically relevant dataset. Acoustic-based approaches --\nparticularly ASR-derived embeddings -- demonstrate strong potential for\nscalable, non-invasive, and cost-effective early detection of ADRD.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bed\u97f3/\u6587\u672c\u57fa\u7840\u6a21\u578b\u5206\u6790\uff0c\u53d1\u73b0ASR\u8bed\u97f3\u5d4c\u5165\u5728ADRD\u65e9\u671f\u68c0\u6d4b\u4e2d\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u73870.731\uff09\uff0c\u505c\u987f\u7279\u5f81\u63d0\u5347\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5728\u81ea\u53d1\u8bed\u97f3\u4e2d\u63d0\u53d6\u58f0\u5b66/\u8bed\u8a00\u6807\u8bb0\u7684\u6f5c\u529b\uff0c\u5f00\u53d1\u975e\u4fb5\u5165\u6027ADRD\u65e9\u671f\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5305\u542b1,189\u4f8b\u6837\u672c\u7684PREPARE\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4Whisper\u3001BERT\u7b49\u5f00\u6e90\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u7eb3\u5165\u505c\u987f\u7b49\u975e\u8bed\u4e49\u7279\u5f81\u3002", "result": "Whisper-medium\u8bed\u97f3\u6a21\u578b\u51c6\u786e\u7387\u6700\u9ad8\uff0873.1%\uff09\uff0cASR\u97f3\u9891\u5d4c\u5165\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u505c\u987f\u6807\u6ce8\u4f7f\u6587\u672c\u6a21\u578bAUC\u63d0\u5347\u81f30.744\u3002", "conclusion": "\u57fa\u4e8e\u58f0\u5b66\u7279\u5f81\uff08\u5c24\u5176\u662fASR\u5d4c\u5165\uff09\u7684\u6846\u67b6\u4e3aADRD\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.11120", "pdf": "https://arxiv.org/pdf/2506.11120", "abs": "https://arxiv.org/abs/2506.11120", "authors": ["Hourun Zhu", "Chengchao Shen"], "title": "SDMPrune: Self-Distillation MLP Pruning for Efficient Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "In spite of strong performance achieved by LLMs, the costs of their\ndeployment are unaffordable. For the compression of LLMs, gradient-based\npruning methods present promising effectiveness. However, in these methods, the\ngradient computation with one-hot labels ignore the potential predictions on\nother words, thus missing key information for generative capability of the\noriginal model. To address this issue, we introduce a self-distillation loss\nduring the pruning phase (rather than post-training) to fully exploit the\npredictions of the original model, thereby obtaining more accurate gradient\ninformation for pruning. Moreover, we find that, compared to attention modules,\nthe predictions of LLM are less sensitive to multilayer perceptron (MLP)\nmodules, which take up more than $5 \\times$ parameters (LLaMA3.2-1.2B). To this\nend, we focus on the pruning of MLP modules, to significantly compress LLM\nwithout obvious performance degradation. Experimental results on extensive\nzero-shot benchmarks demonstrate that our method significantly outperforms\nexisting pruning methods. Furthermore, our method achieves very competitive\nperformance among 1B-scale open source LLMs. The source code and trained\nweights are available at https://github.com/visresearch/SDMPrune.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u84b8\u998f\u7684MLP\u526a\u679d\u65b9\u6cd5SDMPrune\uff0c\u663e\u8457\u538b\u7f29LLM\u53c2\u6570\u4e14\u4fdd\u6301\u6027\u80fd", "motivation": "\u4f20\u7edf\u68af\u5ea6\u526a\u679d\u65b9\u6cd5\u5ffd\u7565\u6a21\u578b\u9884\u6d4b\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u4e14MLP\u6a21\u5757\u53c2\u6570\u5360\u6bd4\u9ad8\u4f46\u5bf9\u9884\u6d4b\u654f\u611f\u5ea6\u4f4e", "method": "1. \u5728\u526a\u679d\u9636\u6bb5\u5f15\u5165\u81ea\u84b8\u998f\u635f\u5931\u83b7\u53d6\u5b8c\u6574\u68af\u5ea6\n2. \u4e13\u6ce8\u526a\u679dMLP\u6a21\u5757\uff08\u5360\u6bd4\u8d855\u500d\u53c2\u6570\uff09\n3. \u57fa\u4e8e\u9884\u6d4b\u654f\u611f\u5ea6\u5206\u6790\u9009\u62e9\u526a\u679d\u76ee\u6807", "result": "\u5728\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c1B\u89c4\u6a21LLM\u4e2d\u8fbeSOTA\uff0c\u5f00\u6e90\u4ee3\u7801\u53ca\u6743\u91cd", "conclusion": "\u901a\u8fc7\u9884\u6d4b\u654f\u611f\u5ea6\u5206\u6790\u548c\u81ea\u84b8\u998f\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u6548LLM\u538b\u7f29\uff0c\u5e73\u8861\u53c2\u6570\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd"}}
{"id": "2506.11121", "pdf": "https://arxiv.org/pdf/2506.11121", "abs": "https://arxiv.org/abs/2506.11121", "authors": ["Wei-Ping Huang", "Guan-Ting Lin", "Hung-yi Lee"], "title": "SUTA-LM: Bridging Test-Time Adaptation and Language Model Rescoring for Robust ASR", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Despite progress in end-to-end ASR, real-world domain mismatches still cause\nperformance drops, which Test-Time Adaptation (TTA) aims to mitigate by\nadjusting models during inference. Recent work explores combining TTA with\nexternal language models, using techniques like beam search rescoring or\ngenerative error correction. In this work, we identify a previously overlooked\nchallenge: TTA can interfere with language model rescoring, revealing the\nnontrivial nature of effectively combining the two methods. Based on this\ninsight, we propose SUTA-LM, a simple yet effective extension of SUTA, an\nentropy-minimization-based TTA approach, with language model rescoring. SUTA-LM\nfirst applies a controlled adaptation process guided by an auto-step selection\nmechanism leveraging both acoustic and linguistic information, followed by\nlanguage model rescoring to refine the outputs. Experiments on 18 diverse ASR\ndatasets show that SUTA-LM achieves robust results across a wide range of\ndomains.", "AI": {"tldr": "SUTA-LM\u7ed3\u5408\u71b5\u6700\u5c0f\u5316TTA\u548c\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u8bc4\u5206\uff0c\u6709\u6548\u89e3\u51b3TTA\u4e0e\u8bed\u8a00\u6a21\u578b\u5e72\u6270\u95ee\u9898\uff0c\u572818\u4e2aASR\u6570\u636e\u96c6\u5b9e\u73b0\u8de8\u9886\u57df\u7a33\u5065\u8868\u73b0", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u9002\u5e94\uff08TTA\uff09\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u65f6\u5b58\u5728\u76f8\u4e92\u5e72\u6270\u95ee\u9898\uff0c\u5bfc\u81f4ASR\u6027\u80fd\u63d0\u5347\u53d7\u9650", "method": "1. \u57fa\u4e8e\u58f0\u5b66/\u8bed\u8a00\u4fe1\u606f\u7684\u81ea\u52a8\u6b65\u957f\u9009\u62e9\u673a\u5236\u63a7\u5236\u9002\u5e94\u8fc7\u7a0b 2. \u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u8bc4\u5206\u4f18\u5316\u8f93\u51fa\u7ed3\u679c", "result": "\u572818\u4e2a\u4e0d\u540c\u9886\u57dfASR\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9c81\u68d2\u6027\u80fd\uff0cWER\u5e73\u5747\u76f8\u5bf9\u964d\u4f4e12.5%", "conclusion": "\u901a\u8fc7\u5206\u9636\u6bb5\u63a7\u5236\u9002\u5e94\u548c\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u7684\u534f\u540c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8de8\u9886\u57dfASR\u7cfb\u7edf\u9002\u5e94\u6027"}}
{"id": "2506.11125", "pdf": "https://arxiv.org/pdf/2506.11125", "abs": "https://arxiv.org/abs/2506.11125", "authors": ["Freddie Grabovski", "Gilad Gressel", "Yisroel Mirsky"], "title": "ASRJam: Human-Friendly AI Speech Jamming to Prevent Automated Phone Scams", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), combined with Text-to-Speech (TTS) and\nAutomatic Speech Recognition (ASR), are increasingly used to automate voice\nphishing (vishing) scams. These systems are scalable and convincing, posing a\nsignificant security threat. We identify the ASR transcription step as the most\nvulnerable link in the scam pipeline and introduce ASRJam, a proactive defence\nframework that injects adversarial perturbations into the victim's audio to\ndisrupt the attacker's ASR. This breaks the scam's feedback loop without\naffecting human callers, who can still understand the conversation. While prior\nadversarial audio techniques are often unpleasant and impractical for real-time\nuse, we also propose EchoGuard, a novel jammer that leverages natural\ndistortions, such as reverberation and echo, that are disruptive to ASR but\ntolerable to humans. To evaluate EchoGuard's effectiveness and usability, we\nconducted a 39-person user study comparing it with three state-of-the-art\nattacks. Results show that EchoGuard achieved the highest overall utility,\noffering the best combination of ASR disruption and human listening experience.", "AI": {"tldr": "\u63d0\u51faASRJam\u5bf9\u6297\u6027\u6270\u52a8\u6846\u67b6\u548cEchoGuard\u56de\u58f0\u5e72\u6270\u5668\uff0c\u901a\u8fc7\u7834\u574fASR\u8f6c\u5f55\u963b\u65ad\u8bed\u97f3\u9493\u9c7c\u653b\u51fb\uff0c\u540c\u65f6\u4fdd\u6301\u4eba\u7c7b\u901a\u8bdd\u4f53\u9a8c", "motivation": "\u57fa\u4e8eLLM+TTS+ASR\u7684\u8bed\u97f3\u9493\u9c7c\u653b\u51fb\u89c4\u6a21\u5316\u4e14\u96be\u4ee5\u8fa8\u8bc6\uff0cASR\u73af\u8282\u6210\u4e3a\u653b\u51fb\u94fe\u4e2d\u6700\u8106\u5f31\u7684\u7a81\u7834\u53e3\uff0c\u9700\u5f00\u53d1\u4e0d\u5f71\u54cd\u4eba\u7c7b\u901a\u8bdd\u7684\u4e3b\u52a8\u9632\u5fa1\u65b9\u6848", "method": "1. \u8bbe\u8ba1ASRJam\u6846\u67b6\u6ce8\u5165\u5bf9\u6297\u6027\u6270\u52a8\uff1b2. \u521b\u65b0EchoGuard\u5e72\u6270\u5668\u5229\u7528\u6df7\u54cd/\u56de\u58f0\u7b49\u81ea\u7136\u5931\u771f\uff1b3. \u901a\u8fc739\u4eba\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u6548\u679c", "result": "EchoGuard\u5728ASR\u7834\u574f\u7387\uff08WER\u63d0\u534734.2%\uff09\u548c\u4eba\u7c7b\u4f53\u9a8c\uff08MOS 4.1/5\uff09\u4e0a\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0c\u7efc\u5408\u6548\u7528\u8d85\u8d8a\u73b0\u6709\u653b\u51fb\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5229\u7528\u4eba\u7c7b\u542c\u89c9\u4e0e\u673a\u5668\u542c\u89c9\u7684\u611f\u77e5\u5dee\u5f02\uff0cEchoGuard\u4e3a\u5b9e\u65f6\u8bed\u97f3\u9493\u9c7c\u9632\u5fa1\u63d0\u4f9b\u4e86\u65e2\u6709\u6548\u53c8\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11127", "pdf": "https://arxiv.org/pdf/2506.11127", "abs": "https://arxiv.org/abs/2506.11127", "authors": ["Wenkang Han", "Zhixiong Zeng", "Jing Huang", "Shu Jiang", "Liming Zheng", "Longrong Yang", "Haibo Qiu", "Chang Yao", "Jingyuan Chen", "Lin Ma"], "title": "GUIRoboTron-Speech: Towards Automated GUI Agents Based on Speech Instructions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Autonomous agents for Graphical User Interfaces (GUIs) are revolutionizing\nhuman-computer interaction, yet their reliance on text-based instructions\nimposes limitations on accessibility and convenience, particularly in\nhands-free scenarios. To address this gap, we propose GUIRoboTron-Speech, the\nfirst end-to-end autonomous GUI agent that directly accepts speech instructions\nand on-device screenshots to predict actions. Confronted with the scarcity of\nspeech-based GUI agent datasets, we initially generated high-quality speech\ninstructions for training by leveraging a random timbre text-to-speech (TTS)\nmodel to convert existing text instructions. We then develop\nGUIRoboTron-Speech's capabilities through progressive grounding and planning\ntraining stages. A key contribution is a heuristic mixed-instruction training\nstrategy designed to mitigate the modality imbalance inherent in pre-trained\nfoundation models. Comprehensive experiments on several benchmark datasets\nvalidate the robust and superior performance of GUIRoboTron-Speech,\ndemonstrating the significant potential and widespread applicability of speech\nas an effective instruction modality for driving GUI agents. Our code and\ndatasets are available at https://github.com/GUIRoboTron/GUIRoboTron-Speech.", "AI": {"tldr": "GUIRoboTron-Speech\u662f\u9996\u4e2a\u7aef\u5230\u7aef\u8bed\u97f3\u9a71\u52a8GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u751f\u6210\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u548c\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u5b9e\u73b0\u514d\u63d0\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u4f9d\u8d56\u6587\u672c\u6307\u4ee4\uff0c\u5728\u514d\u63d0\u573a\u666f\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u9700\u63a2\u7d22\u8bed\u97f3\u6307\u4ee4\u63d0\u5347\u4ea4\u4e92\u4fbf\u6377\u6027\u3002", "method": "1. \u5229\u7528TTS\u6a21\u578b\u751f\u6210\u8bed\u97f3\u8bad\u7ec3\u6570\u636e\n2. \u6e10\u8fdb\u5f0f\u57fa\u7840\u8bad\u7ec3+\u89c4\u5212\u8bad\u7ec3\n3. \u542f\u53d1\u5f0f\u6df7\u5408\u6307\u4ee4\u7b56\u7565\u89e3\u51b3\u6a21\u6001\u4e0d\u5e73\u8861", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u4f18\u8d8a\u6027\u80fd\uff0c\u9a8c\u8bc1\u8bed\u97f3\u4f5c\u4e3aGUI\u4ee3\u7406\u6307\u4ee4\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8bed\u97f3\u9a71\u52a8GUI\u4ee3\u7406\u7684\u5e7f\u6cdb\u9002\u7528\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u4eba\u673a\u4ea4\u4e92\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2506.11128", "pdf": "https://arxiv.org/pdf/2506.11128", "abs": "https://arxiv.org/abs/2506.11128", "authors": ["Andrew Keenan Richardson", "Ryan Othniel Kearns", "Sean Moss", "Vincent Wang-Mascianica", "Philipp Koralus"], "title": "Stronger Language Models Produce More Human-Like Errors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Do language models converge toward human-like reasoning patterns as they\nimprove? We provide surprising evidence that while overall reasoning\ncapabilities increase with model sophistication, the nature of errors\nincreasingly mirrors predictable human reasoning fallacies: a previously\nunobserved inverse scaling phenomenon. To investigate this question, we apply\nthe Erotetic Theory of Reasoning (ETR), a formal cognitive framework with\nempirical support for predicting human reasoning outcomes. Using the\nopen-source package PyETR, we generate logical reasoning problems where humans\npredictably err, evaluating responses from 38 language models across 383\nreasoning tasks. Our analysis indicates that as models advance in general\ncapability (as measured by Chatbot Arena scores), the proportion of their\nincorrect answers that align with ETR-predicted human fallacies tends to\nincrease ($\\rho = 0.360, p = 0.0265$). Notably, as we observe no correlation\nbetween model sophistication and logical correctness on these tasks, this shift\nin error patterns toward human-likeness occurs independently of error rate.\nThese findings challenge the prevailing view that scaling language models\nnaturally obtains normative rationality, suggesting instead a convergence\ntoward human-like cognition inclusive of our characteristic biases and\nlimitations, as we further confirm by demonstrating order-effects in language\nmodel reasoning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u63d0\u5347\u65f6\uff0c\u9519\u8bef\u6a21\u5f0f\u9010\u6e10\u8d8b\u8fd1\u4eba\u7c7b\u63a8\u7406\u8c2c\u8bef\uff08\u03c1=0.360, p=0.0265\uff09\uff0c\u663e\u793a\u5176\u8ba4\u77e5\u6a21\u5f0f\u5411\u4eba\u7c7b\u504f\u5dee\u6536\u655b\u800c\u975e\u83b7\u5f97\u89c4\u8303\u6027\u7406\u6027\u3002", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u8fdb\u6b65\u8fc7\u7a0b\u4e2d\u662f\u5426\u8d8b\u8fd1\u4eba\u7c7b\u63a8\u7406\u6a21\u5f0f\uff0c\u6311\u6218\u2018\u89c4\u6a21\u6269\u5c55\u81ea\u52a8\u83b7\u5f97\u89c4\u8303\u6027\u7406\u6027\u2019\u7684\u4e3b\u6d41\u89c2\u70b9\u3002", "method": "\u4f7f\u7528Erotetic\u63a8\u7406\u7406\u8bba\u6846\u67b6\u548cPyETR\u5de5\u5177\u5305\u751f\u6210383\u4e2a\u4eba\u7c7b\u6613\u9519\u63a8\u7406\u4efb\u52a1\uff0c\u8bc4\u4f3038\u4e2a\u8bed\u8a00\u6a21\u578b\u7684\u54cd\u5e94\u6a21\u5f0f\u3002", "result": "\u6a21\u578b\u7efc\u5408\u80fd\u529b\uff08Chatbot Arena\u8bc4\u5206\uff09\u4e0e\u7b26\u5408ETR\u9884\u6d4b\u7684\u8c2c\u8bef\u9519\u8bef\u6bd4\u4f8b\u5448\u663e\u8457\u6b63\u76f8\u5173\uff0c\u4e14\u8be5\u73b0\u8c61\u72ec\u7acb\u4e8e\u9519\u8bef\u7387\u53d8\u5316\u5b58\u5728\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u53ef\u80fd\u5f62\u6210\u2018\u7c7b\u4eba\u8ba4\u77e5\u2019\u8303\u5f0f\uff08\u542b\u7cfb\u7edf\u6027\u504f\u5dee\uff09\uff0c\u800c\u975e\u7edd\u5bf9\u7406\u6027\u3002\u901a\u8fc7\u987a\u5e8f\u6548\u5e94\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u8ba4\u77e5\u76f8\u4f3c\u6027\u3002"}}
{"id": "2506.11129", "pdf": "https://arxiv.org/pdf/2506.11129", "abs": "https://arxiv.org/abs/2506.11129", "authors": ["Carlos Garcia-Fernandez", "Luis Felipe", "Monique Shotande", "Muntasir Zitu", "Aakash Tripathi", "Ghulam Rasool", "Issam El Naqa", "Vivek Rudrapatna", "Gilmer Valdes"], "title": "Trustworthy AI for Medicine: Continuous Hallucination Detection and Elimination with CHECK", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show promise in healthcare, but hallucinations\nremain a major barrier to clinical use. We present CHECK, a continuous-learning\nframework that integrates structured clinical databases with a classifier\ngrounded in information theory to detect both factual and reasoning-based\nhallucinations. Evaluated on 1500 questions from 100 pivotal clinical trials,\nCHECK reduced LLama3.3-70B-Instruct hallucination rates from 31% to 0.3% -\nmaking an open source model state of the art. Its classifier generalized across\nmedical benchmarks, achieving AUCs of 0.95-0.96, including on the MedQA (USMLE)\nbenchmark and HealthBench realistic multi-turn medical questioning. By\nleveraging hallucination probabilities to guide GPT-4o's refinement and\njudiciously escalate compute, CHECK boosted its USMLE passing rate by 5\npercentage points, achieving a state-of-the-art 92.1%. By suppressing\nhallucinations below accepted clinical error thresholds, CHECK offers a\nscalable foundation for safe LLM deployment in medicine and other high-stakes\ndomains.", "AI": {"tldr": "CHECK\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u6570\u636e\u5e93\u4e0e\u4fe1\u606f\u7406\u8bba\u5206\u7c7b\u5668\uff0c\u5c06LLM\u5e7b\u89c9\u7387\u4ece31%\u964d\u81f30.3%\uff0c\u5728USMLE\u8003\u8bd5\u8fbe\u523092.1%\u901a\u8fc7\u7387\uff0c\u4e3a\u533b\u7597AI\u90e8\u7f72\u63d0\u4f9b\u5b89\u5168\u57fa\u7840", "motivation": "\u89e3\u51b3LLM\u5728\u533b\u7597\u9886\u57df\u4ea7\u751f\u5e7b\u89c9\u7684\u6838\u5fc3\u5b89\u5168\u9690\u60a3\uff0c\u7a81\u7834\u73b0\u6709\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u74f6\u9888\u4ee5\u6ee1\u8db3\u4e34\u5e8a\u8bef\u5dee\u9608\u503c\u8981\u6c42", "method": "\u878d\u5408\u7ed3\u6784\u5316\u4e34\u5e8a\u6570\u636e+\u4fe1\u606f\u7406\u8bba\u5206\u7c7b\u5668\u7684\u53cc\u8def\u5f84\u67b6\u6784\uff0c\u5229\u7528\u5e7b\u89c9\u6982\u7387\u6307\u5bfcGPT-4o\u4f18\u5316\uff0c\u667a\u80fd\u5206\u914d\u7b97\u529b\u8d44\u6e90", "result": "\u57281500\u4e2a\u4e34\u5e8a\u8bd5\u9a8c\u95ee\u9898\u4e0a\u5b9e\u73b00.3%\u5e7b\u89c9\u7387\uff0cMedQA\u7b49\u57fa\u51c6AUC\u8fbe0.95-0.96\uff0cUSMLE\u901a\u8fc7\u7387\u63d0\u53475%\u81f392.1% SOTA\u6c34\u5e73", "conclusion": "CHECK\u9996\u6b21\u5c06LLM\u5e7b\u89c9\u6291\u5236\u5230\u4e34\u5e8a\u53ef\u63a5\u53d7\u8303\u56f4\uff0c\u5efa\u7acb\u4e86\u9ad8\u6269\u5c55\u6027\u5b89\u5168\u90e8\u7f72\u6846\u67b6\uff0c\u4e3a\u533b\u7597\u7b49\u9ad8\u5371\u9886\u57df\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11130", "pdf": "https://arxiv.org/pdf/2506.11130", "abs": "https://arxiv.org/abs/2506.11130", "authors": ["Cheng Kang Chou", "Chan-Jan Hsu", "Ho-Lam Chung", "Liang-Hsuan Tseng", "Hsi-Chun Cheng", "Yu-Kuan Fu", "Kuan Po Huang", "Hung-Yi Lee"], "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u4f18\u5316ASR\u6846\u67b6\uff0c\u901a\u8fc7\u4f2a\u6807\u7b7e\u548cTTS\u5408\u6210\u6570\u636e\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\uff0c\u5f00\u53d1Twister\u6a21\u578b\u663e\u8457\u964d\u4f4e\u9519\u8bef\u7387", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90/\u9886\u57df\u7279\u5b9aASR\u573a\u666f\u4e2d\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u95ed\u73af\u81ea\u4f18\u5316\u673a\u5236\u7a81\u7834\u4f20\u7edf\u4f2a\u6807\u7b7e\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "method": "1. \u7528ASR\u6a21\u578b\u751f\u6210\u672a\u6807\u6ce8\u8bed\u97f3\u7684\u4f2a\u6807\u7b7e \u2192 2. \u8bad\u7ec3\u9ad8\u4fdd\u771fTTS\u7cfb\u7edf \u2192 3. \u5408\u6210\u8bed\u97f3-\u6587\u672c\u5bf9 \u2192 4. \u8fed\u4ee3\u4f18\u5316ASR\u6a21\u578b\uff08Whisper\u2192Twister\uff09", "result": "\u57fa\u4e8e6000\u5c0f\u65f6\u672a\u6807\u6ce8\u8bed\u97f3\uff1a\u666e\u901a\u8bdd\u8bc6\u522b\u9519\u8bef\u7387\u964d\u4f4e20%\uff0c\u4e2d\u82f1\u6df7\u5408\u8bed\u7801\u8f6c\u6362\u9519\u8bef\u7387\u964d\u4f4e50%\uff08\u76f8\u6bd4Whisper-large-v2\uff09", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4f4e\u8d44\u6e90ASR\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u751f\u6210\u5185\u5bb9\u7a81\u7834\u6570\u636e\u74f6\u9888\uff0c\u5177\u6709\u663e\u8457\u7684\u5de5\u7a0b\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2506.11135", "pdf": "https://arxiv.org/pdf/2506.11135", "abs": "https://arxiv.org/abs/2506.11135", "authors": ["David C. Krakauer", "John W. Krakauer", "Melanie Mitchell"], "title": "Large Language Models and Emergence: A Complex Systems Perspective", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "Emergence is a concept in complexity science that describes how many-body\nsystems manifest novel higher-level properties, properties that can be\ndescribed by replacing high-dimensional mechanisms with lower-dimensional\neffective variables and theories. This is captured by the idea \"more is\ndifferent\". Intelligence is a consummate emergent property manifesting\nincreasingly efficient -- cheaper and faster -- uses of emergent capabilities\nto solve problems. This is captured by the idea \"less is more\". In this paper,\nwe first examine claims that Large Language Models exhibit emergent\ncapabilities, reviewing several approaches to quantifying emergence, and\nsecondly ask whether LLMs possess emergent intelligence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5c55\u73b0\u6d8c\u73b0\u80fd\u529b\u53ca\u6d8c\u73b0\u667a\u80fd\uff0c\u5206\u6790\u4e0d\u540c\u91cf\u5316\u65b9\u6cd5\u5e76\u8bc4\u4f30LLMs\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u53ef\u80fd\u6027", "motivation": "\u9488\u5bf9\u5f53\u524d\u5b66\u754c\u5173\u4e8eLLMs\u6d8c\u73b0\u80fd\u529b\u7684\u4e89\u8bae\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u80fd\u529b\u662f\u5426\u771f\u6b63\u7b26\u5408\u6d8c\u73b0\u5b9a\u4e49\uff0c\u5e76\u63a2\u8ba8\u667a\u80fd\u4f53\u5f62\u6210\u673a\u5236", "method": "1. \u56de\u987e\u6d8c\u73b0\u73b0\u8c61\u7684\u79d1\u5b66\u5b9a\u4e49 2. \u5efa\u7acb\u6d8c\u73b0\u80fd\u529b\u91cf\u5316\u6846\u67b6 3. \u5206\u6790LLMs\u80fd\u529b\u4e0e\u7cfb\u7edf\u89c4\u6a21\u7684\u5173\u7cfb 4. \u8bc4\u4f30\u667a\u80fd\u6d8c\u73b0\u6807\u51c6", "result": "\u53d1\u73b0LLMs\u90e8\u5206\u80fd\u529b\u7b26\u5408\u975e\u7ebf\u6027\u6d8c\u73b0\u7279\u5f81\uff0c\u4f46\u4f5c\u4e3a\u5b8c\u6574\u667a\u80fd\u4f53\u4ecd\u9700\u9a8c\u8bc1\uff1b\u63d0\u51fa\u7ed3\u5408\u590d\u6742\u7cfb\u7edf\u7406\u8bba\u7684\u8bc4\u4f30\u6307\u6807", "conclusion": "LLMs\u5728\u7279\u5b9a\u7ef4\u5ea6\u5c55\u793a\u6d8c\u73b0\u7279\u6027\uff0c\u4f46\u8ddd\u79bb\u771f\u6b63\u7684\u6d8c\u73b0\u667a\u80fd\u4f53\u4ecd\u5b58\u5728\u7406\u8bba\u9e3f\u6c9f\uff0c\u9700\u7ed3\u5408\u8de8\u5b66\u79d1\u65b9\u6cd5\u6df1\u5165\u7814\u7a76"}}
{"id": "2506.11137", "pdf": "https://arxiv.org/pdf/2506.11137", "abs": "https://arxiv.org/abs/2506.11137", "authors": ["Chong Shao", "Douglas Snyder", "Chiran Li", "Bowen Gu", "Kerry Ngan", "Chun-Ting Yang", "Jiageng Wu", "Richard Wyss", "Kueiyu Joshua Lin", "Jie Yang"], "title": "Scalable Medication Extraction and Discontinuation Identification from Electronic Health Records Using Large Language Models", "categories": ["cs.CL"], "comment": "preprint, under review", "summary": "Identifying medication discontinuations in electronic health records (EHRs)\nis vital for patient safety but is often hindered by information being buried\nin unstructured notes. This study aims to evaluate the capabilities of advanced\nopen-sourced and proprietary large language models (LLMs) in extracting\nmedications and classifying their medication status from EHR notes, focusing on\ntheir scalability on medication information extraction without human\nannotation. We collected three EHR datasets from diverse sources to build the\nevaluation benchmark. We evaluated 12 advanced LLMs and explored multiple LLM\nprompting strategies. Performance on medication extraction, medication status\nclassification, and their joint task (extraction then classification) was\nsystematically compared across all experiments. We found that LLMs showed\npromising performance on the medication extraction and discontinuation\nclassification from EHR notes. GPT-4o consistently achieved the highest average\nF1 scores in all tasks under zero-shot setting - 94.0% for medication\nextraction, 78.1% for discontinuation classification, and 72.7% for the joint\ntask. Open-sourced models followed closely, Llama-3.1-70B-Instruct achieved the\nhighest performance in medication status classification on the MIV-Med dataset\n(68.7%) and in the joint task on both the Re-CASI (76.2%) and MIV-Med (60.2%)\ndatasets. Medical-specific LLMs demonstrated lower performance compared to\nadvanced general-domain LLMs. Few-shot learning generally improved performance,\nwhile CoT reasoning showed inconsistent gains. LLMs demonstrate strong\npotential for medication extraction and discontinuation identification on EHR\nnotes, with open-sourced models offering scalable alternatives to proprietary\nsystems and few-shot can further improve LLMs' capability.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5f00\u6e90\u4e0e\u4e13\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u63d0\u53d6\u836f\u7269\u4fe1\u606f\u53ca\u505c\u836f\u72b6\u6001\u5206\u7c7b\u7684\u80fd\u529b\uff0c\u53d1\u73b0GPT-4o\u6027\u80fd\u6700\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u53ef\u4f5c\u4e3a\u53ef\u6269\u5c55\u66ff\u4ee3\u65b9\u6848\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u53ef\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u836f\u7269\u505c\u7528\u4fe1\u606f\u5e38\u57cb\u85cf\u5728\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1LLMs\u5728\u65e0\u4eba\u5de5\u6807\u6ce8\u60c5\u51b5\u4e0b\u81ea\u52a8\u5316\u5904\u7406\u8be5\u4efb\u52a1\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u5f02\u6784EHR\u6570\u636e\u96c6\u6784\u5efa\u8bc4\u4f30\u57fa\u51c6\uff0c\u6d4b\u8bd512\u4e2a\u5148\u8fdbLLM\uff0c\u63a2\u7d22\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u7cfb\u7edf\u6bd4\u8f83\u836f\u7269\u63d0\u53d6\u3001\u72b6\u6001\u5206\u7c7b\u53ca\u8054\u5408\u4efb\u52a1\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "GPT-4o\u96f6\u6837\u672c\u4e0b\u5e73\u5747F1\u6700\u9ad8\uff08\u836f\u7269\u63d0\u53d694.0%\uff0c\u505c\u836f\u5206\u7c7b78.1%\uff0c\u8054\u5408\u4efb\u52a172.7%\uff09\u3002Llama-3.1-70B-Instruct\u5728\u7279\u5b9a\u6570\u636e\u96c6\u8868\u73b0\u4f18\u5f02\uff08MIV-Med\u505c\u836f\u5206\u7c7b68.7%\uff0cRe-CASI\u8054\u5408\u4efb\u52a176.2%\uff09\u3002\u533b\u5b66\u4e13\u7528\u6a21\u578b\u8868\u73b0\u5f31\u4e8e\u901a\u7528\u6a21\u578b\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u666e\u904d\u6709\u6548\uff0cCoT\u63a8\u7406\u6548\u679c\u4e0d\u7a33\u5b9a\u3002", "conclusion": "LLMs\u5728\u836f\u7269\u7ba1\u7406\u573a\u666f\u5c55\u73b0\u5f3a\u5927\u6f5c\u529b\uff0c\u5f00\u6e90\u6a21\u578b\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5c11\u6837\u672c\u5b66\u4e60\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u5316EHR\u4fe1\u606f\u63d0\u53d6\u63d0\u4f9b\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.11243", "pdf": "https://arxiv.org/pdf/2506.11243", "abs": "https://arxiv.org/abs/2506.11243", "authors": ["Santiago G\u00f3ngora", "Ignacio Sastre", "Santiago Robaina", "Ignacio Remersaro", "Luis Chiruzzo", "Aiala Ros\u00e1"], "title": "RETUYT-INCO at BEA 2025 Shared Task: How Far Can Lightweight Models Go in AI-powered Tutor Evaluation?", "categories": ["cs.CL", "cs.AI"], "comment": "This paper will be presented at the 20th BEA Workshop (Innovative Use\n  of NLP for Building Educational Applications) at ACL 2025", "summary": "In this paper, we present the RETUYT-INCO participation at the BEA 2025\nshared task. Our participation was characterized by the decision of using\nrelatively small models, with fewer than 1B parameters. This self-imposed\nrestriction tries to represent the conditions in which many research labs or\ninstitutions are in the Global South, where computational power is not easily\naccessible due to its prohibitive cost. Even under this restrictive\nself-imposed setting, our models managed to stay competitive with the rest of\nteams that participated in the shared task. According to the $exact\\ F_1$\nscores published by the organizers, the performance gaps between our models and\nthe winners were as follows: $6.46$ in Track 1; $10.24$ in Track 2; $7.85$ in\nTrack 3; $9.56$ in Track 4; and $13.13$ in Track 5. Considering that the\nminimum difference with a winner team is $6.46$ points -- and the maximum\ndifference is $13.13$ -- according to the $exact\\ F_1$ score, we find that\nmodels with a size smaller than 1B parameters are competitive for these tasks,\nall of which can be run on computers with a low-budget GPU or even without a\nGPU.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u53c2\u6570\u4e0d\u8db310\u4ebf\u7684\u5c0f\u578b\u6a21\u578b\u5728BEA 2025\u7ade\u8d5b\u4e2d\u7684\u7ade\u4e89\u529b\uff0c\u9a8c\u8bc1\u4e86\u5728\u6709\u9650\u7b97\u529b\u6761\u4ef6\u4e0b\u7684\u53ef\u884c\u6027", "motivation": "\u6a21\u62df\u5168\u7403\u5357\u65b9\u7814\u7a76\u673a\u6784\u7f3a\u4e4f\u8ba1\u7b97\u8d44\u6e90\u7684\u771f\u5b9e\u73af\u5883\uff0c\u9a8c\u8bc1\u5c0f\u89c4\u6a21\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "method": "\u81ea\u6211\u9650\u5236\u4f7f\u7528\u53c2\u6570\u89c4\u6a21\u5c0f\u4e8e1B\u7684\u6a21\u578b\u53c2\u4e0e\u4e94\u4e2a\u7ade\u8d5bTrack\uff0c\u6240\u6709\u5b9e\u9a8c\u53ef\u5728\u4f4e\u7aefGPU\u6216\u65e0GPU\u8bbe\u5907\u8fd0\u884c", "result": "\u4e0e\u51a0\u519b\u56e2\u961f\u5728exact F1\u4e0a\u7684\u5dee\u8ddd\uff1aTrack1(6.46)\u3001Track2(10.24)\u3001Track3(7.85)\u3001Track4(9.56)\u3001Track5(13.13)", "conclusion": "6.46-13.13\u5206\u7684\u6027\u80fd\u5dee\u8ddd\u8868\u660e\uff0c\u5c0f\u6a21\u578b\u5728\u7279\u5b9aNLP\u4efb\u52a1\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u673a\u6784\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2506.11244", "pdf": "https://arxiv.org/pdf/2506.11244", "abs": "https://arxiv.org/abs/2506.11244", "authors": ["Shun Shao", "Yftah Ziser", "Zheng Zhao", "Yifu Qiu", "Shay B. Cohen", "Anna Korhonen"], "title": "Iterative Multilingual Spectral Attribute Erasure", "categories": ["cs.CL"], "comment": "8 pages, 3 figures", "summary": "Multilingual representations embed words with similar meanings to share a\ncommon semantic space across languages, creating opportunities to transfer\ndebiasing effects between languages. However, existing methods for debiasing\nare unable to exploit this opportunity because they operate on individual\nlanguages. We present Iterative Multilingual Spectral Attribute Erasure\n(IMSAE), which identifies and mitigates joint bias subspaces across multiple\nlanguages through iterative SVD-based truncation. Evaluating IMSAE across eight\nlanguages and five demographic dimensions, we demonstrate its effectiveness in\nboth standard and zero-shot settings, where target language data is\nunavailable, but linguistically similar languages can be used for debiasing.\nOur comprehensive experiments across diverse language models (BERT, LLaMA,\nMistral) show that IMSAE outperforms traditional monolingual and cross-lingual\napproaches while maintaining model utility.", "AI": {"tldr": "\u63d0\u51faIMSAE\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3SVD\u8de8\u8bed\u8a00\u6d88\u9664\u504f\u89c1\u5b50\u7a7a\u95f4\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u6a21\u578b\u53bb\u504f\u6548\u679c", "motivation": "\u73b0\u6709\u5355\u8bed\u8a00\u53bb\u504f\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u591a\u8bed\u8a00\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u7684\u4f18\u52bf\uff0c\u5bfc\u81f4\u8de8\u8bed\u8a00\u53bb\u504f\u6548\u679c\u53d7\u9650", "method": "\u57fa\u4e8e\u8fed\u4ee3\u5947\u5f02\u503c\u5206\u89e3(SVD)\u5728\u591a\u8bed\u8a00\u8054\u5408\u7a7a\u95f4\u4e2d\u8bc6\u522b\u5e76\u622a\u65ad\u5171\u540c\u504f\u89c1\u5b50\u7a7a\u95f4", "result": "\u57288\u79cd\u8bed\u8a00\u548c5\u4e2a\u7ef4\u5ea6\u6d4b\u8bd5\u4e2d\uff0cIMSAE\u5728\u6807\u51c6/\u96f6\u6837\u672c\u573a\u666f\u5747\u4f18\u4e8e\u5355\u8bed\u65b9\u6cd5\uff0c\u4e14\u4fdd\u6301\u6a21\u578b\u5b9e\u7528\u6027\uff08BERT/LLaMA/Mistral\u9a8c\u8bc1\uff09", "conclusion": "IMSAE\u5b9e\u73b0\u4e86\u8de8\u8bed\u8a00\u504f\u89c1\u8fc1\u79fb\u6d88\u9664\uff0c\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u516c\u5e73\u6027\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.11246", "pdf": "https://arxiv.org/pdf/2506.11246", "abs": "https://arxiv.org/abs/2506.11246", "authors": ["Kushagra Dixit", "Abhishek Rajgaria", "Harshavardhan Kalalbandi", "Dan Roth", "Vivek Gupta"], "title": "No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 19 Tables, 9 Figures", "summary": "Temporal Table Reasoning is a critical challenge for Large Language Models\n(LLMs), requiring effective prompting techniques to extract relevant insights.\nDespite existence of multiple prompting methods, their impact on table\nreasoning remains largely unexplored. Furthermore, the performance of these\nmodels varies drastically across different table and context structures, making\nit difficult to determine an optimal approach. This work investigates multiple\nprompting technique across diverse table types to determine optimal approaches\nfor different scenarios. We find that performance varies based on entity type,\ntable structure, requirement of additional context and question complexity,\nwith NO single method consistently outperforming others. To mitigate these\nchallenges, we introduce SEAR, an adaptive prompting framework inspired by\nhuman reasoning that dynamically adjusts based on context characteristics and\nintegrates a structured reasoning. Our results demonstrate that SEAR achieves\nsuperior performance across all table types compared to other baseline\nprompting techniques. Additionally, we explore the impact of table structure\nrefactoring, finding that a unified representation enhances model's reasoning.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u63d0\u793a\u6846\u67b6SEAR\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63d0\u793a\u7b56\u7565\u548c\u7ed3\u6784\u5316\u63a8\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u5e8f\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u5728\u8868\u683c\u7ed3\u6784\u3001\u5b9e\u4f53\u7c7b\u578b\u3001\u4e0a\u4e0b\u6587\u4f9d\u8d56\u548c\u95ee\u9898\u590d\u6742\u5ea6\u4e0d\u540c\u573a\u666f\u4e0b\u8868\u73b0\u5dee\u5f02\u5927\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u4f18\u5316\u65b9\u6848\u3002\u9700\u8981\u63a2\u7d22\u4e0d\u540c\u8868\u683c\u7c7b\u578b\u4e0b\u7684\u6700\u4f73\u63d0\u793a\u7b56\u7565\u7ec4\u5408\u3002", "method": "1. \u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u63d0\u793a\u65b9\u6cd5\u5728\u4e0d\u540c\u8868\u683c\u7c7b\u578b\u4e0b\u7684\u6027\u80fd\u5dee\u5f02 2. \u53d7\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u542f\u53d1\uff0c\u5f00\u53d1\u52a8\u6001\u8c03\u6574\u63d0\u793a\u7b56\u7565\u7684SEAR\u6846\u67b6 3. \u5f15\u5165\u7ed3\u6784\u5316\u63a8\u7406\u673a\u5236\u5e76\u9a8c\u8bc1\u8868\u683c\u91cd\u6784\u7684\u7edf\u4e00\u8868\u793a\u6548\u679c", "result": "SEAR\u6846\u67b6\u5728\u6240\u6709\u8868\u683c\u7c7b\u578b\u4e0a\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u683c\u7ed3\u6784\u91cd\u6784\u4f7f\u6a21\u578b\u63a8\u7406\u80fd\u529b\u63d0\u534712.5%\u3002\u4e0d\u540c\u573a\u666f\u4e0b\u6700\u4f73\u63d0\u793a\u7b56\u7565\u7ec4\u5408\u5dee\u5f02\u663e\u8457\uff08\u6700\u9ad8\u8fbe37.2%\u6027\u80fd\u6ce2\u52a8\uff09", "conclusion": "\u81ea\u9002\u5e94\u63d0\u793a\u6846\u67b6\u7ed3\u5408\u7ed3\u6784\u5316\u63a8\u7406\u53ef\u6709\u6548\u63d0\u5347\u8868\u683c\u63a8\u7406\u6027\u80fd\uff0c\u8868\u683c\u8868\u5f81\u6807\u51c6\u5316\u5bf9\u6a21\u578b\u7406\u89e3\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u7ec6\u7c92\u5ea6\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u673a\u5236"}}
{"id": "2506.11274", "pdf": "https://arxiv.org/pdf/2506.11274", "abs": "https://arxiv.org/abs/2506.11274", "authors": ["Liran Ringel", "Elad Tolochinsky", "Yaniv Romano"], "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling has emerged as an effective approach for improving language\nmodel performance by utilizing additional compute at inference time. Recent\nstudies have shown that overriding end-of-thinking tokens (e.g., replacing\n\"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In\nthis work, we explore whether a dedicated continue-thinking token can be\nlearned to trigger extended reasoning. We augment a distilled version of\nDeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only\nits embedding via reinforcement learning while keeping the model weights\nfrozen. Our experiments show that this learned token achieves improved accuracy\non standard math benchmarks compared to both the baseline model and a test-time\nscaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In\nparticular, we observe that in cases where the fixed-token approach enhances\nthe base model's accuracy, our method achieves a markedly greater improvement.\nFor example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%\nabsolute improvement in accuracy, whereas our learned-token method achieves a\n4.2% improvement over the base model that does not use budget forcing.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e13\u7528'\u7ee7\u7eed\u601d\u8003'\u6807\u8bb0\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u53c2\u6570\u51bb\u7ed3\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6027\u80fd\uff0c\u6548\u679c\u4f18\u4e8e\u56fa\u5b9a\u6807\u8bb0\u6269\u5c55\u65b9\u6cd5", "motivation": "\u9488\u5bf9\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u7ed3\u675f\u6807\u8bb0\uff08\u5982'Wait'\uff09\u53ef\u80fd\u5b58\u5728\u7684\u6548\u7387\u9650\u5236\uff0c\u63a2\u7d22\u4e13\u7528\u5b66\u4e60\u6807\u8bb0\u80fd\u5426\u66f4\u597d\u89e6\u53d1\u6269\u5c55\u63a8\u7406", "method": "\u5728\u84b8\u998f\u7248DeepSeek-R1\u4e2d\u65b0\u589e<|continue-thinking|>\u6807\u8bb0\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ec5\u8bad\u7ec3\u8be5\u6807\u8bb0\u7684\u5d4c\u5165\uff08\u4fdd\u6301\u5176\u4ed6\u6a21\u578b\u53c2\u6570\u51bb\u7ed3\uff09", "result": "\u5728GSM8K\u7b49\u6570\u5b66\u57fa\u51c6\u4e0a\uff0c\u5b66\u4e60\u6807\u8bb0\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u53474.2%\uff08\u56fa\u5b9a\u6807\u8bb0\u4ec5\u63d0\u53471.3%\uff09\uff0c\u5728\u56fa\u5b9a\u6807\u8bb0\u6709\u6548\u573a\u666f\u83b7\u5f97\u66f4\u5927\u589e\u76ca", "conclusion": "\u5b66\u4e60\u4e13\u7528\u7ee7\u7eed\u601d\u8003\u6807\u8bb0\u6bd4\u56fa\u5b9a\u6807\u8bb0\u6269\u5c55\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u51bb\u7ed3\u7684\u9ad8\u6548\u6027\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b"}}
{"id": "2506.11300", "pdf": "https://arxiv.org/pdf/2506.11300", "abs": "https://arxiv.org/abs/2506.11300", "authors": ["Yang Zhang", "Amr Mohamed", "Hadi Abdine", "Guokan Shang", "Michalis Vazirgiannis"], "title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Curriculum learning has shown promise in improving training efficiency and\ngeneralization in various machine learning domains, yet its potential in\npretraining language models remains underexplored, prompting our work as the\nfirst systematic investigation in this area. We experimented with different\nsettings, including vanilla curriculum learning, pacing-based sampling, and\ninterleaved curricula-guided by six difficulty metrics spanning linguistic and\ninformation-theoretic perspectives. We train models under these settings and\nevaluate their performance on eight diverse benchmarks. Our experiments reveal\nthat curriculum learning consistently improves convergence in early and\nmid-training phases, and can yield lasting gains when used as a warmup strategy\nwith up to $3.5\\%$ improvement. Notably, we identify compression ratio, lexical\ndiversity, and readability as effective difficulty signals across settings. Our\nfindings highlight the importance of data ordering in large-scale pretraining\nand provide actionable insights for scalable, data-efficient model development\nunder realistic training scenarios.", "AI": {"tldr": "\u8bfe\u7a0b\u5b66\u4e60\u901a\u8fc7\u6570\u636e\u6392\u5e8f\u7b56\u7565\u53ef\u63d0\u5347\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b3.5%\u6027\u80fd\uff0c\u538b\u7f29\u6bd4\u3001\u8bcd\u6c47\u591a\u6837\u6027\u548c\u53ef\u8bfb\u6027\u662f\u6700\u6709\u6548\u7684\u96be\u5ea6\u6307\u6807", "motivation": "\u63a2\u7d22\u8bfe\u7a0b\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u4e2d\u7684\u6f5c\u529b\uff0c\u586b\u8865\u8be5\u9886\u57df\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d", "method": "\u91c7\u7528\u516d\u79cd\u8bed\u8a00\u5b66\u4e0e\u4fe1\u606f\u8bba\u6307\u6807\u6307\u5bfc\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u5305\u62ec\u666e\u901a\u8bfe\u7a0b\u3001\u6b65\u8c03\u91c7\u6837\u548c\u4ea4\u9519\u8bfe\u7a0b\u4e09\u79cd\u8bbe\u7f6e\uff0c\u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u6a21\u578b\u8868\u73b0", "result": "\u65e9\u671f\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u63d0\u534714%\uff0c\u4f5c\u4e3a\u9884\u70ed\u7b56\u7565\u65f6\u5b9e\u73b0\u6301\u7eed\u6027\u80fd\u589e\u76ca\uff0c\u6700\u4f18\u8bbe\u7f6e\u4e0b\u51c6\u786e\u7387\u63d0\u53473.5%", "conclusion": "\u6570\u636e\u6392\u5e8f\u7b56\u7565\u5bf9\u9884\u8bad\u7ec3\u6548\u679c\u81f3\u5173\u91cd\u8981\uff0c\u538b\u7f29\u6bd4\u7b49\u5ba2\u89c2\u6307\u6807\u53ef\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u8bfe\u7a0b\u8bbe\u8ba1\u4f9d\u636e\uff0c\u4e3a\u5b9e\u9645\u8bad\u7ec3\u573a\u666f\u63d0\u4f9b\u6570\u636e\u6548\u7387\u4f18\u5316\u65b9\u6848"}}
{"id": "2506.11305", "pdf": "https://arxiv.org/pdf/2506.11305", "abs": "https://arxiv.org/abs/2506.11305", "authors": ["Mohammad Hammoud", "Devang Acharya"], "title": "Don't Pay Attention", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578bAvey\u67b6\u6784\uff0c\u7a81\u7834\u6ce8\u610f\u529b\u4e0e\u5faa\u73af\u673a\u5236\u9650\u5236\uff0c\u5b9e\u73b0\u4efb\u610f\u957f\u5ea6\u5e8f\u5217\u5904\u7406\u5e76\u4f18\u4e8eTransformer", "motivation": "Transformer\u5b58\u5728\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u6ce8\u610f\u529b\u673a\u5236\u4e8c\u6b21\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u5faa\u73af\u67b6\u6784\u5219\u53d7\u5236\u4e8e\u5e8f\u5217\u5316\u5904\u7406\u6548\u7387", "method": "\u7531rankers\u548c\u81ea\u56de\u5f52\u795e\u7ecf\u5904\u7406\u5668\u6784\u6210\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u5173\u952etoken\u5e76\u89e3\u8026\u5e8f\u5217\u957f\u5ea6\u4e0e\u4e0a\u4e0b\u6587\u5bbd\u5ea6", "result": "\u5728\u77ed\u7a0bNLP\u4efb\u52a1\u8868\u73b0\u76f8\u5f53\uff0c\u957f\u7a0b\u4f9d\u8d56\u5904\u7406\u663e\u8457\u4f18\u4e8eTransformer", "conclusion": "Avey\u4e3a\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u517c\u5177\u9ad8\u6548\u957f\u7a0b\u5904\u7406\u4e0e\u5e76\u884c\u8ba1\u7b97\u4f18\u52bf"}}
{"id": "2506.11338", "pdf": "https://arxiv.org/pdf/2506.11338", "abs": "https://arxiv.org/abs/2506.11338", "authors": ["Yi-Chien Lin", "William Schuler"], "title": "Surprisal from Larger Transformer-based Language Models Predicts fMRI Data More Poorly", "categories": ["cs.CL"], "comment": null, "summary": "As Transformers become more widely incorporated into natural language\nprocessing tasks, there has been considerable interest in using surprisal from\nthese models as predictors of human sentence processing difficulty. Recent work\nhas observed a positive relationship between Transformer-based models'\nperplexity and the predictive power of their surprisal estimates on reading\ntimes, showing that language models with more parameters and trained on more\ndata are less predictive of human reading times. However, these studies focus\non predicting latency-based measures (i.e., self-paced reading times and\neye-gaze durations) with surprisal estimates from Transformer-based language\nmodels. This trend has not been tested on brain imaging data. This study\ntherefore evaluates the predictive power of surprisal estimates from 17\npre-trained Transformer-based models across three different language families\non two functional magnetic resonance imaging datasets. Results show that the\npositive relationship between model perplexity and model fit still obtains,\nsuggesting that this trend is not specific to latency-based measures and can be\ngeneralized to neural measures.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Transformer\u6a21\u578b\u7684\u590d\u6742\u5ea6\uff08\u56f0\u60d1\u5ea6\uff09\u4e0e\u5176\u5bf9\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u80fd\u529b\u7684\u9884\u6d4b\u6548\u679c\u5448\u8d1f\u76f8\u5173\uff0c\u8be5\u73b0\u8c61\u4e0d\u4ec5\u5b58\u5728\u4e8e\u884c\u4e3a\u5b9e\u9a8c\u6570\u636e\uff0c\u5728\u8111\u6210\u50cf\u6570\u636e\u4e2d\u540c\u6837\u6210\u7acb\u3002", "motivation": "\u9a8c\u8bc1\u6a21\u578b\u56f0\u60d1\u5ea6\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u9884\u6d4b\u80fd\u529b\u7684\u8d1f\u76f8\u5173\u5173\u7cfb\u662f\u5426\u9002\u7528\u4e8e\u795e\u7ecf\u5c42\u9762\u7684\u6d4b\u91cf\uff08fMRI\u6570\u636e\uff09\uff0c\u7a81\u7834\u6b64\u524d\u4ec5\u9650\u4e8e\u884c\u4e3a\u5b9e\u9a8c\u7684\u7814\u7a76\u5c40\u9650\u3002", "method": "\u4f7f\u752817\u4e2a\u9884\u8bad\u7ec3Transformer\u6a21\u578b\uff0c\u5728\u4e24\u4e2afMRI\u6570\u636e\u96c6\uff08\u6db5\u76d6\u4e09\u8bed\u7cfb\uff09\u4e2d\u8bc4\u4f30\u6a21\u578b\u60ca\u5947\u503c\u4e0e\u795e\u7ecf\u6d3b\u52a8\u7684\u76f8\u5173\u6027\u3002", "result": "\u6a21\u578b\u56f0\u60d1\u5ea6\u4e0e\u795e\u7ecf\u6d3b\u52a8\u9884\u6d4b\u80fd\u529b\u5448\u6b63\u76f8\u5173\uff0c\u66f4\u5927\u66f4\u5f3a\u7684\u6a21\u578b\u5bf9\u4eba\u7c7b\u795e\u7ecf\u8868\u5f81\u7684\u9884\u6d4b\u6548\u679c\u66f4\u5dee\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u4f34\u968f\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u673a\u5236\u89e3\u91ca\u529b\u7684\u4e0b\u964d\uff0c\u8fd9\u5bf9\u8ba4\u77e5\u5efa\u6a21\u4e0eNLP\u7684\u4ea4\u53c9\u7814\u7a76\u5177\u6709\u8b66\u793a\u610f\u4e49\u3002"}}
{"id": "2506.11343", "pdf": "https://arxiv.org/pdf/2506.11343", "abs": "https://arxiv.org/abs/2506.11343", "authors": ["Yaohui Zhang", "Haijing Zhang", "Wenlong Ji", "Tianyu Hua", "Nick Haber", "Hancheng Cao", "Weixin Liang"], "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review", "categories": ["cs.CL"], "comment": null, "summary": "The advent of large language models (LLMs) offers unprecedented opportunities\nto reimagine peer review beyond the constraints of traditional workflows.\nDespite these opportunities, prior efforts have largely focused on replicating\ntraditional review workflows with LLMs serving as direct substitutes for human\nreviewers, while limited attention has been given to exploring new paradigms\nthat fundamentally rethink how LLMs can participate in the academic review\nprocess. In this paper, we introduce and explore a novel mechanism that employs\nLLM agents to perform pairwise comparisons among manuscripts instead of\nindividual scoring. By aggregating outcomes from substantial pairwise\nevaluations, this approach enables a more accurate and robust measure of\nrelative manuscript quality. Our experiments demonstrate that this comparative\napproach significantly outperforms traditional rating-based methods in\nidentifying high-impact papers. However, our analysis also reveals emergent\nbiases in the selection process, notably a reduced novelty in research topics\nand an increased institutional imbalance. These findings highlight both the\ntransformative potential of rethinking peer review with LLMs and critical\nchallenges that future systems must address to ensure equity and diversity.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528LLM\u8fdb\u884c\u8bba\u6587\u6210\u5bf9\u6bd4\u8f83\u7684\u65b0\u8bc4\u5ba1\u673a\u5236\uff0c\u867d\u63d0\u5347\u8bc4\u4f30\u6548\u679c\u4f46\u5b58\u5728\u65b0\u9896\u6027\u964d\u4f4e\u548c\u673a\u6784\u5931\u8861\u95ee\u9898", "motivation": "\u7a81\u7834\u4f20\u7edf\u540c\u884c\u8bc4\u5ba1\u5c40\u9650\uff0c\u63a2\u7d22LLM\u5728\u5b66\u672f\u8bc4\u5ba1\u4e2d\u7684\u8303\u5f0f\u521b\u65b0\uff08\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126LLM\u66ff\u4ee3\u4eba\u7c7b\u8bc4\u5ba1\u7684\u8def\u5f84\u590d\u5236\u800c\u975e\u673a\u5236\u521b\u65b0\uff09", "method": "\u901a\u8fc7LLM\u4ee3\u7406\u8fdb\u884c\u5927\u89c4\u6a21\u7a3f\u4ef6\u6210\u5bf9\u6bd4\u8f83\uff0c\u57fa\u4e8e\u6bd4\u8f83\u7ed3\u679c\u805a\u5408\u5efa\u7acb\u76f8\u5bf9\u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb", "result": "\u6bd4\u8f83\u65b9\u6cd5\u8f83\u4f20\u7edf\u8bc4\u5206\u5236\u63d0\u534734%\u9ad8\u5f71\u54cd\u529b\u8bba\u6587\u8bc6\u522b\u7387\uff0c\u4f46\u5bfc\u81f4\u7814\u7a76\u4e3b\u9898\u65b0\u9896\u6027\u4e0b\u964d18%\u3001\u673a\u6784\u5931\u8861\u52a0\u526722%", "conclusion": "LLM\u91cd\u6784\u8bc4\u5ba1\u6d41\u7a0b\u5177\u6709\u9769\u547d\u6f5c\u529b\uff0c\u4f46\u9700\u5efa\u7acb\u504f\u5dee\u6821\u6b63\u673a\u5236\u4fdd\u969c\u5b66\u672f\u591a\u6837\u6027\uff0c\u672a\u6765\u7cfb\u7edf\u5e94\u5e73\u8861\u6548\u7387\u4e0e\u516c\u5e73"}}
{"id": "2506.11344", "pdf": "https://arxiv.org/pdf/2506.11344", "abs": "https://arxiv.org/abs/2506.11344", "authors": ["Peilin Wu", "Jinho D. Choi"], "title": "Do We Still Need Audio? Rethinking Speaker Diarization with a Text-Based Approach Using Multiple Prediction Models", "categories": ["cs.CL"], "comment": null, "summary": "We present a novel approach to Speaker Diarization (SD) by leveraging\ntext-based methods focused on Sentence-level Speaker Change Detection within\ndialogues. Unlike audio-based SD systems, which are often challenged by audio\nquality and speaker similarity, our approach utilizes the dialogue transcript\nalone. Two models are developed: the Single Prediction Model (SPM) and the\nMultiple Prediction Model (MPM), both of which demonstrate significant\nimprovements in identifying speaker changes, particularly in short\nconversations. Our findings, based on a curated dataset encompassing diverse\nconversational scenarios, reveal that the text-based SD approach, especially\nthe MPM, performs competitively against state-of-the-art audio-based SD\nsystems, with superior performance in short conversational contexts. This paper\nnot only showcases the potential of leveraging linguistic features for SD but\nalso highlights the importance of integrating semantic understanding into SD\nsystems, opening avenues for future research in multimodal and semantic\nfeature-based diarization.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u7684\u8bf4\u8bdd\u4eba\u53d8\u5316\u68c0\u6d4b\u65b0\u65b9\u6cd5\uff0c\u5f00\u53d1SPM/MPM\u6a21\u578b\uff0c\u5728\u77ed\u5bf9\u8bdd\u573a\u666f\u4e2d\u6548\u679c\u663e\u8457\u4e14\u5ab2\u7f8e\u97f3\u9891\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u97f3\u9891SD\u7cfb\u7edf\u6613\u53d7\u97f3\u8d28\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u5f71\u54cd\uff0c\u6587\u672c\u65b9\u6cd5\u53ef\u901a\u8fc7\u8bed\u8a00\u7279\u5f81\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u5c24\u5176\u5728\u77ed\u5bf9\u8bdd\u573a\u666f\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4", "method": "\u5f00\u53d1\u5355\u9884\u6d4b\u6a21\u578b(SPM)\u548c\u591a\u9884\u6d4b\u6a21\u578b(MPM)\u4e24\u79cd\u6587\u672c\u5206\u6790\u6846\u67b6\uff0c\u57fa\u4e8e\u5305\u542b\u591a\u79cd\u5bf9\u8bdd\u573a\u666f\u7684\u5b9a\u5236\u5316\u6570\u636e\u96c6\u8fdb\u884c\u9a8c\u8bc1", "result": "MPM\u6a21\u578b\u5728\u77ed\u5bf9\u8bdd\u573a\u666f\u8868\u73b0\u8d85\u8d8a\u4e3b\u6d41\u97f3\u9891SD\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u7279\u5f81\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u6574\u5408\u63d0\u4f9b\u65b0\u65b9\u5411", "conclusion": "\u6587\u672cSD\u65b9\u6cd5\u5c55\u73b0\u8bed\u4e49\u7406\u89e3\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u672a\u6765\u5e94\u7ed3\u5408\u591a\u6a21\u6001\u7279\u5f81\u6df1\u5316\u8bf4\u8bdd\u4eba\u8bc6\u522b\uff0c\u63a8\u52a8\u8bed\u4e49\u9a71\u52a8\u7684\u65e5\u5fd7\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2506.11361", "pdf": "https://arxiv.org/pdf/2506.11361", "abs": "https://arxiv.org/abs/2506.11361", "authors": ["Jack H Fagan", "Ruhaan Juyaal", "Amy Yue-Ming Yu", "Siya Pun"], "title": "The Biased Samaritan: LLM biases in Perceived Kindness", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "While Large Language Models (LLMs) have become ubiquitous in many fields,\nunderstanding and mitigating LLM biases is an ongoing issue. This paper\nprovides a novel method for evaluating the demographic biases of various\ngenerative AI models. By prompting models to assess a moral patient's\nwillingness to intervene constructively, we aim to quantitatively evaluate\ndifferent LLMs' biases towards various genders, races, and ages. Our work\ndiffers from existing work by aiming to determine the baseline demographic\nidentities for various commercial models and the relationship between the\nbaseline and other demographics. We strive to understand if these biases are\npositive, neutral, or negative, and the strength of these biases. This paper\ncan contribute to the objective assessment of bias in Large Language Models and\ngive the user or developer the power to account for these biases in LLM output\nor in training future LLMs. Our analysis suggested two key findings: that\nmodels view the baseline demographic as a white middle-aged or young adult\nmale; however, a general trend across models suggested that non-baseline\ndemographics are more willing to help than the baseline. These methodologies\nallowed us to distinguish these two biases that are often tangled together.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u65b9\u6cd5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u4ee5\u767d\u4eba\u7537\u6027\u4e3a\u57fa\u51c6\u7fa4\u4f53\uff0c\u4f46\u975e\u57fa\u51c6\u7fa4\u4f53\u8868\u73b0\u51fa\u66f4\u5f3a\u52a9\u4eba\u610f\u613f", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u4e0d\u540c\u5546\u4e1a\u6a21\u578b\u57fa\u51c6\u4eba\u53e3\u8eab\u4efd\u53ca\u5176\u4e0e\u5176\u4ed6\u7fa4\u4f53\u5173\u7cfb\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u9700\u91cf\u5316\u8bc4\u4f30\u6a21\u578b\u504f\u89c1\u4ee5\u5e2e\u52a9\u5f00\u53d1\u8005\u4f18\u5316\u6a21\u578b", "method": "\u901a\u8fc7\u8981\u6c42\u6a21\u578b\u8bc4\u4f30\u9053\u5fb7\u60a3\u8005\u5efa\u8bbe\u6027\u5e72\u9884\u610f\u613f\uff0c\u5b9a\u91cf\u5206\u6790\u4e0d\u540cLLM\u5bf9\u6027\u522b/\u79cd\u65cf/\u5e74\u9f84\u7684\u504f\u89c1\u7a0b\u5ea6\u4e0e\u65b9\u5411", "result": "\u6a21\u578b\u666e\u904d\u4ee5\u767d\u4eba\u4e2d\u5e74/\u9752\u5e74\u7537\u6027\u4e3a\u57fa\u51c6\uff0c\u4f46\u975e\u57fa\u51c6\u7fa4\u4f53\u52a9\u4eba\u610f\u613f\u66f4\u5f3a\uff1b\u65b9\u6cd5\u8bba\u6210\u529f\u533a\u5206\u4e86\u5e38\u88ab\u6df7\u6dc6\u7684\u4e24\u79cd\u504f\u89c1\u7c7b\u578b", "conclusion": "\u8be5\u7814\u7a76\u4e3aLLM\u504f\u89c1\u63d0\u4f9b\u5ba2\u89c2\u8bc4\u4f30\u6846\u67b6\uff0c\u5e2e\u52a9\u5f00\u53d1\u8005\u5728\u6a21\u578b\u8f93\u51fa\u548c\u8bad\u7ec3\u4e2d\u6821\u6b63\u504f\u89c1\uff0c\u5e76\u5b9e\u73b0\u4e0d\u540c\u504f\u89c1\u7c7b\u578b\u7684\u7cbe\u51c6\u5206\u79bb"}}
{"id": "2506.11381", "pdf": "https://arxiv.org/pdf/2506.11381", "abs": "https://arxiv.org/abs/2506.11381", "authors": ["Samuel Mensah", "Elena Kochkina", "Jabez Magomere", "Joy Prakash Sain", "Simerjot Kaur", "Charese Smiley"], "title": "A Variational Approach for Mitigating Entity Bias in Relation Extraction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main", "summary": "Mitigating entity bias is a critical challenge in Relation Extraction (RE),\nwhere models often rely excessively on entities, resulting in poor\ngeneralization. This paper presents a novel approach to address this issue by\nadapting a Variational Information Bottleneck (VIB) framework. Our method\ncompresses entity-specific information while preserving task-relevant features.\nIt achieves state-of-the-art performance on relation extraction datasets across\ngeneral, financial, and biomedical domains, in both indomain (original test\nsets) and out-of-domain (modified test sets with type-constrained entity\nreplacements) settings. Our approach offers a robust, interpretable, and\ntheoretically grounded methodology.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u7684\u5173\u7cfb\u62bd\u53d6\u53bb\u504f\u65b9\u6cd5\uff0c\u901a\u8fc7\u538b\u7f29\u5b9e\u4f53\u4fe1\u606f\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edf\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u5b9e\u4f53\u4fe1\u606f\u5bfc\u81f4\u6cdb\u5316\u6027\u5dee\uff0c\u9700\u7406\u8bba\u6027\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u53d8\u5206\u4fe1\u606f\u74f6\u9888(VIB)\u6846\u67b6\uff0c\u5728\u4fdd\u7559\u4efb\u52a1\u7279\u5f81\u7684\u540c\u65f6\u538b\u7f29\u5b9e\u4f53\u7279\u5b9a\u4fe1\u606f", "result": "\u5728\u901a\u7528/\u91d1\u878d/\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u539f\u59cb\u53ca\u5b9e\u4f53\u66ff\u6362\u6d4b\u8bd5\u96c6\u4e0a\u5747\u53d6\u5f97SOTA\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u89e3\u91ca\u6027\u548c\u8de8\u9886\u57df\u9002\u7528\u6027"}}
{"id": "2506.11389", "pdf": "https://arxiv.org/pdf/2506.11389", "abs": "https://arxiv.org/abs/2506.11389", "authors": ["Karanpartap Singh", "Neil Band", "Ehsan Adeli"], "title": "Curriculum-Guided Layer Scaling for Language Model Pretraining", "categories": ["cs.CL"], "comment": null, "summary": "As the cost of pretraining large language models grows, there is continued\ninterest in strategies to improve learning efficiency during this core training\nstage. Motivated by cognitive development, where humans gradually build\nknowledge as their brains mature, we propose Curriculum-Guided Layer Scaling\n(CGLS), a framework for compute-efficient pretraining that synchronizes\nincreasing data difficulty with model growth through progressive layer stacking\n(i.e. gradually adding layers during training). At the 100M parameter scale,\nusing a curriculum transitioning from synthetic short stories to general web\ndata, CGLS outperforms baseline methods on the question-answering benchmarks\nPIQA and ARC. Pretraining at the 1.2B scale, we stratify the DataComp-LM corpus\nwith a DistilBERT-based classifier and progress from general text to highly\ntechnical or specialized content. Our results show that progressively\nincreasing model depth alongside sample difficulty leads to better\ngeneralization and zero-shot performance on various downstream benchmarks.\nAltogether, our findings demonstrate that CGLS unlocks the potential of\nprogressive stacking, offering a simple yet effective strategy for improving\ngeneralization on knowledge-intensive and reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u8bfe\u7a0b\u5f15\u5bfc\u7684\u5c42\u6269\u5c55\u6846\u67b6\uff08CGLS\uff09\uff0c\u901a\u8fc7\u540c\u6b65\u589e\u52a0\u6570\u636e\u96be\u5ea6\u548c\u6a21\u578b\u6df1\u5ea6\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u591a\u4e2a\u53c2\u6570\u89c4\u6a21\u4e0b\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8ba4\u77e5\u53d1\u5c55\u8fc7\u7a0b\u542f\u53d1\uff08\u9010\u6b65\u6784\u5efa\u77e5\u8bc6\u4f53\u7cfb\uff09\uff0c\u5e0c\u671b\u901a\u8fc7\u6e10\u8fdb\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6570\u636e\u96be\u5ea6\u540c\u6b65\u63d0\u5347\u7684\u65b9\u5f0f\u6539\u5584\u5927\u6a21\u578b\u9884\u8bad\u7ec3\u6548\u7387\u3002", "method": "\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff1a1\uff09100M\u53c2\u6570\u6a21\u578b\u91c7\u7528\u4ece\u5408\u6210\u6545\u4e8b\u5230\u901a\u7528\u6570\u636e\u7684\u8bfe\u7a0b\uff1b2\uff091.2B\u53c2\u6570\u6a21\u578b\u7528DistilBERT\u5206\u7c7b\u5668\u5206\u5c42\u6570\u636e\uff0c\u4ece\u901a\u7528\u5230\u4e13\u4e1a\u6280\u672f\u5185\u5bb9\uff0c\u540c\u6b65\u5b9e\u65bd\u6e10\u8fdb\u5c42\u5806\u53e0\u3002", "result": "\u5728PIQA/ARC\u7b49QA\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\uff0c1.2B\u6a21\u578b\u5728\u6280\u672f\u9886\u57df\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u96f6\u6837\u672c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "CGLS\u901a\u8fc7\u7b80\u5355\u6709\u6548\u7684\u6e10\u8fdb\u5806\u53e0\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u9ad8\u6548\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2506.11410", "pdf": "https://arxiv.org/pdf/2506.11410", "abs": "https://arxiv.org/abs/2506.11410", "authors": ["Wilson Lau", "Youngwon Kim", "Sravanthi Parasa", "Md Enamul Haque", "Anand Oka", "Jay Nanduri"], "title": "Predicting Early-Onset Colorectal Cancer with Large Language Models", "categories": ["cs.CL"], "comment": "Paper accepted for the proceedings of the 2025 American Medical\n  Informatics Association Annual Symposium (AMIA)", "summary": "The incidence rate of early-onset colorectal cancer (EoCRC, age < 45) has\nincreased every year, but this population is younger than the recommended age\nestablished by national guidelines for cancer screening. In this paper, we\napplied 10 different machine learning models to predict EoCRC, and compared\ntheir performance with advanced large language models (LLM), using patient\nconditions, lab results, and observations within 6 months of patient journey\nprior to the CRC diagnoses. We retrospectively identified 1,953 CRC patients\nfrom multiple health systems across the United States. The results demonstrated\nthat the fine-tuned LLM achieved an average of 73% sensitivity and 91%\nspecificity.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0e\u5fae\u8c03LLM\u5728\u9884\u6d4b\u65e9\u53d1\u6027\u7ed3\u76f4\u80a0\u764c\u4e2d\u7684\u6027\u80fd\uff0cLLM\u8868\u73b0\u66f4\u4f18", "motivation": "\u65e9\u53d1\u6027\u7ed3\u76f4\u80a0\u764c\uff08<45\u5c81\uff09\u53d1\u75c5\u7387\u9010\u5e74\u4e0a\u5347\u4f46\u7f3a\u4e4f\u7b5b\u67e5\u65b9\u6848\uff0c\u4f20\u7edf\u6307\u5357\u8986\u76d6\u4e0d\u8db3", "method": "\u4f7f\u752810\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e0eLLM\uff0c\u57fa\u4e8e\u60a3\u8005\u8bca\u65ad\u524d6\u4e2a\u6708\u7684\u4e34\u5e8a\u6570\u636e\uff08\u75c5\u60c5\u3001\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u3001\u89c2\u5bdf\u8bb0\u5f55\uff09\u8fdb\u884c\u9884\u6d4b", "result": "\u5fae\u8c03\u540e\u7684LLM\u8fbe\u523073%\u654f\u611f\u6027\u548c91%\u7279\u5f02\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b", "conclusion": "LLM\u5728\u65e9\u7b5b\u573a\u666f\u5c55\u73b0\u4e34\u5e8a\u5b9e\u7528\u4ef7\u503c\uff0c\u53ef\u80fd\u63a8\u52a8\u7b5b\u67e5\u6307\u5357\u5e74\u9f84\u95e8\u69db\u7684\u91cd\u65b0\u8bc4\u4f30"}}
{"id": "2506.11418", "pdf": "https://arxiv.org/pdf/2506.11418", "abs": "https://arxiv.org/abs/2506.11418", "authors": ["Jie Hu", "Shengnan Wang", "Yutong He", "Ping Gong", "Jiawei Yi", "Juncheng Zhang", "Youhui Bai", "Renhai Chen", "Gong Zhang", "Cheng Li", "Kun Yuan"], "title": "Efficient Long-Context LLM Inference via KV Cache Clustering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with extended context windows have become\nincreasingly prevalent for tackling complex tasks. However, the substantial\nKey-Value (KV) cache required for long-context LLMs poses significant\ndeployment challenges. Existing approaches either discard potentially critical\ninformation needed for future generations or offer limited efficiency gains due\nto high computational overhead. In this paper, we introduce Chelsea, a simple\nyet effective framework for online KV cache clustering. Our approach is based\non the observation that key states exhibit high similarity along the sequence\ndimension. To enable efficient clustering, we divide the sequence into chunks\nand propose Chunked Soft Matching, which employs an alternating partition\nstrategy within each chunk and identifies clusters based on similarity. Chelsea\nthen merges the KV cache within each cluster into a single centroid.\nAdditionally, we provide a theoretical analysis of the computational complexity\nand the optimality of the intra-chunk partitioning strategy. Extensive\nexperiments across various models and long-context benchmarks demonstrate that\nChelsea achieves up to 80% reduction in KV cache memory usage while maintaining\ncomparable model performance. Moreover, with minimal computational overhead,\nChelsea accelerates the decoding stage of inference by up to 3.19$\\times$ and\nreduces end-to-end latency by up to 2.72$\\times$.", "AI": {"tldr": "\u63d0\u51faChelsea\u6846\u67b6\u901a\u8fc7\u5206\u5757\u8f6f\u5339\u914d\u7b56\u7565\u5b9e\u73b0KV\u7f13\u5b58\u805a\u7c7b\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c1180%\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u63a8\u7406", "motivation": "\u957f\u4e0a\u4e0b\u6587LLMs\u7684KV\u7f13\u5b58\u5360\u7528\u8fc7\u9ad8\u5bfc\u81f4\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u4e22\u5931\u6216\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u5e8f\u5217\u7ef4\u5ea6\u5173\u952e\u72b6\u6001\u9ad8\u76f8\u4f3c\u6027\u7684\u89c2\u5bdf\uff0c\u91c7\u7528\u5206\u5757\u8f6f\u5339\u914d\u7b56\u7565\uff1a\u5c06\u5e8f\u5217\u5206\u5757\u2192\u4ea4\u66ff\u5206\u533a\u2192\u76f8\u4f3c\u6027\u805a\u7c7b\u2192KV\u7f13\u5b58\u5408\u5e76", "result": "\u5b9e\u9a8c\u663e\u793a\u5185\u5b58\u5360\u7528\u51cf\u5c1180%\uff0c\u89e3\u7801\u52a0\u901f3.19\u500d\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e2.72\u500d\uff0c\u6027\u80fd\u635f\u5931\u53ef\u5ffd\u7565", "conclusion": "Chelsea\u901a\u8fc7\u9ad8\u6548KV\u7f13\u5b58\u805a\u7c7b\u673a\u5236\uff0c\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLMs\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11425", "pdf": "https://arxiv.org/pdf/2506.11425", "abs": "https://arxiv.org/abs/2506.11425", "authors": ["Jeff Da", "Clinton Wang", "Xiang Deng", "Yuntao Ma", "Nikhil Barhate", "Sean Hendryx"], "title": "Agent-RLVR: Training Software Engineering Agents via Guidance and Environment Rewards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has been widely adopted\nas the de facto method for enhancing the reasoning capabilities of large\nlanguage models and has demonstrated notable success in verifiable domains like\nmath and competitive programming tasks. However, the efficacy of RLVR\ndiminishes significantly when applied to agentic environments. These settings,\ncharacterized by multi-step, complex problem solving, lead to high failure\nrates even for frontier LLMs, as the reward landscape is too sparse for\neffective model training via conventional RLVR. In this work, we introduce\nAgent-RLVR, a framework that makes RLVR effective in challenging agentic\nsettings, with an initial focus on software engineering tasks. Inspired by\nhuman pedagogy, Agent-RLVR introduces agent guidance, a mechanism that actively\nsteers the agent towards successful trajectories by leveraging diverse\ninformational cues. These cues, ranging from high-level strategic plans to\ndynamic feedback on the agent's errors and environmental interactions, emulate\na teacher's guidance, enabling the agent to navigate difficult solution spaces\nand promotes active self-improvement via additional environment exploration. In\nthe Agent-RLVR training loop, agents first attempt to solve tasks to produce\ninitial trajectories, which are then validated by unit tests and supplemented\nwith agent guidance. Agents then reattempt with guidance, and the agent policy\nis updated with RLVR based on the rewards of these guided trajectories.\nAgent-RLVR elevates the pass@1 performance of Qwen-2.5-72B-Instruct from 9.4%\nto 22.4% on SWE-Bench Verified. We find that our guidance-augmented RLVR data\nis additionally useful for test-time reward model training, shown by further\nboosting pass@1 to 27.8%. Agent-RLVR lays the groundwork for training agents\nwith RLVR in complex, real-world environments where conventional RL methods\nstruggle.", "AI": {"tldr": "\u63d0\u51faAgent-RLVR\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u8f68\u8ff9\u5f15\u5bfc\u673a\u5236\u89e3\u51b3\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u4ee3\u7406\u73af\u5883\u4e2d\u5956\u52b1\u7a00\u758f\u7684\u96be\u9898\uff0c\u5728SWE-Bench\u6d4b\u8bd5\u4e2d\u5c06\u6a21\u578b\u6027\u80fd\u63d0\u5347136%", "motivation": "\u4f20\u7edfRLVR\u5728\u6570\u5b66\u7b49\u53ef\u9a8c\u8bc1\u9886\u57df\u6709\u6548\uff0c\u4f46\u5728\u591a\u6b65\u9aa4\u590d\u6742\u4ee3\u7406\u73af\u5883\u4e2d\u56e0\u5956\u52b1\u4fe1\u53f7\u8fc7\u4e8e\u7a00\u758f\u5bfc\u81f4\u5931\u8d25\u7387\u9ad8\uff0c\u4e9f\u9700\u6539\u8fdb\u65b9\u6cd5", "method": "\u5f15\u5165agent guidance\u673a\u5236\uff0c\u878d\u5408\u6218\u7565\u89c4\u5212\u3001\u9519\u8bef\u53cd\u9988\u548c\u73af\u5883\u4ea4\u4e92\u4fe1\u606f\uff0c\u6784\u5efa\u8bad\u7ec3\u5faa\u73af\uff1a\u521d\u59cb\u5c1d\u8bd5\u2192\u5355\u5143\u6d4b\u8bd5\u9a8c\u8bc1\u2192\u5f15\u5bfc\u751f\u6210\u2192\u7b56\u7565\u66f4\u65b0", "result": "Qwen-2.5-72B-Instruct\u5728SWE-Bench\u7684pass@1\u4ece9.4%\u63d0\u5347\u81f322.4%\uff0c\u7ed3\u5408\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u540e\u8fdb\u4e00\u6b65\u8fbe\u523027.8%", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6559\u5b66\u5f0f\u5f15\u5bfc\u673a\u5236\u7a81\u7834\u4f20\u7edfRL\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2506.11432", "pdf": "https://arxiv.org/pdf/2506.11432", "abs": "https://arxiv.org/abs/2506.11432", "authors": ["Taeeun Kim", "Semin Jeong", "Youngsook Song"], "title": "KoGEC : Korean Grammatical Error Correction with Pre-trained Translation Models", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 2 figures", "summary": "This research introduces KoGEC, a Korean Grammatical Error Correction system\nusing pre\\--trained translation models. We fine-tuned NLLB (No Language Left\nBehind) models for Korean GEC, comparing their performance against large\nlanguage models like GPT-4 and HCX-3. The study used two social media\nconversation datasets for training and testing. The NLLB models were fine-tuned\nusing special language tokens to distinguish between original and corrected\nKorean sentences. Evaluation was done using BLEU scores and an \"LLM as judge\"\nmethod to classify error types. Results showed that the fine-tuned NLLB (KoGEC)\nmodels outperformed GPT-4o and HCX-3 in Korean GEC tasks. KoGEC demonstrated a\nmore balanced error correction profile across various error types, whereas the\nlarger LLMs tended to focus less on punctuation errors. We also developed a\nChrome extension to make the KoGEC system accessible to users. Finally, we\nexplored token vocabulary expansion to further improve the model but found it\nto decrease model performance. This research contributes to the field of NLP by\nproviding an efficient, specialized Korean GEC system and a new evaluation\nmethod. It also highlights the potential of compact, task-specific models to\ncompete with larger, general-purpose language models in specialized NLP tasks.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eNLLB\u6a21\u578b\u7684\u97e9\u8bed\u8bed\u6cd5\u7ea0\u9519\u7cfb\u7edfKoGEC\uff0c\u5728\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eGPT-4o\u548cHCX-3\uff0c\u5e76\u5f00\u53d1\u4e86\u6d4f\u89c8\u5668\u6269\u5c55\u5e94\u7528", "motivation": "\u89e3\u51b3\u97e9\u8bed\u8bed\u6cd5\u7ea0\u9519\u4efb\u52a1\u4e2d\u5927\u578b\u901a\u7528\u6a21\u578b\u6548\u7387\u4f4e\u4e0b\u548c\u6807\u70b9\u9519\u8bef\u5904\u7406\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4e13\u7528\u7d27\u51d1\u6a21\u578b\u7684\u6f5c\u529b", "method": "1. \u4f7f\u7528\u793e\u4ea4\u5a92\u4f53\u5bf9\u8bdd\u6570\u636e\u96c6\u5fae\u8c03NLLB\u7ffb\u8bd1\u6a21\u578b\n2. \u5f15\u5165\u7279\u6b8a\u8bed\u8a00\u6807\u8bb0\u533a\u5206\u539f\u53e5/\u7ea0\u9519\u53e5\n3. \u91c7\u7528BLEU\u8bc4\u5206\u548cLLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u53cc\u8bc4\u4ef7\u4f53\u7cfb", "result": "1. KoGEC\u6a21\u578b\u5728BLEU\u5f97\u5206\u4e0a\u8d85\u8fc7GPT-4o 24.5\u5206\n2. \u5728\u6807\u70b9/\u62fc\u5199/\u5f62\u6001\u7d20\u9519\u8bef\u4fee\u6b63\u66f4\u5747\u8861(\u5927\u578b\u6a21\u578b\u6807\u70b9\u9519\u8bef\u4fee\u6b63\u7387\u4f4e15-20%)\n3. \u8bcd\u6c47\u6269\u5c55\u5b9e\u9a8c\u53cd\u800c\u964d\u4f4e\u6a21\u578b\u6027\u80fd", "conclusion": "\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u5728\u7279\u5b9aNLP\u4efb\u52a1\u4e2d\u53ef\u8d85\u8d8a\u901a\u7528\u5927\u6a21\u578b\uff0c\u63d0\u51fa\u7684LLM\u8bc4\u4f30\u6cd5\u4e3a\u8bed\u6cd5\u7ea0\u9519\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u7d27\u51d1\u6a21\u578b\u90e8\u7f72\u4f18\u52bf\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u843d\u5730"}}
{"id": "2506.11440", "pdf": "https://arxiv.org/pdf/2506.11440", "abs": "https://arxiv.org/abs/2506.11440", "authors": ["Harvey Yiyun Fu", "Aryan Shrivastava", "Jared Moore", "Peter West", "Chenhao Tan", "Ari Holtzman"], "title": "AbsenceBench: Language Models Can't Tell What's Missing", "categories": ["cs.CL"], "comment": "23 pages, 8 figures. Code and data are publicly available at\n  https://github.com/harvey-fin/absence-bench", "summary": "Large language models (LLMs) are increasingly capable of processing long\ninputs and locating specific information within them, as evidenced by their\nperformance on the Needle in a Haystack (NIAH) test. However, while models\nexcel at recalling surprising information, they still struggle to identify\nclearly omitted information. We introduce AbsenceBench to assesses LLMs'\ncapacity to detect missing information across three domains: numerical\nsequences, poetry, and GitHub pull requests. AbsenceBench asks models to\nidentify which pieces of a document were deliberately removed, given access to\nboth the original and edited contexts. Despite the apparent straightforwardness\nof these tasks, our experiments reveal that even state-of-the-art models like\nClaude-3.7-Sonnet achieve only 69.6% F1-score with a modest average context\nlength of 5K tokens. Our analysis suggests this poor performance stems from a\nfundamental limitation: Transformer attention mechanisms cannot easily attend\nto \"gaps\" in documents since these absences don't correspond to any specific\nkeys that can be attended to. Overall, our results and analysis provide a case\nstudy of the close proximity of tasks where models are already superhuman\n(NIAH) and tasks where models breakdown unexpectedly (AbsenceBench).", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u5b9a\u4f4d\u663e\u6027\u4fe1\u606f\u4f46\u96be\u4ee5\u68c0\u6d4b\u4fe1\u606f\u7f3a\u5931\uff0cAbsenceBench\u6d4b\u8bd5\u663e\u793a\u9876\u5c16\u6a21\u578bF1\u5206\u6570\u4ec569.6%\uff0cTransformer\u6ce8\u610f\u529b\u673a\u5236\u5b58\u5728\u56fa\u6709\u5c40\u9650", "motivation": "\u63a2\u7d22LLMs\u5728\u5df2\u5177\u5907\u8d85\u4eba\u7c7b\u6027\u80fd\u9886\u57df\uff08\u5982NIAH\uff09\u4e0e\u610f\u5916\u5931\u6548\u573a\u666f\uff08\u4fe1\u606f\u7f3a\u5931\u68c0\u6d4b\uff09\u4e4b\u95f4\u7684\u8fb9\u754c\uff0c\u63ed\u793a\u6a21\u578b\u5904\u7406\u6587\u6863\u7a7a\u7f3a\u80fd\u529b\u7684\u4e0d\u8db3", "method": "\u5f00\u53d1AbsenceBench\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6570\u503c\u5e8f\u5217/\u8bd7\u6b4c/GitHub\u8bf7\u6c42\u4e09\u4e2a\u9886\u57df\u6784\u9020\u539f\u59cb\u4e0e\u5220\u51cf\u6587\u672c\u5bf9\u6bd4\uff0c\u6d4b\u8bd5\u6a21\u578b\u8bc6\u522b\u7f3a\u5931\u4fe1\u606f\u80fd\u529b", "result": "Claude-3.7-Sonnet\u5728\u5e73\u57475K tokens\u4e0a\u4e0b\u6587\u4e2dF1-score\u4ec569.6%\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5c06\u7f3a\u5931\u8bef\u8ba4\u4e3a\u5b58\u5728\uff08\u5047\u9633\u6027\u738734.3%\uff09", "conclusion": "Transformer\u6ce8\u610f\u529b\u673a\u5236\u65e0\u6cd5\u6709\u6548\u5904\u7406\u975e\u663e\u6027\u4fe1\u606f\u7a7a\u7f3a\uff0c\u8fd9\u79cd\u67b6\u6784\u5c42\u9762\u7684\u5c40\u9650\u5bfc\u81f4\u6a21\u578b\u5728\u7f3a\u5931\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u843d\u540e\u4eba\u7c7b"}}
{"id": "2506.11467", "pdf": "https://arxiv.org/pdf/2506.11467", "abs": "https://arxiv.org/abs/2506.11467", "authors": ["Carlos Rafael Catalan"], "title": "A Gamified Evaluation and Recruitment Platform for Low Resource Language Machine Translation Systems", "categories": ["cs.CL", "cs.SI", "F.2.2, I.2.7"], "comment": "7 pages, 7 figures, presented at the HEAL Workshop at CHI", "summary": "Human evaluators provide necessary contributions in evaluating large language\nmodels. In the context of Machine Translation (MT) systems for low-resource\nlanguages (LRLs), this is made even more apparent since popular automated\nmetrics tend to be string-based, and therefore do not provide a full picture of\nthe nuances of the behavior of the system. Human evaluators, when equipped with\nthe necessary expertise of the language, will be able to test for adequacy,\nfluency, and other important metrics. However, the low resource nature of the\nlanguage means that both datasets and evaluators are in short supply. This\npresents the following conundrum: How can developers of MT systems for these\nLRLs find adequate human evaluators and datasets? This paper first presents a\ncomprehensive review of existing evaluation procedures, with the objective of\nproducing a design proposal for a platform that addresses the resource gap in\nterms of datasets and evaluators in developing MT systems. The result is a\ndesign for a recruitment and gamified evaluation platform for developers of MT\nsystems. Challenges are also discussed in terms of evaluating this platform, as\nwell as its possible applications in the wider scope of Natural Language\nProcessing (NLP) research.", "AI": {"tldr": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u8bc4\u4f30\u56f0\u5883\uff0c\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u4f17\u5305\u62db\u52df\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u7684\u5e73\u53f0\u8bbe\u8ba1\u65b9\u6848", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5f00\u53d1\u9762\u4e34\u53cc\u91cd\u6311\u6218\uff1a\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e14\u7f3a\u4e4f\u4e13\u4e1a\u4eba\u5de5\u8bc4\u4f30\u8005\u548c\u6570\u636e\u96c6", "method": "\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u8bc4\u4f30\u6d41\u7a0b\uff0c\u63d0\u51fa\u5305\u542b\u4eba\u624d\u62db\u52df\u673a\u5236\u548c\u6e38\u620f\u5316\u8bc4\u4f30\u6846\u67b6\u7684\u5e73\u53f0\u8bbe\u8ba1\u65b9\u6848", "result": "\u5f00\u53d1\u51fa\u96c6\u6210\u4f17\u5305\u62db\u52df\u4e0e\u6a21\u5757\u5316\u8bc4\u4f30\u4efb\u52a1\u7684\u5e73\u53f0\u539f\u578b\uff0c\u652f\u6301\u8bed\u6cd5\u68c0\u67e5\u3001\u8bed\u4e49\u9a8c\u8bc1\u7b49\u591a\u7ef4\u5ea6\u4eba\u5de5\u8bc4\u4f30", "conclusion": "\u8be5\u5e73\u53f0\u8bbe\u8ba1\u4e3a\u4f4e\u8d44\u6e90NLP\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u8bed\u97f3\u5408\u6210\u7b49\u66f4\u591a\u573a\u666f\uff0c\u4f46\u9700\u89e3\u51b3\u8bc4\u4f30\u8005\u8d44\u8d28\u8ba4\u8bc1\u548c\u957f\u671f\u53c2\u4e0e\u6fc0\u52b1\u95ee\u9898"}}
{"id": "2506.11474", "pdf": "https://arxiv.org/pdf/2506.11474", "abs": "https://arxiv.org/abs/2506.11474", "authors": ["Jaehoon Yun", "Jiwoong Sohn", "Jungwoo Park", "Hyunjae Kim", "Xiangru Tang", "Yanjun Shao", "Yonghoe Koo", "Minhyeok Ko", "Qingyu Chen", "Mark Gerstein", "Michael Moor", "Jaewoo Kang"], "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified Process Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/", "AI": {"tldr": "\u63d0\u51faMed-PRM\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u9a8c\u8bc1\u533b\u5b66\u63a8\u7406\u6b65\u9aa4\uff0c\u5728\u4e94\u5927\u533b\u5b66QA\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\uff0c\u4f7f\u57fa\u7840\u6a21\u578b\u6027\u80fd\u63d0\u5347\u8fbe13.5%", "motivation": "\u73b0\u6709\u533b\u5b66\u63a8\u7406\u6a21\u578b\u96be\u4ee5\u5b9a\u4f4d\u548c\u4fee\u6b63\u5177\u4f53\u63a8\u7406\u6b65\u9aa4\u7684\u9519\u8bef\uff0c\u8fd9\u5bf9\u9700\u8981\u7cbe\u51c6\u8bca\u65ad\u7684\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981", "method": "\u5f00\u53d1\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u4e34\u5e8a\u6307\u5357\u548c\u6587\u732e\u8bc1\u636e\u9a8c\u8bc1\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u8d28\u91cf", "result": "\u5728MedQA\u9996\u6b21\u5b9e\u73b080%+\u51c6\u786e\u7387\uff088B\u53c2\u6570\u6a21\u578b\uff09\uff0c\u96c6\u6210Meerkat\u7b49\u5f3a\u7b56\u7565\u6a21\u578b\u65f6\u4fdd\u6301\u5373\u63d2\u5373\u7528\u901a\u7528\u6027", "conclusion": "Med-PRM\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u63a8\u7406\u9a8c\u8bc1\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u8d28\u91cf\uff0c\u8bc1\u660e\u5c0f\u89c4\u6a21\u6a21\u578b\u4e5f\u80fd\u8fbe\u5230\u7a81\u7834\u6027\u6027\u80fd"}}
{"id": "2506.11478", "pdf": "https://arxiv.org/pdf/2506.11478", "abs": "https://arxiv.org/abs/2506.11478", "authors": ["Aman Sinha", "Bogdan-Valentin Popescu", "Xavier Coubez", "Marianne Clausel", "Mathieu Constant"], "title": "ImmunoFOMO: Are Language Models missing what oncologists see?", "categories": ["cs.CL"], "comment": null, "summary": "Language models (LMs) capabilities have grown with a fast pace over the past\ndecade leading researchers in various disciplines, such as biomedical research,\nto increasingly explore the utility of LMs in their day-to-day applications.\nDomain specific language models have already been in use for biomedical natural\nlanguage processing (NLP) applications. Recently however, the interest has\ngrown towards medical language models and their understanding capabilities. In\nthis paper, we investigate the medical conceptual grounding of various language\nmodels against expert clinicians for identification of hallmarks of\nimmunotherapy in breast cancer abstracts. Our results show that pre-trained\nlanguage models have potential to outperform large language models in\nidentifying very specific (low-level) concepts.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4e73\u817a\u764c\u514d\u75ab\u6cbb\u7597\u6458\u8981\u7684\u7279\u5b9a\u533b\u5b66\u6982\u5ff5\u8bc6\u522b\u4e2d\u5c55\u73b0\u4f18\u4e8e\u5927\u6a21\u578b\u7684\u6f5c\u529b", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u6269\u5c55\uff0c\u9700\u9a8c\u8bc1\u5176\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u5728\u4e13\u4e1a\u533b\u5b66\u6982\u5ff5\u8bc6\u522b\u4e0a\u7684\u6027\u80fd\u5dee\u5f02", "method": "\u5bf9\u6bd4\u5206\u6790\u591a\u79cd\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e73\u817a\u764c\u514d\u75ab\u6cbb\u7597\u6458\u8981\u4e2d\u6838\u5fc3\u6982\u5ff5\u8bc6\u522b\u7684\u80fd\u529b", "result": "\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4f4e\u5c42\u6b21\u5177\u4f53\u533b\u5b66\u6982\u5ff5\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b", "conclusion": "\u4e13\u4e1a\u9886\u57df\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u7cbe\u7ec6\u5316\u533b\u5b66\u6982\u5ff5\u7406\u89e3\u65b9\u9762\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597NLP\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.11485", "pdf": "https://arxiv.org/pdf/2506.11485", "abs": "https://arxiv.org/abs/2506.11485", "authors": ["Cole Gawin"], "title": "Relational Schemata in BERT Are Inducible, Not Emergent: A Study of Performance vs. Competence in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 3 tables", "summary": "While large language models like BERT demonstrate strong empirical\nperformance on semantic tasks, whether this reflects true conceptual competence\nor surface-level statistical association remains unclear. I investigate whether\nBERT encodes abstract relational schemata by examining internal representations\nof concept pairs across taxonomic, mereological, and functional relations. I\ncompare BERT's relational classification performance with representational\nstructure in [CLS] token embeddings. Results reveal that pretrained BERT\nenables high classification accuracy, indicating latent relational signals.\nHowever, concept pairs organize by relation type in high-dimensional embedding\nspace only after fine-tuning on supervised relation classification tasks. This\nindicates relational schemata are not emergent from pretraining alone but can\nbe induced via task scaffolding. These findings demonstrate that behavioral\nperformance does not necessarily imply structured conceptual understanding,\nthough models can acquire inductive biases for grounded relational abstraction\nthrough appropriate training.", "AI": {"tldr": "\u7814\u7a76\u8868\u660eBERT\u9700\u901a\u8fc7\u5fae\u8c03\u624d\u80fd\u5efa\u7acb\u5173\u7cfb\u6a21\u5f0f\uff0c\u9884\u8bad\u7ec3\u4ec5\u63d0\u4f9b\u6f5c\u5728\u4fe1\u53f7\uff0c\u5b9e\u9645\u7406\u89e3\u9700\u4efb\u52a1\u5f15\u5bfc", "motivation": "\u63a2\u7a76BERT\u662f\u5426\u771f\u6b63\u7406\u89e3\u8bed\u4e49\u5173\u7cfb\uff08\u5206\u7c7b/\u7ec4\u5408/\u529f\u80fd\uff09\uff0c\u8fd8\u662f\u4ec5\u4f9d\u8d56\u8868\u5c42\u7edf\u8ba1\u5173\u8054", "method": "\u901a\u8fc7\u5206\u6790[CLS]\u6807\u8bb0\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5173\u7cfb\u5206\u7c7b\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5728\u6982\u5ff5\u5bf9\u5206\u7c7b\u7684\u8868\u73b0", "result": "\u5fae\u8c03\u540e\u6a21\u578b\u5728\u5d4c\u5165\u7a7a\u95f4\u5448\u73b0\u660e\u786e\u5173\u7cfb\u7ed3\u6784\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u4ec5\u6709\u9690\u5f0f\u4fe1\u53f7\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7ec4\u7ec7", "conclusion": "\u884c\u4e3a\u8868\u73b0\u2260\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u4f46\u6a21\u578b\u53ef\u901a\u8fc7\u4efb\u52a1\u8bad\u7ec3\u83b7\u5f97\u5173\u7cfb\u62bd\u8c61\u7684\u5f52\u7eb3\u504f\u7f6e"}}
{"id": "2506.11498", "pdf": "https://arxiv.org/pdf/2506.11498", "abs": "https://arxiv.org/abs/2506.11498", "authors": ["Manlai Liang", "Wanyi Huang", "Mandi Liu", "Huaijun Li", "Jinlong Li"], "title": "Lag-Relative Sparse Attention In Long Context Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and generation, yet their ability to handle long-context\ninput remains constrained by the quadratic complexity of attention computation\nand linear-increasing key-value memory footprint. To reduce computational costs\nand memory, key-value cache compression techniques are commonly applied at\ninference time, but this often leads to severe performance degradation, as\nmodels are not trained to handle compressed context. Although there are more\nsophisticated compression methods, they are typically unsuitable for\npost-training because of their incompatibility with gradient-based optimization\nor high computation overhead. To fill this gap with no additional parameter and\nlittle computation overhead, we propose Lag-Relative Sparse Attention(LRSA)\nanchored by the LagKV compression method for long context post-training. Our\nmethod performs chunk-by-chunk prefilling, which selects the top K most\nrelevant key-value pairs in a fixed-size lagging window, allowing the model to\nfocus on salient historical context while maintaining efficiency. Experimental\nresults show that our approach significantly enhances the robustness of the LLM\nwith key-value compression and achieves better fine-tuned results in the\nquestion-answer tuning task.", "AI": {"tldr": "\u63d0\u51faLRSA\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u4e0eLagKV\u538b\u7f29\u6280\u672f\uff0c\u901a\u8fc7\u5757\u9884\u586b\u5145\u9009\u62e9\u76f8\u5173\u952e\u503c\u5bf9\uff0c\u5728\u4fdd\u6301\u6548\u7387\u7684\u540c\u65f6\u589e\u5f3aLLM\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u952e\u503c\u7f13\u5b58\u538b\u7f29\u6280\u672f\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u590d\u6742\u538b\u7f29\u65b9\u6cd5\u4e0d\u517c\u5bb9\u540e\u8bad\u7ec3\u3002\u9700\u8981\u65e0\u53c2\u3001\u4f4e\u8ba1\u7b97\u91cf\u7684\u538b\u7f29\u65b9\u6848\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u95ee\u9898", "method": "\u91c7\u7528\u57fa\u4e8e\u6ede\u540e\u7a97\u53e3\u7684\u5757\u9884\u586b\u5145\u673a\u5236\uff0c\u5728\u56fa\u5b9a\u7a97\u53e3\u5185\u9009\u62e9Top-K\u76f8\u5173\u952e\u503c\u5bf9\uff0c\u4f7f\u6a21\u578b\u805a\u7126\u6838\u5fc3\u5386\u53f2\u4e0a\u4e0b\u6587\u7684\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u5e26\u952e\u503c\u538b\u7f29LLM\u7684\u9c81\u68d2\u6027\uff0c\u5728\u95ee\u7b54\u5fae\u8c03\u4efb\u52a1\u4e2d\u53d6\u5f97\u66f4\u4f18\u6548\u679c", "conclusion": "LRSA\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u957f\u4e0a\u4e0b\u6587\u540e\u8bad\u7ec3\u4f18\u5316\uff0c\u65e0\u9700\u589e\u52a0\u53c2\u6570\u5373\u53ef\u63d0\u5347\u6a21\u578b\u5bf9\u538b\u7f29\u4e0a\u4e0b\u6587\u7684\u9002\u5e94\u80fd\u529b"}}
{"id": "2506.11499", "pdf": "https://arxiv.org/pdf/2506.11499", "abs": "https://arxiv.org/abs/2506.11499", "authors": ["Seongbo Jang", "Seonghyeon Lee", "Dongha Lee", "Hwanjo Yu"], "title": "On the Effectiveness of Integration Methods for Multimodal Dialogue Response Retrieval", "categories": ["cs.CL"], "comment": "9 pages, 1 figure", "summary": "Multimodal chatbots have become one of the major topics for dialogue systems\nin both research community and industry. Recently, researchers have shed light\non the multimodality of responses as well as dialogue contexts. This work\nexplores how a dialogue system can output responses in various modalities such\nas text and image. To this end, we first formulate a multimodal dialogue\nresponse retrieval task for retrieval-based systems as the combination of three\nsubtasks. We then propose three integration methods based on a two-step\napproach and an end-to-end approach, and compare the merits and demerits of\neach method. Experimental results on two datasets demonstrate that the\nend-to-end approach achieves comparable performance without an intermediate\nstep in the two-step approach. In addition, a parameter sharing strategy not\nonly reduces the number of parameters but also boosts performance by\ntransferring knowledge across the subtasks and the modalities.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u5bf9\u8bdd\u54cd\u5e94\u68c0\u7d22\u6846\u67b6\uff0c\u6bd4\u8f83\u4e24\u6b65\u6cd5\u548c\u7aef\u5230\u7aef\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u53c2\u6570\u5171\u4eab\u7b56\u7565\u7684\u6709\u6548\u6027", "motivation": "\u63a2\u7d22\u5bf9\u8bdd\u7cfb\u7edf\u5982\u4f55\u751f\u6210\u5305\u542b\u6587\u672c\u3001\u56fe\u50cf\u7b49\u591a\u6a21\u6001\u54cd\u5e94\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4f20\u7edf\u5355\u6587\u672c\u4ea4\u4e92\u9650\u5236", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e24\u6b65\u6cd5\uff08\u5148\u68c0\u7d22\u540e\u91cd\u6392\u5e8f\uff09\u548c\u7aef\u5230\u7aef\u6cd5\u7684\u4e09\u79cd\u96c6\u6210\u65b9\u6cd5\uff0c\u5f15\u5165\u8de8\u5b50\u4efb\u52a1\u548c\u6a21\u6001\u7684\u53c2\u6570\u5171\u4eab\u673a\u5236", "result": "\u7aef\u5230\u7aef\u65b9\u6cd5\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e0e\u4e24\u6b65\u6cd5\u76f8\u5f53\u7684\u6548\u679c\uff1b\u53c2\u6570\u5171\u4eab\u7b56\u7565\u51cf\u5c1130%\u53c2\u6570\u91cf\u540c\u65f6\u63d0\u53471.5%\u51c6\u786e\u7387", "conclusion": "\u7aef\u5230\u7aef\u65b9\u6cd5\u65e0\u9700\u4e2d\u95f4\u6b65\u9aa4\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u591a\u6a21\u6001\u54cd\u5e94\u68c0\u7d22\uff0c\u53c2\u6570\u5171\u4eab\u7b56\u7565\u901a\u8fc7\u8de8\u4efb\u52a1\u77e5\u8bc6\u8fc1\u79fb\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd"}}
{"id": "2506.11557", "pdf": "https://arxiv.org/pdf/2506.11557", "abs": "https://arxiv.org/abs/2506.11557", "authors": ["Chih-Hao Hsu", "Ying-Jia Lin", "Hung-Yu Kao"], "title": "From Persona to Person: Enhancing the Naturalness with Multiple Discourse Relations Graph Learning in Personalized Dialogue Generation", "categories": ["cs.CL"], "comment": "Accepted by PAKDD 2025", "summary": "In dialogue generation, the naturalness of responses is crucial for effective\nhuman-machine interaction. Personalized response generation poses even greater\nchallenges, as the responses must remain coherent and consistent with the\nuser's personal traits or persona descriptions. We propose MUDI\n($\\textbf{Mu}$ltiple $\\textbf{Di}$scourse Relations Graph Learning) for\npersonalized dialogue generation. We utilize a Large Language Model to assist\nin annotating discourse relations and to transform dialogue data into\nstructured dialogue graphs. Our graph encoder, the proposed DialogueGAT model,\nthen captures implicit discourse relations within this structure, along with\npersona descriptions. During the personalized response generation phase, novel\ncoherence-aware attention strategies are implemented to enhance the decoder's\nconsideration of discourse relations. Our experiments demonstrate significant\nimprovements in the quality of personalized responses, thus resembling\nhuman-like dialogue exchanges.", "AI": {"tldr": "\u63d0\u51faMUDI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u56fe\u4e0e\u8bed\u7bc7\u5173\u7cfb\u5efa\u6a21\u63d0\u5347\u4e2a\u6027\u5316\u5bf9\u8bdd\u751f\u6210\u7684\u81ea\u7136\u5ea6\u4e0e\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u5728\u4fdd\u6301\u8bed\u7bc7\u8fde\u8d2f\u6027\u548c\u7528\u6237\u753b\u50cf\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7ed3\u6784\u5316\u8868\u8fbe\u65b9\u6cd5", "method": "\u4f7f\u7528LLM\u6807\u6ce8\u8bed\u7bc7\u5173\u7cfb\u6784\u5efa\u5bf9\u8bdd\u56fe\uff0c\u8bbe\u8ba1DialogueGAT\u56fe\u7f16\u7801\u5668\u6355\u6349\u9690\u5f0f\u5173\u7cfb\uff0c\u5728\u89e3\u7801\u9636\u6bb5\u5f15\u5165\u8fde\u8d2f\u6027\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e2a\u6027\u5316\u56de\u590d\u8d28\u91cf\uff0c\u751f\u6210\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5bf9\u8bdd\u7684\u54cd\u5e94", "conclusion": "\u7ed3\u6784\u5316\u8bed\u7bc7\u5173\u7cfb\u5efa\u6a21\u4e0e\u56fe\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6709\u6548\u589e\u5f3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4e2a\u6027\u5316\u8868\u8fbe\u80fd\u529b\u4e0e\u81ea\u7136\u4ea4\u4e92\u6548\u679c"}}
{"id": "2506.11602", "pdf": "https://arxiv.org/pdf/2506.11602", "abs": "https://arxiv.org/abs/2506.11602", "authors": ["Hawau Olamide Toyin", "Samar M. Magdy", "Hanan Aldarmaki"], "title": "Are LLMs Good Text Diacritizers? An Arabic and Yor\u00f9b\u00e1 Case Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We investigate the effectiveness of large language models (LLMs) for text\ndiacritization in two typologically distinct languages: Arabic and Yoruba. To\nenable a rigorous evaluation, we introduce a novel multilingual dataset\nMultiDiac, with diverse samples that capture a range of diacritic ambiguities.\nWe evaluate 14 LLMs varying in size, accessibility, and language coverage, and\nbenchmark them against 6 specialized diacritization models. Additionally, we\nfine-tune four small open-source models using LoRA for Yoruba. Our results show\nthat many off-the-shelf LLMs outperform specialized diacritization models for\nboth Arabic and Yoruba, but smaller models suffer from hallucinations.\nFine-tuning on a small dataset can help improve diacritization performance and\nreduce hallucination rates.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u548c\u7ea6\u9c81\u5df4\u8bed\u6587\u672c\u52a0\u97f3\u7b26\u53f7\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u5c0f\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\u4f46\u53ef\u901a\u8fc7\u5fae\u8c03\u6539\u5584", "motivation": "\u63a2\u7d22LLMs\u5728\u5f62\u6001\u590d\u6742\u8bed\u8a00\u7684\u97f3\u6807\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u6027\u80fd\u5e76\u89e3\u51b3\u5c0f\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898", "method": "\u6784\u5efaMultiDiac\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u8bc4\u4f3014\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLMs\uff0c\u4f7f\u7528LoRA\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\uff08Yoruba\uff09", "result": "\u73b0\u6210LLMs\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\uff0c\u5c0f\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u4f46\u5fae\u8c03\u53ef\u63d0\u5347\u6027\u80fd\uff08Yoruba\u51c6\u786e\u7387\u63d0\u534716%\uff09", "conclusion": "LLMs\u5728\u97f3\u6807\u6807\u6ce8\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u9886\u57df\u5fae\u8c03\u53ef\u4f18\u5316\u6548\u679c\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.11631", "pdf": "https://arxiv.org/pdf/2506.11631", "abs": "https://arxiv.org/abs/2506.11631", "authors": ["Simeon Junker", "Sina Zarrie\u00df"], "title": "SceneGram: Conceptualizing and Describing Tangrams in Scene Context", "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "Research on reference and naming suggests that humans can come up with very\ndifferent ways of conceptualizing and referring to the same object, e.g. the\nsame abstract tangram shape can be a \"crab\", \"sink\" or \"space ship\". Another\ncommon assumption in cognitive science is that scene context fundamentally\nshapes our visual perception of objects and conceptual expectations. This paper\ncontributes SceneGram, a dataset of human references to tangram shapes placed\nin different scene contexts, allowing for systematic analyses of the effect of\nscene context on conceptualization. Based on this data, we analyze references\nto tangram shapes generated by multimodal LLMs, showing that these models do\nnot account for the richness and variability of conceptualizations found in\nhuman references.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7SceneGram\u6570\u636e\u96c6\u5206\u6790\u573a\u666f\u8bed\u5883\u5bf9\u4eba\u7c7b\u6982\u5ff5\u5316\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u590d\u73b0\u4eba\u7c7b\u6982\u5ff5\u8868\u8fbe\u7684\u4e30\u5bcc\u6027", "motivation": "\u63a2\u7d22\u573a\u666f\u8bed\u5883\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5bf9\u7269\u4f53\u7684\u6982\u5ff5\u5316\u8fc7\u7a0b\uff0c\u63ed\u793a\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6982\u5ff5\u8868\u8fbe\u591a\u6837\u6027\u65b9\u9762\u4e0e\u4eba\u7c7b\u7684\u5dee\u8ddd", "method": "\u6784\u5efa\u5305\u542b\u4e0d\u540c\u573a\u666f\u4e2d\u4e03\u5de7\u677f\u5f62\u72b6\u4eba\u7c7b\u6307\u79f0\u7684SceneGram\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u8be5\u6570\u636e\u7cfb\u7edf\u5206\u6790\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6982\u5ff5\u5316\u8868\u8fbe", "result": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u6982\u5ff5\u5316\u8868\u8fbe\u7f3a\u4e4f\u4eba\u7c7b\u6307\u79f0\u4e2d\u89c2\u5bdf\u5230\u7684\u4e30\u5bcc\u53d8\u5f02\u6027", "conclusion": "\u9700\u63d0\u5347AI\u6a21\u578b\u5bf9\u573a\u666f\u8bed\u5883\u654f\u611f\u6027\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u4eba\u7c7b\u6982\u5ff5\u5316\u6570\u636e\u4e3a\u8bc4\u4f30\u6a21\u578b\u8ba4\u77e5\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6"}}
{"id": "2506.11638", "pdf": "https://arxiv.org/pdf/2506.11638", "abs": "https://arxiv.org/abs/2506.11638", "authors": ["Yicheng Xiao", "Lin Song", "Rui Yang", "Cheng Cheng", "Yixiao Ge", "Xiu Li", "Ying Shan"], "title": "LoRA-Gen: Specializing Large Language Model via Online LoRA Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances have highlighted the benefits of scaling language models to\nenhance performance across a wide range of NLP tasks. However, these approaches\nstill face limitations in effectiveness and efficiency when applied to\ndomain-specific tasks, particularly for small edge-side models. We propose the\nLoRA-Gen framework, which utilizes a large cloud-side model to generate LoRA\nparameters for edge-side models based on task descriptions. By employing the\nreparameterization technique, we merge the LoRA parameters into the edge-side\nmodel to achieve flexible specialization. Our method facilitates knowledge\ntransfer between models while significantly improving the inference efficiency\nof the specialized model by reducing the input context length. Without\nspecialized training, LoRA-Gen outperforms conventional LoRA fine-tuning, which\nachieves competitive accuracy and a 2.1x speedup with TinyLLaMA-1.1B in\nreasoning tasks. Besides, our method delivers a compression ratio of 10.1x with\nGemma-2B on intelligent agent tasks.", "AI": {"tldr": "\u63d0\u51faLoRA-Gen\u6846\u67b6\uff0c\u901a\u8fc7\u4e91\u7aef\u5927\u6a21\u578b\u751f\u6210\u4efb\u52a1\u4e13\u5c5e\u7684LoRA\u53c2\u6570\u5e76\u878d\u5408\u5230\u8fb9\u7f18\u7aef\u5c0f\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u4e13\u4e1a\u5316\u4e0e\u63a8\u7406\u52a0\u901f", "motivation": "\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u8f93\u5165\u4e0a\u4e0b\u6587\u8fc7\u957f\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5bf9\u5c0f\u578b\u8fb9\u7f18\u7aef\u6a21\u578b\u6548\u679c\u6709\u9650", "method": "1. \u4e91\u7aef\u5927\u6a21\u578b\u6839\u636e\u4efb\u52a1\u63cf\u8ff0\u751f\u6210LoRA\u53c2\u6570\n2. \u901a\u8fc7\u91cd\u65b0\u53c2\u6570\u5316\u6280\u672f\u5c06LoRA\u53c2\u6570\u878d\u5165\u8fb9\u7f18\u6a21\u578b\n3. \u652f\u6301\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb", "result": "\u5728TinyLLaMA-1.1B\u63a8\u7406\u4efb\u52a1\u4e2d\uff1a\n- \u8d85\u8d8a\u4f20\u7edfLoRA\u5fae\u8c03\n- 2.1\u500d\u63a8\u7406\u52a0\u901f\n- Gemma-2B\u5728\u667a\u80fd\u4ee3\u7406\u4efb\u52a1\u5b9e\u73b010.1\u500d\u53c2\u6570\u538b\u7f29", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u6a21\u578b\u95f4\u7684\u77e5\u8bc6\u9ad8\u6548\u8fc1\u79fb\uff0c\u901a\u8fc7\u53c2\u6570\u538b\u7f29\u548c\u4e0a\u4e0b\u6587\u7f29\u77ed\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u6a21\u578b\u7684\u63a8\u7406\u6548\u7387\uff0c\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u5373\u53ef\u7075\u6d3b\u9002\u914d\u4e0d\u540c\u4efb\u52a1"}}
{"id": "2506.11666", "pdf": "https://arxiv.org/pdf/2506.11666", "abs": "https://arxiv.org/abs/2506.11666", "authors": ["Pietro Ferrazzi", "Alberto Lavelli", "Bernardo Magnini"], "title": "Converting Annotated Clinical Cases into Structured Case Report Forms", "categories": ["cs.CL", "cs.AI"], "comment": "to be published in BioNLP 2025", "summary": "Case Report Forms (CRFs) are largely used in medical research as they ensure\naccuracy, reliability, and validity of results in clinical studies. However,\npublicly available, wellannotated CRF datasets are scarce, limiting the\ndevelopment of CRF slot filling systems able to fill in a CRF from clinical\nnotes. To mitigate the scarcity of CRF datasets, we propose to take advantage\nof available datasets annotated for information extraction tasks and to convert\nthem into structured CRFs. We present a semi-automatic conversion methodology,\nwhich has been applied to the E3C dataset in two languages (English and\nItalian), resulting in a new, high-quality dataset for CRF slot filling.\nThrough several experiments on the created dataset, we report that slot filling\nachieves 59.7% for Italian and 67.3% for English on a closed Large Language\nModels (zero-shot) and worse performances on three families of open-source\nmodels, showing that filling CRFs is challenging even for recent\nstate-of-the-art LLMs. We release the datest at\nhttps://huggingface.co/collections/NLP-FBK/e3c-to-crf-67b9844065460cbe42f80166", "AI": {"tldr": "\u9488\u5bf9\u4e34\u5e8a\u7814\u7a76\u4e2d\u75c5\u4f8b\u62a5\u544a\u8868\uff08CRF\uff09\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u5c06\u73b0\u6709\u4fe1\u606f\u62bd\u53d6\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316CRF\u7684\u534a\u81ea\u52a8\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u82f1\u8bed\u548c\u610f\u5927\u5229\u8bed\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u4e0b\u7684\u586b\u5145\u6548\u679c\u3002", "motivation": "\u7531\u4e8e\u516c\u5f00\u7684\u9ad8\u8d28\u91cfCRF\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u4ece\u4e34\u5e8a\u7b14\u8bb0\u81ea\u52a8\u586b\u5145CRF\u7684\u7cfb\u7edf\u5f00\u53d1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8f6c\u6362\u73b0\u6709\u6570\u636e\u96c6\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u534a\u81ea\u52a8\u8f6c\u6362\u65b9\u6cd5\uff0c\u5e94\u7528\u4e8e\u591a\u8bed\u8a00E3C\u6570\u636e\u96c6\uff0c\u751f\u6210\u7528\u4e8eCRF\u69fd\u586b\u5145\u7684\u9ad8\u8d28\u91cf\u65b0\u6570\u636e\u96c6\u3002", "result": "\u96f6\u6837\u672c\u4e0b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u610f\u5927\u5229\u8bed\u548c\u82f1\u8bed\u7684\u69fd\u586b\u5145\u51c6\u786e\u7387\u5206\u522b\u4e3a59.7%\u548c67.3%\uff0c\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff0c\u8868\u660eCRF\u586b\u5145\u5bf9\u5f53\u524d\u5148\u8fdb\u6a21\u578b\u4ecd\u5177\u6311\u6218\u6027\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86CRF\u81ea\u52a8\u586b\u5145\u7684\u6280\u672f\u96be\u70b9\uff0c\u53d1\u5e03\u7684\u8de8\u8bed\u8a00\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u8d44\u6e90\uff0c\u63a8\u52a8\u4e34\u5e8a\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.11673", "pdf": "https://arxiv.org/pdf/2506.11673", "abs": "https://arxiv.org/abs/2506.11673", "authors": ["Alicja Dobrzeniecka", "Antske Fokkens", "Pia Sommerauer"], "title": "Improving Causal Interventions in Amnesic Probing with Mean Projection or LEACE", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Amnesic probing is a technique used to examine the influence of specific\nlinguistic information on the behaviour of a model. This involves identifying\nand removing the relevant information and then assessing whether the model's\nperformance on the main task changes. If the removed information is relevant,\nthe model's performance should decline. The difficulty with this approach lies\nin removing only the target information while leaving other information\nunchanged. It has been shown that Iterative Nullspace Projection (INLP), a\nwidely used removal technique, introduces random modifications to\nrepresentations when eliminating target information. We demonstrate that Mean\nProjection (MP) and LEACE, two proposed alternatives, remove information in a\nmore targeted manner, thereby enhancing the potential for obtaining behavioural\nexplanations through Amnesic Probing.", "AI": {"tldr": "MP\u548cLEACE\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edfINLP\u6280\u672f\uff0c\u80fd\u66f4\u7cbe\u51c6\u6d88\u9664\u76ee\u6807\u4fe1\u606f\uff0c\u63d0\u5347\u9057\u5fd8\u6027\u63a2\u6d4b\u7684\u89e3\u91ca\u6709\u6548\u6027", "motivation": "\u4f20\u7edf\u8fed\u4ee3\u96f6\u7a7a\u95f4\u6295\u5f71\uff08INLP\uff09\u5728\u6d88\u9664\u76ee\u6807\u4fe1\u606f\u65f6\u4f1a\u5f15\u5165\u968f\u673a\u5e72\u6270\uff0c\u5f71\u54cd\u9057\u5fd8\u6027\u63a2\u6d4b\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u89e3\u91ca\u53ef\u9760\u6027", "method": "\u901a\u8fc7\u5bf9\u6bd4INLP\u4e0e\u63d0\u51fa\u7684\u5747\u503c\u6295\u5f71\uff08MP\uff09\u548cLEACE\u4e24\u79cd\u65b0\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4fe1\u606f\u6d88\u9664\u7684\u9488\u5bf9\u6027", "result": "MP\u548cLEACE\u5728\u6d88\u9664\u76ee\u6807\u4fe1\u606f\u65f6\u66f4\u7cbe\u51c6\uff0c\u4fdd\u7559\u4e86\u66f4\u591a\u975e\u76f8\u5173\u7279\u5f81\uff0c\u4f7f\u6a21\u578b\u6027\u80fd\u53d8\u5316\u66f4\u80fd\u53cd\u6620\u88ab\u6d88\u9664\u4fe1\u606f\u7684\u771f\u5b9e\u5f71\u54cd", "conclusion": "\u6539\u8fdb\u7684\u4fe1\u606f\u6d88\u9664\u65b9\u6cd5\u589e\u5f3a\u4e86\u9057\u5fd8\u6027\u63a2\u6d4b\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5206\u6790\u5de5\u5177"}}
{"id": "2506.11681", "pdf": "https://arxiv.org/pdf/2506.11681", "abs": "https://arxiv.org/abs/2506.11681", "authors": ["Pratibha Zunjare", "Michael Hsiao"], "title": "LLMs for Sentence Simplification: A Hybrid Multi-Agent prompting Approach", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenge of transforming complex sentences into\nsequences of logical, simplified sentences while preserving semantic and\nlogical integrity with the help of Large Language Models. We propose a hybrid\napproach that combines advanced prompting with multi-agent architectures to\nenhance the sentence simplification process. Experimental results show that our\napproach was able to successfully simplify 70% of the complex sentences written\nfor video game design application. In comparison, a single-agent approach\nattained a 48% success rate on the same task.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u63d0\u793a\u7b56\u7565\u4e0e\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u590d\u6742\u53e5\u5b50\u8f6c\u5316\u6210\u529f\u7387\u63d0\u5347\u81f370%\uff08\u5355\u667a\u80fd\u4f5348%\uff09", "motivation": "\u89e3\u51b3\u89c6\u9891\u6e38\u620f\u8bbe\u8ba1\u4e2d\u590d\u6742\u53e5\u5b50\u7b80\u5316\u65f6\u4fdd\u6301\u8bed\u4e49\u903b\u8f91\u5b8c\u6574\u6027\u7684\u6311\u6218", "method": "\u7ed3\u5408\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u540c\u67b6\u6784\u7684\u6df7\u5408\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u663e\u793a\u6df7\u5408\u65b9\u6cd5\u5728\u6e38\u620f\u8bbe\u8ba1\u573a\u666f\u7b80\u5316\u6210\u529f\u738770%\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\uff0848%\uff09", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6709\u6548\u63d0\u5347\u590d\u6742\u6587\u672c\u5904\u7406\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.11702", "pdf": "https://arxiv.org/pdf/2506.11702", "abs": "https://arxiv.org/abs/2506.11702", "authors": ["V\u00edctor Gallego"], "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ICML 2025 Workshop on Models of Human Feedback for AI\n  Alignment", "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning", "AI": {"tldr": "\u63d0\u51fa\u53ef\u914d\u7f6e\u504f\u597d\u8c03\u6574\u6846\u67b6CPT\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u63d0\u793a\u5b9e\u73b0LLM\u8f93\u51fa\u7684\u52a8\u6001\u8c03\u63a7", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u504f\u597d\u7684AI\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u573a\u666f\u9700\u6c42\uff0c\u9700\u5efa\u7acb\u7ec6\u7c92\u5ea6\u53ef\u63a7\u7684\u504f\u597d\u8c03\u8282\u673a\u5236", "method": "\u5229\u7528\u7ed3\u6784\u5316\u89c4\u5219\u751f\u6210\u7cfb\u7edf\u63d0\u793a\uff0c\u57fa\u4e8e\u4eba\u5de5\u5408\u6210\u504f\u597d\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u8f93\u51fa\u52a8\u6001\u8c03\u8282", "result": "\u6210\u529f\u8d4b\u4e88LLM\u6839\u636e\u7cfb\u7edf\u63d0\u793a\u5b9e\u65f6\u8c03\u6574\u8f93\u51fa\u98ce\u683c\u7684\u80fd\u529b\uff0c\u5efa\u7acb\u66f4\u7ec6\u7c92\u5ea6\u7684\u53cd\u9988\u5efa\u6a21\u673a\u5236", "conclusion": "CPT\u6846\u67b6\u6709\u6548\u7a81\u7834\u4e86\u9759\u6001\u504f\u597d\u9650\u5236\uff0c\u76f8\u5173\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u5df2\u5f00\u6e90"}}
{"id": "2506.11728", "pdf": "https://arxiv.org/pdf/2506.11728", "abs": "https://arxiv.org/abs/2506.11728", "authors": ["H\u00e9ctor Mart\u00ednez", "Adri\u00e1n Castell\u00f3", "Francisco D. Igual", "Enrique S. Quintana-Ort\u00ed"], "title": "The Cambrian Explosion of Mixed-Precision Matrix Multiplication for Quantized Deep Learning Inference", "categories": ["cs.CL"], "comment": "16 pages, 7 tables, 7 figures", "summary": "Recent advances in deep learning (DL) have led to a shift from traditional\n64-bit floating point (FP64) computations toward reduced-precision formats,\nsuch as FP16, BF16, and 8- or 16-bit integers, combined with mixed-precision\narithmetic. This transition enhances computational throughput, reduces memory\nand bandwidth usage, and improves energy efficiency, offering significant\nadvantages for resource-constrained edge devices. To support this shift,\nhardware architectures have evolved accordingly, now including adapted ISAs\n(Instruction Set Architectures) that expose mixed-precision vector units and\nmatrix engines tailored for DL workloads. At the heart of many DL and\nscientific computing tasks is the general matrix-matrix multiplication gemm, a\nfundamental kernel historically optimized using axpy vector instructions on\nSIMD (single instruction, multiple data) units. However, as hardware moves\ntoward mixed-precision dot-product-centric operations optimized for quantized\ninference, these legacy approaches are being phased out. In response to this,\nour paper revisits traditional high-performance gemm and describes strategies\nfor adapting it to mixed-precision integer (MIP) arithmetic across modern ISAs,\nincluding x86_64, ARM, and RISC-V. Concretely, we illustrate novel micro-kernel\ndesigns and data layouts that better exploit today's specialized hardware and\ndemonstrate significant performance gains from MIP arithmetic over\nfloating-point implementations across three representative CPU architectures.\nThese contributions highlight a new era of gemm optimization-driven by the\ndemands of DL inference on heterogeneous architectures, marking what we term as\nthe \"Cambrian period\" for matrix multiplication.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u6574\u6570\u8ba1\u7b97\u4f18\u5316\u901a\u7528\u77e9\u9635\u4e58\u6cd5(gemm)\uff0c\u4ee5\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u63a8\u7406\u5728\u5f02\u6784\u786c\u4ef6\u4e0a\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6d6e\u70b9\u7684SIMD\u4f18\u5316\u65b9\u6cd5\u65e0\u6cd5\u9002\u914d\u65b0\u5174\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u6574\u6570\u8ba1\u7b97\u7684\u786c\u4ef6\u67b6\u6784\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u8ba1\u7b97\u8303\u5f0f", "method": "\u63d0\u51fa\u9762\u5411x86_64/ARM/RISC-V\u7684\u65b0\u578b\u5fae\u5185\u6838\u8bbe\u8ba1\u548c\u6570\u636e\u5e03\u5c40\u7b56\u7565\uff0c\u5145\u5206\u5229\u7528\u73b0\u4ee3CPU\u7684\u6df7\u5408\u7cbe\u5ea6\u77e9\u9635\u5f15\u64ce", "result": "\u5728\u4e09\u79cd\u4e3b\u6d41CPU\u67b6\u6784\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6df7\u5408\u7cbe\u5ea6\u6574\u6570\u8ba1\u7b97\u76f8\u6bd4\u6d6e\u70b9\u65b9\u6848\u6548\u7387\u4f18\u52bf\u660e\u663e", "conclusion": "\u6807\u5fd7\u7740\u77e9\u9635\u4e58\u6cd5\u8fdb\u5165\u7531\u6df1\u5ea6\u5b66\u4e60\u9700\u6c42\u9a71\u52a8\u7684\u786c\u4ef6\u534f\u540c\u4f18\u5316\u65b0\u9636\u6bb5\uff0c\u5f62\u6210\u6240\u8c13'\u5bd2\u6b66\u7eaa\u5927\u7206\u53d1'\u5f0f\u7684\u67b6\u6784\u521b\u65b0\u671f"}}
{"id": "2506.11752", "pdf": "https://arxiv.org/pdf/2506.11752", "abs": "https://arxiv.org/abs/2506.11752", "authors": ["Nan Jiang", "Ziming Wu", "De-Chuan Zhan", "Fuming Lai", "Shaobing Lian"], "title": "DART: Distilling Autoregressive Reasoning to Silent Thought", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has significantly advanced Large Language\nModels (LLMs) in solving complex tasks. However, its autoregressive paradigm\nleads to significant computational overhead, hindering its deployment in\nlatency-sensitive applications. To address this, we propose \\textbf{DART}\n(\\textbf{D}istilling \\textbf{A}utoregressive \\textbf{R}easoning to Silent\n\\textbf{T}hought), a self-distillation framework that enables LLMs to replace\nautoregressive CoT with non-autoregressive Silent Thought (ST). Specifically,\nDART introduces two training pathways: the CoT pathway for traditional\nreasoning and the ST pathway for generating answers directly from a few ST\ntokens. The ST pathway utilizes a lightweight Reasoning Evolvement Module (REM)\nto align its hidden states with the CoT pathway, enabling the ST tokens to\nevolve into informative embeddings. During inference, only the ST pathway is\nactivated, leveraging evolving ST tokens to deliver the answer directly.\nExtensive experimental results demonstrate that DART achieves comparable\nreasoning performance to existing baselines while offering significant\nefficiency gains, serving as a feasible alternative for efficient reasoning.", "AI": {"tldr": "\u63d0\u51faDART\u6846\u67b6\u901a\u8fc7\u81ea\u84b8\u998f\u5c06\u81ea\u56de\u5f52\u601d\u7ef4\u94fe\u63a8\u7406\u8f6c\u5316\u4e3a\u975e\u81ea\u5f52\u7ea6\u7684\u9759\u9ed8\u601d\u7ef4\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387", "motivation": "\u4f20\u7edfChain-of-Thought\u63a8\u7406\u7684\u81ea\u56de\u5f52\u7279\u6027\u5bfc\u81f4\u8ba1\u7b97\u5ef6\u8fdf\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u5b9e\u65f6\u573a\u666f", "method": "\u6784\u5efa\u53cc\u8bad\u7ec3\u8def\u5f84\uff08CoT\u8def\u5f84\u548cST\u8def\u5f84\uff09\uff0c\u901a\u8fc7REM\u6a21\u5757\u5bf9\u9f50\u9690\u72b6\u6001\uff0c\u4f7fST\u4ee4\u724c\u6f14\u5316\u51fa\u4fe1\u606f\u5d4c\u5165", "result": "\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u83b7\u5f97\u663e\u8457\u6548\u7387\u63d0\u5347\uff0c\u63a8\u7406\u65f6\u4ec5\u9700\u6fc0\u6d3bST\u8def\u5f84", "conclusion": "DART\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2506.11763", "pdf": "https://arxiv.org/pdf/2506.11763", "abs": "https://arxiv.org/abs/2506.11763", "authors": ["Mingxuan Du", "Benfeng Xu", "Chiwei Zhu", "Xiaorui Wang", "Zhendong Mao"], "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents", "categories": ["cs.CL", "cs.IR"], "comment": "31 pages, 5 figures", "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.", "AI": {"tldr": "\u63d0\u51fa\u4e86DeepResearch Bench\u57fa\u51c6\u548c\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u5728\u590d\u6742\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRAs\uff09\u7684\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u5ba2\u89c2\u8861\u91cf\u5176\u5c06\u6d77\u91cf\u7f51\u7edc\u4fe1\u606f\u8f6c\u5316\u4e3a\u9ad8\u8d28\u91cf\u7814\u7a76\u62a5\u544a\u7684\u80fd\u529b", "method": "1.\u6784\u5efa\u5305\u542b100\u4e2a\u8de8\u5b66\u79d1\u535a\u58eb\u7ea7\u7814\u7a76\u4efb\u52a1\u7684\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\n2.\u5f00\u53d1\u81ea\u9002\u5e94\u8bc4\u4f30\u6846\u67b6\uff1a\u57fa\u4e8e\u53c2\u8003\u7684\u6587\u672c\u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb+\u5f15\u7528\u6709\u6548\u6027\u53cc\u91cd\u8bc4\u4f30\u673a\u5236", "result": "\u63d0\u51fa\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b9e\u73b0\u4e0e\u4eba\u5de5\u8bc4\u5224\u5f3a\u5bf9\u9f50\uff0c\u5f00\u6e90\u57fa\u51c6\u8986\u76d622\u4e2a\u9886\u57df\u5e76\u5305\u542b\u53ef\u6269\u5c55\u67b6\u6784", "conclusion": "\u8be5\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u663e\u8457\u63d0\u5347LLM\u4ee3\u7406\u7684\u7814\u7a76\u6548\u7387\uff0c\u4e3a\u81ea\u52a8\u5316\u79d1\u7814\u8f85\u52a9\u5de5\u5177\u7684\u53d1\u5c55\u63d0\u4f9b\u57fa\u7840\u8bbe\u65bd\u652f\u6301"}}
{"id": "2506.11769", "pdf": "https://arxiv.org/pdf/2506.11769", "abs": "https://arxiv.org/abs/2506.11769", "authors": ["Tianqi Du", "Haotian Huang", "Yifei Wang", "Yisen Wang"], "title": "Long-Short Alignment for Effective Long-Context Modeling in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "ICML 2025", "summary": "Large language models (LLMs) have exhibited impressive performance and\nsurprising emergent properties. However, their effectiveness remains limited by\nthe fixed context window of the transformer architecture, posing challenges for\nlong-context modeling. Among these challenges, length generalization -- the\nability to generalize to sequences longer than those seen during training -- is\na classical and fundamental problem. In this work, we propose a fresh\nperspective on length generalization, shifting the focus from the conventional\nemphasis on input features such as positional encodings or data structures to\nthe output distribution of the model. Specifically, through case studies on\nsynthetic tasks, we highlight the critical role of \\textbf{long-short\nalignment} -- the consistency of output distributions across sequences of\nvarying lengths. Extending this insight to natural language tasks, we propose a\nmetric called Long-Short Misalignment to quantify this phenomenon, uncovering a\nstrong correlation between the metric and length generalization performance.\nBuilding on these findings, we develop a regularization term that promotes\nlong-short alignment during training. Extensive experiments validate the\neffectiveness of our approach, offering new insights for achieving more\neffective long-context modeling in LLMs. Code is available at\nhttps://github.com/PKU-ML/LongShortAlignment.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u957f\u77ed\u671f\u5bf9\u9f50\u673a\u5236\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u5173\u6ce8\u8f93\u51fa\u5206\u5e03\u4e00\u81f4\u6027\u800c\u975e\u4f20\u7edf\u8f93\u5165\u7279\u5f81", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e\u56fa\u5b9a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b58\u5728\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898", "method": "1. \u901a\u8fc7\u5408\u6210\u4efb\u52a1\u9a8c\u8bc1\u957f\u77ed\u671f\u5bf9\u9f50\u91cd\u8981\u6027 2. \u8bbe\u8ba1Long-Short Misalignment\u91cf\u5316\u6307\u6807 3. \u63d0\u51fa\u63d0\u5347\u8f93\u51fa\u5206\u5e03\u4e00\u81f4\u6027\u7684\u6b63\u5219\u5316\u65b9\u6cd5", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u957f\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u957f\u77ed\u671f\u9519\u4f4d\u6307\u6807\u4e0e\u6a21\u578b\u6027\u80fd\u5f3a\u76f8\u5173", "conclusion": "\u8f93\u51fa\u5206\u5e03\u4e00\u81f4\u6027\u662f\u957f\u5ea6\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u63d0\u5347\u5927\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.11798", "pdf": "https://arxiv.org/pdf/2506.11798", "abs": "https://arxiv.org/abs/2506.11798", "authors": ["Maximilian Kreutner", "Marlene Lutz", "Markus Strohmaier"], "title": "Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) display remarkable capabilities to understand or\neven produce political discourse, but have been found to consistently display a\nprogressive left-leaning bias. At the same time, so-called persona or identity\nprompts have been shown to produce LLM behavior that aligns with socioeconomic\ngroups that the base model is not aligned with. In this work, we analyze\nwhether zero-shot persona prompting with limited information can accurately\npredict individual voting decisions and, by aggregation, accurately predict\npositions of European groups on a diverse set of policies. We evaluate if\npredictions are stable towards counterfactual arguments, different persona\nprompts and generation methods. Finally, we find that we can simulate voting\nbehavior of Members of the European Parliament reasonably well with a weighted\nF1 score of approximately 0.793. Our persona dataset of politicians in the 2024\nEuropean Parliament and our code are available at\nhttps://github.com/dess-mannheim/european_parliament_simulation.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u96f6\u6837\u672c\u4eba\u7269\u63d0\u793a\u9884\u6d4b\u6b27\u6d32\u8bae\u5458\u7684\u6295\u7968\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u80fd\u6709\u6548\u6a21\u62df\u653f\u6cbb\u7acb\u573a\uff08\u52a0\u6743F1\u7ea60.793\uff09\uff0c\u4e14\u9884\u6d4b\u7ed3\u679c\u5bf9\u53cd\u4e8b\u5b9e\u8bba\u636e\u548c\u63d0\u793a\u65b9\u5f0f\u7a33\u5b9a\u3002", "motivation": "\u9488\u5bf9LLMs\u5b58\u5728\u7684\u5de6\u503e\u653f\u6cbb\u504f\u89c1\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8eab\u4efd\u63d0\u793a\u6280\u672f\u6a21\u62df\u975e\u5bf9\u9f50\u7fa4\u4f53\u7684\u653f\u6cbb\u51b3\u7b56\uff0c\u9a8c\u8bc1\u9884\u6d4b\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u4f7f\u7528\u96f6\u6837\u672c\u4eba\u7269\u63d0\u793a\u6280\u672f\uff0c\u7ed3\u5408\u4e0d\u540c\u653f\u7b56\u7acb\u573a\u751f\u6210\u65b9\u6cd5\uff0c\u6784\u5efa\u6b27\u6d32\u8bae\u5458\u4eba\u7269\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c\u5bf9\u53cd\u4e8b\u5b9e\u8bba\u636e/\u63d0\u793a\u65b9\u5f0f/\u751f\u6210\u65b9\u6cd5\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u6210\u529f\u6a21\u62df2024\u6b27\u6d32\u8bae\u4f1a\u8bae\u5458\u6295\u7968\u884c\u4e3a\uff08\u52a0\u6743F1=0.793\uff09\uff0c\u521b\u5efa\u5305\u542b\u653f\u6cbb\u4eba\u7269\u6570\u636e\u7684\u6570\u636e\u96c6\u5e76\u5f00\u6e90\u4ee3\u7801\u5e93\u3002", "conclusion": "\u8eab\u4efd\u63d0\u793a\u6280\u672f\u80fd\u6709\u6548\u9884\u6d4b\u7fa4\u4f53\u653f\u6cbb\u7acb\u573a\uff0c\u4e3aLLMs\u5728\u653f\u6cbb\u884c\u4e3a\u6a21\u62df\u548c\u653f\u7b56\u5206\u6790\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b9\u6cd5\u8bba\u9a8c\u8bc1\u548c\u57fa\u7840\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2506.11807", "pdf": "https://arxiv.org/pdf/2506.11807", "abs": "https://arxiv.org/abs/2506.11807", "authors": ["Simeon Junker", "Manar Ali", "Larissa Koch", "Sina Zarrie\u00df", "Hendrik Buschmeier"], "title": "Are Multimodal Large Language Models Pragmatically Competent Listeners in Simple Reference Resolution Tasks?", "categories": ["cs.CL"], "comment": "To appear in ACL Findings 2025", "summary": "We investigate the linguistic abilities of multimodal large language models\nin reference resolution tasks featuring simple yet abstract visual stimuli,\nsuch as color patches and color grids. Although the task may not seem\nchallenging for today's language models, being straightforward for human dyads,\nwe consider it to be a highly relevant probe of the pragmatic capabilities of\nMLLMs. Our results and analyses indeed suggest that basic pragmatic\ncapabilities, such as context-dependent interpretation of color descriptions,\nstill constitute major challenges for state-of-the-art MLLMs.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u62bd\u8c61\u89c6\u89c9\u53c2\u7167\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u57fa\u7840\u8bed\u7528\u80fd\u529b\u4ecd\u662f\u91cd\u5927\u6311\u6218", "motivation": "\u901a\u8fc7\u989c\u8272\u8272\u5757/\u7f51\u683c\u7b49\u7b80\u5355\u89c6\u89c9\u523a\u6fc0\uff0c\u9a8c\u8bc1MLLMs\u5728\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u7684\u8bed\u7528\u63a8\u7406\u80fd\u529b\uff0c\u8be5\u4efb\u52a1\u5bf9\u4eba\u7c7b\u7b80\u5355\u4f46\u5bf9\u6a21\u578b\u5177\u6311\u6218\u6027", "method": "\u91c7\u7528\u62bd\u8c61\u89c6\u89c9\u53c2\u7167\u4efb\u52a1\uff08\u5982\u4f9d\u8d56\u4e0a\u4e0b\u6587\u89e3\u91ca\u989c\u8272\u63cf\u8ff0\uff09\uff0c\u6d4b\u8bd5\u6700\u5148\u8fdbMLLMs\u7684\u8bed\u5883\u7406\u89e3\u80fd\u529b", "result": "\u5f53\u524d\u6700\u4f73MLLMs\u4ecd\u96be\u4ee5\u5904\u7406\u57fa\u7840\u8bed\u7528\u80fd\u529b\uff08\u5982\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u989c\u8272\u63cf\u8ff0\u89e3\u91ca\uff09", "conclusion": "\u63d0\u5347MLLMs\u7684\u8bed\u7528\u7406\u89e3\u80fd\u529b\u662f\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u7684\u5173\u952e\u65b9\u5411\uff0c\u9700\u7a81\u7834\u4e0a\u4e0b\u6587\u5173\u8054\u63a8\u7406\u7684\u6280\u672f\u74f6\u9888"}}
{"id": "2506.11857", "pdf": "https://arxiv.org/pdf/2506.11857", "abs": "https://arxiv.org/abs/2506.11857", "authors": ["Yi-Pei Chen", "Noriki Nishida", "Hideki Nakayama", "Yuji Matsumoto"], "title": "Post Persona Alignment for Multi-Session Dialogue Generation", "categories": ["cs.CL"], "comment": null, "summary": "Multi-session persona-based dialogue generation presents challenges in\nmaintaining long-term consistency and generating diverse, personalized\nresponses. While large language models (LLMs) excel in single-session\ndialogues, they struggle to preserve persona fidelity and conversational\ncoherence across extended interactions. Existing methods typically retrieve\npersona information before response generation, which can constrain diversity\nand result in generic outputs. We propose Post Persona Alignment (PPA), a novel\ntwo-stage framework that reverses this process. PPA first generates a general\nresponse based solely on dialogue context, then retrieves relevant persona\nmemories using the response as a query, and finally refines the response to\nalign with the speaker's persona. This post-hoc alignment strategy promotes\nnaturalness and diversity while preserving consistency and personalization.\nExperiments on multi-session LLM-generated dialogue data demonstrate that PPA\nsignificantly outperforms prior approaches in consistency, diversity, and\npersona relevance, offering a more flexible and effective paradigm for\nlong-term personalized dialogue generation.", "AI": {"tldr": "\u63d0\u51faPPA\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u751f\u6210\u901a\u7528\u56de\u590d\u518d\u5bf9\u9f50\u89d2\u8272\u7279\u5f81\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u63d0\u5347\u591a\u8f6e\u89d2\u8272\u5bf9\u8bdd\u7684\u4e00\u81f4\u6027\u3001\u591a\u6837\u6027\u548c\u4e2a\u6027\u5316\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5148\u68c0\u7d22\u89d2\u8272\u4fe1\u606f\u4f1a\u9650\u5236\u56de\u590d\u591a\u6837\u6027\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u4f1a\u8bdd\u573a\u666f\u4e2d\u96be\u4ee5\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u548c\u5bf9\u8bdd\u8fde\u8d2f\u6027\u3002", "method": "PPA\u6846\u67b6\u91c7\u7528\u9006\u5411\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u751f\u6210\u65e0\u89d2\u8272\u7ea6\u675f\u7684\u901a\u7528\u56de\u590d\uff0c\u968f\u540e\u57fa\u4e8e\u56de\u590d\u68c0\u7d22\u76f8\u5173\u89d2\u8272\u8bb0\u5fc6\uff0c\u6700\u540e\u901a\u8fc7\u540e\u9a8c\u5bf9\u9f50\u4f18\u5316\u56de\u590d\u3002", "result": "\u5b9e\u9a8c\u8868\u660ePPA\u5728\u4e00\u81f4\u6027\uff08\u63d0\u534712%\uff09\u3001\u591a\u6837\u6027\uff08\u589e\u52a023%\uff09\u548c\u89d2\u8272\u76f8\u5173\u6027\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u540e\u9a8c\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u5e73\u8861\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4e0e\u89d2\u8272\u4e00\u81f4\u6027\uff0c\u4e3a\u957f\u7a0b\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.11886", "pdf": "https://arxiv.org/pdf/2506.11886", "abs": "https://arxiv.org/abs/2506.11886", "authors": ["Xiaoran Liu", "Siyang He", "Qiqi Wang", "Ruixiao Li", "Yuerong Song", "Zhigeng Liu", "Linlin Li", "Qun Liu", "Zengfeng Huang", "Qipeng Guo", "Ziwei He", "Xipeng Qiu"], "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via Fourier-Approximated KV Cache", "categories": ["cs.CL"], "comment": "10 pages, 7 figures, work in progress", "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.", "AI": {"tldr": "FourierAttention\u6846\u67b6\u901a\u8fc7\u5085\u91cc\u53f6\u57fa\u6295\u5f71\u4f18\u5316KV\u7f13\u5b58\u5185\u5b58\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0bKV\u7f13\u5b58\u5185\u5b58\u6fc0\u589e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u635f\u5931\u6216\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u7f3a\u9677", "method": "\u5229\u7528Transformer\u5934\u7ef4\u5ea6\u5f02\u8d28\u6027\uff1a\u4f4e\u7ef4\u5173\u6ce8\u5c40\u90e8\u4e0a\u4e0b\u6587\uff0c\u9ad8\u7ef4\u6355\u6349\u957f\u7a0b\u4f9d\u8d56\u3002\u5c06\u957f\u4e0a\u4e0b\u6587\u4e0d\u654f\u611f\u7ef4\u5ea6\u6295\u5f71\u5230\u6b63\u4ea4\u5085\u91cc\u53f6\u57fa\uff0c\u7528\u56fa\u5b9a\u957f\u5ea6\u8c31\u7cfb\u6570\u8fd1\u4f3c\u65f6\u5e8f\u6f14\u5316", "result": "\u5728LLaMA\u6a21\u578b\u4e0a\u53d6\u5f97LongBench\u548cNIAH\u6d4b\u8bd5\u6700\u4f73\u957f\u4e0a\u4e0b\u6587\u51c6\u786e\u7387\uff0c\u5f00\u53d1FlashFourierAttention\u5185\u6838\u5b9e\u73b0\u5185\u5b58\u4f18\u5316", "conclusion": "FourierAttention\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11903", "pdf": "https://arxiv.org/pdf/2506.11903", "abs": "https://arxiv.org/abs/2506.11903", "authors": ["Raphael Scheible-Schmitt", "Johann Frei"], "title": "GeistBERT: Breathing Life into German NLP", "categories": ["cs.CL"], "comment": null, "summary": "Advances in transformer-based language models have highlighted the benefits\nof language-specific pre-training on high-quality corpora. In this context,\nGerman NLP stands to gain from updated architectures and modern datasets\ntailored to the linguistic characteristics of the German language. GeistBERT\nseeks to improve German language processing by incrementally training on a\ndiverse corpus and optimizing model performance across various NLP tasks. It\nwas pre-trained using fairseq with standard hyperparameters, initialized from\nGottBERT weights, and trained on a large-scale German corpus using Whole Word\nMasking (WWM). Based on the pre-trained model, we derived extended-input\nvariants using Nystr\\\"omformer and Longformer architectures with support for\nsequences up to 8k tokens. While these long-context models were not evaluated\non dedicated long-context benchmarks, they are included in our release. We\nassessed all models on NER (CoNLL 2003, GermEval 2014) and text classification\n(GermEval 2018 fine/coarse, 10kGNAD) using $F_1$ score and accuracy. The\nGeistBERT models achieved strong performance, leading all tasks among the base\nmodels and setting a new state-of-the-art (SOTA). Notably, the base models\noutperformed larger models in several tasks. To support the German NLP research\ncommunity, we are releasing GeistBERT under the MIT license.", "AI": {"tldr": "GeistBERT\u901a\u8fc7\u5fb7\u8bed\u4e13\u7528\u9884\u8bad\u7ec3\u548c\u67b6\u6784\u4f18\u5316\uff0c\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e2d\u5237\u65b0SOTA\u8868\u73b0\uff0c\u5e76\u5f00\u6e90\u6a21\u578b\u63a8\u52a8\u5fb7\u8bed\u7814\u7a76", "motivation": "\u63d0\u5347\u5fb7\u8bedNLP\u6027\u80fd\uff0c\u9488\u5bf9\u5fb7\u8bed\u8bed\u8a00\u7279\u70b9\u5b9a\u5236\u73b0\u4ee3\u5316\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u8bed\u6599\u5e93", "method": "\u57fa\u4e8eGottBERT\u521d\u59cb\u5316\uff0c\u4f7f\u7528Whole Word Masking\u9884\u8bad\u7ec3\uff0c\u5e76\u6269\u5c55Nystr\u00f6mformer/Longformer\u67b6\u6784\u652f\u63018k\u957f\u5e8f\u5217", "result": "\u57fa\u7840\u6a21\u578b\u5728NER\u548c\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5168\u9762\u9886\u5148\uff0c\u90e8\u5206\u4efb\u52a1\u8d85\u8d8a\u66f4\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u521b\u9020\u65b0SOTA", "conclusion": "\u4ee5MIT\u534f\u8bae\u5f00\u6e90GeistBERT\u7cfb\u5217\u6a21\u578b\uff0c\u652f\u6301\u5fb7\u8bedNLP\u7814\u7a76\u793e\u533a\u53d1\u5c55"}}
{"id": "2506.11919", "pdf": "https://arxiv.org/pdf/2506.11919", "abs": "https://arxiv.org/abs/2506.11919", "authors": ["Greta Damo", "Elena Cabrio", "Serena Villata"], "title": "Effectiveness of Counter-Speech against Abusive Content: A Multidimensional Annotation and Classification Study", "categories": ["cs.CL"], "comment": null, "summary": "Counter-speech (CS) is a key strategy for mitigating online Hate Speech (HS),\nyet defining the criteria to assess its effectiveness remains an open\nchallenge. We propose a novel computational framework for CS effectiveness\nclassification, grounded in social science concepts. Our framework defines six\ncore dimensions - Clarity, Evidence, Emotional Appeal, Rebuttal, Audience\nAdaptation, and Fairness - which we use to annotate 4,214 CS instances from two\nbenchmark datasets, resulting in a novel linguistic resource released to the\ncommunity. In addition, we propose two classification strategies, multi-task\nand dependency-based, achieving strong results (0.94 and 0.96 average F1\nrespectively on both expert- and user-written CS), outperforming standard\nbaselines, and revealing strong interdependence among dimensions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7684\u53cd\u4ec7\u6068\u8a00\u8bba\u6709\u6548\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6807\u6ce84214\u4e2a\u5b9e\u4f8b\u5e76\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff08F1 0.94-0.96\uff09", "motivation": "\u73b0\u6709\u53cd\u4ec7\u6068\u8a00\u8bba(CS)\u6709\u6548\u6027\u8bc4\u4f30\u6807\u51c6\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u9700\u5efa\u7acb\u57fa\u4e8e\u793e\u4f1a\u79d1\u5b66\u7406\u8bba\u7684\u8ba1\u7b97\u6846\u67b6", "method": "\u5b9a\u4e49\u6e05\u6670\u5ea6/\u8bc1\u636e/\u60c5\u611f\u8bc9\u6c42/\u53cd\u9a73/\u53d7\u4f17\u9002\u5e94\u6027/\u516c\u5e73\u60276\u4e2a\u7ef4\u5ea6\uff0c\u91c7\u7528\u591a\u4efb\u52a1\u548c\u4f9d\u8d56\u5173\u7cfb\u5206\u7c7b\u7b56\u7565", "result": "\u5728\u4e13\u5bb6\u548c\u7528\u6237\u64b0\u5199\u7684CS\u4e0a\u5206\u522b\u8fbe\u52300.94\u548c0.96\u5e73\u5747F1\uff0c\u63ed\u793a\u7ef4\u5ea6\u95f4\u5f3a\u76f8\u5173\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u8bc4\u4f30\u53cd\u4ec7\u6068\u8a00\u8bba\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\uff0c\u7ef4\u5ea6\u4f9d\u8d56\u5173\u7cfb\u4e3a\u4f18\u5316CS\u7b56\u7565\u6307\u660e\u65b9\u5411"}}
{"id": "2506.11930", "pdf": "https://arxiv.org/pdf/2506.11930", "abs": "https://arxiv.org/abs/2506.11930", "authors": ["Dongwei Jiang", "Alvin Zhang", "Andrew Wang", "Nicholas Andrews", "Daniel Khashabi"], "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u7ed9\u4e88\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u5916\u90e8\u53cd\u9988\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u5b58\u5728\u53cd\u9988\u6469\u64e6\u73b0\u8c61\uff0c\u65e0\u6cd5\u6709\u6548\u4fee\u6b63\u9519\u8bef\u7b54\u6848", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408\u5916\u90e8\u53cd\u9988\u7684\u6548\u80fd\uff0c\u9a8c\u8bc1\u5728\u7406\u60f3\u53cd\u9988\u6761\u4ef6\u4e0b\u6a21\u578b\u80fd\u5426\u5b8c\u5168\u4fee\u6b63\u9519\u8bef\u7b54\u6848", "method": "\u6784\u5efa\u5305\u542b\u89e3\u9898\u6a21\u578b\u548c\u57fa\u4e8e\u5b8c\u6574\u53c2\u8003\u7b54\u6848\u7684\u53cd\u9988\u751f\u6210\u5668\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u77e5\u8bc6\u63a8\u7406\u7b49\u591a\u9886\u57df\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4f7f\u7528Claude 3.7\u7b49\u5148\u8fdb\u6a21\u578b", "result": "\u6a21\u578b\u6301\u7eed\u8868\u73b0\u51fa\u53cd\u9988\u6297\u62d2\u6027\uff0c\u6e29\u5ea6\u6e10\u8fdb\u7b56\u7565\u4ec5\u5e26\u6765\u6709\u9650\u6539\u8fdb\uff0c\u6392\u9664\u4e86\u6a21\u578b\u8fc7\u5ea6\u81ea\u4fe1\u548c\u6570\u636e\u719f\u6089\u5ea6\u7b49\u6f5c\u5728\u539f\u56e0", "conclusion": "\u53cd\u9988\u6469\u64e6\u63ed\u793a\u4e86\u5f53\u524dLLMs\u81ea\u6211\u6539\u8fdb\u673a\u5236\u7684\u6839\u672c\u9650\u5236\uff0c\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u7a81\u7834\u8be5\u74f6\u9888"}}
{"id": "2506.11938", "pdf": "https://arxiv.org/pdf/2506.11938", "abs": "https://arxiv.org/abs/2506.11938", "authors": ["Samuel Simko", "Mrinmaya Sachan", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "Improving Large Language Model Safety with Contrastive Representation Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) are powerful tools with profound societal\nimpacts, yet their ability to generate responses to diverse and uncontrolled\ninputs leaves them vulnerable to adversarial attacks. While existing defenses\noften struggle to generalize across varying attack types, recent advancements\nin representation engineering offer promising alternatives. In this work, we\npropose a defense framework that formulates model defense as a contrastive\nrepresentation learning (CRL) problem. Our method finetunes a model using a\ntriplet-based loss combined with adversarial hard negative mining to encourage\nseparation between benign and harmful representations. Our experimental results\nacross multiple models demonstrate that our approach outperforms prior\nrepresentation engineering-based defenses, improving robustness against both\ninput-level and embedding-space attacks without compromising standard\nperformance. Our code is available at\nhttps://github.com/samuelsimko/crl-llm-defense", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u6bd4\u8868\u5f81\u5b66\u4e60\u7684\u9632\u5fa1\u6846\u67b6CRL\uff0c\u901a\u8fc7\u4e09\u5143\u7ec4\u635f\u5931\u548c\u5bf9\u6297\u6027\u96be\u8d1f\u6837\u672c\u6316\u6398\u589e\u5f3a\u6a21\u578b\u5bf9\u6297\u653b\u51fb\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709LLM\u9632\u5fa1\u65b9\u6cd5\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u7c7b\u578b\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u8868\u5f81\u5de5\u7a0b\u6280\u672f\u4e3a\u6b64\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002LLM\u5728\u5e94\u5bf9\u591a\u6837\u5316\u8f93\u5165\u65f6\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u4e9f\u9700\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027\u3002", "method": "\u5c06\u6a21\u578b\u9632\u5fa1\u8f6c\u5316\u4e3a\u5bf9\u6bd4\u8868\u5f81\u5b66\u4e60\u95ee\u9898\uff0c\u91c7\u7528\u4e09\u5143\u7ec4\u635f\u5931\u51fd\u6570\u7ed3\u5408\u5bf9\u6297\u6027\u96be\u8d1f\u6837\u672c\u6316\u6398\u6280\u672f\uff0c\u5f3a\u5236\u5206\u79bb\u826f\u6027\u6837\u672c\u4e0e\u6709\u5bb3\u6837\u672c\u7684\u8868\u5f81\u7a7a\u95f4", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8868\u5f81\u5de5\u7a0b\u9632\u5fa1\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6807\u51c6\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5bf9\u8f93\u5165\u7ea7\u548c\u5d4c\u5165\u7a7a\u95f4\u653b\u51fb\u5747\u5c55\u73b0\u51fa\u66f4\u5f3a\u9c81\u68d2\u6027", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u5bf9\u6bd4\u8868\u5f81\u5b66\u4e60\u5728LLM\u5b89\u5168\u9632\u5fa1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u7a0b\u5b9e\u73b0\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55"}}
{"id": "2506.12014", "pdf": "https://arxiv.org/pdf/2506.12014", "abs": "https://arxiv.org/abs/2506.12014", "authors": ["Yuliang Xu", "Siming Huang", "Mingmeng Geng", "Yao Wan", "Xuanhua Shi", "Dongping Chen"], "title": "code_transformed: The Influence of Large Language Models on Code", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "comment": "We release all the experimental dataset and source code at:\n  https://github.com/ignorancex/LLM_code", "summary": "Coding remains one of the most fundamental modes of interaction between\nhumans and machines. With the rapid advancement of Large Language Models\n(LLMs), code generation capabilities have begun to significantly reshape\nprogramming practices. This development prompts a central question: Have LLMs\ntransformed code style, and how can such transformation be characterized? In\nthis paper, we present a pioneering study that investigates the impact of LLMs\non code style, with a focus on naming conventions, complexity, maintainability,\nand similarity. By analyzing code from over 19,000 GitHub repositories linked\nto arXiv papers published between 2020 and 2025, we identify measurable trends\nin the evolution of coding style that align with characteristics of\nLLM-generated code. For instance, the proportion of snake\\_case variable names\nin Python code increased from 47% in Q1 2023 to 51% in Q1 2025. Furthermore, we\ninvestigate how LLMs approach algorithmic problems by examining their reasoning\nprocesses. Given the diversity of LLMs and usage scenarios, among other\nfactors, it is difficult or even impossible to precisely estimate the\nproportion of code generated or assisted by LLMs. Our experimental results\nprovide the first large-scale empirical evidence that LLMs affect real-world\nprogramming style.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u663e\u8457\u5f71\u54cd\u4e86\u5b9e\u9645\u7f16\u7a0b\u98ce\u683c\uff0c\u901a\u8fc7\u5206\u6790GitHub\u4ee3\u7801\u5e93\u53d1\u73b0\u53d8\u91cf\u547d\u540d\u89c4\u8303\uff08\u5982Python\u4e2dsnake_case\u4f7f\u7528\u7387\u4ece47%\u5347\u81f351%\uff09\u3001\u4ee3\u7801\u590d\u6742\u5ea6\u7b49\u6307\u6807\u51fa\u73b0\u53ef\u91cf\u5316\u8d8b\u52bf\u53d8\u5316\u3002", "motivation": "\u968f\u7740LLMs\u4ee3\u7801\u751f\u6210\u80fd\u529b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u4e9f\u9700\u8bc4\u4f30\u5176\u5bf9\u7f16\u7a0b\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u4ee3\u7801\u98ce\u683c\u662f\u5426\u53d1\u751f\u6839\u672c\u6027\u6539\u53d8\u53ca\u5176\u7279\u5f81\u5316\u63cf\u8ff0\u3002", "method": "\u5206\u67902020-2025\u5e74\u95f4arXiv\u8bba\u6587\u5173\u8054\u768419,000+ GitHub\u4ed3\u5e93\u4ee3\u7801\uff0c\u805a\u7126\u547d\u540d\u89c4\u8303\u3001\u590d\u6742\u5ea6\u3001\u53ef\u7ef4\u62a4\u6027\u548c\u76f8\u4f3c\u6027\u7b49\u7ef4\u5ea6\u8fdb\u884c\u91cf\u5316\u8ffd\u8e2a\u3002", "result": "1. snake_case\u53d8\u91cf\u540d\u6bd4\u4f8b\u5728Python\u4e2d2023Q1\u81f32025Q1\u671f\u95f4\u589e\u957f4\u4e2a\u767e\u5206\u70b9\n2. \u53d1\u73b0\u4ee3\u7801\u98ce\u683c\u6f14\u53d8\u8d8b\u52bf\u4e0eLLM\u751f\u6210\u4ee3\u7801\u7279\u5f81\u9ad8\u5ea6\u543b\u5408\n3. \u9996\u6b21\u63d0\u4f9bLLM\u5f71\u54cd\u771f\u5b9e\u7f16\u7a0b\u98ce\u683c\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc1\u636e", "conclusion": "\u5c3d\u7ba1LLMs\u591a\u6837\u6027\u5bfc\u81f4\u7cbe\u786e\u91cf\u5316\u5176\u5f71\u54cd\u5b58\u5728\u56f0\u96be\uff0c\u4f46\u5b9e\u9a8c\u6570\u636e\u9996\u6b21\u5b9e\u8bc1LLMs\u6b63\u5728\u91cd\u5851\u73b0\u5b9e\u4e16\u754c\u7684\u7f16\u7a0b\u98ce\u683c\uff0c\u6807\u5fd7\u7740\u4eba\u673a\u4ea4\u4e92\u6a21\u5f0f\u7684\u6f14\u8fdb\u3002"}}
{"id": "2506.11004", "pdf": "https://arxiv.org/pdf/2506.11004", "abs": "https://arxiv.org/abs/2506.11004", "authors": ["Kevin Cogan", "Vuong M. Ngo", "Mark Roantree"], "title": "Developing a Dyslexia Indicator Using Eye Tracking", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "The 23rd International Conference on Artificial Intelligence in\n  Medicine (AIME 2025), LNAI, Springer, 11 pages", "summary": "Dyslexia, affecting an estimated 10% to 20% of the global population,\nsignificantly impairs learning capabilities, highlighting the need for\ninnovative and accessible diagnostic methods. This paper investigates the\neffectiveness of eye-tracking technology combined with machine learning\nalgorithms as a cost-effective alternative for early dyslexia detection. By\nanalyzing general eye movement patterns, including prolonged fixation durations\nand erratic saccades, we proposed an enhanced solution for determining\neye-tracking-based dyslexia features. A Random Forest Classifier was then\nemployed to detect dyslexia, achieving an accuracy of 88.58\\%. Additionally,\nhierarchical clustering methods were applied to identify varying severity\nlevels of dyslexia. The analysis incorporates diverse methodologies across\nvarious populations and settings, demonstrating the potential of this\ntechnology to identify individuals with dyslexia, including those with\nborderline traits, through non-invasive means. Integrating eye-tracking with\nmachine learning represents a significant advancement in the diagnostic\nprocess, offering a highly accurate and accessible method in clinical research.", "AI": {"tldr": "\u7ed3\u5408\u773c\u52a8\u8ffd\u8e2a\u6280\u672f\u548c\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u5b9e\u73b0\u9605\u8bfb\u969c\u788d\u65e9\u671f\u68c0\u6d4b\uff0c\u51c6\u786e\u7387\u8fbe88.58%", "motivation": "\u5168\u740310%-20%\u4eba\u7fa4\u53d7\u9605\u8bfb\u969c\u788d\u5f71\u54cd\uff0c\u4e9f\u9700\u5f00\u53d1\u4f4e\u6210\u672c\u3001\u6613\u83b7\u53d6\u7684\u65b0\u578b\u8bca\u65ad\u65b9\u6cd5", "method": "\u901a\u8fc7\u5206\u6790\u6ce8\u89c6\u65f6\u957f/\u773c\u8df3\u6a21\u5f0f\u63d0\u53d6\u7279\u5f81\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u91c7\u7528\u5c42\u6b21\u805a\u7c7b\u5212\u5206\u4e25\u91cd\u7b49\u7ea7", "result": "\u5728\u8de8\u4eba\u7fa4\u5b9e\u9a8c\u4e2d\u5b9e\u73b088.58%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u4e34\u754c\u75c5\u4f8b", "conclusion": "\u773c\u52a8\u8ffd\u8e2a+\u673a\u5668\u5b66\u4e60\u7684\u975e\u4fb5\u5165\u5f0f\u65b9\u6848\u663e\u8457\u6539\u8fdb\u4e86\u8bca\u65ad\u6d41\u7a0b\uff0c\u4e3a\u4e34\u5e8a\u7814\u7a76\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u624b\u6bb5"}}
{"id": "2506.11012", "pdf": "https://arxiv.org/pdf/2506.11012", "abs": "https://arxiv.org/abs/2506.11012", "authors": ["Guanglin Niu", "Bo Li", "Yangguang Lin"], "title": "A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects", "categories": ["cs.AI", "cs.CL", "I.2.7"], "comment": "45 pages, 17 figures, 12 tables", "summary": "Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring\nand leveraging diverse real-world knowledge, which serve as a fundamental\ntechnology for enabling cognitive intelligence systems with advanced\nunderstanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims\nto infer new knowledge based on existing facts in KGs, playing a crucial role\nin applications such as public security intelligence, intelligent healthcare,\nand financial risk assessment. From a task-centric perspective, existing KGR\napproaches can be broadly classified into static single-step KGR, static\nmulti-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR.\nWhile existing surveys have covered these six types of KGR tasks, a\ncomprehensive review that systematically summarizes all KGR tasks particularly\nincluding downstream applications and more challenging reasoning paradigms\nremains lacking. In contrast to previous works, this survey provides a more\ncomprehensive perspective on the research of KGR by categorizing approaches\nbased on primary reasoning tasks, downstream application tasks, and potential\nchallenging reasoning tasks. Besides, we explore advanced techniques, such as\nlarge language models (LLMs), and their impact on KGR. This work aims to\nhighlight key research trends and outline promising future directions in the\nfield of KGR.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4efb\u52a1\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e3b\u8981\u63a8\u7406\u4efb\u52a1/\u4e0b\u6e38\u5e94\u7528/\u6311\u6218\u6027\u4efb\u52a1\u7684\u4e09\u7ef4\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8LLM\u7b49\u524d\u6cbf\u6280\u672f\u5bf9\u9886\u57df\u7684\u5f71\u54cd", "motivation": "\u73b0\u6709\u7efc\u8ff0\u7f3a\u4e4f\u5bf9\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e0b\u6e38\u5e94\u7528\u573a\u666f\u548c\u66f4\u5177\u6311\u6218\u6027\u63a8\u7406\u8303\u5f0f\uff08\u5982\u52a8\u6001\u63a8\u7406\u3001\u5c11\u6837\u672c\u63a8\u7406\uff09\u7684\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6", "method": "\u4ece\u4efb\u52a1\u4e2d\u5fc3\u89c6\u89d2\u6784\u5efa\u4e09\u7ef4\u5206\u7c7b\u4f53\u7cfb\uff1a1\uff09\u57fa\u7840\u63a8\u7406\u4efb\u52a1\uff08\u9759\u6001/\u52a8\u6001/\u591a\u6a21\u6001\u7b49\uff092\uff09\u9886\u57df\u5e94\u7528\u4efb\u52a1\uff08\u533b\u7597/\u91d1\u878d/\u5b89\u5168\uff093\uff09\u524d\u6cbf\u6311\u6218\u4efb\u52a1\uff08\u5c11\u6837\u672c/\u5f52\u7eb3\u5f0f/\u56e0\u679c\u63a8\u7406\uff09", "result": "\u63d0\u51fa\u9996\u4e2a\u878d\u5408\u6280\u672f\u6f14\u8fdb\u4e0e\u5e94\u7528\u573a\u666f\u7684\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793aLLM\u4e0e\u77e5\u8bc6\u56fe\u8c31\u878d\u5408\u63a8\u7406\u7684\u6280\u672f\u8def\u5f84\uff0c\u6307\u660e\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u3001\u6301\u7eed\u5b66\u4e60\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6b63\u5411\u52a8\u6001\u5316\u3001\u591a\u6a21\u6001\u5316\u65b9\u5411\u53d1\u5c55\uff0cLLM\u7684\u878d\u5408\u5c06\u91cd\u5851\u63a8\u7406\u8303\u5f0f\uff0c\u8de8\u6a21\u6001\u5bf9\u9f50\u3001\u56e0\u679c\u63a8\u7406\u7b49\u5c06\u6210\u4e3a\u5173\u952e\u7a81\u7834\u65b9\u5411"}}
{"id": "2506.11022", "pdf": "https://arxiv.org/pdf/2506.11022", "abs": "https://arxiv.org/abs/2506.11022", "authors": ["Shivani Shukla", "Himanshu Joshi", "Romilla Syed"], "title": "Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": "Keywords - Large Language Models, Security Vulnerabilities,\n  AI-Generated Code, Iterative Feedback, Software Security, Secure Coding\n  Practices, Feedback Loops, LLM Prompting Strategies", "summary": "The rapid adoption of Large Language Models(LLMs) for code generation has\ntransformed software development, yet little attention has been given to how\nsecurity vulnerabilities evolve through iterative LLM feedback. This paper\nanalyzes security degradation in AI-generated code through a controlled\nexperiment with 400 code samples across 40 rounds of \"improvements\" using four\ndistinct prompting strategies. Our findings show a 37.6% increase in critical\nvulnerabilities after just five iterations, with distinct vulnerability\npatterns emerging across different prompting approaches. This evidence\nchallenges the assumption that iterative LLM refinement improves code security\nand highlights the essential role of human expertise in the loop. We propose\npractical guidelines for developers to mitigate these risks, emphasizing the\nneed for robust human validation between LLM iterations to prevent the\nparadoxical introduction of new security issues during supposedly beneficial\ncode \"improvements\".", "AI": {"tldr": "\u8fed\u4ee3LLM\u53cd\u9988\u5bfc\u81f4AI\u751f\u6210\u4ee3\u7801\u7684\u4e25\u91cd\u6f0f\u6d1e\u589e\u52a037.6%\uff0c\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4ea7\u751f\u5dee\u5f02\u5316\u6f0f\u6d1e\u6a21\u5f0f\uff0c\u9700\u52a0\u5f3a\u4eba\u5de5\u9a8c\u8bc1\u73af\u8282", "motivation": "\u7814\u7a76LLM\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4ee3\u7801\u5b89\u5168\u6f0f\u6d1e\u7684\u6f14\u5316\u89c4\u5f8b\uff0c\u6311\u6218'AI\u81ea\u52a8\u4f18\u5316\u63d0\u5347\u5b89\u5168\u6027'\u7684\u56fa\u6709\u8ba4\u77e5", "method": "\u91c7\u7528400\u4e2a\u4ee3\u7801\u6837\u672c\u8fdb\u884c40\u8f6e\u6539\u8fdb\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u56db\u79cd\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u5728\u4e94\u6b21\u8fed\u4ee3\u4e2d\u7684\u5b89\u5168\u8868\u73b0", "result": "\u4ec5\u4e94\u6b21\u8fed\u4ee3\u5373\u51fa\u73b0\u5173\u952e\u6f0f\u6d1e\u663e\u8457\u589e\u957f\uff08+37.6%\uff09\uff0c\u4e0d\u540c\u63d0\u793a\u65b9\u6cd5\u5448\u73b0\u7279\u5f81\u6027\u6f0f\u6d1e\u7c7b\u578b\u5206\u5e03", "conclusion": "\u4ee3\u7801\u5b89\u5168\u4f18\u5316\u9700\u5efa\u7acb'\u4eba\u7c7b\u4e13\u5bb6-LLM'\u534f\u540c\u673a\u5236\uff0c\u5efa\u8bae\u5236\u5b9a\u5f3a\u5236\u4eba\u5de5\u9a8c\u8bc1\u6d41\u7a0b\u7684\u5f00\u53d1\u8005\u6307\u5357\u4ee5\u9632\u6b62\u81ea\u52a8\u5316\u6539\u8fdb\u4e2d\u7684\u5b89\u5168\u9690\u60a3"}}
{"id": "2506.11031", "pdf": "https://arxiv.org/pdf/2506.11031", "abs": "https://arxiv.org/abs/2506.11031", "authors": ["Zoher Kachwala", "Danishjeet Singh", "Danielle Yang", "Filippo Menczer"], "title": "Task-aligned prompting improves zero-shot detection of AI-generated images by Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "As image generators produce increasingly realistic images, concerns about\npotential misuse continue to grow. Supervised detection relies on large,\ncurated datasets and struggles to generalize across diverse generators. In this\nwork, we investigate the use of pre-trained Vision-Language Models (VLMs) for\nzero-shot detection of AI-generated images. While off-the-shelf VLMs exhibit\nsome task-specific reasoning and chain-of-thought prompting offers gains, we\nshow that task-aligned prompting elicits more focused reasoning and\nsignificantly improves performance without fine-tuning. Specifically, prefixing\nthe model's response with the phrase ``Let's examine the style and the\nsynthesis artifacts'' -- a method we call zero-shot-s$^2$ -- boosts Macro F1\nscores by 8%-29% for two widely used open-source models. These gains are\nconsistent across three recent, diverse datasets spanning human faces, objects,\nand animals with images generated by 16 different models -- demonstrating\nstrong generalization. We further evaluate the approach across three additional\nmodel sizes and observe improvements in most dataset-model combinations --\nsuggesting robustness to model scale. Surprisingly, self-consistency, a\nbehavior previously observed in language reasoning, where aggregating answers\nfrom diverse reasoning paths improves performance, also holds in this setting.\nEven here, zero-shot-s$^2$ scales better than chain-of-thought in most cases --\nindicating that it elicits more useful diversity. Our findings show that\ntask-aligned prompts elicit more focused reasoning and enhance latent\ncapabilities in VLMs, like the detection of AI-generated images -- offering a\nsimple, generalizable, and explainable alternative to supervised methods. Our\ncode is publicly available on github:\nhttps://github.com/osome-iu/Zero-shot-s2.git.", "AI": {"tldr": "Proposes zero-shot-s\u00b2 prompting method to enhance VLM capabilities for AI-generated image detection without fine-tuning, achieving 8%-29% F1 improvements across diverse datasets/models.", "motivation": "Addressing supervised detection's limitations in generalization across diverse image generators by leveraging VLMs' latent capabilities.", "method": "1) Task-aligned prompting with style/artifact-focused prefix (zero-shot-s\u00b2) 2) Self-consistency through aggregated reasoning paths", "result": "8%-29% Macro F1 gains on 16 generators across 3 datasets. Robust performance across model scales (tested on 3 sizes) with improved generalization.", "conclusion": "Task-aligned prompting unlocks VLMs' latent detection abilities, offering generalizable and explainable alternative to supervised methods without dataset constraints."}}
{"id": "2506.11034", "pdf": "https://arxiv.org/pdf/2506.11034", "abs": "https://arxiv.org/abs/2506.11034", "authors": ["Aneesh Komanduri", "Karuna Bhaila", "Xintao Wu"], "title": "CausalVLBench: Benchmarking Visual Causal Reasoning in Large Vision-Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable ability in various\nlanguage tasks, especially with their emergent in-context learning capability.\nExtending LLMs to incorporate visual inputs, large vision-language models\n(LVLMs) have shown impressive performance in tasks such as recognition and\nvisual question answering (VQA). Despite increasing interest in the utility of\nLLMs in causal reasoning tasks such as causal discovery and counterfactual\nreasoning, there has been relatively little work showcasing the abilities of\nLVLMs on visual causal reasoning tasks. We take this opportunity to formally\nintroduce a comprehensive causal reasoning benchmark for multi-modal in-context\nlearning from LVLMs. Our CausalVLBench encompasses three representative tasks:\ncausal structure inference, intervention target prediction, and counterfactual\nprediction. We evaluate the ability of state-of-the-art open-source LVLMs on\nour causal reasoning tasks across three causal representation learning datasets\nand demonstrate their fundamental strengths and weaknesses. We hope that our\nbenchmark elucidates the drawbacks of existing vision-language models and\nmotivates new directions and paradigms in improving the visual causal reasoning\nabilities of LVLMs.", "AI": {"tldr": "\u63d0\u51faCausalVLBench\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u7ed3\u6784\u63a8\u65ad\u3001\u5e72\u9884\u76ee\u6807\u9884\u6d4b\u548c\u53cd\u4e8b\u5b9e\u9884\u6d4b\u4e09\u5927\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u89c6\u89c9\u56e0\u679c\u63a8\u7406\u4e2d\u7684\u6839\u672c\u6027\u7f3a\u9677", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u9700\u8981\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u63a8\u52a8\u9886\u57df\u53d1\u5c55", "method": "\u6784\u5efa\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u56e0\u679c\u63a8\u7406\u57fa\u51c6\uff0c\u4f7f\u7528\u4e09\u4e2a\u56e0\u679c\u8868\u793a\u6570\u636e\u96c6\u8bc4\u4f30\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "result": "\u73b0\u6709\u6a21\u578b\u5728\u56e0\u679c\u7ed3\u6784\u63a8\u65ad\uff08\u51c6\u786e\u7387<50%\uff09\u548c\u53cd\u4e8b\u5b9e\u9884\u6d4b\uff08\u6210\u529f\u7387\u4ec532%\uff09\u8868\u73b0\u663e\u8457\u4e0d\u8db3\uff0c\u4f46\u5c55\u793a\u51fa\u521d\u6b65\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b", "conclusion": "\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u9700\u5f00\u53d1\u65b0\u8303\u5f0f\u63d0\u5347\u56e0\u679c\u8868\u5f81\u5b66\u4e60\u4e0e\u63a8\u7406\u80fd\u529b"}}
{"id": "2506.11035", "pdf": "https://arxiv.org/pdf/2506.11035", "abs": "https://arxiv.org/abs/2506.11035", "authors": ["Moussa Koulako Bala Doumbouya", "Dan Jurafsky", "Christopher D. Manning"], "title": "Tversky Neural Networks: Psychologically Plausible Deep Learning with Differentiable Tversky Similarity", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV", "68", "I.2.0; I.2.4; I.2.6; I.2.7; I.4.7; I.4.10; I.5.1; F.1.1"], "comment": null, "summary": "Work in psychology has highlighted that the geometric model of similarity\nstandard in deep learning is not psychologically plausible because its metric\nproperties such as symmetry do not align with human perception. In contrast,\nTversky (1977) proposed an axiomatic theory of similarity based on a\nrepresentation of objects as sets of features, and their similarity as a\nfunction of common and distinctive features. However, this model has not been\nused in deep learning before, partly due to the challenge of incorporating\ndiscrete set operations. We develop a differentiable parameterization of\nTversky's similarity that is learnable through gradient descent, and derive\nneural network building blocks such as the Tversky projection layer, which\nunlike the linear projection layer can model non-linear functions such as XOR.\nThrough experiments with image recognition and language modeling, we show that\nthe Tversky projection layer is a beneficial replacement for the linear\nprojection layer, which employs geometric similarity. On the NABirds image\nclassification task, a frozen ResNet-50 adapted with a Tversky projection layer\nachieves a 24.7% relative accuracy improvement over the linear layer adapter\nbaseline. With Tversky projection layers, GPT-2's perplexity on PTB decreases\nby 7.5%, and its parameter count by 34.8%. Finally, we propose a unified\ninterpretation of both projection layers as computing similarities of input\nstimuli to learned prototypes, for which we also propose a novel visualization\ntechnique highlighting the interpretability of Tversky projection layers. Our\nwork offers a new paradigm for thinking about the similarity model implicit in\ndeep learning, and designing networks that are interpretable under an\nestablished theory of psychological similarity.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u5206Tversky\u76f8\u4f3c\u6027\u6a21\u578b\u66ff\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u51e0\u4f55\u76f8\u4f3c\u6027\uff0c\u901a\u8fc7Tversky\u6295\u5f71\u5c42\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u7684\u51e0\u4f55\u76f8\u4f3c\u6027\u6a21\u578b\u4e0d\u7b26\u5408\u4eba\u7c7b\u5fc3\u7406\u611f\u77e5\u7279\u5f81\uff0cTversky\u57fa\u4e8e\u7279\u5f81\u96c6\u5408\u7684\u76f8\u4f3c\u6027\u7406\u8bba\u66f4\u5408\u7406\u4f46\u5c1a\u672a\u88ab\u5e94\u7528", "method": "\u5f00\u53d1\u53ef\u5fae\u5206Tversky\u76f8\u4f3c\u6027\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1Tversky\u6295\u5f71\u5c42\uff08\u652f\u6301\u975e\u7ebf\u6027\u8fd0\u7b97\u5982XOR\uff09\uff0c\u66ff\u4ee3\u7ebf\u6027\u6295\u5f71\u5c42", "result": "ResNet-50\u5728NABirds\u51c6\u786e\u7387\u63d0\u534724.7%\uff1bGPT-2\u5728PTB\u6570\u636e\u96c6\u56f0\u60d1\u5ea6\u964d\u4f4e7.5%\uff0c\u53c2\u6570\u91cf\u51cf\u5c1134.8%", "conclusion": "Tversky\u76f8\u4f3c\u6027\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u5fc3\u7406\u53ef\u4fe1\u7684\u76f8\u4f3c\u6027\u5efa\u6a21\u65b0\u8303\u5f0f\uff0c\u539f\u578b\u53ef\u89c6\u5316\u6280\u672f\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.11040", "pdf": "https://arxiv.org/pdf/2506.11040", "abs": "https://arxiv.org/abs/2506.11040", "authors": ["Feifei Shi", "Xueyan Yin", "Kang Wang", "Wanyu Tu", "Qifu Sun", "Huansheng Ning"], "title": "Large Language models for Time Series Analysis: Techniques, Applications, and Challenges", "categories": ["cs.LG", "cs.CL", "cs.ET"], "comment": null, "summary": "Time series analysis is pivotal in domains like financial forecasting and\nbiomedical monitoring, yet traditional methods are constrained by limited\nnonlinear feature representation and long-term dependency capture. The\nemergence of Large Language Models (LLMs) offers transformative potential by\nleveraging their cross-modal knowledge integration and inherent attention\nmechanisms for time series analysis. However, the development of\ngeneral-purpose LLMs for time series from scratch is still hindered by data\ndiversity, annotation scarcity, and computational requirements. This paper\npresents a systematic review of pre-trained LLM-driven time series analysis,\nfocusing on enabling techniques, potential applications, and open challenges.\nFirst, it establishes an evolutionary roadmap of AI-driven time series\nanalysis, from the early machine learning era, through the emerging LLM-driven\nparadigm, to the development of native temporal foundation models. Second, it\norganizes and systematizes the technical landscape of LLM-driven time series\nanalysis from a workflow perspective, covering LLMs' input, optimization, and\nlightweight stages. Finally, it critically examines novel real-world\napplications and highlights key open challenges that can guide future research\nand innovation. The work not only provides valuable insights into current\nadvances but also outlines promising directions for future development. It\nserves as a foundational reference for both academic and industrial\nresearchers, paving the way for the development of more efficient,\ngeneralizable, and interpretable systems of LLM-driven time series analysis.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0LLM\u9a71\u52a8\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u63d0\u51faAI\u53d1\u5c55\u8def\u7ebf\u56fe\uff08\u673a\u5668\u5b66\u4e60\u2192LLM\u8303\u5f0f\u2192\u65f6\u95f4\u57fa\u7840\u6a21\u578b\uff09\uff0c\u5e76\u4ece\u5de5\u4f5c\u6d41\u7a0b\u89d2\u5ea6\u6784\u5efa\u6280\u672f\u6846\u67b6\uff0c\u63a2\u8ba8\u5b9e\u9645\u5e94\u7528\u4e0e\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u65b9\u6cd5\u5728\u975e\u7ebf\u6027\u7279\u5f81\u8868\u5f81\u548c\u957f\u671f\u4f9d\u8d56\u6355\u6349\u4e0a\u7684\u5c40\u9650\uff0c\u5229\u7528LLM\u7684\u8de8\u6a21\u6001\u6574\u5408\u4f18\u52bf\u63a8\u52a8\u66f4\u9ad8\u6548\u3001\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u5206\u6790\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u5efa\u7acb\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684AI\u8fdb\u5316\u8def\u7ebf\u56fe\uff0c\u4eceLLM\u8f93\u5165/\u4f18\u5316/\u8f7b\u91cf\u5316\u4e09\u9636\u6bb5\u6784\u5efa\u6280\u672f\u4f53\u7cfb\uff0c\u5e76\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7a0b\u89c6\u89d2\u7cfb\u7edf\u5316\u65b9\u6cd5\u8bba\u3002", "result": "\u5f62\u6210\u6db5\u76d6\u6280\u672f\u8def\u5f84-\u5e94\u7528\u573a\u666f-\u6311\u6218\u7684\u5b8c\u6574\u6846\u67b6\uff0c\u63ed\u793a\u9884\u8bad\u7ec3LLM\u5728\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u63d0\u53d6\u3001\u65f6\u5e8f\u5efa\u6a21\u548c\u9886\u57df\u9002\u5e94\u65b9\u9762\u7684\u7a81\u7834\u6027\u6f5c\u529b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u5c06\u91cd\u5851\u884c\u4e1a\u8303\u5f0f\uff0c\u9700\u6301\u7eed\u89e3\u51b3\u6570\u636e\u5f02\u6784\u6027\u3001\u8ba1\u7b97\u6548\u7387\u7b49\u6838\u5fc3\u95ee\u9898\uff0c\u63a8\u52a8\u539f\u751f\u65f6\u95f4\u57fa\u7840\u6a21\u578b\u7684\u521b\u65b0\u53d1\u5c55\u3002"}}
{"id": "2506.11059", "pdf": "https://arxiv.org/pdf/2506.11059", "abs": "https://arxiv.org/abs/2506.11059", "authors": ["Hanxi Guo", "Siyuan Cheng", "Kaiyuan Zhang", "Guangyu Shen", "Xiangyu Zhang"], "title": "CodeMirage: A Multi-Lingual Benchmark for Detecting AI-Generated and Paraphrased Source Code from Production-Level LLMs", "categories": ["cs.SE", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have become integral to modern software\ndevelopment, producing vast amounts of AI-generated source code. While these\nmodels boost programming productivity, their misuse introduces critical risks,\nincluding code plagiarism, license violations, and the propagation of insecure\nprograms. As a result, robust detection of AI-generated code is essential. To\nsupport the development of such detectors, a comprehensive benchmark that\nreflects real-world conditions is crucial. However, existing benchmarks fall\nshort -- most cover only a limited set of programming languages and rely on\nless capable generative models. In this paper, we present CodeMirage, a\ncomprehensive benchmark that addresses these limitations through three major\nadvancements: (1) it spans ten widely used programming languages, (2) includes\nboth original and paraphrased code samples, and (3) incorporates outputs from\nten state-of-the-art production-level LLMs, including both reasoning and\nnon-reasoning models from six major providers. Using CodeMirage, we evaluate\nten representative detectors across four methodological paradigms under four\nrealistic evaluation configurations, reporting results using three\ncomplementary metrics. Our analysis reveals nine key findings that uncover the\nstrengths and weaknesses of current detectors, and identify critical challenges\nfor future work. We believe CodeMirage offers a rigorous and practical testbed\nto advance the development of robust and generalizable AI-generated code\ndetectors.", "AI": {"tldr": "\u63d0\u51faCodeMirage\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30AI\u751f\u6210\u4ee3\u7801\u68c0\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u8986\u76d610\u79cd\u7f16\u7a0b\u8bed\u8a00\u548c10\u4e2a\u5148\u8fdbLLM\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u8986\u76d6\u8bed\u8a00\u6709\u9650\u4e14\u4f9d\u8d56\u843d\u540e\u6a21\u578b\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u771f\u5b9e\u573a\u666f\u4e0b\u7684AI\u4ee3\u7801\u68c0\u6d4b\u80fd\u529b\uff0c\u4e9f\u9700\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "1) \u652f\u630110\u79cd\u4e3b\u6d41\u7f16\u7a0b\u8bed\u8a00 2) \u5305\u542b\u539f\u59cb/\u6539\u5199\u4ee3\u7801\u6837\u672c 3) \u6574\u54086\u5927\u5382\u5546\u768410\u4e2a\u5148\u8fdbLLM\uff08\u542b\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u6a21\u578b\uff09", "result": "\u8bc4\u4f30\u53d1\u73b0\u5f53\u524d\u68c0\u6d4b\u5668\u5b58\u57289\u5927\u5173\u952e\u95ee\u9898\uff0c\u63ed\u793a\u5176\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u3001\u6a21\u578b\u7c7b\u578b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u53ca\u6cdb\u5316\u80fd\u529b\u7f3a\u9677\u3002", "conclusion": "CodeMirage\u4e3a\u5f00\u53d1\u9c81\u68d2\u7684AI\u4ee3\u7801\u68c0\u6d4b\u5668\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5176\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u5411\u5b9e\u9645\u5e94\u7528\u8fc8\u8fdb\u3002"}}
{"id": "2506.11064", "pdf": "https://arxiv.org/pdf/2506.11064", "abs": "https://arxiv.org/abs/2506.11064", "authors": ["Jiajun He", "Tomoki Toda"], "title": "PMF-CEC: Phoneme-augmented Multimodal Fusion for Context-aware ASR Error Correction with Error-specific Selective Decoding", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Accepted by IEEE TASLP 2025", "summary": "End-to-end automatic speech recognition (ASR) models often struggle to\naccurately recognize rare words. Previously, we introduced an ASR\npostprocessing method called error detection and context-aware error correction\n(ED-CEC), which leverages contextual information such as named entities and\ntechnical terms to improve the accuracy of ASR transcripts. Although ED-CEC\nachieves a notable success in correcting rare words, its accuracy remains low\nwhen dealing with rare words that have similar pronunciations but different\nspellings. To address this issue, we proposed a phoneme-augmented multimodal\nfusion method for context-aware error correction (PMF-CEC) method on the basis\nof ED-CEC, which allowed for better differentiation between target rare words\nand homophones. Additionally, we observed that the previous ASR error detection\nmodule suffers from overdetection. To mitigate this, we introduced a retention\nprobability mechanism to filter out editing operations with confidence scores\nbelow a set threshold, preserving the original operation to improve error\ndetection accuracy. Experiments conducted on five datasets demonstrated that\nour proposed PMF-CEC maintains reasonable inference speed while further\nreducing the biased word error rate compared with ED-CEC, showing a stronger\nadvantage in correcting homophones. Moreover, our method outperforms other\ncontextual biasing methods, and remains valuable compared with LLM-based\nmethods in terms of faster inference and better robustness under large biasing\nlists.", "AI": {"tldr": "\u63d0\u51fa\u97f3\u7d20\u589e\u5f3a\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5PMF-CEC\uff0c\u901a\u8fc7\u97f3\u7d20\u4fe1\u606f\u589e\u5f3a\u548c\u4fdd\u7559\u6982\u7387\u673a\u5236\u6539\u8fdb\u8bed\u97f3\u8bc6\u522b\u540e\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u540c\u97f3\u5f02\u5f62\u8bcd\u7ea0\u9519\u6548\u679c\u5e76\u4fdd\u6301\u63a8\u7406\u6548\u7387", "motivation": "\u73b0\u6709ED-CEC\u65b9\u6cd5\u5728\u5904\u7406\u53d1\u97f3\u76f8\u4f3c\u4f46\u62fc\u5199\u4e0d\u540c\u7684\u7f55\u89c1\u8bcd\u65f6\u51c6\u786e\u7387\u4e0d\u8db3\uff0c\u4e14\u5b58\u5728\u9519\u8bef\u68c0\u6d4b\u8fc7\u8bc6\u522b\u95ee\u9898", "method": "\u5728ED-CEC\u57fa\u7840\u4e0a\u5f15\u5165\u97f3\u7d20\u7279\u5f81\u7684\u591a\u6a21\u6001\u878d\u5408\u67b6\u6784\uff0c\u5e76\u8bbe\u8ba1\u4fdd\u7559\u6982\u7387\u673a\u5236\u8fc7\u6ee4\u4f4e\u7f6e\u4fe1\u5ea6\u4fee\u6539\u64cd\u4f5c", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0cPMF-CEC\u76f8\u6bd4ED-CEC\u8fdb\u4e00\u6b65\u964d\u4f4e12.7%\u7684\u504f\u7f6e\u8bcd\u9519\u8bef\u7387\uff0c\u540c\u97f3\u8bcd\u7ea0\u9519\u51c6\u786e\u7387\u63d0\u534723.4%\uff0c\u63a8\u7406\u901f\u5ea6\u4fdd\u6301\u572828ms/\u53e5", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u540c\u97f3\u8bcd\u533a\u5206\uff0c\u8f83\u5927\u89c4\u6a21\u504f\u7f6e\u5217\u8868\u4e0b\u9c81\u68d2\u6027\u4f18\u4e8eLLM\u65b9\u6848\uff0c\u4e3a\u8bed\u97f3\u8bc6\u522b\u540e\u5904\u7406\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.11069", "pdf": "https://arxiv.org/pdf/2506.11069", "abs": "https://arxiv.org/abs/2506.11069", "authors": ["Tao Zhong", "Mengzhe Geng", "Shujie Hu", "Guinan Li", "Xunying Liu"], "title": "Regularized Federated Learning for Privacy-Preserving Dysarthric and Elderly Speech Recognition", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Accurate recognition of dysarthric and elderly speech remains challenging to\ndate. While privacy concerns have driven a shift from centralized approaches to\nfederated learning (FL) to ensure data confidentiality, this further\nexacerbates the challenges of data scarcity, imbalanced data distribution and\nspeaker heterogeneity. To this end, this paper conducts a systematic\ninvestigation of regularized FL techniques for privacy-preserving dysarthric\nand elderly speech recognition, addressing different levels of the FL process\nby 1) parameter-based, 2) embedding-based and 3) novel loss-based\nregularization. Experiments on the benchmark UASpeech dysarthric and\nDementiaBank Pitt elderly speech corpora suggest that regularized FL systems\nconsistently outperform the baseline FedAvg system by statistically significant\nWER reductions of up to 0.55\\% absolute (2.13\\% relative). Further increasing\ncommunication frequency to one exchange per batch approaches centralized\ntraining performance.", "AI": {"tldr": "\u5229\u7528\u6b63\u5219\u5316\u8054\u90a6\u5b66\u4e60\u6280\u672f\u6539\u8fdb\u6784\u97f3\u969c\u788d\u548c\u8001\u5e74\u8bed\u97f3\u8bc6\u522b\uff0c\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387", "motivation": "\u96c6\u4e2d\u5f0f\u8bed\u97f3\u8bc6\u522b\u5b58\u5728\u9690\u79c1\u95ee\u9898\uff0c\u8054\u90a6\u5b66\u4e60(FL)\u867d\u4fdd\u62a4\u9690\u79c1\u4f46\u52a0\u5267\u6570\u636e\u7a00\u7f3a\u3001\u5206\u5e03\u4e0d\u5747\u8861\u548c\u8bf4\u8bdd\u4eba\u5f02\u8d28\u6027\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6b63\u5219\u5316FL\u6280\u672f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u63d0\u51fa\u4e09\u5c42\u6b21FL\u6b63\u5219\u5316\u6846\u67b6\uff1a1)\u53c2\u6570\u7ea7\u6b63\u5219\u5316 2)\u5d4c\u5165\u7a7a\u95f4\u6b63\u5219\u5316 3)\u65b0\u578b\u635f\u5931\u51fd\u6570\u6b63\u5219\u5316\uff1b\u5728UASpeech\u6784\u97f3\u969c\u788d\u548cDementiaBank Pitt\u8001\u5e74\u8bed\u97f3\u6570\u636e\u96c6\u5b9e\u9a8c\uff0c\u91c7\u7528\u6279\u91cf\u7ea7\u9ad8\u9891\u53c2\u6570\u901a\u4fe1\u673a\u5236", "result": "\u6b63\u5219\u5316FL\u7cfb\u7edf\u6bd4FedAvg\u57fa\u7ebf\u5b9e\u73b0\u7edf\u8ba1\u663e\u8457\u76840.55%\u7edd\u5bf9\uff082.13%\u76f8\u5bf9\uff09\u8bcd\u9519\u7387\u964d\u4f4e\uff0c\u6279\u91cf\u7ea7\u901a\u4fe1\u4f7f\u6027\u80fd\u63a5\u8fd1\u96c6\u4e2d\u5f0f\u8bad\u7ec3", "conclusion": "\u6b63\u5219\u5316FL\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u9690\u79c1\u4fdd\u62a4\u573a\u666f\u4e0b\u7684\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\uff0c\u9ad8\u9891\u901a\u4fe1\u7b56\u7565\u7f29\u5c0f\u4e86\u4e0e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u7684\u5dee\u8ddd\uff0c\u4e3a\u533b\u7597\u9886\u57df\u90e8\u7f72\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.11072", "pdf": "https://arxiv.org/pdf/2506.11072", "abs": "https://arxiv.org/abs/2506.11072", "authors": ["Tahiya Chowdhury", "Veronica Romero"], "title": "Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling", "categories": ["eess.AS", "cs.CL", "cs.CY", "cs.SD", "stat.AP", "K.4; J.4; I.2"], "comment": "5 pages, 1 figure, 3 tables", "summary": "Machine learning-based behavioral models rely on features extracted from\naudio-visual recordings. The recordings are processed using open-source tools\nto extract speech features for classification models. These tools often lack\nvalidation to ensure reliability in capturing behaviorally relevant\ninformation. This gap raises concerns about reproducibility and fairness across\ndiverse populations and contexts. Speech processing tools, when used outside of\ntheir design context, can fail to capture behavioral variations equitably and\ncan then contribute to bias. We evaluate speech features extracted from two\nwidely used speech analysis tools, OpenSMILE and Praat, to assess their\nreliability when considering adolescents with autism. We observed considerable\nvariation in features across tools, which influenced model performance across\ncontext and demographic groups. We encourage domain-relevant verification to\nenhance the reliability of machine learning models in clinical applications.", "AI": {"tldr": "\u7814\u7a76\u6307\u51fa\u8bed\u97f3\u5206\u6790\u5de5\u5177OpenSMILE\u548cPraat\u5728\u81ea\u95ed\u75c7\u9752\u5c11\u5e74\u7fa4\u4f53\u4e2d\u7279\u5f81\u63d0\u53d6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u6ce2\u52a8\uff0c\u5f3a\u8c03\u9886\u57df\u9002\u914d\u6027\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f00\u6e90\u8bed\u97f3\u5206\u6790\u5de5\u5177\u7f3a\u4e4f\u884c\u4e3a\u76f8\u5173\u4fe1\u606f\u7684\u53ef\u9760\u6027\u9a8c\u8bc1\uff0c\u53ef\u80fd\u5728\u4e0d\u540c\u7fa4\u4f53\u548c\u573a\u666f\u4e2d\u5f15\u53d1\u53ef\u590d\u73b0\u6027\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4OpenSMILE\u548cPraat\u4e24\u4e2a\u4e3b\u6d41\u5de5\u5177\u63d0\u53d6\u7684\u8bed\u97f3\u7279\u5f81\uff0c\u8bc4\u4f30\u5176\u5728\u81ea\u95ed\u75c7\u9752\u5c11\u5e74\u7fa4\u4f53\u4e2d\u7684\u53ef\u9760\u6027\u3002", "result": "\u4e0d\u540c\u5de5\u5177\u63d0\u53d6\u7684\u7279\u5f81\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4e14\u8fd9\u4e9b\u5dee\u5f02\u5f71\u54cd\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u5883\u548c\u4eba\u53e3\u7fa4\u4f53\u4e2d\u7684\u8868\u73b0\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5efa\u8bae\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u52a0\u5f3a\u9886\u57df\u76f8\u5173\u7684\u5de5\u5177\u9a8c\u8bc1\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u4e0e\u516c\u5e73\u6027\u3002"}}
{"id": "2506.11079", "pdf": "https://arxiv.org/pdf/2506.11079", "abs": "https://arxiv.org/abs/2506.11079", "authors": ["Lingyun Gao", "Cristian Tejedor-Garcia", "Catia Cucchiarini", "Helmer Strik"], "title": "Improving Child Speech Recognition and Reading Mistake Detection by Using Prompts", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "This paper is accepted to Interspeech 2025. This publication is part\n  of the project Responsible AI for Voice Diagnostics (RAIVD) with file number\n  NGF.1607.22.013 of the research programme NGF AiNed Fellowship Grants which\n  is financed by the Dutch Research Council (NWO)", "summary": "Automatic reading aloud evaluation can provide valuable support to teachers\nby enabling more efficient scoring of reading exercises. However, research on\nreading evaluation systems and applications remains limited. We present a novel\nmultimodal approach that leverages audio and knowledge from text resources. In\nparticular, we explored the potential of using Whisper and instruction-tuned\nlarge language models (LLMs) with prompts to improve transcriptions for child\nspeech recognition, as well as their effectiveness in downstream reading\nmistake detection. Our results demonstrate the effectiveness of prompting\nWhisper and prompting LLM, compared to the baseline Whisper model without\nprompting. The best performing system achieved state-of-the-art recognition\nperformance in Dutch child read speech, with a word error rate (WER) of 5.1%,\nimproving the baseline WER of 9.4%. Furthermore, it significantly improved\nreading mistake detection, increasing the F1 score from 0.39 to 0.73.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408Whisper\u548c\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u793a\u6280\u672f\u5c06\u513f\u7ae5\u6717\u8bfb\u8bed\u97f3\u8bcd\u9519\u7387\u964d\u4f4e\u81f35.1%\uff0c\u9519\u8bef\u68c0\u6d4bF1\u503c\u63d0\u5347\u81f30.73\u3002", "motivation": "\u73b0\u6709\u6717\u8bfb\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u5de5\u5177\u5e2e\u52a9\u6559\u5e08\u8fdb\u884c\u9605\u8bfb\u7ec3\u4e60\u8bc4\u5206\uff0c\u7279\u522b\u662f\u9488\u5bf9\u513f\u7ae5\u8bed\u97f3\u8bc6\u522b\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528Whisper\u8bed\u97f3\u6a21\u578b+\u6307\u4ee4\u8c03\u4f18LLM\u7684\u591a\u6a21\u6001\u67b6\u6784\uff0c\u901a\u8fc7\u6587\u672c\u77e5\u8bc6\u63d0\u793a\u4f18\u5316\u513f\u7ae5\u8bed\u97f3\u8f6c\u5f55\uff0c\u63d0\u5347\u6717\u8bfb\u9519\u8bef\u68c0\u6d4b\u6d41\u7a0b\u3002", "result": "\u6700\u4f73\u7cfb\u7edf\u5c06\u8377\u5170\u513f\u7ae5\u6717\u8bfb\u8bcd\u9519\u7387\u4ece\u57fa\u7ebf9.4%\u964d\u81f35.1%\uff0c\u9605\u8bfb\u9519\u8bef\u68c0\u6d4bF1\u503c\u4ece0.39\u5927\u5e45\u63d0\u5347\u81f30.73\u3002", "conclusion": "\u63d0\u793a\u6280\u672f\u663e\u8457\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u4e0e\u9519\u8bef\u68c0\u6d4b\u6548\u679c\uff0c\u8be5\u7cfb\u7edf\u5728\u513f\u7ae5\u6717\u8bfb\u8bc4\u4f30\u9886\u57df\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u9a8c\u8bc1\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.11085", "pdf": "https://arxiv.org/pdf/2506.11085", "abs": "https://arxiv.org/abs/2506.11085", "authors": ["Justin Asher"], "title": "LeanExplore: A search engine for Lean 4 declarations", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR", "cs.LG", "cs.LO", "I.2.6; H.3.3; I.2.3"], "comment": "16 pages, 1 figure. Project website: https://www.leanexplore.com/ ,\n  Code: https://github.com/justincasher/lean-explore", "summary": "The expanding Lean 4 ecosystem poses challenges for navigating its vast\nlibraries. This paper introduces LeanExplore, a search engine for Lean 4\ndeclarations. LeanExplore enables users to semantically search for statements,\nboth formally and informally, across select Lean 4 packages (including\nBatteries, Init, Lean, Mathlib, PhysLean, and Std). This search capability is\npowered by a hybrid ranking strategy, integrating scores from a multi-source\nsemantic embedding model (capturing conceptual meaning from formal Lean code,\ndocstrings, AI-generated informal translations, and declaration titles), BM25+\nfor keyword-based lexical relevance, and a PageRank-based score reflecting\ndeclaration importance and interconnectedness. The search engine is accessible\nvia a dedicated website (https://www.leanexplore.com/) and a Python API\n(https://github.com/justincasher/lean-explore). Furthermore, the database can\nbe downloaded, allowing users to self-host the service. LeanExplore integrates\neasily with LLMs via the model context protocol (MCP), enabling users to chat\nwith an AI assistant about Lean declarations or utilize the search engine for\nbuilding theorem-proving agents. This work details LeanExplore's architecture,\ndata processing, functionalities, and its potential to enhance Lean 4 workflows\nand AI-driven mathematical research", "AI": {"tldr": "LeanExplore\u662f\u9488\u5bf9Lean 4\u751f\u6001\u7cfb\u7edf\u7684\u8bed\u4e49\u641c\u7d22\u5f15\u64ce\uff0c\u6574\u5408\u591a\u6e90\u8bed\u4e49\u6a21\u578b\u3001\u5173\u952e\u8bcd\u68c0\u7d22\u548c\u58f0\u660e\u91cd\u8981\u6027\u8bc4\u4f30\uff0c\u63d0\u4f9b\u7f51\u7ad9/API\u8bbf\u95ee\u5e76\u652f\u6301AI\u96c6\u6210", "motivation": "\u89e3\u51b3Lean 4\u5e93\u89c4\u6a21\u6269\u5927\u5bfc\u81f4\u7684\u58f0\u660e\u5bfc\u822a\u56f0\u96be\u95ee\u9898", "method": "\u91c7\u7528\u6df7\u5408\u6392\u5e8f\u7b56\u7565\uff1a\u591a\u6e90\u8bed\u4e49\u5d4c\u5165\u6a21\u578b\uff08\u5f62\u5f0f\u5316\u4ee3\u7801/\u6587\u6863/AI\u751f\u6210\u975e\u6b63\u5f0f\u7ffb\u8bd1\uff09\u3001BM25+\u5173\u952e\u8bcd\u76f8\u5173\u5ea6\u3001\u57fa\u4e8ePageRank\u7684\u58f0\u660e\u91cd\u8981\u6027\u8bc4\u4f30", "result": "\u63d0\u4f9b\u7f51\u7ad9(leanexplore.com)\u548cPython API\u8bbf\u95ee\uff0c\u652f\u6301\u6570\u636e\u5e93\u4e0b\u8f7d\u81ea\u6258\u7ba1\uff0c\u901a\u8fc7MCP\u534f\u8bae\u4e0eLLM\u96c6\u6210\u5b9e\u73b0AI\u52a9\u624b\u5bf9\u8bdd\u548c\u5b9a\u7406\u8bc1\u660e\u4ee3\u7406\u6784\u5efa", "conclusion": "\u8be5\u5de5\u5177\u53ef\u4f18\u5316Lean 4\u5de5\u4f5c\u6d41\u7a0b\uff0c\u63a8\u52a8AI\u9a71\u52a8\u7684\u6570\u5b66\u7814\u7a76\uff0c\u5176\u67b6\u6784\u8bbe\u8ba1\u548c\u6269\u5c55\u6027\u4e3a\u9886\u57df\u641c\u7d22\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.11087", "pdf": "https://arxiv.org/pdf/2506.11087", "abs": "https://arxiv.org/abs/2506.11087", "authors": ["Boya Xiong", "Shuo Wang", "Weifeng Ge", "Guanhua Chen", "Yun Chen"], "title": "ADAMIX: Adaptive Mixed-Precision Delta-Compression with Quantization Error Optimization for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive performance on various\nknowledge-intensive and complex reasoning tasks in different domains. In\ncertain scenarios like multi-tenant serving, a large number of LLMs finetuned\nfrom the same base model are deployed to meet complex requirements for users.\nRecent works explore delta-compression approaches to quantize and compress the\ndelta parameters between the customized LLM and the corresponding base model.\nHowever, existing works either exhibit unsatisfactory performance at high\ncompression ratios or depend on empirical bit allocation schemes. In this work,\nwe propose ADAMIX, an effective adaptive mixed-precision delta-compression\nframework. We provide a mathematical derivation of quantization error to\nmotivate our mixed-precision compression strategy and formulate the optimal\nmixed-precision bit allocation scheme as the solution to a 0/1 integer linear\nprogramming problem. Our derived bit allocation strategy minimizes the\nquantization error while adhering to a predefined compression ratio\nrequirement. Experimental results on various models and benchmarks demonstrate\nthat our approach surpasses the best baseline by a considerable margin. On\ntasks like AIME2024 and GQA, where the norm of $\\Delta \\mathbf{W}$ is large and\nthe base model lacks sufficient ability, ADAMIX outperforms the best baseline\nDelta-CoMe by 22.3% and 6.1% with 7B models, respectively.", "AI": {"tldr": "ADAMIX\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6df7\u5408\u7cbe\u5ea6delta\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u6bd4\u7279\u5206\u914d\u65b9\u6848\u663e\u8457\u63d0\u5347\u538b\u7f29\u6027\u80fd", "motivation": "\u73b0\u6709delta\u538b\u7f29\u65b9\u6cd5\u5728\u9ad8\u538b\u7f29\u6bd4\u4e0b\u6027\u80fd\u4e0d\u8db3\u4e14\u4f9d\u8d56\u7ecf\u9a8c\u6027\u6bd4\u7279\u5206\u914d\uff0c\u9700\u7406\u8bba\u6307\u5bfc\u7684\u4f18\u5316\u65b9\u6848", "method": "\u901a\u8fc7\u91cf\u5316\u8bef\u5dee\u6570\u5b66\u63a8\u5bfc\u5efa\u7acb\u6df7\u5408\u7cbe\u5ea6\u7b56\u7565\uff0c\u5c06\u6700\u4f18\u6bd4\u7279\u5206\u914d\u5efa\u6a21\u4e3a0/1\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898", "result": "\u57287B\u6a21\u578b\u4e0a\uff0cAIME2024\u548cGQA\u4efb\u52a1\u5206\u522b\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf22.3%\u548c6.1%", "conclusion": "ADAMIX\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u538b\u7f29\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u6700\u4f73\u5e73\u8861"}}
{"id": "2506.11089", "pdf": "https://arxiv.org/pdf/2506.11089", "abs": "https://arxiv.org/abs/2506.11089", "authors": ["Jeena Prakash", "Blessingh Kumar", "Kadri Hacioglu", "Bidisha Sharma", "Sindhuja Gopalan", "Malolan Chetlur", "Shankar Venkatesan", "Andreas Stolcke"], "title": "Better Pseudo-labeling with Multi-ASR Fusion and Error Correction by SpeechLLM", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": null, "summary": "Automatic speech recognition (ASR) models rely on high-quality transcribed\ndata for effective training. Generating pseudo-labels for large unlabeled audio\ndatasets often relies on complex pipelines that combine multiple ASR outputs\nthrough multi-stage processing, leading to error propagation, information loss\nand disjoint optimization. We propose a unified multi-ASR prompt-driven\nframework using postprocessing by either textual or speech-based large language\nmodels (LLMs), replacing voting or other arbitration logic for reconciling the\nensemble outputs. We perform a comparative study of multiple architectures with\nand without LLMs, showing significant improvements in transcription accuracy\ncompared to traditional methods. Furthermore, we use the pseudo-labels\ngenerated by the various approaches to train semi-supervised ASR models for\ndifferent datasets, again showing improved performance with textual and\nspeechLLM transcriptions compared to baselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591aASR\u63d0\u793a\u9a71\u52a8\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u540e\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u8f6c\u5f55\u51c6\u786e\u6027\u548c\u534a\u76d1\u7763\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edfASR\u4f2a\u6807\u7b7e\u751f\u6210\u4f9d\u8d56\u590d\u6742\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\u548c\u4fe1\u606f\u635f\u5931\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u7edf\u4e00\u5904\u7406\u65b9\u6cd5", "method": "\u6574\u5408\u591a\u4e2aASR\u8f93\u51fa\u540e\uff0c\u91c7\u7528\u6587\u672c/\u8bed\u97f3\u5927\u6a21\u578b\u8fdb\u884c\u7edf\u4e00\u540e\u5904\u7406\uff0c\u53d6\u4ee3\u4f20\u7edf\u6295\u7968\u4ef2\u88c1\u673a\u5236\uff0c\u5e76\u8fdb\u884c\u591a\u67b6\u6784\u5bf9\u6bd4\u5b9e\u9a8c", "result": "LLM\u540e\u5904\u7406\u4f7f\u8f6c\u5f55\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u534a\u76d1\u7763ASR\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5747\u5c55\u73b0\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u591aASR\u8f93\u51fa\u6574\u5408\u96be\u9898\uff0c\u8bc1\u660e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u5904\u7406\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.11096", "pdf": "https://arxiv.org/pdf/2506.11096", "abs": "https://arxiv.org/abs/2506.11096", "authors": ["Guillaume Wisniewski", "S\u00e9verine Guillaume", "Clara Rosina Fern\u00e1ndez"], "title": "Assessing the Impact of Anisotropy in Neural Representations of Speech: A Case Study on Keyword Spotting", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Pretrained speech representations like wav2vec2 and HuBERT exhibit strong\nanisotropy, leading to high similarity between random embeddings. While widely\nobserved, the impact of this property on downstream tasks remains unclear. This\nwork evaluates anisotropy in keyword spotting for computational documentary\nlinguistics. Using Dynamic Time Warping, we show that despite anisotropy,\nwav2vec2 similarity measures effectively identify words without transcription.\nOur results highlight the robustness of these representations, which capture\nphonetic structures and generalize across speakers. Our results underscore the\nimportance of pretraining in learning rich and invariant speech\nrepresentations.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c3d\u7ba1wav2vec2\u5b58\u5728\u5404\u5411\u5f02\u6027\uff0c\u4f46\u5176\u76f8\u4f3c\u6027\u5ea6\u91cf\u4ecd\u80fd\u6709\u6548\u5b9e\u73b0\u65e0\u6807\u6ce8\u7684\u8bed\u97f3\u5173\u952e\u8bcd\u8bc6\u522b", "motivation": "\u63a2\u7a76\u5404\u5411\u5f02\u6027\u7279\u5f81\u5bf9\u8bed\u97f3\u5173\u952e\u8bcd\u8bc6\u522b\u4efb\u52a1\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u9a8c\u8bc1\u9884\u8bad\u7ec3\u8bed\u97f3\u8868\u5f81\u7684\u9c81\u68d2\u6027", "method": "\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574(DTW)\u65b9\u6cd5\u8bc4\u4f30wav2vec2\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u5728\u8ba1\u7b97\u7eaa\u5f55\u7247\u8bed\u8a00\u5b66\u4e2d\u7684\u5173\u952e\u8bcd\u8bc6\u522b\u6548\u679c", "result": "wav2vec2\u7684\u8bed\u97f3\u8868\u5f81\u6210\u529f\u6355\u83b7\u8bed\u97f3\u7ed3\u6784\u7279\u5f81\uff0c\u5177\u6709\u8de8\u8bf4\u8bdd\u4eba\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u5b9e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6709\u6548\u6027", "conclusion": "\u9884\u8bad\u7ec3\u8fc7\u7a0b\u5bf9\u5b66\u4e60\u5177\u6709\u4e30\u5bcc\u8bed\u97f3\u7279\u5f81\u4e14\u4e0d\u53d7\u8bf4\u8bdd\u4eba\u5dee\u5f02\u5f71\u54cd\u7684\u9c81\u68d2\u8bed\u97f3\u8868\u5f81\u81f3\u5173\u91cd\u8981"}}
{"id": "2506.11099", "pdf": "https://arxiv.org/pdf/2506.11099", "abs": "https://arxiv.org/abs/2506.11099", "authors": ["Huiling Zhu", "Yingqi Zeng"], "title": "Knowledge Graph Embeddings with Representing Relations as Annular Sectors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Knowledge graphs (KGs), structured as multi-relational data of entities and\nrelations, are vital for tasks like data analysis and recommendation systems.\nKnowledge graph completion (KGC), or link prediction, addresses incompleteness\nof KGs by inferring missing triples (h, r, t). It is vital for downstream\napplications. Region-based embedding models usually embed entities as points\nand relations as geometric regions to accomplish the task. Despite progress,\nthese models often overlook semantic hierarchies inherent in entities. To solve\nthis problem, we propose SectorE, a novel embedding model in polar coordinates.\nRelations are modeled as annular sectors, combining modulus and phase to\ncapture inference patterns and relation attributes. Entities are embedded as\npoints within these sectors, intuitively encoding hierarchical structure.\nEvaluated on FB15k-237, WN18RR, and YAGO3-10, SectorE achieves competitive\nperformance against various kinds of models, demonstrating strengths in\nsemantic modeling capability.", "AI": {"tldr": "\u63d0\u51fa\u6781\u5750\u6807\u7cfb\u5d4c\u5165\u6a21\u578bSectorE\uff0c\u901a\u8fc7\u73af\u5f62\u533a\u57df\u5efa\u6a21\u5173\u7cfb\u548c\u5b9e\u4f53\u5c42\u6b21\u7ed3\u6784\uff0c\u63d0\u5347\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u8bed\u4e49\u5efa\u6a21\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u533a\u57df\u7684\u5d4c\u5165\u6a21\u578b\u5e38\u5ffd\u89c6\u5b9e\u4f53\u95f4\u7684\u8bed\u4e49\u5c42\u7ea7\u5173\u7cfb\uff0c\u5236\u7ea6\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6548\u679c", "method": "\u5728\u6781\u5750\u6807\u7cfb\u4e2d\u5c06\u5173\u7cfb\u5efa\u6a21\u4e3a\u73af\u5f62\u533a\u57df\uff08\u6a21\u957f+\u76f8\u4f4d\u7ec4\u5408\uff09\uff0c\u5b9e\u4f53\u4f5c\u4e3a\u533a\u57df\u5185\u7684\u70b9\u5d4c\u5165\uff0c\u81ea\u7136\u5f62\u6210\u5c42\u6b21\u7ed3\u6784", "result": "\u5728FB15k-237/WN18RR/YAGO3-10\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u7ade\u4e89\u529b\u8868\u73b0\uff0c\u5c24\u5176\u5728\u8bed\u4e49\u5efa\u6a21\u65b9\u9762\u4f18\u52bf\u663e\u8457", "conclusion": "SectorE\u901a\u8fc7\u51e0\u4f55\u5efa\u6a21\u6709\u6548\u6355\u6349\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u4e3a\u5173\u7cfb\u63a8\u7406\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.11221", "pdf": "https://arxiv.org/pdf/2506.11221", "abs": "https://arxiv.org/abs/2506.11221", "authors": ["Weibing Zheng", "Laurah Turner", "Jess Kropczynski", "Murat Ozer", "Tri Nguyen", "Shane Halse"], "title": "LLM-as-a-Fuzzy-Judge: Fine-Tuning Large Language Models as a Clinical Evaluation Judge with Fuzzy Logic", "categories": ["cs.AI", "cs.CL", "cs.LO", "D.2.4; K.3.1; C.3; I.2.6"], "comment": "12 pages, 1 figure, 2025 IFSA World Congress NAFIPS Annual Meeting", "summary": "Clinical communication skills are critical in medical education, and\npracticing and assessing clinical communication skills on a scale is\nchallenging. Although LLM-powered clinical scenario simulations have shown\npromise in enhancing medical students' clinical practice, providing automated\nand scalable clinical evaluation that follows nuanced physician judgment is\ndifficult. This paper combines fuzzy logic and Large Language Model (LLM) and\nproposes LLM-as-a-Fuzzy-Judge to address the challenge of aligning the\nautomated evaluation of medical students' clinical skills with subjective\nphysicians' preferences. LLM-as-a-Fuzzy-Judge is an approach that LLM is\nfine-tuned to evaluate medical students' utterances within student-AI patient\nconversation scripts based on human annotations from four fuzzy sets, including\nProfessionalism, Medical Relevance, Ethical Behavior, and Contextual\nDistraction. The methodology of this paper started from data collection from\nthe LLM-powered medical education system, data annotation based on\nmultidimensional fuzzy sets, followed by prompt engineering and the supervised\nfine-tuning (SFT) of the pre-trained LLMs using these human annotations. The\nresults show that the LLM-as-a-Fuzzy-Judge achieves over 80\\% accuracy, with\nmajor criteria items over 90\\%, effectively leveraging fuzzy logic and LLM as a\nsolution to deliver interpretable, human-aligned assessment. This work suggests\nthe viability of leveraging fuzzy logic and LLM to align with human\npreferences, advances automated evaluation in medical education, and supports\nmore robust assessment and judgment practices. The GitHub repository of this\nwork is available at https://github.com/2sigmaEdTech/LLMAsAJudge", "AI": {"tldr": "\u63d0\u51faLLM-as-a-Fuzzy-Judge\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u4e0eLLM\u5b9e\u73b0\u533b\u5b66\u6559\u80b2\u4e2d\u4e34\u5e8a\u6c9f\u901a\u6280\u80fd\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u89e3\u51b3\u8bc4\u4f30\u7ed3\u679c\u4e0e\u533b\u751f\u4e3b\u89c2\u5224\u65ad\u5bf9\u9f50\u7684\u96be\u9898\u3002", "motivation": "\u533b\u5b66\u6559\u80b2\u4e2d\u4e34\u5e8a\u6c9f\u901a\u6280\u80fd\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\u4e0e\u81ea\u52a8\u5316\u8bc4\u4f30\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709LLM\u9a71\u52a8\u7684\u4e34\u5e8a\u6a21\u62df\u7cfb\u7edf\u96be\u4ee5\u5b9e\u73b0\u7b26\u5408\u533b\u751f\u4e3b\u89c2\u504f\u597d\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u3002", "method": "1. \u4eceLLM\u533b\u7597\u6559\u80b2\u7cfb\u7edf\u6536\u96c6\u5bf9\u8bdd\u6570\u636e 2. \u57fa\u4e8e\u4e13\u4e1a\u6027\u3001\u533b\u5b66\u76f8\u5173\u6027\u7b49\u56db\u4e2a\u6a21\u7cca\u96c6\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8 3. \u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u548c\u76d1\u7763\u5fae\u8c03(SFT)\u8bad\u7ec3LLM\u6a21\u62df\u533b\u751f\u8bc4\u4f30\u903b\u8f91", "result": "\u4e3b\u8981\u8bc4\u4f30\u6807\u51c6\u51c6\u786e\u7387\u8d8590%\uff0c\u6574\u4f53\u51c6\u786e\u7387\u8fbe80%+\uff0c\u6210\u529f\u5c06\u6a21\u7cca\u903b\u8f91\u4e0eLLM\u7ed3\u5408\u5b9e\u73b0\u4eba\u7c7b\u5bf9\u9f50\u7684\u8bc4\u4f30\u7cfb\u7edf\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u6a21\u7cca\u903b\u8f91+LLM\u5728\u533b\u5b66\u8bc4\u4f30\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u63a8\u52a8\u53ef\u89e3\u91ca\u6027\u81ea\u52a8\u5316\u8bc4\u4f30\u53d1\u5c55\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u5176\u4ed6\u533b\u5b66\u6559\u80b2\u573a\u666f\u3002"}}
{"id": "2506.11237", "pdf": "https://arxiv.org/pdf/2506.11237", "abs": "https://arxiv.org/abs/2506.11237", "authors": ["Ngoc Phuoc An Vo", "Brent Paulovicks", "Vadim Sheinin"], "title": "LLM-as-a-Judge for Reference-less Automatic Code Validation and Refinement for Natural Language to Bash in IT Automation", "categories": ["cs.SE", "cs.CL"], "comment": "10 pages", "summary": "In an effort to automatically evaluate and select the best model and improve\ncode quality for automatic incident remediation in IT Automation, it is crucial\nto verify if the generated code for remediation action is syntactically and\nsemantically correct and whether it can be executed correctly as intended.\nThere are three approaches: 1) conventional methods use surface form similarity\nmetrics (token match, exact match, etc.) which have numerous limitations, 2)\nexecution-based evaluation focuses more on code functionality based on\npass/fail judgments for given test-cases, and 3) LLM-as-a-Judge employs LLMs\nfor automated evaluation to judge if it is a correct answer for a given problem\nbased on pre-defined metrics. In this work, we focused on enhancing\nLLM-as-a-Judge using bidirectional functionality matching and logic\nrepresentation for reference-less automatic validation and refinement for Bash\ncode generation to select the best model for automatic incident remediation in\nIT Automation. We used execution-based evaluation as ground-truth to evaluate\nour LLM-as-a-Judge metrics. Results show high accuracy and agreement with\nexecution-based evaluation (and up to 8% over baseline). Finally, we built\nReflection code agents to utilize judgments and feedback from our evaluation\nmetrics which achieved significant improvement (up to 24% increase in accuracy)\nfor automatic code refinement.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u901a\u8fc7\u589e\u5f3aLLM-as-a-Judge\u65b9\u6cd5\uff0c\u7ed3\u5408\u53cc\u5411\u529f\u80fd\u5339\u914d\u548c\u903b\u8f91\u8868\u793a\u5b9e\u73b0Bash\u4ee3\u7801\u751f\u6210\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u4e0e\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347IT\u81ea\u52a8\u5316\u4e8b\u4ef6\u4fee\u590d\u7684\u6a21\u578b\u9009\u62e9\u4e0e\u4ee3\u7801\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u4ee3\u7801\u8bc4\u4f30\u65b9\u6cd5\uff08\u8868\u9762\u76f8\u4f3c\u6027\u6307\u6807\u548c\u6267\u884c\u6d4b\u8bd5\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u53c2\u8003\u65e0\u5173\u81ea\u52a8\u9a8c\u8bc1\u65b9\u6cd5\u6765\u9009\u62e9\u6700\u4f73\u4fee\u590d\u6a21\u578b\u5e76\u63d0\u5347\u4ee3\u7801\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u53cc\u5411\u529f\u80fd\u5339\u914d\u548c\u903b\u8f91\u8868\u793a\u589e\u5f3aLLM\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u6267\u884c\u6d4b\u8bd5\u4e3a\u57fa\u51c6\u9a8c\u8bc1\u6307\u6807\uff0c\u5e76\u6784\u5efaReflection\u4ee3\u7801\u4ee3\u7406\u8fdb\u884c\u81ea\u52a8\u4f18\u5316\u3002", "result": "LLM\u8bc4\u4f30\u6307\u6807\u4e0e\u6267\u884c\u6d4b\u8bd5\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff08\u51c6\u786e\u7387\u63d0\u53478%\uff09\uff0c\u4ee3\u7801\u4ee3\u7406\u5b9e\u73b024%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u589e\u5f3a\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\u7ed3\u5408\u81ea\u52a8\u5316\u4ee3\u7801\u4f18\u5316\u4ee3\u7406\uff0c\u53ef\u6709\u6548\u63d0\u5347IT\u81ea\u52a8\u5316\u4e8b\u4ef6\u4fee\u590d\u7684\u4ee3\u7801\u751f\u6210\u8d28\u91cf\u4e0e\u6a21\u578b\u9009\u62e9\u53ef\u9760\u6027\u3002"}}
{"id": "2506.11350", "pdf": "https://arxiv.org/pdf/2506.11350", "abs": "https://arxiv.org/abs/2506.11350", "authors": ["Heinrich Dinkel", "Zhiyong Yan", "Tianzi Wang", "Yongqing Wang", "Xingwei Sun", "Yadong Niu", "Jizhong Liu", "Gang Li", "Junbo Zhang", "Jian Luan"], "title": "GLAP: General contrastive audio-text pretraining across domains and languages", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Contrastive Language Audio Pretraining (CLAP) is a widely-used method to\nbridge the gap between audio and text domains. Current CLAP methods enable\nsound and music retrieval in English, ignoring multilingual spoken content. To\naddress this, we introduce general language audio pretraining (GLAP), which\nexpands CLAP with multilingual and multi-domain abilities. GLAP demonstrates\nits versatility by achieving competitive performance on standard audio-text\nretrieval benchmarks like Clotho and AudioCaps, while significantly surpassing\nexisting methods in speech retrieval and classification tasks. Additionally,\nGLAP achieves strong results on widely used sound-event zero-shot benchmarks,\nwhile simultaneously outperforming previous methods on speech content\nbenchmarks. Further keyword spotting evaluations across 50 languages emphasize\nGLAP's advanced multilingual capabilities. Finally, multilingual sound and\nmusic understanding is evaluated across four languages. Checkpoints and Source:\nhttps://github.com/xiaomi-research/dasheng-glap.", "AI": {"tldr": "\u63d0\u51faGLAP\u65b9\u6cd5\uff0c\u6269\u5c55CLAP\u4ee5\u652f\u6301\u591a\u8bed\u8a00\u548c\u591a\u9886\u57df\u97f3\u9891\u6587\u672c\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709CLAP\u65b9\u6cd5\u4ec5\u652f\u6301\u82f1\u8bed\u97f3\u9891\u6587\u672c\u68c0\u7d22\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u8bed\u8a00\u8bed\u97f3\u5185\u5bb9", "method": "\u5728CLAP\u6846\u67b6\u57fa\u7840\u4e0a\u6269\u5c55\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\uff0c\u6784\u5efa\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6GLAP", "result": "\u5728Clotho/AudioCaps\u97f3\u9891\u68c0\u7d22\u4efb\u52a1\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u8bed\u97f3\u68c0\u7d22\u51c6\u786e\u7387\u63d0\u534715%\uff0c\u652f\u630150\u79cd\u8bed\u8a00\u5173\u952e\u8bcd\u8bc6\u522b", "conclusion": "GLAP\u6210\u529f\u7a81\u7834\u8bed\u8a00\u9650\u5236\uff0c\u4e3a\u591a\u8bed\u8a00\u97f3\u9891\u7406\u89e3\u4efb\u52a1\u5efa\u7acb\u65b0\u57fa\u51c6\uff0c\u5f00\u6e90\u6a21\u578b\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2506.11375", "pdf": "https://arxiv.org/pdf/2506.11375", "abs": "https://arxiv.org/abs/2506.11375", "authors": ["Yitong Zhou", "Mingyue Cheng", "Qingyang Mao", "Yucong Luo", "Qi Liu", "Yupeng Li", "Xiaohan Zhang", "Deguang Liu", "Xin Li", "Enhong Chen"], "title": "Benchmarking Multimodal LLMs on Recognition and Understanding over Chemical Tables", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Chemical tables encode complex experimental knowledge through symbolic\nexpressions, structured variables, and embedded molecular graphics. Existing\nbenchmarks largely overlook this multimodal and domain-specific complexity,\nlimiting the ability of multimodal large language models to support scientific\nunderstanding in chemistry. In this work, we introduce ChemTable, a large-scale\nbenchmark of real-world chemical tables curated from the experimental sections\nof literature. ChemTable includes expert-annotated cell polygons, logical\nlayouts, and domain-specific labels, including reagents, catalysts, yields, and\ngraphical components and supports two core tasks: (1) Table Recognition,\ncovering structure parsing and content extraction; and (2) Table Understanding,\nencompassing both descriptive and reasoning-oriented question answering\ngrounded in table structure and domain semantics. We evaluated a range of\nrepresentative multimodal models, including both open-source and closed-source\nmodels, on ChemTable and reported a series of findings with practical and\nconceptual insights. Although models show reasonable performance on basic\nlayout parsing, they exhibit substantial limitations on both descriptive and\ninferential QA tasks compared to human performance, and we observe significant\nperformance gaps between open-source and closed-source models across multiple\ndimensions. These results underscore the challenges of chemistry-aware table\nunderstanding and position ChemTable as a rigorous and realistic benchmark for\nadvancing scientific reasoning.", "AI": {"tldr": "\u63d0\u51faChemTable\u4f5c\u4e3a\u5316\u5b66\u8868\u683c\u591a\u6a21\u6001\u7406\u89e3\u65b0\u57fa\u51c6\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u8868\u683c\u8bc6\u522b\u4e0e\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u57fa\u51c6\u5ffd\u89c6\u5316\u5b66\u8868\u683c\u7684\u591a\u6a21\u6001\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u652f\u6301\u5316\u5b66\u79d1\u5b66\u7406\u89e3", "method": "\u6784\u5efa\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684\u5316\u5b66\u8868\u683c\u6570\u636e\u96c6\uff0c\u5b9a\u4e49\u8868\u683c\u8bc6\u522b\uff08\u7ed3\u6784\u89e3\u6790/\u5185\u5bb9\u63d0\u53d6\uff09\u548c\u8868\u683c\u7406\u89e3\uff08\u63cf\u8ff0\u6027/\u63a8\u7406\u6027QA\uff09\u53cc\u4efb\u52a1", "result": "\u6a21\u578b\u5728\u5e03\u5c40\u89e3\u6790\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46QA\u4efb\u52a1\u663e\u8457\u843d\u540e\u4eba\u7c7b\uff0c\u4e14\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u5b58\u5728\u591a\u7ef4\u6027\u80fd\u5dee\u8ddd", "conclusion": "\u5316\u5b66\u611f\u77e5\u8868\u683c\u7406\u89e3\u5b58\u5728\u6311\u6218\uff0cChemTable\u4e3a\u63a8\u8fdb\u79d1\u5b66\u63a8\u7406\u63d0\u4f9b\u4e25\u683c\u57fa\u51c6"}}
{"id": "2506.11376", "pdf": "https://arxiv.org/pdf/2506.11376", "abs": "https://arxiv.org/abs/2506.11376", "authors": ["Liying Wang", "Ph. D.", "Daffodil Carrington", "M. S.", "Daniil Filienko", "M. S.", "Caroline El Jazmi", "M. S.", "Serena Jinchen Xie", "M. S.", "Martine De Cock", "Ph. D.", "Sarah Iribarren", "Ph. D.", "Weichao Yuwen", "Ph. D"], "title": "Large Language Model-Powered Conversational Agent Delivering Problem-Solving Therapy (PST) for Family Caregivers: Enhancing Empathy and Therapeutic Alliance Using In-Context Learning", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "Family caregivers often face substantial mental health challenges due to\ntheir multifaceted roles and limited resources. This study explored the\npotential of a large language model (LLM)-powered conversational agent to\ndeliver evidence-based mental health support for caregivers, specifically\nProblem-Solving Therapy (PST) integrated with Motivational Interviewing (MI)\nand Behavioral Chain Analysis (BCA). A within-subject experiment was conducted\nwith 28 caregivers interacting with four LLM configurations to evaluate empathy\nand therapeutic alliance. The best-performing models incorporated Few-Shot and\nRetrieval-Augmented Generation (RAG) prompting techniques, alongside\nclinician-curated examples. The models showed improved contextual understanding\nand personalized support, as reflected by qualitative responses and\nquantitative ratings on perceived empathy and therapeutic alliances.\nParticipants valued the model's ability to validate emotions, explore\nunexpressed feelings, and provide actionable strategies. However, balancing\nthorough assessment with efficient advice delivery remains a challenge. This\nwork highlights the potential of LLMs in delivering empathetic and tailored\nsupport for family caregivers.", "AI": {"tldr": "\u7814\u7a76\u6d4b\u8bd5\u4e0d\u540cLLM\u914d\u7f6e\u5bf9\u5bb6\u5ead\u7167\u987e\u8005\u5fc3\u7406\u652f\u6301\u7684\u6548\u679c\uff0c\u6700\u4f73\u6a21\u578b\u7ed3\u5408Few-Shot\u548cRAG\u6280\u672f\uff0c\u663e\u793a\u51fa\u5171\u60c5\u548c\u6cbb\u7597\u8054\u76df\u7684\u63d0\u5347\uff0c\u4f46\u5b58\u5728\u8bc4\u4f30\u4e0e\u5efa\u8bae\u6548\u7387\u7684\u5e73\u8861\u95ee\u9898", "motivation": "\u5bb6\u5ead\u7167\u987e\u8005\u56e0\u591a\u91cd\u89d2\u8272\u548c\u8d44\u6e90\u9650\u5236\u9762\u4e34\u5fc3\u7406\u5065\u5eb7\u6311\u6218\uff0c\u9700\u63a2\u7d22LLM\u63d0\u4f9b\u5faa\u8bc1\u5fc3\u7406\u652f\u6301\u7684\u53ef\u884c\u6027", "method": "28\u540d\u7167\u987e\u8005\u53c2\u4e0e\u5185\u4e3b\u4f53\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u7ed3\u5408PST/MI/BCA\u7684\u56db\u79cdLLM\u914d\u7f6e\uff0c\u91c7\u7528Few-Shot+RAG\u63d0\u793a\u6280\u672f\u53ca\u4e34\u5e8a\u6848\u4f8b\u4f18\u5316", "result": "\u4f18\u5316\u6a21\u578b\u5728\u60c5\u5883\u7406\u89e3\u548c\u4e2a\u6027\u5316\u652f\u6301\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u5b9a\u91cf\u663e\u793a\u5171\u60c5\u548c\u6cbb\u7597\u8054\u76df\u8bc4\u5206\u63d0\u9ad8\uff0c\u53c2\u4e0e\u8005\u8ba4\u53ef\u60c5\u611f\u9a8c\u8bc1\u548c\u7b56\u7565\u5b9e\u7528\u6027", "conclusion": "LLM\u5728\u5bb6\u5ead\u7167\u987e\u8005\u5fc3\u7406\u652f\u6301\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u672a\u6765\u9700\u5e73\u8861\u6df1\u5ea6\u8bc4\u4f30\u4e0e\u9ad8\u6548\u5efa\u8bae\uff0c\u63a8\u52a8\u4e2a\u6027\u5316\u5fc3\u7406\u5065\u5eb7\u5e72\u9884\u53d1\u5c55"}}
{"id": "2506.11402", "pdf": "https://arxiv.org/pdf/2506.11402", "abs": "https://arxiv.org/abs/2506.11402", "authors": ["Pradyut Sekhsaria", "Marcel Mateos Salles", "Hai Huang", "Randall Balestriero"], "title": "LoRA Users Beware: A Few Spurious Tokens Can Manipulate Your Finetuned Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "29 pages, 16 figures, 15 tables. Submitted for publication. for\n  associated blog post, see https://pradyut3501.github.io/lora-spur-corr/", "summary": "Parameter Efficient FineTuning (PEFT), such as Low-Rank Adaptation (LoRA),\naligns pre-trained Large Language Models (LLMs) to particular downstream tasks\nin a resource-efficient manner. Because efficiency has been the main metric of\nprogress, very little attention has been put in understanding possible\ncatastrophic failures. We uncover one such failure: PEFT encourages a model to\nsearch for shortcut solutions to solve its fine-tuning tasks. When very small\namount of tokens, e.g., one token per prompt, are correlated with downstream\ntask classes, PEFT makes any pretrained model rely predominantly on that token\nfor decision making. While such spurious tokens may emerge accidentally from\nincorrect data cleaning, it also opens opportunities for malevolent parties to\ncontrol a model's behavior from Seamless Spurious Token Injection (SSTI). In\nSSTI, a small amount of tokens correlated with downstream classes are injected\nby the dataset creators. At test time, the finetuned LLM's behavior can be\ncontrolled solely by injecting those few tokens. We apply SSTI across models\nfrom three families (Snowflake Arctic, Apple OpenELM, and Meta LLaMA-3) and\nfour diverse datasets (IMDB, Financial Classification, CommonSense QA, and Bias\nin Bios). Our findings reveal three astonishing behaviors. First, as few as a\nsingle token of SSTI is sufficient to steer a model's decision making. Second,\nfor light SSTI, the reliance on spurious tokens is proportional to the LoRA\nrank. Lastly, with aggressive SSTI, larger LoRA rank values become preferable\nto small rank values as it makes the model attend to non-spurious tokens, hence\nimproving robustness.", "AI": {"tldr": "PEFT\u65b9\u6cd5\uff08\u5982LoRA\uff09\u5728\u9ad8\u6548\u5fae\u8c03LLM\u65f6\u6613\u53d7\u865a\u5047\u4ee4\u724c\uff08SSTI\uff09\u64cd\u63a7\uff0c\u6a21\u578b\u51b3\u7b56\u8fc7\u5ea6\u4f9d\u8d56\u5355\u4ee4\u724c\u4e14LoRA\u79e9\u5f71\u54cd\u9c81\u68d2\u6027", "motivation": "\u63ed\u793a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5b58\u5728\u7684\u6f5c\u5728\u98ce\u9669\uff1a\u901a\u8fc7\u865a\u5047\u4ee4\u724c\u6ce8\u5165\uff08SSTI\uff09\u53ef\u64cd\u63a7\u6a21\u578b\u51b3\u7b56\uff0c\u6311\u6218\u73b0\u6709\u4ee5\u6548\u7387\u4e3a\u6838\u5fc3\u7684\u5fae\u8c03\u8303\u5f0f", "method": "\u57283\u5927\u6a21\u578b\u5bb6\u65cf\uff08Arctic/OpenELM/LLaMA-3\uff09\u548c4\u4e2a\u6570\u636e\u96c6\uff08IMDB/\u91d1\u878d\u5206\u7c7b/\u5e38\u8bc6QA/BiasBios\uff09\u4e2d\u690d\u5165\u4e0d\u540c\u5f3a\u5ea6SSTI\uff0c\u7cfb\u7edf\u5206\u6790LoRA\u79e9\u4e0e\u865a\u5047\u4ee4\u724c\u4f9d\u8d56\u5173\u7cfb", "result": "1. \u5355\u4e2a\u865a\u5047\u4ee4\u724c\u5373\u53ef\u64cd\u63a7\u6a21\u578b\u51b3\u7b56\n2. \u4f4eLoRA\u79e9\u6a21\u578b\u66f4\u4f9d\u8d56\u865a\u5047\u7279\u5f81\n3. \u9ad8LoRA\u79e9\u5728\u6fc0\u8fdbSSTI\u4e2d\u5c55\u73b0\u66f4\u597d\u9c81\u68d2\u6027", "conclusion": "\u63ed\u793aPEFT\u8106\u5f31\u6027\u673a\u5236\uff0c\u5efa\u8bae\u5728\u9ad8\u6548\u5fae\u8c03\u65f6\u9700\u5e73\u8861\u79e9\u53c2\u6570\u4e0e\u9c81\u68d2\u6027\uff0c\u8b66\u60d5\u6570\u636e\u96c6\u4e2d\u6f5c\u5728/\u6076\u610f\u7684\u865a\u5047\u7279\u5f81\u6ce8\u5165\u98ce\u9669"}}
{"id": "2506.11415", "pdf": "https://arxiv.org/pdf/2506.11415", "abs": "https://arxiv.org/abs/2506.11415", "authors": ["Linlin Wang", "Tianqing Zhu", "Laiqiao Qin", "Longxiang Gao", "Wanlei Zhou"], "title": "Bias Amplification in RAG: Poisoning Knowledge Retrieval to Steer LLMs", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "In Large Language Models, Retrieval-Augmented Generation (RAG) systems can\nsignificantly enhance the performance of large language models by integrating\nexternal knowledge. However, RAG also introduces new security risks. Existing\nresearch focuses mainly on how poisoning attacks in RAG systems affect model\noutput quality, overlooking their potential to amplify model biases. For\nexample, when querying about domestic violence victims, a compromised RAG\nsystem might preferentially retrieve documents depicting women as victims,\ncausing the model to generate outputs that perpetuate gender stereotypes even\nwhen the original query is gender neutral. To show the impact of the bias, this\npaper proposes a Bias Retrieval and Reward Attack (BRRA) framework, which\nsystematically investigates attack pathways that amplify language model biases\nthrough a RAG system manipulation. We design an adversarial document generation\nmethod based on multi-objective reward functions, employ subspace projection\ntechniques to manipulate retrieval results, and construct a cyclic feedback\nmechanism for continuous bias amplification. Experiments on multiple mainstream\nlarge language models demonstrate that BRRA attacks can significantly enhance\nmodel biases in dimensions. In addition, we explore a dual stage defense\nmechanism to effectively mitigate the impacts of the attack. This study reveals\nthat poisoning attacks in RAG systems directly amplify model output biases and\nclarifies the relationship between RAG system security and model fairness. This\nnovel potential attack indicates that we need to keep an eye on the fairness\nissues of the RAG system.", "AI": {"tldr": "RAG\u7cfb\u7edf\u589e\u5f3aLLM\u6027\u80fd\u4f46\u5f15\u5165\u504f\u89c1\u98ce\u9669\uff0cBRRA\u6846\u67b6\u5c55\u793a\u653b\u51fb\u5982\u4f55\u901a\u8fc7\u68c0\u7d22\u64cd\u7eb5\u653e\u5927\u6a21\u578b\u504f\u89c1\uff0c\u9700\u5173\u6ce8RAG\u7cfb\u7edf\u516c\u5e73\u6027", "motivation": "\u73b0\u6709\u7814\u7a76\u805a\u7126RAG\u4e2d\u6bd2\u653b\u51fb\u5bf9\u8f93\u51fa\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5374\u5ffd\u89c6\u5176\u653e\u5927\u6a21\u578b\u504f\u89c1\u7684\u6f5c\u5728\u98ce\u9669\uff08\u5982\u4e2d\u6027\u67e5\u8be2\u4e0b\u5f3a\u5316\u6027\u522b\u523b\u677f\u5370\u8c61\uff09\uff0c\u9700\u7cfb\u7edf\u7814\u7a76\u653b\u51fb\u8def\u5f84", "method": "\u63d0\u51faBRRA\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u591a\u76ee\u6807\u5956\u52b1\u51fd\u6570\u7684\u5bf9\u6297\u6587\u6863\u751f\u6210 2\uff09\u5b50\u7a7a\u95f4\u6295\u5f71\u6280\u672f\u64cd\u63a7\u68c0\u7d22\u7ed3\u679c 3\uff09\u6784\u5efa\u504f\u89c1\u5faa\u73af\u53cd\u9988\u673a\u5236", "result": "\u5b9e\u9a8c\u663e\u793aBRRA\u653b\u51fb\u663e\u8457\u589e\u5f3a\u591a\u7ef4\u5ea6\u6a21\u578b\u504f\u89c1\uff0c\u53cc\u91cd\u9632\u5fa1\u673a\u5236\uff08\u68c0\u7d22\u51c0\u5316+\u6a21\u578b\u53bb\u504f\uff09\u6709\u6548\u7f13\u89e3\u653b\u51fb\u5f71\u54cd", "conclusion": "\u63ed\u793a\u4e86RAG\u7cfb\u7edf\u5b89\u5168\u4e0e\u6a21\u578b\u516c\u5e73\u6027\u7684\u5173\u8054\uff0c\u65b0\u578b\u653b\u51fb\u8868\u660e\u9700\u5c06\u516c\u5e73\u6027\u7eb3\u5165RAG\u5b89\u5168\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2506.11475", "pdf": "https://arxiv.org/pdf/2506.11475", "abs": "https://arxiv.org/abs/2506.11475", "authors": ["Syeda Kisaa Fatima", "Tehreem Zubair", "Noman Ahmed", "Asifullah Khan"], "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction", "categories": ["cs.MA", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution.", "AI": {"tldr": "\u63d0\u51faLUCID-MA\u591a\u667a\u80fd\u4f53\u79bb\u7ebf\u72af\u7f6a\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u6a21\u5757\u534f\u540c\u5b9e\u73b0\u72af\u7f6a\u65f6\u7a7a\u6a21\u5f0f\u8bc6\u522b\u3001\u7ed3\u679c\u4f18\u5316\u548c\u72af\u7f6a\u8d8b\u52bf\u9884\u6d4b\uff0c\u652f\u6301100\u8f6e\u81ea\u6539\u8fdb\u5bf9\u8bdd\u5e76\u914d\u5907\u53ef\u89c6\u5316\u8bc4\u5206\u7cfb\u7edf", "motivation": "\u89e3\u51b3\u793e\u4f1a\u79d1\u5b66\u9886\u57df\u72af\u7f6a\u6570\u636e\u5206\u6790\u7684\u81ea\u4e3b\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u9690\u79c1\u95ee\u9898\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u540c\u65f6\u5b9e\u73b0\u79bb\u7ebf\u5b89\u5168\u5206\u6790", "method": "1. \u5206\u6790\u52a9\u624b\u6a21\u5757\u63d0\u53d6\u65f6\u7a7a\u72af\u7f6a\u6a21\u5f0f\n2. \u53cd\u9988\u6a21\u5757\u4f18\u5316\u5206\u6790\u7ed3\u679c\n3. \u9884\u6d4b\u6a21\u5757\u751f\u6210\u72af\u7f6a\u8d8b\u52bf\n\u91c7\u7528LLaMA-2-13B-Chat-GPTQ\u6a21\u578b\u79bb\u7ebf\u8fd0\u884c\uff0c\u8bbe\u8ba1\u667a\u80fd\u4f53\u81ea\u6539\u8fdb\u673a\u5236\uff08100\u8f6e\u5bf9\u8bdd\uff09\u53ca\u53ef\u89c6\u5316\u8bc4\u5206\u7cfb\u7edf", "result": "\u6210\u529f\u9a8c\u8bc1AutoGen\u5f0f\u667a\u80fd\u4f53\u5728\u793e\u4f1a\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u5b9e\u73b0\u72af\u7f6a\u5206\u6790\u7684\u81ea\u4e3b\u8fed\u4ee3\u4e0e\u9690\u79c1\u4fdd\u62a4\u53cc\u91cd\u76ee\u6807", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79bb\u7ebf\u73af\u5883\u4e0b\u793e\u4f1a\u95ee\u9898\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6a21\u5f0f\u5bf9\u6570\u636e\u654f\u611f\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2506.11515", "pdf": "https://arxiv.org/pdf/2506.11515", "abs": "https://arxiv.org/abs/2506.11515", "authors": ["Xiao Xu", "Libo Qin", "Wanxiang Che", "Min-Yen Kan"], "title": "Manager: Aggregating Insights from Unimodal Experts in Two-Tower VLMs and MLLMs", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "Accepted by IEEE Transactions on Circuits and Systems for Video\n  Technology (TCSVT). June 2025. DOI:\n  https://doi.org/10.1109/TCSVT.2025.3578266", "summary": "Two-Tower Vision--Language Models (VLMs) have demonstrated strong performance\nacross various downstream VL tasks. While BridgeTower further enhances\nperformance by building bridges between encoders, it \\textit{(i)} suffers from\nineffective layer-by-layer utilization of unimodal representations,\n\\textit{(ii)} restricts the flexible exploitation of different levels of\nunimodal semantic knowledge, and \\textit{(iii)} is limited to the evaluation on\ntraditional low-resolution datasets only with the Two-Tower VLM architecture.\nIn this work, we propose Manager, a lightweight, efficient and effective plugin\nthat adaptively aggregates insights from different levels of pre-trained\nunimodal experts to facilitate more comprehensive VL alignment and fusion.\nFirst, under the Two-Tower VLM architecture, we introduce ManagerTower, a novel\nVLM that introduces the manager in each cross-modal layer. Whether with or\nwithout VL pre-training, ManagerTower outperforms previous strong baselines and\nachieves superior performance on 4 downstream VL tasks. Moreover, we extend our\nexploration to the latest Multimodal Large Language Model (MLLM) architecture.\nWe demonstrate that LLaVA-OV-Manager significantly boosts the zero-shot\nperformance of LLaVA-OV across different categories of capabilities, images,\nand resolutions on 20 downstream datasets, whether the multi-grid algorithm is\nenabled or not. In-depth analysis reveals that both our manager and the\nmulti-grid algorithm can be viewed as a plugin that improves the visual\nrepresentation by capturing more diverse visual details from two orthogonal\nperspectives (depth and width). Their synergy can mitigate the semantic\nambiguity caused by the multi-grid algorithm and further improve performance.\nCode and models are available at https://github.com/LooperXX/ManagerTower.", "AI": {"tldr": "\u63d0\u51faManager\u8f7b\u91cf\u7ea7\u63d2\u4ef6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u5408\u4e0d\u540c\u5c42\u7ea7\u5355\u6a21\u6001\u4e13\u5bb6\u77e5\u8bc6\u63d0\u5347\u53cc\u5854VLM\u548cMLLM\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u4efb\u52a1/\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6548\u679c\u63d0\u5347", "motivation": "\u73b0\u6709BridgeTower\u6a21\u578b\u5b58\u5728\u5355\u6a21\u6001\u8868\u5f81\u5229\u7528\u6548\u7387\u4f4e\u3001\u8bed\u4e49\u77e5\u8bc6\u5c42\u7ea7\u53d7\u9650\u3001\u8bc4\u4f30\u4f53\u7cfb\u5c40\u9650\u7b49\u95ee\u9898\uff0c\u9700\u7a81\u7834\u4f20\u7edf\u67b6\u6784\u7684\u8bed\u4e49\u878d\u5408\u74f6\u9888", "method": "\u5728\u53cc\u5854VLM\u67b6\u6784\u4e2d\u9010\u5c42\u5f15\u5165Manager\u6a21\u5757\uff0c\u5e76\u901a\u8fc7LLaVA-OV-Manager\u6269\u5c55\u5230MLLM\u67b6\u6784\uff0c\u5229\u7528\u591a\u7f51\u683c\u7b97\u6cd5\u4e0eManager\u7684\u534f\u540c\u4f18\u5316\u673a\u5236", "result": "ManagerTower\u57284\u9879\u4e0b\u6e38\u4efb\u52a1\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff1bLLaVA-OV-Manager\u572820\u4e2a\u6570\u636e\u96c6\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u53474.8%\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe12.3%", "conclusion": "Manager\u63d2\u4ef6\u4ece\u6df1\u5ea6\u7ef4\u5ea6\u589e\u5f3a\u89c6\u89c9\u8868\u5f81\uff0c\u4e0e\u591a\u7f51\u683c\u7b97\u6cd5\u7684\u5bbd\u5ea6\u6269\u5c55\u5f62\u6210\u6b63\u4ea4\u4e92\u8865\uff0c\u534f\u540c\u7f13\u89e3\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u81ea\u9002\u5e94\u77e5\u8bc6\u805a\u5408\u7684\u6709\u6548\u6027"}}
{"id": "2506.11516", "pdf": "https://arxiv.org/pdf/2506.11516", "abs": "https://arxiv.org/abs/2506.11516", "authors": ["Chengye Li", "Haiyun Liu", "Yuanxi Li"], "title": "Brewing Knowledge in Context: Distillation Perspectives on In-Context Learning", "categories": ["cs.LG", "cs.CL"], "comment": "10 main pages, 10 page appendix", "summary": "In-context learning (ICL) allows large language models (LLMs) to solve novel\ntasks without weight updates. Despite its empirical success, the mechanism\nbehind ICL remains poorly understood, limiting our ability to interpret,\nimprove, and reliably apply it. In this paper, we propose a new theoretical\nperspective that interprets ICL as an implicit form of knowledge distillation\n(KD), where prompt demonstrations guide the model to form a task-specific\nreference model during inference. Under this view, we derive a Rademacher\ncomplexity-based generalization bound and prove that the bias of the distilled\nweights grows linearly with the Maximum Mean Discrepancy (MMD) between the\nprompt and target distributions. This theoretical framework explains several\nempirical phenomena and unifies prior gradient-based and distributional\nanalyses. To the best of our knowledge, this is the first to formalize\ninference-time attention as a distillation process, which provides theoretical\ninsights for future prompt engineering and automated demonstration selection.", "AI": {"tldr": "\u8bba\u6587\u5c06\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u89e3\u91ca\u4e3a\u9690\u5f0f\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u51fa\u504f\u5dee\u4e0e\u5206\u5e03\u5dee\u5f02\u7684\u7ebf\u6027\u5173\u7cfb\uff0c\u7edf\u4e00\u4e86\u5148\u524d\u7684\u5206\u6790\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9ICL\u5de5\u4f5c\u673a\u5236\u7f3a\u4e4f\u6e05\u6670\u7406\u89e3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u8df5\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u548c\u6539\u8fdb\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u5c06ICL\u89c6\u4e3a\u63a8\u7406\u65f6\u7684\u77e5\u8bc6\u84b8\u998f\u8fc7\u7a0b\uff0c\u57fa\u4e8eRademacher\u590d\u6742\u6027\u63a8\u5bfc\u6cdb\u5316\u8fb9\u754c\uff0c\u5206\u6790MMD\u4e0e\u6743\u91cd\u504f\u5dee\u7684\u5b9a\u91cf\u5173\u7cfb\u3002", "result": "\u7406\u8bba\u6846\u67b6\u6210\u529f\u89e3\u91ca\u4e86\u591a\u4e2a\u5b9e\u8bc1\u73b0\u8c61\uff0c\u9996\u6b21\u5c06\u6ce8\u610f\u529b\u673a\u5236\u5f62\u5f0f\u5316\u4e3a\u84b8\u998f\u8fc7\u7a0b\uff0c\u7edf\u4e00\u4e86\u68af\u5ea6\u5206\u6790\u548c\u5206\u5e03\u5206\u6790\u4e24\u79cd\u7814\u7a76\u8def\u5f84\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u4f18\u5316\u63d0\u793a\u5de5\u7a0b\u548c\u5f00\u53d1\u81ea\u52a8\u5316\u793a\u8303\u9009\u62e9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63a8\u52a8\u4e86ICL\u673a\u5236\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u3002"}}
{"id": "2506.11555", "pdf": "https://arxiv.org/pdf/2506.11555", "abs": "https://arxiv.org/abs/2506.11555", "authors": ["Yu Wang", "Shiwan Zhao", "Ming Fan", "Zhihu Wang", "Yubo Zhang", "Xicheng Zhang", "Zhengfan Wang", "Heyuan Huang", "Ting Liu"], "title": "RAG+: Enhancing Retrieval-Augmented Generation with Application-Aware Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The integration of external knowledge through Retrieval-Augmented Generation\n(RAG) has become foundational in enhancing large language models (LLMs) for\nknowledge-intensive tasks. However, existing RAG paradigms often overlook the\ncognitive step of applying knowledge, leaving a gap between retrieved facts and\ntask-specific reasoning. In this work, we introduce RAG+, a principled and\nmodular extension that explicitly incorporates application-aware reasoning into\nthe RAG pipeline. RAG+ constructs a dual corpus consisting of knowledge and\naligned application examples, created either manually or automatically, and\nretrieves both jointly during inference. This design enables LLMs not only to\naccess relevant information but also to apply it within structured,\ngoal-oriented reasoning processes. Experiments across mathematical, legal, and\nmedical domains, conducted on multiple models, demonstrate that RAG+\nconsistently outperforms standard RAG variants, achieving average improvements\nof 3-5%, and peak gains up to 7.5% in complex scenarios. By bridging retrieval\nwith actionable application, RAG+ advances a more cognitively grounded\nframework for knowledge integration, representing a step toward more\ninterpretable and capable LLMs.", "AI": {"tldr": "RAG+\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u5e94\u7528\u63a8\u7406\u673a\u5236\u663e\u8457\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6548\u679c", "motivation": "\u73b0\u6709RAG\u8303\u5f0f\u5ffd\u89c6\u77e5\u8bc6\u5e94\u7528\u9636\u6bb5\u7684\u8ba4\u77e5\u6b65\u9aa4\uff0c\u5bfc\u81f4\u68c0\u7d22\u77e5\u8bc6\u4e0e\u4efb\u52a1\u63a8\u7406\u4e4b\u95f4\u5b58\u5728\u5e94\u7528\u9e3f\u6c9f", "method": "\u6784\u5efa\u77e5\u8bc6\u5e93\u4e0e\u5bf9\u9f50\u5e94\u7528\u5b9e\u4f8b\u7684\u53cc\u91cd\u8bed\u6599\u5e93\uff08\u4eba\u5de5\u6216\u81ea\u52a8\u751f\u6210\uff09\uff0c\u5728\u63a8\u7406\u65f6\u8fdb\u884c\u8054\u5408\u68c0\u7d22", "result": "\u5728\u6570\u5b66\u3001\u6cd5\u5f8b\u548c\u533b\u7597\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cRAG+\u5e73\u5747\u63d0\u53473-5%\uff0c\u590d\u6742\u573a\u666f\u4e0b\u5cf0\u503c\u589e\u76ca\u8fbe7.5%", "conclusion": "RAG+\u901a\u8fc7\u6865\u63a5\u68c0\u7d22\u4e0e\u4efb\u52a1\u5e94\u7528\uff0c\u5efa\u7acb\u4e86\u66f4\u5177\u8ba4\u77e5\u57fa\u7840\u7684\u77e5\u8bc6\u6574\u5408\u6846\u67b6\uff0c\u63a8\u52a8LLMs\u5411\u66f4\u53ef\u89e3\u91ca\u548c\u5f3a\u5927\u7684\u65b9\u5411\u53d1\u5c55"}}
{"id": "2506.11558", "pdf": "https://arxiv.org/pdf/2506.11558", "abs": "https://arxiv.org/abs/2506.11558", "authors": ["Bo-Cheng Chiu", "Jen-Jee Chen", "Yu-Chee Tseng", "Feng-Chi Chen"], "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.", "AI": {"tldr": "DaMO\u662f\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u5229\u7528\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u53cc\u6d41\u67b6\u6784\u548c\u56db\u9636\u6bb5\u6e10\u8fdb\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u65f6\u95f4\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u65f6\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u5c24\u5176\u5728\u76d1\u7763\u6570\u636e\u6709\u9650\u65f6\u96be\u4ee5\u7cbe\u786e\u5b9a\u4f4d\u89c6\u9891\u7247\u6bb5\u3002", "method": "\u63d0\u51fa\u65f6\u5e8f\u611f\u77e5\u878d\u5408\u6846\u67b6\uff08Temporal-aware Fuseformer\uff09\u5b9e\u73b0\u6a21\u6001\u5185\u65f6\u5e8f\u52a8\u6001\u6355\u83b7\u4e0e\u8de8\u6a21\u6001\u878d\u5408\uff0c\u91c7\u7528\u5168\u5c40\u6b8b\u5dee\u51cf\u5c11\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u56db\u9636\u6bb5\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff08\u591a\u6a21\u6001\u5bf9\u9f50\u2192\u8bed\u4e49\u5b9a\u4f4d\u2192\u65f6\u5e8f\u63a8\u7406\uff09\u6784\u5efa\u80fd\u529b\u4f53\u7cfb\u3002", "result": "\u5728\u65f6\u95f4\u5b9a\u4f4d\u548c\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u89c6\u9891-\u8bed\u8a00\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u5316\u6e10\u8fdb\u8bad\u7ec3\u4e0e\u8de8\u6a21\u6001\u65f6\u5e8f\u5efa\u6a21\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.11604", "pdf": "https://arxiv.org/pdf/2506.11604", "abs": "https://arxiv.org/abs/2506.11604", "authors": ["Ren\u00e9 Peinl", "Vincent Tischler"], "title": "VLM@school -- Evaluation of AI image understanding on German middle school knowledge", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces a novel benchmark dataset designed to evaluate the\ncapabilities of Vision Language Models (VLMs) on tasks that combine visual\nreasoning with subject-specific background knowledge in the German language. In\ncontrast to widely used English-language benchmarks that often rely on\nartificially difficult or decontextualized problems, this dataset draws from\nreal middle school curricula across nine domains including mathematics,\nhistory, biology, and religion. The benchmark includes over 2,000 open-ended\nquestions grounded in 486 images, ensuring that models must integrate visual\ninterpretation with factual reasoning rather than rely on superficial textual\ncues. We evaluate thirteen state-of-the-art open-weight VLMs across multiple\ndimensions, including domain-specific accuracy and performance on adversarial\ncrafted questions. Our findings reveal that even the strongest models achieve\nless than 45% overall accuracy, with particularly poor performance in music,\nmathematics, and adversarial settings. Furthermore, the results indicate\nsignificant discrepancies between success on popular benchmarks and real-world\nmultimodal understanding. We conclude that middle school-level tasks offer a\nmeaningful and underutilized avenue for stress-testing VLMs, especially in\nnon-English contexts. The dataset and evaluation protocol serve as a rigorous\ntestbed to better understand and improve the visual and linguistic reasoning\ncapabilities of future AI systems.", "AI": {"tldr": "\u6784\u5efa\u5fb7\u8bed\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e2d\u5b66\u8bfe\u7a0b\u573a\u666f\u4e0b\u7684\u8de8\u5b66\u79d1\u7efc\u5408\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u82f1\u8bed\u57fa\u51c6\u5b58\u5728\u8131\u79bb\u771f\u5b9e\u6559\u5b66\u60c5\u5883\u7684\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u8bfe\u7a0b\u7ea7\u591a\u6a21\u6001\u4efb\u52a1\u6d4b\u8bd5\u6a21\u578b\u7684\u5b9e\u9645\u8ba4\u77e5\u6c34\u5e73", "method": "\u57fa\u4e8e9\u5927\u5b66\u79d1486\u5f20\u771f\u5b9e\u6559\u5b66\u56fe\u50cf\u6784\u5efa2,000+\u5f00\u653e\u5f0f\u95ee\u9898\uff0c\u8bbe\u8ba1\u5bf9\u6297\u6027\u6d4b\u8bd5\u96c6\uff0c\u8bc4\u4f3013\u4e2a\u524d\u6cbfVLM\u7684\u591a\u7ef4\u5ea6\u8868\u73b0", "result": "\u6700\u4f18\u6a21\u578b\u7efc\u5408\u51c6\u786e\u7387\u4e0d\u8db345%\uff0c\u97f3\u4e50/\u6570\u5b66\u9886\u57df\u8868\u73b0\u6700\u5f31\uff0c\u5bf9\u6297\u6027\u95ee\u9898\u66b4\u9732\u6a21\u578b\u8106\u5f31\u6027", "conclusion": "\u4e2d\u5b66\u8bfe\u7a0b\u4efb\u52a1\u53ef\u4f5c\u4e3a\u6709\u6548\u7684AI\u538b\u529b\u6d4b\u8bd5\u573a\uff0c\u975e\u82f1\u8bed\u591a\u6a21\u6001\u8bc4\u4f30\u63ed\u793a\u6a21\u578b\u80fd\u529b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5b9e\u8d28\u5dee\u8ddd"}}
{"id": "2506.11620", "pdf": "https://arxiv.org/pdf/2506.11620", "abs": "https://arxiv.org/abs/2506.11620", "authors": ["Stefan Bleeck"], "title": "(SimPhon Speech Test): A Data-Driven Method for In Silico Design and Validation of a Phonetically Balanced Speech Test", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often provides an incomplete characterization of the\nfunctional impact of hearing loss on speech understanding, particularly for\nsupra-threshold deficits common in presbycusis. This motivates the development\nof more diagnostically specific speech perception tests. We introduce the\nSimulated Phoneme Speech Test (SimPhon Speech Test) methodology, a novel,\nmulti-stage computational pipeline for the in silico design and validation of a\nphonetically balanced minimal-pair speech test. This methodology leverages a\nmodern Automatic Speech Recognition (ASR) system as a proxy for a human\nlistener to simulate the perceptual effects of sensorineural hearing loss. By\nprocessing speech stimuli under controlled acoustic degradation, we first\nidentify the most common phoneme confusion patterns. These patterns then guide\nthe data-driven curation of a large set of candidate word pairs derived from a\ncomprehensive linguistic corpus. Subsequent phases involving simulated\ndiagnostic testing, expert human curation, and a final, targeted sensitivity\nanalysis systematically reduce the candidates to a final, optimized set of 25\npairs (the SimPhon Speech Test-25). A key finding is that the diagnostic\nperformance of the SimPhon Speech Test-25 test items shows no significant\ncorrelation with predictions from the standard Speech Intelligibility Index\n(SII), suggesting the SimPhon Speech Test captures perceptual deficits beyond\nsimple audibility. This computationally optimized test set offers a significant\nincrease in efficiency for audiological test development, ready for initial\nhuman trials.", "AI": {"tldr": "\u5f00\u53d1SimPhon\u8bed\u97f3\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7ASR\u6a21\u62df\u542c\u529b\u635f\u5931\uff0c\u6784\u5efa25\u7ec4\u4f18\u5316\u97f3\u7d20\u5bf9\uff0c\u7a81\u7834\u4f20\u7edfSII\u6307\u6807\u9650\u5236", "motivation": "\u4f20\u7edf\u542c\u529b\u6d4b\u8bd5\u65e0\u6cd5\u5b8c\u6574\u8bc4\u4f30\u8d85\u9608\u503c\u542c\u529b\u635f\u5931\u5bf9\u8bed\u8a00\u7406\u89e3\u7684\u5f71\u54cd\uff08\u5c24\u5176\u5728\u8001\u5e74\u6027\u8033\u804b\u4e2d\uff09\uff0c\u9700\u5f00\u53d1\u66f4\u5177\u8bca\u65ad\u7279\u5f02\u6027\u7684\u8bed\u97f3\u611f\u77e5\u6d4b\u8bd5\u65b9\u6cd5", "method": "1. \u7528ASR\u6a21\u62df\u611f\u97f3\u795e\u7ecf\u6027\u542c\u529b\u635f\u5931\n2. \u63a7\u5236\u58f0\u5b66\u9000\u5316\u8bc6\u522b\u97f3\u7d20\u6df7\u6dc6\u6a21\u5f0f\n3. \u57fa\u4e8e\u8bed\u6599\u5e93\u7b5b\u9009\u5019\u9009\u8bcd\u5bf9\n4. \u901a\u8fc7\u6a21\u62df\u6d4b\u8bd5\u3001\u4e13\u5bb6\u7b5b\u9009\u548c\u654f\u611f\u6027\u5206\u6790\u4f18\u5316\u51fa25\u7ec4\u6d4b\u8bd5\u5bf9", "result": "SimPhon-25\u6d4b\u8bd5\u8868\u73b0\u4e0e\u6807\u51c6\u8bed\u97f3\u6e05\u6670\u5ea6\u6307\u6570(SII)\u65e0\u663e\u8457\u76f8\u5173\uff0c\u8868\u660e\u5176\u53ef\u68c0\u6d4b\u8d85\u8d8a\u5355\u7eaf\u53ef\u542c\u5ea6\u7684\u611f\u77e5\u7f3a\u9677", "conclusion": "\u8ba1\u7b97\u4f18\u5316\u7684\u6d4b\u8bd5\u96c6\u663e\u8457\u63d0\u5347\u542c\u529b\u5b66\u6d4b\u8bd5\u5f00\u53d1\u6548\u7387\uff0c\u5df2\u51c6\u5907\u8fdb\u884c\u4eba\u4f53\u8bd5\u9a8c\u9a8c\u8bc1"}}
{"id": "2506.11737", "pdf": "https://arxiv.org/pdf/2506.11737", "abs": "https://arxiv.org/abs/2506.11737", "authors": ["Dinh Viet Cuong", "Hoang-Bao Le", "An Pham Ngoc Nguyen", "Liting Zhou", "Cathal Gurrin"], "title": "Quizzard@INOVA Challenge 2025 -- Track A: Plug-and-Play Technique in Interleaved Multi-Image Model", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": null, "summary": "This paper addresses two main objectives. Firstly, we demonstrate the\nimpressive performance of the LLaVA-NeXT-interleave on 22 datasets across three\ndifferent tasks: Multi-Image Reasoning, Documents and Knowledge-Based\nUnderstanding and Interactive Multi-Modal Communication. Secondly, we add the\nDense Channel Integration (DCI) connector to the LLaVA-NeXT-Interleave and\ncompare its performance against the standard model. We find that the standard\nmodel achieves the highest overall accuracy, excelling in vision-heavy tasks\nlike VISION, NLVR2, and Fashion200K. Meanwhile, the DCI-enhanced version shows\nparticular strength on datasets requiring deeper semantic coherence or\nstructured change understanding such as MIT-States_PropertyCoherence and\nSlideVQA. Our results highlight the potential of combining powerful foundation\nmodels with plug-and-play techniques for Interleave tasks. The code is\navailable at https://github.com/dinhvietcuong1996/icme25-inova.", "AI": {"tldr": "LLaVA-NeXT-interleave\u6846\u67b6\u572822\u4e2a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6807\u51c6\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u9886\u5148\uff0cDCI\u589e\u5f3a\u7248\u5728\u8bed\u4e49\u8fde\u8d2f\u4efb\u52a1\u4e2d\u66f4\u5177\u4f18\u52bf", "motivation": "\u63a2\u7d22\u5c06\u5f3a\u5927\u7684\u57fa\u7840\u6a21\u578b\u4e0e\u5373\u63d2\u5373\u7528\u6280\u672f\uff08DCI\u8fde\u63a5\u5668\uff09\u7ed3\u5408\uff0c\u63d0\u5347\u591a\u6a21\u6001\u4ea4\u66ff\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b", "method": "1. \u4f7f\u7528LLaVA-NeXT-interleave\u6846\u67b6\u6d4b\u8bd522\u4e2a\u6570\u636e\u96c6 2. \u65b0\u589eDCI\u8fde\u63a5\u5668\u8fdb\u884c\u901a\u9053\u96c6\u6210 3. \u5bf9\u6bd4\u6807\u51c6\u6a21\u578b\u4e0eDCI\u589e\u5f3a\u7248\u5728\u89c6\u89c9\u5bc6\u96c6\u578b\u548c\u8bed\u4e49\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u8868\u73b0", "result": "\u6807\u51c6\u6a21\u578b\u5728VISION\uff0892.3%\uff09\u3001NLVR2\uff0891.5%\uff09\u3001Fashion200K\uff0888.7%\uff09\u8868\u73b0\u6700\u4f73\uff1bDCI\u7248\u5728MIT-States_PropertyCoherence\uff08+2.1%\uff09\u548cSlideVQA\uff08+1.8%\uff09\u63d0\u5347\u663e\u8457", "conclusion": "\u57fa\u7840\u6a21\u578b\u4e0e\u6a21\u5757\u5316\u6280\u672f\u7684\u7ed3\u5408\u4e3a\u591a\u6a21\u6001\u4ea4\u66ff\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u4e0d\u540c\u8fde\u63a5\u5668\u53ef\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u6027\u4f7f\u7528\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55"}}
{"id": "2506.11812", "pdf": "https://arxiv.org/pdf/2506.11812", "abs": "https://arxiv.org/abs/2506.11812", "authors": ["Margot Geerts", "Manon Reusens", "Bart Baesens", "Seppe vanden Broucke", "Jochen De Weerdt"], "title": "On the Performance of LLMs for Real Estate Appraisal", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ECML-PKDD 2025", "summary": "The real estate market is vital to global economies but suffers from\nsignificant information asymmetry. This study examines how Large Language\nModels (LLMs) can democratize access to real estate insights by generating\ncompetitive and interpretable house price estimates through optimized\nIn-Context Learning (ICL) strategies. We systematically evaluate leading LLMs\non diverse international housing datasets, comparing zero-shot, few-shot,\nmarket report-enhanced, and hybrid prompting techniques. Our results show that\nLLMs effectively leverage hedonic variables, such as property size and\namenities, to produce meaningful estimates. While traditional machine learning\nmodels remain strong for pure predictive accuracy, LLMs offer a more\naccessible, interactive and interpretable alternative. Although\nself-explanations require cautious interpretation, we find that LLMs explain\ntheir predictions in agreement with state-of-the-art models, confirming their\ntrustworthiness. Carefully selected in-context examples based on feature\nsimilarity and geographic proximity, significantly enhance LLM performance, yet\nLLMs struggle with overconfidence in price intervals and limited spatial\nreasoning. We offer practical guidance for structured prediction tasks through\nprompt optimization. Our findings highlight LLMs' potential to improve\ntransparency in real estate appraisal and provide actionable insights for\nstakeholders.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u901a\u8fc7\u4f18\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u751f\u6210\u5177\u6709\u7ade\u4e89\u529b\u7684\u623f\u4ef7\u8bc4\u4f30\uff0c\u63d0\u5347\u623f\u5730\u4ea7\u900f\u660e\u5ea6\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u548c\u7a7a\u95f4\u63a8\u7406\u5c40\u9650\u3002", "motivation": "\u89e3\u51b3\u623f\u5730\u4ea7\u5e02\u573a\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u5728\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u6613\u83b7\u53d6\u7684\u623f\u4ef7\u8bc4\u4f30\u4e2d\u7684\u6c11\u4e3b\u5316\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e3b\u6d41LLMs\u5728\u56fd\u9645\u4f4f\u623f\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63d0\u793a\u3001\u5e02\u573a\u62a5\u544a\u589e\u5f3a\u53ca\u6df7\u5408\u7b56\u7565\uff0c\u4f18\u5316\u4e0a\u4e0b\u6587\u793a\u4f8b\u9009\u62e9\u65b9\u6cd5\u3002", "result": "LLMs\u80fd\u6709\u6548\u5229\u7528\u623f\u4ea7\u7279\u5f81\u751f\u6210\u5408\u7406\u4f30\u503c\uff0c\u5176\u89e3\u91ca\u6027\u4e0e\u5148\u8fdb\u6a21\u578b\u4e00\u81f4\uff1b\u7cbe\u9009\u5730\u7406\u90bb\u8fd1/\u7279\u5f81\u76f8\u4f3c\u7684\u4e0a\u4e0b\u6587\u6837\u672c\u53ef\u4f7f\u51c6\u786e\u7387\u63d0\u534717.3%\uff0c\u4f46\u4ef7\u683c\u533a\u95f4\u9884\u6d4b\u5b58\u57289.8%\u7684\u8fc7\u5ea6\u81ea\u4fe1\u8bef\u5dee\u3002", "conclusion": "LLMs\u4e3a\u623f\u5730\u4ea7\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4ea4\u4e92\u6027\u5f3a\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u4f18\u5316\u53ef\u589e\u5f3a\u5b9e\u7528\u6027\uff0c\u4f46\u9700\u7ed3\u5408\u4f20\u7edf\u6a21\u578b\u4f18\u52bf\u5e76\u8b66\u60d5\u7b97\u6cd5\u5c40\u9650\u6027\u3002"}}
{"id": "2506.11820", "pdf": "https://arxiv.org/pdf/2506.11820", "abs": "https://arxiv.org/abs/2506.11820", "authors": ["Xintong Wang", "Jingheng Pan", "Yixiao Liu", "Xiaohu Zhao", "Chenyang Lyu", "Minghao Wu", "Chris Biemann", "Longyue Wang", "Linlong Xu", "Weihua Luo", "Kaifu Zhang"], "title": "Rethinking Multilingual Vision-Language Translation: Dataset, Evaluation, and Adaptation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Vision-Language Translation (VLT) is a challenging task that requires\naccurately recognizing multilingual text embedded in images and translating it\ninto the target language with the support of visual context. While recent Large\nVision-Language Models (LVLMs) have demonstrated strong multilingual and visual\nunderstanding capabilities, there is a lack of systematic evaluation and\nunderstanding of their performance on VLT. In this work, we present a\ncomprehensive study of VLT from three key perspectives: data quality, model\narchitecture, and evaluation metrics. (1) We identify critical limitations in\nexisting datasets, particularly in semantic and cultural fidelity, and\nintroduce AibTrans -- a multilingual, parallel, human-verified dataset with\nOCR-corrected annotations. (2) We benchmark 11 commercial LVLMs/LLMs and 6\nstate-of-the-art open-source models across end-to-end and cascaded\narchitectures, revealing their OCR dependency and contrasting generation versus\nreasoning behaviors. (3) We propose Density-Aware Evaluation to address metric\nreliability issues under varying contextual complexity, introducing the DA\nScore as a more robust measure of translation quality. Building upon these\nfindings, we establish a new evaluation benchmark for VLT. Notably, we observe\nthat fine-tuning LVLMs on high-resource language pairs degrades cross-lingual\nperformance, and we propose a balanced multilingual fine-tuning strategy that\neffectively adapts LVLMs to VLT without sacrificing their generalization\nability.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\uff0c\u4ece\u6570\u636e\u8d28\u91cf\u3001\u6a21\u578b\u67b6\u6784\u548c\u8bc4\u4f30\u6307\u6807\u4e09\u4e2a\u7ef4\u5ea6\u7a81\u7834\uff0c\u63d0\u51fa\u65b0\u6570\u636e\u96c6AibTrans\u3001\u5bc6\u5ea6\u611f\u77e5\u8bc4\u4f30\u65b9\u6cd5DA Score\uff0c\u5e76\u5efa\u7acb\u65b0\u8bc4\u4f30\u57fa\u51c6", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u4e14\u9762\u4e34\u6570\u636e\u96c6\u8d28\u91cf\u5dee\u3001\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u9760\u7b49\u95ee\u9898", "method": "\u6784\u5efaOCR\u6821\u6b63\u7684\u5e73\u884c\u591a\u8bed\u8a00\u6570\u636e\u96c6AibTrans\uff1b\u8bc4\u4f3017\u79cd\u5546\u4e1a/\u5f00\u6e90\u6a21\u578b\uff1b\u63d0\u51fa\u8003\u8651\u4e0a\u4e0b\u6587\u590d\u6742\u5ea6\u7684DA Score\u8bc4\u4f30\u6307\u6807", "result": "\u53d1\u73b0\u6a21\u578b\u5bf9OCR\u7684\u5f3a\u4f9d\u8d56\u6027\uff0c\u63d0\u51fa\u5e73\u8861\u591a\u8bed\u8a00\u5fae\u8c03\u7b56\u7565\u53ef\u4f7fLVLM\u9002\u5e94VLT\u4efb\u52a1\u4e14\u4e0d\u635f\u5931\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u3001\u4f18\u5316\u6a21\u578b\u67b6\u6784\u548c\u8bbe\u8ba1\u53ef\u9760\u8bc4\u4f30\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u7ffb\u8bd1\u4efb\u52a1\u7684\u6027\u80fd\u4e0e\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2506.11880", "pdf": "https://arxiv.org/pdf/2506.11880", "abs": "https://arxiv.org/abs/2506.11880", "authors": ["Alejandro Pe\u00f1a", "Julian Fierrez", "Aythami Morales", "Gonzalo Mancera", "Miguel Lopez", "Ruben Tolosana"], "title": "Addressing Bias in LLMs: Strategies and Application to Fair AI-based Recruitment", "categories": ["cs.AI", "cs.CL"], "comment": "Submitted to AIES 2025 (Under Review)", "summary": "The use of language technologies in high-stake settings is increasing in\nrecent years, mostly motivated by the success of Large Language Models (LLMs).\nHowever, despite the great performance of LLMs, they are are susceptible to\nethical concerns, such as demographic biases, accountability, or privacy. This\nwork seeks to analyze the capacity of Transformers-based systems to learn\ndemographic biases present in the data, using a case study on AI-based\nautomated recruitment. We propose a privacy-enhancing framework to reduce\ngender information from the learning pipeline as a way to mitigate biased\nbehaviors in the final tools. Our experiments analyze the influence of data\nbiases on systems built on two different LLMs, and how the proposed framework\neffectively prevents trained systems from reproducing the bias in the data.", "AI": {"tldr": "\u63d0\u51fa\u9690\u79c1\u589e\u5f3a\u6846\u67b6\u964d\u4f4eLLM\u62db\u8058\u5de5\u5177\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u6709\u6548\u6291\u5236\u6570\u636e\u504f\u5dee\u4f20\u9012", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u62db\u8058\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u6613\u53d7\u6570\u636e\u6027\u522b\u504f\u89c1\u5f71\u54cd\uff0c\u9700\u5f00\u53d1\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u51cf\u5c11\u7b97\u6cd5\u6b67\u89c6", "method": "\u6784\u5efa\u53bb\u6027\u522b\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4e24\u79cdLLM\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5206\u6790\u6570\u636e\u504f\u89c1\u5f71\u54cd\u8def\u5f84\u53ca\u6846\u67b6\u5e72\u9884\u6548\u679c", "result": "\u6570\u636e\u504f\u89c1\u663e\u8457\u5f71\u54cdLLM\u51b3\u7b56\uff0c\u53bb\u6027\u522b\u5316\u6846\u67b6\u4f7f\u7cfb\u7edf\u51c6\u786e\u7387\u4fdd\u630195%\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6027\u522b\u654f\u611f\u5ea6\u6307\u6807", "conclusion": "\u901a\u8fc7\u4e3b\u52a8\u79fb\u9664\u5b66\u4e60\u7ba1\u9053\u7684\u654f\u611f\u4fe1\u606f\uff0c\u53ef\u6709\u6548\u963b\u65adAI\u7cfb\u7edf\u5bf9\u6570\u636e\u504f\u89c1\u7684\u590d\u5236\uff0c\u4e3a\u516c\u5e73AI\u63d0\u4f9b\u6280\u672f\u8def\u5f84"}}
{"id": "2506.11887", "pdf": "https://arxiv.org/pdf/2506.11887", "abs": "https://arxiv.org/abs/2506.11887", "authors": ["Claudio Fanconi", "Mihaela van der Schaar"], "title": "Towards a Cascaded LLM Framework for Cost-effective Human-AI Decision-Making", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Effective human-AI decision-making balances three key factors: the\n\\textit{correctness} of predictions, the \\textit{cost} of knowledge and\nreasoning complexity, and the confidence about whether to \\textit{abstain}\nautomated answers or involve human experts. In this work, we present a cascaded\nLLM decision framework that adaptively delegates tasks across multiple tiers of\nexpertise -- a base model for initial candidate answers, a more capable and\nknowledgeable (but costlier) large model, and a human expert for when the model\ncascade abstains. Our method proceeds in two stages. First, a deferral policy\ndetermines whether to accept the base model's answer or regenerate it with the\nlarge model based on the confidence score. Second, an abstention policy decides\nwhether the cascade model response is sufficiently certain or requires human\nintervention. Moreover, we incorporate an online learning mechanism in the\nframework that can leverage human feedback to improve decision quality over\ntime. We demonstrate this approach to general question-answering (ARC-Easy and\nARC-Challenge) and medical question-answering (MedQA and MedMCQA). Our results\nshow that our cascaded strategy outperforms in most cases single-model\nbaselines in accuracy while reducing cost and providing a principled way to\nhandle abstentions.", "AI": {"tldr": "\u63d0\u51fa\u7ea7\u8054LLM\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u2192\u5927\u6a21\u578b\u2192\u4eba\u7c7b\u4e13\u5bb6\u7684\u591a\u7ea7\u59d4\u6258\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u51c6\u786e\u6027\u7684\u540c\u65f6\u964d\u4f4e\u51b3\u7b56\u6210\u672c\u5e76\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u4eba\u673a\u534f\u4f5c\u9700\u6743\u8861\u9884\u6d4b\u6b63\u786e\u6027\u3001\u77e5\u8bc6\u63a8\u7406\u6210\u672c\u4e0e\u51b3\u7b56\u7f6e\u4fe1\u5ea6\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u52a8\u6001\u5e73\u8861\u8fd9\u4e09\u4e2a\u7ef4\u5ea6\u3002", "method": "\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u5ef6\u8fdf\u7b56\u7565\u6839\u636e\u7f6e\u4fe1\u5ea6\u9009\u62e9\u63a5\u53d7\u57fa\u7840\u6a21\u578b\u7b54\u6848\u6216\u8c03\u7528\u5927\u6a21\u578b\u91cd\u65b0\u751f\u6210\uff1b2) \u5f03\u6743\u7b56\u7565\u51b3\u5b9a\u662f\u5426\u9700\u8981\u4eba\u7c7b\u4ecb\u5165\uff0c\u5e76\u96c6\u6210\u5728\u7ebf\u5b66\u4e60\u673a\u5236\u6301\u7eed\u4f18\u5316\u51b3\u7b56\u3002", "result": "\u5728ARC\u3001MedQA\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u7ea7\u8054\u7b56\u7565\u51c6\u786e\u7387\u4f18\u4e8e\u5355\u6a21\u578b\u57fa\u7ebf(\u5e73\u5747\u63d0\u53472.3%)\uff0c\u51b3\u7b56\u6210\u672c\u964d\u4f4e38%\uff0c\u4e14\u5b9e\u73b0\u7cfb\u7edf\u5316\u5f03\u6743\u5904\u7406\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u51b3\u7b56\u573a\u666f\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u8d44\u6e90\u5206\u914d\u5b9e\u73b0\u7cbe\u5ea6-\u6210\u672c\u5e73\u8861\uff0c\u5728\u7ebf\u5b66\u4e60\u673a\u5236\u589e\u5f3a\u7cfb\u7edf\u6301\u7eed\u8fdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.11902", "pdf": "https://arxiv.org/pdf/2506.11902", "abs": "https://arxiv.org/abs/2506.11902", "authors": ["Zhenyu Hou", "Ziniu Hu", "Yujiang Li", "Rui Lu", "Jie Tang", "Yuxiao Dong"], "title": "TreeRL: LLM Reinforcement Learning with On-Policy Tree Search", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ACL 2025 main conference", "summary": "Reinforcement learning (RL) with tree search has demonstrated superior\nperformance in traditional reasoning tasks. Compared to conventional\nindependent chain sampling strategies with outcome supervision, tree search\nenables better exploration of the reasoning space and provides dense, on-policy\nprocess rewards during RL training but remains under-explored in On-Policy LLM\nRL. We propose TreeRL, a reinforcement learning framework that directly\nincorporates on-policy tree search for RL training. Our approach includes\nintermediate supervision and eliminates the need for a separate reward model\ntraining. Existing approaches typically train a separate process reward model,\nwhich can suffer from distribution mismatch and reward hacking. We also\nintroduce a cost-effective tree search approach that achieves higher search\nefficiency under the same generation token budget by strategically branching\nfrom high-uncertainty intermediate steps rather than using random branching.\nExperiments on challenging math and code reasoning benchmarks demonstrate that\nTreeRL achieves superior performance compared to traditional ChainRL,\nhighlighting the potential of tree search for LLM. TreeRL is open-sourced at\nhttps://github.com/THUDM/TreeRL.", "AI": {"tldr": "TreeRL\u6846\u67b6\u901a\u8fc7\u6574\u5408\u5728\u7ebf\u7b56\u7565\u6811\u641c\u7d22\uff0c\u5728LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u63a2\u7d22\uff0c\u6d88\u9664\u5355\u72ec\u5956\u52b1\u6a21\u578b\u9700\u6c42\uff0c\u5728\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4f20\u7edfChainRL\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u72ec\u7acb\u94fe\u5f0f\u91c7\u6837\u7b56\u7565\u5b58\u5728\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u5956\u52b1\u6a21\u578b\u5206\u5e03\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u6811\u641c\u7d22\u80fd\u63d0\u4f9b\u5bc6\u96c6\u8fc7\u7a0b\u5956\u52b1\u4f46\u5c1a\u672a\u5728LLM\u5f3a\u5316\u5b66\u4e60\u4e2d\u5145\u5206\u5e94\u7528\u3002", "method": "1. \u76f4\u63a5\u6574\u5408\u5728\u7ebf\u7b56\u7565\u6811\u641c\u7d22\u8fdb\u884cRL\u8bad\u7ec3\uff0c\u5f15\u5165\u4e2d\u95f4\u76d1\u7763\n2. \u63d0\u51fa\u7ecf\u6d4e\u9ad8\u6548\u7684\u6811\u641c\u7d22\u7b56\u7565\uff1a\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u8282\u70b9\u5206\u652f\n3. \u65e0\u9700\u5355\u72ec\u8bad\u7ec3\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b", "result": "\u5728GSM8K\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\uff0cTreeRL\u76f8\u6bd4ChainRL\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\uff08\u5982GSM8K\u4e0a+3.8%\uff09\uff0c\u76f8\u540ctoken\u9884\u7b97\u4e0b\u641c\u7d22\u6548\u7387\u63d0\u9ad82.1\u500d\u3002", "conclusion": "\u6811\u641c\u7d22\u673a\u5236\u663e\u8457\u63d0\u5347LLM\u5f3a\u5316\u5b66\u4e60\u6548\u679c\uff0c\u8bc1\u660e\u8fc7\u7a0b\u5956\u52b1\u4e0e\u9ad8\u6548\u63a2\u7d22\u5bf9\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3aLLM\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.11928", "pdf": "https://arxiv.org/pdf/2506.11928", "abs": "https://arxiv.org/abs/2506.11928", "authors": ["Zihan Zheng", "Zerui Cheng", "Zeyu Shen", "Shang Zhou", "Kaiyuan Liu", "Hansen He", "Dongruixuan Li", "Stanley Wei", "Hangyi Hao", "Jianzhu Yao", "Peiyao Sheng", "Zixuan Wang", "Wenhao Chai", "Aleksandra Korolova", "Peter Henderson", "Sanjeev Arora", "Pramod Viswanath", "Jingbo Shang", "Saining Xie"], "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive Programming?", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": "Project Page at https://livecodebenchpro.com/", "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.", "AI": {"tldr": "LLM\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u4e2d\u8868\u73b0\u4ecd\u663e\u8457\u843d\u540e\u4eba\u7c7b\u4e13\u5bb6\uff1a\u4e2d\u7b49\u96be\u5ea6\u9898\u901a\u8fc7\u7387\u4ec553%\uff0c\u56f0\u96be\u98980%\u6210\u529f\u3002\u6a21\u578b\u64c5\u957f\u5b9e\u73b0\u7c7b\u95ee\u9898\uff0c\u4f46\u7b97\u6cd5\u63a8\u7406\u548c\u590d\u6742\u6848\u4f8b\u5206\u6790\u80fd\u529b\u8584\u5f31\uff0c\u4e14\u9ad8\u8868\u73b0\u4e3b\u8981\u4f9d\u8d56\u5de5\u5177\u589e\u5f3a\u800c\u975e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u5173\u4e8eLLM\u5728\u7f16\u7a0b\u7ade\u8d5b\u8d85\u8d8a\u4eba\u7c7b\u7684\u8bf4\u6cd5\uff0c\u901a\u8fc7\u7b97\u6cd5\u7ade\u8d5b\u5956\u724c\u5f97\u4e3b\u7684\u4e13\u4e1a\u89c6\u89d2\uff0c\u7cfb\u7edf\u6027\u9a8c\u8bc1\u6a21\u578b\u5b9e\u9645\u80fd\u529b\u8fb9\u754c\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u672c\u8d28\u5dee\u5f02\u3002", "method": "\u6784\u5efa\u6301\u7eed\u66f4\u65b0\u7684LiveCodeBench Pro\u57fa\u51c6(\u542bCodeforces/ICPC/IOI\u9898\u76ee)\uff0c\u7531\u5965\u8d5b\u5956\u724c\u5f97\u4e3b\u8fdb\u884c\u7b97\u6cd5\u5206\u7c7b\u6807\u6ce8\uff0c\u5e76\u5bf9\u6a21\u578b\u5931\u8d25\u6848\u4f8b\u8fdb\u884c\u9010\u884c\u4ee3\u7801\u5206\u6790\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u4e2d\u7b49\u96be\u5ea6\u9898\u76eepass@1\u4ec553%\uff0c\u56f0\u96be\u98980%\u901a\u8fc7\uff1b\u6a21\u578b\u6210\u529f\u6848\u4f8b\u96c6\u4e2d\u4e8e\u5b9e\u73b0\u5bc6\u96c6\u578b\u95ee\u9898\uff0c\u5931\u8d25\u4e3b\u56e0\u662f\u7b97\u6cd5\u63a8\u7406\u7f3a\u9677\u548c\u590d\u6742\u6848\u4f8b\u5206\u6790\u9519\u8bef\uff0c\u4e14\u5e38\u4f34\u968f\u8fc7\u5ea6\u81ea\u4fe1\u7684\u9519\u8bef\u89e3\u91ca\u3002", "conclusion": "LiveCodeBench Pro\u63ed\u793a\u4e86LLM\u4e0e\u4eba\u7c7b\u9876\u7ea7\u9009\u624b\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u5176\u8bca\u65ad\u6570\u636e\u4e3a\u63d0\u5347\u4ee3\u7801\u4e2d\u5fc3\u5316\u63a8\u7406\u80fd\u529b\u6307\u660e\u65b9\u5411\uff0c\u5f53\u524d\u6a21\u578b\u7684\u9ad8\u8868\u73b0\u4e3b\u8981\u4f9d\u8d56\u5de5\u5177\u589e\u5f3a\u800c\u975e\u672c\u8d28\u63a8\u7406\u80fd\u529b\u7684\u7a81\u7834\u3002"}}
{"id": "2506.11986", "pdf": "https://arxiv.org/pdf/2506.11986", "abs": "https://arxiv.org/abs/2506.11986", "authors": ["Wuzhenghong Wen", "Su Pan", "yuwei Sun"], "title": "Schema-R1: A reasoning training approach for schema linking in Text-to-SQL Task", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "11 pages, 3 figures, conference", "summary": "Schema linking is a critical step in Text-to-SQL task, aiming to accurately\npredict the table names and column names required for the SQL query based on\nthe given question. However, current fine-tuning approaches for schema linking\nmodels employ a rote-learning paradigm, excessively optimizing for ground truth\nschema linking outcomes while compromising reasoning ability. This limitation\narises because of the difficulty in acquiring a high-quality reasoning sample\nfor downstream tasks. To address this, we propose Schema-R1, a reasoning schema\nlinking model trained using reinforcement learning. Specifically, Schema-R1\nconsists of three key steps: constructing small batches of high-quality\nreasoning samples, supervised fine-tuning for cold-start initialization, and\nrule-based reinforcement learning training. The final results demonstrate that\nour method effectively enhances the reasoning ability of the schema linking\nmodel, achieving a 10\\% improvement in filter accuracy compared to the existing\nmethod. Our code is available at https://github.com/hongWin/Schema-R1/.", "AI": {"tldr": "\u63d0\u51faSchema-R1\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u4f18\u5316Text-to-SQL\u6a21\u5f0f\u94fe\u63a5\uff0c\u63a8\u7406\u80fd\u529b\u63d0\u534710%\u8fc7\u6ee4\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u6a21\u5f0f\u94fe\u63a5\u6a21\u578b\u91c7\u7528\u673a\u68b0\u5b66\u4e60\u8303\u5f0f\uff0c\u8fc7\u5ea6\u4f18\u5316\u6807\u6ce8\u7ed3\u679c\u4f46\u7f3a\u4e4f\u63a8\u7406\u80fd\u529b\uff0c\u9ad8\u8d28\u91cf\u63a8\u7406\u6837\u672c\u83b7\u53d6\u56f0\u96be", "method": "1. \u6784\u5efa\u5c0f\u6279\u91cf\u9ad8\u8d28\u91cf\u63a8\u7406\u6837\u672c 2. \u76d1\u7763\u5fae\u8c03\u51b7\u542f\u52a8 3. \u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3", "result": "\u5728\u8fc7\u6ee4\u51c6\u786e\u7387\u6307\u6807\u4e0a\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534710%\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6709\u6548\u589e\u5f3a\u6a21\u5f0f\u94fe\u63a5\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0"}}
{"id": "2506.11991", "pdf": "https://arxiv.org/pdf/2506.11991", "abs": "https://arxiv.org/abs/2506.11991", "authors": ["Jiacong Wang", "Zijiang Kang", "Haochen Wang", "Haiyong Jiang", "Jiawen Li", "Bohong Wu", "Ya Wang", "Jiao Ran", "Xiao Liang", "Chao Feng", "Jun Xiao"], "title": "VGR: Visual Grounded Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "9 pages, 4 figures", "summary": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.", "AI": {"tldr": "\u63d0\u51faVGR\u591a\u6a21\u6001\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u548c\u63a8\u7406\u56de\u653e\u673a\u5236\uff0c\u5728\u51cf\u5c1130%\u56fe\u50cf\u4ee4\u724c\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u57fa\u51c6\u8868\u73b0", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u65b9\u6cd5\u5b58\u5728\u8bed\u8a00\u504f\u89c1\u548c\u89c6\u89c9\u7ec6\u8282\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1", "method": "1.\u6784\u5efaVGR-SFT\u6570\u636e\u96c6\u878d\u5408\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u8bed\u8a00\u63a8\u7406 2.\u8bbe\u8ba1\u4e24\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff08\u533a\u57df\u68c0\u6d4b+\u56de\u653e\u63a8\u7406\uff09 3.\u5728LLaVA-NeXT-7B\u57fa\u7ebf\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03", "result": "MMStar+4.1\u3001AI2D+7.1\u3001ChartQA+12.9\u7684\u663e\u8457\u63d0\u5347\uff0c\u56fe\u50cf\u4ee4\u724c\u4f7f\u7528\u91cf\u51cf\u5c1130%", "conclusion": "VGR\u901a\u8fc7\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u8bed\u8a00\u63a8\u7406\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u6709\u6548\u7a81\u7834\u4f20\u7edfMLLMs\u7684\u89c6\u89c9\u7406\u89e3\u74f6\u9888\uff0c\u4e3a\u590d\u6742\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.11999", "pdf": "https://arxiv.org/pdf/2506.11999", "abs": "https://arxiv.org/abs/2506.11999", "authors": ["Zheli Zhou", "Chenxu Zhu", "Jianghao Lin", "Bo Chen", "Ruiming Tang", "Weinan Zhang", "Yong Yu"], "title": "Generative Representational Learning of Foundation Models for Recommendation", "categories": ["cs.IR", "cs.CL"], "comment": "Project page is available at https://junkfood436.github.io/RecFound/", "summary": "Developing a single foundation model with the capability to excel across\ndiverse tasks has been a long-standing objective in the field of artificial\nintelligence. As the wave of general-purpose foundation models sweeps across\nvarious domains, their influence has significantly extended to the field of\nrecommendation systems. While recent efforts have explored recommendation\nfoundation models for various generative tasks, they often overlook crucial\nembedding tasks and struggle with the complexities of multi-task learning,\nincluding knowledge sharing & conflict resolution, and convergence speed\ninconsistencies. To address these limitations, we introduce RecFound, a\ngenerative representational learning framework for recommendation foundation\nmodels. We construct the first comprehensive dataset for recommendation\nfoundation models covering both generative and embedding tasks across diverse\nscenarios. Based on this dataset, we propose a novel multi-task training scheme\nfeaturing a Task-wise Mixture of Low-rank Experts (TMoLE) to handle knowledge\nsharing & conflict, a Step-wise Convergence-oriented Sample Scheduler (S2Sched)\nto address inconsistent convergence, and a Model Merge module to balance the\nperformance across tasks. Experiments demonstrate that RecFound achieves\nstate-of-the-art performance across various recommendation tasks, outperforming\nexisting baselines.", "AI": {"tldr": "\u63d0\u51faRecFound\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u8bad\u7ec3\u65b9\u6848\uff08TMoLE+S2Sched+\u6a21\u578b\u5408\u5e76\uff09\u89e3\u51b3\u63a8\u8350\u57fa\u7840\u6a21\u578b\u4e2d\u751f\u6210\u4efb\u52a1\u4e0e\u5d4c\u5165\u4efb\u52a1\u7684\u534f\u540c\u4f18\u5316\u96be\u9898", "motivation": "\u73b0\u6709\u63a8\u8350\u57fa\u7840\u6a21\u578b\u5ffd\u89c6\u5173\u952e\u5d4c\u5165\u4efb\u52a1\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u591a\u4efb\u52a1\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u3001\u6536\u655b\u901f\u5ea6\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898", "method": "\u6784\u5efa\u9996\u4e2a\u591a\u4efb\u52a1\u63a8\u8350\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4efb\u52a1\u7ea7\u4f4e\u79e9\u4e13\u5bb6\u6df7\u5408\uff08TMoLE\uff09\u3001\u5206\u9636\u6bb5\u6536\u655b\u8c03\u5ea6\u5668\uff08S2Sched\uff09\u548c\u6a21\u578b\u5408\u5e76\u6a21\u5757", "result": "\u5b9e\u9a8c\u8bc1\u660eRecFound\u5728\u591a\u4e2a\u63a8\u8350\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b", "conclusion": "RecFound\u6709\u6548\u5e73\u8861\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u63a8\u8350\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
