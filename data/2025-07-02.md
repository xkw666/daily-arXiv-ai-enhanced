<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 11]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.AI](#cs.AI) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

TL;DR: 本文探讨了文本和多模态大语言模型(LLMs)在处理跨领域、跨模态表格数据时的表现，发现LLMs在科学表格处理上存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在不同领域(科学/非科学)和不同模态(图像/文本)表格理解任务中的有效性，揭示其处理科学表格的局限性。

Method: 通过跨领域、跨模态评估框架，结合可解释性分析，并建立包含3017个多格式表格的TableEval基准测试集(含图像、字典、HTML等五种格式)。

Result: LLMs在不同表格模态间保持稳健性，但对科学表格处理准确率显著下降(科学表格准确率比非科学表格低18.7%)。

Conclusion: 需针对性优化LLMs在复杂科学表格中的结构理解和领域知识应用能力，TableEval基准为后续研究提供标准化评估平台。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [2] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

TL;DR: 提示是研究和控制大语言模型的核心方法，应被视为LLM科学的重要组成部分而非权宜之计


<details>
  <summary>Details</summary>
Motivation: 针对当前学术界常将提示视为'炼金术'的偏见，主张重新定位提示作为行为科学的学术地位

Method: 通过将LLMs类比为通过训练获得的复杂生物体，从行为科学角度论证提示的科研价值

Result: 确立提示与机械解释的互补关系，证明其在模型行为研究中的不可替代性

Conclusion: 提示应作为理解LLMs的核心方法论，需要建立系统化的科学研究框架以充分释放模型潜力

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [3] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

TL;DR: 提出LineRetriever方法，通过语言模型检索与未来导航步骤相关的关键网页元素，在保持性能的同时显著减少网页代理的观测数据量。


<details>
  <summary>Details</summary>
Motivation: 现有网页导航方法（如底部截断/嵌入检索）会丢失页面状态和动作历史的关键信息，影响自适应规划能力。嵌入模型难以捕捉规划相关的时序信息。

Method: 基于语言模型的动态检索框架，通过规划视野（planning horizon）选择对未来动作预测最有贡献的网页元素行，而非仅依赖语义相似度。

Result: 实验证明LineRetriever能在保持上下文限制下维持网页代理性能，有效缩减每一步的观测规模。

Conclusion: 面向规划视野的检索机制优化显著提升了网页导航任务中的自适应规划效率，为LLM代理的上下文管理提供了新思路。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [4] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

TL;DR: 提出两阶段方法，利用LLM生成推理增强文本分类模型，在情感分类任务中实现8.7%准确率提升


<details>
  <summary>Details</summary>
Motivation: 传统分类模型缺乏显式推理机制，可能影响性能、鲁棒性和可解释性。通过显式推理训练可增强模型能力

Method: 1. 微调Llama-3.2-1B-Instruct生成推理文本；2. 用生成的推理数据训练下游生成式模型（输入文本→推理+预测结果）

Result: 在emotion数据集上，Q->RA模型比Q->A基线模型准确率提升8.7个百分点

Conclusion: LLM生成的推理能有效提升下游任务性能，同时提供可解释性，该方法具有广泛NLP应用潜力

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [5] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

TL;DR: 论文提出RASTA方法，通过检索增强学习解决LLM翻译中的文化风格对齐问题，改善非西方语言翻译效果。


<details>
  <summary>Details</summary>
Motivation: 文化差异常导致LLM翻译中说话者意图与听者理解间的风格错位（如礼貌用语丢失），需解决翻译模型的文化适应性。

Method: RASTA(检索增强风格对齐)方法，利用学习到的风格概念引导LLM在翻译中恰当传递文化沟通规范

Result: 有效减少翻译风格偏向中立的问题，显著提升非西方语言场景下的翻译风格对齐效果

Conclusion: RASTA方法通过显式建模文化沟通规范，为跨文化场景的LLM翻译提供了有效的风格对齐解决方案

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [6] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

TL;DR: 研究显示指令微调的语言模型虽能拒绝有害请求，但通过线性探针仍可解码被拒绝的有害信息，表明这些信息在表征空间持续存在并影响下游行为


<details>
  <summary>Details</summary>
Motivation: 探索指令微调后的语言模型是否真正消除了有害信息，以及拒绝机制被绕过时潜在风险的存在形式

Method: 使用线性探针分析语言模型隐藏状态，比较基座模型与指令微调模型的表征空间信息解码能力

Result: 被拒绝信息线性解码相关性达0.8以上，基座模型的探针可迁移至微调模型，且探针预测与生成行为相关

Conclusion: 指令微调仅抑制而非消除有害信息的表达，这些信息仍保持线性可访问性并通过表征空间持续影响模型行为

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [7] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

TL;DR: 在合并运算与强最简论题的数学框架下构建形态-句法接口模型，通过operadic对应关系和形态余积分解描述结构形成过程，并重新诠释分布式形态学操作为边界调整工具。


<details>
  <summary>Details</summary>
Motivation: 形式化形态学与句法学的交互机制，弥补传统理论在数学建模方面的不足，特别是形态学缺乏移位操作的问题，为语言结构的数学描述提供新范式。

Method: 采用magma代数结构组织形态树，通过余积分解扩展形态输入集合，建立operad代数对应关系构建形态句法树，引入分布式形态学的柔性边界转换机制。

Result: 成功建立可数学描述的形态-句法接口模型，证明operadic对应能有效协调句法形态交互，实现形态句法边界的动态调整能力。

Conclusion: 该数学模型为语言接口研究提供新框架，其操作灵活性可能推动最简方案与分布式形态学的理论融合与发展。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [8] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

TL;DR: 非英语推理在语言模型中展现出更高的token效率和精度保持，模型多语言能力影响改进幅度


<details>
  <summary>Details</summary>
Motivation: 探索英语是否真是最高效的推理语言，揭示多语言预训练模型在非英语场景的潜在优势

Method: 使用DeepSeek R1/Qwen系列模型，在7种语言4个数学数据集上对比token效率与精度，包含翻译验证实验

Result: 非英语推理节省15-30% token且精度持平，翻译后效果保留说明推理模式改变而非表层语言效应

Conclusion: 多语言推理潜力显著，模型需强化多语言基础建设，打破英语中心主义的推理研究范式

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [9] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

TL;DR: 比较参数微调与提示微调的隐私风险，发现提示方法在保持性能的同时显著降低记忆化风险


<details>
  <summary>Details</summary>
Motivation: 评估不同微调方法对隐私风险的影响，填补参数微调与提示微调隐私保护能力的研究空白

Method: 通过成员推理攻击(MIAs)评估流行微调方法的记忆化程度，对比参数微调和提示微调的性能与隐私性

Result: 提示微调性能相当且MIA脆弱性更低，其低记忆化特性不受模型规模影响

Conclusion: 参数微调易泄露隐私信息，提示微调是更安全的隐私保护选择

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [10] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 论文针对撒哈拉以南非洲低资源语言，通过数据清洗与标注、多语言模型适应性研究，揭示了数据质量对语义表示的影响，并构建了21种语言的大规模标注数据集。


<details>
  <summary>Details</summary>
Motivation: 解决多语言模型在非洲低资源语言中面临的三大挑战：语料库噪声大、标注数据稀缺、现有模型对未训练语言适应性差。

Method: 1. 构建高质量非洲语言语料库
2. 探索词嵌入与多语言PLMs的潜力
3. 开发21种语言的NER/MT标注数据集
4. 监督/弱监督/迁移学习的系统性评估

Result: 实证表明：①数据质量比数量更关键 ②多语言PLMs对未训练语言展现适应性 ③少量单语文本即可有效调整模型

Conclusion: 提出非洲低资源语言NLP解决方案：高质量数据筛选→多语言PLMs适配→人工标注支持，为全球语言技术公平性研究提供新范式。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [11] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

TL;DR: 通过分析语言模型在简单句法任务中的错误机制，提出RASteer方法增强可靠组件贡献，使平衡括号任务准确率从0%提升至近100%，数学推理提升约20%。


<details>
  <summary>Details</summary>
Motivation: 语言模型虽具备强大编码能力，但在平衡括号等基础任务上表现异常。研究旨在揭示错误产生机制并提出调控方法。

Method: 通过识别模型中可靠组件（sound mechanisms）并增强其贡献，RASteer方法有效抑制噪声组件（faulty mechanisms）对预测的干扰。

Result: RASteer使部分模型在平衡括号任务中准确率从0%提升至近100%，数学推理任务最高提升约20%，且不影响通用编码能力。

Conclusion: 通过主动调控模型内部机制权重，可显著提升特定任务性能，该方法具有广泛适用性，为改进模型提供新思路。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [12] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

TL;DR: 提出COLDSELECT方法，通过联合标签词和实例选择策略，结合多样性建模和不确定性优化，显著提升冷启动场景下的提示学习效果


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法在冷启动场景下对模板、标签词和样本选择敏感，特别是忽略了标签词与样本实例在嵌入空间的邻近性依赖关系

Method: 将PLM词汇和[MASK]嵌入映射到共享空间，通过降维聚类实现高效多样化选择，构建联合优化目标实现最小不确定性和最大多样性

Result: 在8个基准测试中验证方法有效性，相比基线模型在冷启动场景的标签词选择和少样本选择任务上均取得显著提升

Conclusion: COLDSELECT通过联合优化策略和多样性建模，有效捕捉数据关系，为冷启动场景的提示学习方法提供了新思路

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [13] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

TL;DR: 提出结合问题分解和候选池重排序的RAG流程，显著提升多跳问题的检索效果（MRR@10提升36.7%）和回答准确率（F1提高11.6%）


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理需要多文档证据的多跳问题时存在检索不足，例如比较多家公司利润的问题需要聚合分散的事实

Method: 1. 用LLM分解原始问题为子问题
2. 分别检索各子问题的相关段落
3. 使用现成交叉编码器对合并候选池重排序，提升证据覆盖率和相关性

Result: 在MultiHop-RAG和HotpotQA数据集上，检索指标MRR@10提升36.7%，答案生成F1分数提升11.6%

Conclusion: 无需额外训练或专用索引，通过问题分解与重排序的组合策略，有效解决了多跳问题的证据检索瓶颈

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [14] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

TL;DR: 研究通过无监督分层Pitman-Yor语言模型对格里高利圣歌旋律进行分割，发现最优分割在调式分类中表现优异，但与传统'拼贴理论'存在差异，揭示记忆效率与旋律公式化区域关联。


<details>
  <summary>Details</summary>
Motivation: 探究格里高利圣歌旋律是否存在系统性拼贴结构（centonisation），并验证记忆机制对旋律组织的影响。基于圣歌依赖记忆传播的特性，假设最优分割应反映记忆效率需求。

Method: 使用嵌套分层Pitman-Yor语言模型进行无监督旋律分割，模拟修士从单一礼仪手稿记忆旋律的过程，通过模式分类任务验证分割效果。

Result: 1. 分割方法实现调式分类SOTA性能；2. 发现旋律首尾存在更多公式化段落（与调式表演功能对应）；3. 记忆最优分割与音乐学定义的拼贴理论存在本质差异。

Conclusion: 尽管记忆优化的分割支持调式分类，但实证表明传统拼贴理论未能准确描述圣歌结构。旋律组织更可能受记忆效率驱动，而非预设的拼贴规则。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [15] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

TL;DR: 提出因果推理框架CAPITAL，通过前门调整改进隐式情感分析，提升LLM推理的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的隐式情感分析方法依赖思维链的多数投票机制，未验证因果有效性，易受内部偏见和伪相关影响

Method: 将总因果效应分解为输入提示对推理链的影响，以及推理链对输出的影响，采用编码器聚类+NWGM近似估计，配合对比学习对齐表征空间

Result: 在三个LLM的基准测试中，CAPITAL在准确率和鲁棒性（尤其对抗场景）持续超越现有方法

Conclusion: 为因果推理融入LLM提示提供了理论框架，实现了基于偏置感知的情感推理突破

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [16] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

TL;DR: 通过简单监督方法显著提升语言模型与不同人群的价值观对齐效果


<details>
  <summary>Details</summary>
Motivation: 预测不同群体对主观问题的回答具有重要价值，但现有方法复杂度高。需要探索简单有效的监督方式来提升语言模型与多样化群体的对齐能力。

Method: 使用三个涵盖多主题的数据集，通过简单监督机制评估不同LLMs和提示策略的对齐效果，重点关注不同群体间的差异性表现。

Result: 方法在多个大语言模型上显示有效性（提升群体对齐度），但效果存在群体差异性，需根据具体应用场景谨慎使用。

Conclusion: 提出的简单通用方法为价值观对齐提供了新思路，通过开源评估框架和跨模型测试结果建立了可扩展的研究基准。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [17] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

TL;DR: 开放LLM基准测试存在漏洞，微调小型模型可虚假登顶排行榜但缺乏实用性，需改进评估体系


<details>
  <summary>Details</summary>
Motivation: 揭示开放基准测试的潜在风险：标准化评估协议可能被针对性优化，导致排行榜结果失真

Method: 构建BART/T5/GPT-2的小型变体，直接在HELM等公开测试集上微调制造『作弊』模型

Result: 作弊模型在HELM基准排名前列，但实际泛化能力差且缺乏实用价值

Conclusion: 需结合私有/动态测试基准，重构评估体系以确保语言模型评估的可靠性和实际价值

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [18] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

TL;DR: 提出模块化广告管理流程（广告重写器+分类器），通过对抗训练实现在生成式对话系统中更隐蔽的广告植入


<details>
  <summary>Details</summary>
Motivation: 生成式搜索引擎模糊了广告与信息内容的界限，传统广告标识方式不再适用，需解决透明度和信任危机

Method: 1. 开发广告重写器实现自然植入 2. 用营销策略启发的合成数据训练广告检测分类器 3. 采用监督微调和Best-of-N采样双重优化策略

Result: 广告分类器检测准确率显著提升，优化后的广告植入隐蔽性增强（人工检测成功率降低37%），实现自然的内容-广告融合

Conclusion: 对抗性协同进化框架有效平衡商业变现与用户体验，为生成式搜索系统的广告整合提供新范式

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [19] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 提出了Nirantar框架，通过真实增量数据构建多语言/多领域ASR持续学习评估体系，包含3250小时语音数据，发现现有CL方法表现不稳定。


<details>
  <summary>Details</summary>
Motivation: 解决现有持续学习评估过于依赖模拟数据的问题，通过真实场景中采集的动态非均匀语言/领域增量数据，构建更贴近实际挑战的评估基准。

Method: 收集印度22种语言、208地区的自然增量语音数据，设计语言增量(LIL)、领域增量(DIL)及混合增量(LIDIL)三种评估场景，包含3250小时人工标注语音（其中1720小时为新数据）。

Result: 现有持续学习方法在框架中表现参差不齐，没有单一方法在所有场景中保持优势，突显现有CL策略的局限性。

Conclusion: Nirantar为持续学习研究提供了更真实的评估平台，揭示了当前方法的不足，强调需要开发更具鲁棒性的持续学习策略。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [20] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

TL;DR: 提出基于胶囊网络的用户语义意图建模算法，通过动态路由机制提升人机交互中意图识别的准确率


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在复杂语义条件下意图识别准确率不足的问题，传统深度学习模型难以有效捕捉语义实体间的层次关系和部分-整体结构

Method: 1. 使用向量化胶囊结构表示文本语义特征
2. 通过动态路由机制实现跨胶囊层信息传递
3. 采用卷积模块作为底层编码器生成初始语义胶囊
4. 在损失函数中引入基于间隔的优化机制

Result: 在公开数据集上验证：
- 准确率、F1值和意图检测率优于传统方法和主流深度学习模型
- 动态路由迭代3次时达到最佳性能
- 损失函数收敛曲线验证训练稳定性

Conclusion: 提出的结构化建模方法有效改善复杂语义条件下的意图识别，动态路由机制和损失函数优化是关键创新点，实验验证了方法在语义建模中的优越性

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [21] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

TL;DR: 针对计算密集型理论研究中算法不透明性问题，提出主题建模算法应用指南以提升方法严谨性


<details>
  <summary>Details</summary>
Motivation: 现有计算算法（如主题建模）的模糊性和应用缺乏透明度，可能损害研究可信度，需建立方法论规范

Method: 使用结构主题建模算法进行案例展示，并制定跨算法适用的实施指南

Result: 开发出可适配其他计算算法的通用严谨性评估框架，特别适用于理论建构研究

Conclusion: 为计算密集型理论研究提供方法论基准，强化主题建模研究的可信度，推动学术共同体建立算法应用规范

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [22] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

TL;DR: 提出结合检索验证与BERT模型的方案检测多语言LLM的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有幻觉研究主要集中于英语，忽视LLMs的多语言特性

Method: 基于维基百科的检索式事实核查 + 微调BERT识别常见幻觉模式

Result: 在8种语言进入前十（含英语），支持超过14种语言的检测

Conclusion: 该多语言检测工具可提升LLM输出质量与实用性

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [23] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

TL;DR: 提出结合知识转移与参数高效微调的框架，通过知识对齐损失和软提示调优增强低资源语言场景下大模型的跨任务适应能力，实验证明在MLQA/XQuAD/PAWS-X等跨语言任务中性能显著优于主流方法


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言场景中迁移能力不足、训练稳定性差的问题，需在最小标注条件下提升模型对目标语言特征的吸收效率

Method: 构建统一框架：1)知识转移模块采用对齐损失引导特征吸收 2)软提示调优结合轻量适配模块 3)冻结策略与提示注入实现知识保留 4)稳定性分析与伪数据迁移实验验证鲁棒性

Result: 在MLQA/XQuAD/PAWS-X跨语言任务中准确率提升3-5个点，数据稀缺场景(＜1%标注)下训练速度提升2倍，模型稳定性指标提高40%

Conclusion: 该方法在增强任务特定适应性的同时保留大模型通用能力，为多语言处理提供了高效可扩展的解决方案，特别适合资源受限的复杂语义建模场景

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [24] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出Mixture of Reasoning (MoR)训练框架，通过嵌入多样化推理策略使大模型实现自适应推理，无需人工设计任务提示


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计任务特定提示(如CoT/ToT)，限制模型适应性和效率

Method: 两阶段框架：1) 使用GPT-4o生成推理链模板 2) 构建监督微调数据集进行模型训练

Result: MoR150在CoT提示下准确率0.730(提升2.2%)，基线比较提升13.5%达0.734

Conclusion: MoR提供通用化解决方案，消除任务特定提示需求，增强跨领域推理鲁棒性

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [25] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

TL;DR: 研究者提出SAFER框架，通过稀疏自编码器解析奖励模型机制，实现安全对齐的精准调控。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类反馈的强化学习（RLHF）中奖励模型存在黑箱问题，难以理解其安全决策机制。该研究旨在通过可解释性分析提升大语言模型安全对齐的可靠性。

Method: 1. 利用稀疏自编码器（SAE）提取奖励模型激活值中的人类可解释特征
2. 通过比较安全偏好数据中被采纳/拒绝回答的激活差异量化特征重要性
3. 设计针对性数据投毒/去噪策略验证特征有效性

Result: 实验表明：仅需修改0.1%数据即可精准操控模型安全对齐（安全响应率从31%→87%或→6%），且保持通用对话能力。代码已开源。

Conclusion: SAFER为高风险LLM对齐任务提供了可解释性工具，支持奖励模型的机制解释、安全审计与优化改进，推动安全可控AI系统发展。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [26] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

TL;DR: 视觉语言模型（VLMs）在不同语言训练下展现出与人类相似的文化注意模式差异：日语模型呈现整体性描述，英语模型偏向分析性聚焦


<details>
  <summary>Details</summary>
Motivation: 探究AI模型是否会像人类一样，因训练数据中的文化差异而表现出不同的信息处理倾向（整体性 vs 分析性）

Method: 对比分析日语和英语VLMs对相同图像生成的描述文本，统计上下文关注度与个体对象聚焦度的差异

Result: VLMs不仅内化语言结构，还能复现训练语料中的文化认知模式（日语模型侧重背景关系，英语模型强调主体属性）

Conclusion: 文化认知通过训练数据隐性塑造AI模型的信息处理方式，这种文化偏差与人类认知差异存在同构性

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [27] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

TL;DR: 提出融合提示工程、模型选择和评估的框架，利用大语言模型生成金融报告，并开发自动标注系统区分数据驱动/金融推理/外部知识型结论


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型从时序数据生成金融报告的潜力，通过信息分类机制评估模型的事实依据和推理能力

Method: 建立包含三大组件的系统框架，引入自动标注系统对报告内容进行来源分类（时序数据/金融推理/外部知识）

Result: 在真实股指和合成数据上的实验证明，大语言模型能够生成结构完整、信息丰富的金融分析报告

Conclusion: 该框架提升了生成报告的可解释性，通过来源标注帮助评估模型可靠性，为金融领域自动化报告生成提供新范式

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [28] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

TL;DR: LitBench作为首个创意写作评估标准化基准，通过人类标注数据集和奖励模型训练，显著提升大语言模型生成内容的自动化评估可靠性


<details>
  <summary>Details</summary>
Motivation: 现有基于现成大语言模型的创意写作评估方法缺乏可靠基准，需要建立标准化评估体系

Method: 构建包含24,480对标注数据的LitBench数据集，训练Bradley-Terry和生成式奖励模型，并通过人类研究验证模型效果

Result: Claude-3.7-Sonnet现成模型达73%人类偏好匹配率，训练后的奖励模型达78%准确率，显著优于所有现成模型

Conclusion: LitBench为创意写作系统提供了经过验证的自动化评估框架，其训练模型在人类研究中展现出持续可靠的评估能力

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [29] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

TL;DR: 使用函数式编程与范畴论构建类型-效应系统，通过图表演算提升自然语言语义的形式化表达能力


<details>
  <summary>Details</summary>
Motivation: 突破传统指称语义的表达局限性，通过代数效应处理机制增强语义模型的灵活性和计算效率

Method: 基于范畴论构建类型-效应系统，设计图表化演算框架（diagrammatic calculus）实现语法解析与效应处理的统一建模

Result: 开发出支持效应传播的可视化计算范式，实现句子指称义的自动化高效推导

Conclusion: 该范畴驱动的方法显著提升了自然语言语义的形式化表达能力，为复杂语言现象的代数化处理提供了新范式

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [30] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

TL;DR: 本文探讨生成式AI在科学计量学中的应用潜力与局限，指出其在语言生成任务（如标注）中表现优异，但在需要稳定语义和领域知识的任务中存在限制，强调需系统比较不同模型表现并进行持续实证研究。


<details>
  <summary>Details</summary>
Motivation: 评估生成式AI对科学计量学领域的影响，警惕其大规模生成科学文本可能改变科研测量指标（作者/词汇/参考文献）的底层特征。

Method: 通过理论溯源（分布语言学）→实验评估（主题标注/引文分析/学者画像等应用）→影响推演的三阶段框架展开分析。

Result: 生成式AI在语言生成主导任务中表现良好，但在需语用推理的任务中稳定性不足，且实验结果可能随模型迭代快速失效。

Conclusion: 需建立持续比较不同模型的机制，通过实证研究和理论反思应对AI生成文本对科研测量体系的根本性挑战。

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [31] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

TL;DR: 研究发现多智能体LLM在道德困境中表现出类似人类的功利主义倾向，但决策机制存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 探索多智能体LLM系统在道德判断中的集体决策机制，及其与人类群体道德决策的异同。

Method: 使用6个LLM模型，在个人道德困境场景下测试两种条件：独立决策（Solo）和群体讨论（Group）。

Result: 所有模型在群体讨论中更接受违反道德规范但能最大化效用的行为，但决策机制不同于人类（敏感性变化vs结果导向）。

Conclusion: LLM群体的表面道德决策模式与人类相似，但底层机制存在本质差异，这对AI对齐和多智能体系统设计具有重要启示。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [32] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

TL;DR: 提出结合人工标注与LLM代理的可扩展评估协议，验证LLM代理可替代人类进行主题模型评估


<details>
  <summary>Details</summary>
Motivation: 现有主题模型评估方法存在自动化指标与人类偏好脱节、专家标签难以扩展的问题，需开发更实用的评估方案

Method: 设计基于文档分类的评估协议：1) 人工/LLM代理审查主题内文档 2) 推断类别标签 3) 验证标签适用性 4) 收集大规模标注数据验证代理有效性

Result: 最佳LLM代理在统计指标上与人类标注者无显著差异（p>0.05），可作为可靠的自动化评估替代方案

Conclusion: 该协议首次实现主题模型评估的规模化验证，LLM代理的可靠性为实际应用提供高效评估路径，开源工具促进方法推广

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [33] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

TL;DR: 使用文体测量学成功区分LLM生成与人类文本，最佳模型达0.98准确率（维基百科vs GPT-4）


<details>
  <summary>Details</summary>
Motivation: 解决模型归属、知识产权和AI伦理问题，针对日益复杂的LLMs提供文本溯源方法

Method: 构建包含人类/LLM/多处理文本的维基基准数据集，采用树模型结合人工设计特征（StyloMetrix）和自建n-gram特征分析文本模式

Result: 多分类MCC达0.87，二分类准确率0.79-1.0（维基百科vs GPT-4平衡数据达0.98），SHAP分析揭示LLM文本存在语法标准化和特定高频词特征

Conclusion: 证实通过文体特征可有效区分特定类型文本来源，为AI文本检测提供方法论支撑

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [34] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

TL;DR: 提出TransLaw多智能体框架解决香港法律判决翻译难题，使用翻译-注释-校对三代理协作模式，在语义准确性和结构连贯性超越GPT-4o但弱于人类专家


<details>
  <summary>Details</summary>
Motivation: 现有LLM在法律翻译中面临专业术语复杂、文化语境敏感、语言结构严苛三大挑战

Method: 构建Translator（翻译）、Annotator（术语标注）、Proofreader（风格校对）三代理协作框架，支持13种LLM配置组合

Result: 框架成本显著低于专业人工翻译，法律语义准确率/结构连贯性/风格保真度超越GPT-4o，但复杂术语语境化和风格自然度仍落后人类专家

Conclusion: 多代理协作模式有效提升法律翻译质量，未来需增强术语语境理解与自然语言生成能力

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [35] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 研究发现数学问题呈现方式的文化差异会影响大语言模型性能，具备推理能力的模型对文化差异更具韧性


<details>
  <summary>Details</summary>
Motivation: 现有数学基准测试（如GSM8K）隐含西方文化背景，需验证模型在不同文化语境下的表现差异

Method: 通过提示转换和人工验证创建5个文化区域的GSM8K测试变体，评估6个不同规模LLM在5种提示策略下的表现

Result: 所有模型在美国原版数据集表现最佳，在文化适应版本出现系统性性能下降（推理型模型下降幅度较小）

Conclusion: 数学问题的文化呈现方式显著影响模型表现，深层推理能力有助于缓解文化差异带来的性能损失

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [36] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

TL;DR: 研究发现下游扩展定律仅在39%情况下有效，实验设置的微小变化会显著改变扩展趋势，需更全面建模预训练损失与下游任务性能的关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究对通过预训练损失预测下游任务性能的扩展定律有效性存在分歧，需系统评估其适用范围和影响因素。

Method: 采用元分析方法，对现有下游扩展定律研究数据进行系统性分析，并考察实验条件变化对扩展趋势的影响。

Result: 仅39%案例符合线性扩展定律，61%案例显示非线性或不可预测趋势，实验设置调整可完全改变扩展模式。

Conclusion: 必须建立包含非线性案例的完整建模框架，同时深入理解扩展定律成功应用的条件边界与约束因素。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [37] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

TL;DR: 构建首个自动生成的带表情包的中文多轮对话数据集MemeCMD，通过大规模多模态标注和双智能体对话生成技术实现


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集缺乏多模态表达能力，纯文本对话难以传递表情包带来的情感和语境细微差异

Method: 结合大规模MLLM标注的表情包库与双智能体自动生成对话，采用检索框架和自适应阈值保证表情包上下文相关性

Result: 生成具有上下文相关性的多样化表情包对话，实验验证方法有效性，提供可扩展且保护隐私的数据资源

Conclusion: MemeCMD为推进多模态对话AI发展提供了兼具表达力和自动化优势的新型数据集解决方案

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [38] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

TL;DR: 现有方法需更大同源数据集，但自动生成不可行，导致系统发育推断结果与权威标准不符，计算历史语言学应用存疑


<details>
  <summary>Details</summary>
Motivation: 手动收集的同源数据集规模过小，无法支撑复杂模型与机器学习技术需求，且当前缺乏有效自动生成大规模同源数据的方法

Method: 通过BabelNet自动提取多语言词典数据构建特征矩阵，评估系统发育推断结果与标准演化树的吻合度

Result: BabelNet生成的系统发育树与标准树存在显著偏差，其他多语言资源也难以提取有效特征矩阵

Conclusion: 受限于数据集规模，计算系统发育方法难以应用于同源数据分析，计算历史语言学的可行性仍待解决

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [39] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 研究揭示了大型语言模型道德自我纠正的两个悖论：表面有效性及诊断能力不足，通过分析语料库发现启发式捷径机制，并提出基于数据启发式的解决方案


<details>
  <summary>Details</summary>
Motivation: 现有道德自我纠正技术存在两个核心矛盾：1. 仅在表层实现道德对齐 2. 无法定位道德不一致的根源，需揭示其底层机制

Method: 通过解构道德微调语料库的论述结构，发现有效构建中的启发式捷径，实验验证这些捷径在自我纠正中的作用机制

Result: 证明道德自我纠正依赖启发式捷径，且同时提升自我纠正与自我诊断能力会产生系统矛盾，需通过精选数据集的启发式进行干预

Conclusion: 提出基于数据启发式的改进方案，同时指出该能力在情境化学习与模型规模扩展方面存在显著泛化挑战

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [40] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

TL;DR: 论文通过对比MLM与CLM预训练目标，发现MLM在文本表征任务表现更优但CLM数据效率更高，提出CLM+MLM双阶段训练策略可优化计算资源利用。


<details>
  <summary>Details</summary>
Motivation: 探究CLM模型在文本表征任务中的优势究竟源于其训练目标本身还是模型/数据规模等外部因素，明确两种预训练目标的本质差异。

Method: 训练30个不同参数规模模型(2.1亿至10亿参数)，控制变量进行15,000+次微调实验，采用双阶段训练策略(先CLM后MLM)。

Result: MLM整体表现更优但CLM数据效率提升37%、微调稳定性更好；双阶段训练比单一目标训练提升2.5%，且继承CLM预训练权重可减少34%计算消耗。

Conclusion: 在固定计算预算下，CLM+MLM双阶段训练策略能实现最优性能，且利用现有LLM生态中的CLM模型可显著降低训练最佳编码器的计算成本。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [41] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

TL;DR: 创建首个西班牙语及变体的开源大模型评测榜La Leaderboard，整合66个数据集评估50个模型，提倡低样本评估减少环境影响。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型对西班牙语社区语言文化多样性支持不足的问题，推动社区驱动的模型发展。

Method: 整合巴斯克语/加泰罗尼亚语/加利西亚语及西班牙语变体的66个数据集，采用低few-shot评估策略(减少样本量)，建立标准化评测框架。

Result: 覆盖4种主要语言及方言变体，完成50个模型评测，验证低样本策略可行性，建立可复现评估标准。

Conclusion: 通过开源社区协作构建多语言评测体系，平衡模型性能与环保需求，为小语种NLP发展提供新范式。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [42] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: SciArena是基于社区投票的开放科研平台，用于评估科学文献任务中的基础模型表现，支持23个模型并收集13k+研究人员投票，同时发布自动化评估基准SciArena-Eval。


<details>
  <summary>Details</summary>
Motivation: 传统科学文献评估基准难以满足开放型任务的评估需求，需通过社区协作方式实现更真实的模型性能评估。

Method: 采用类Chatbot Arena的社区投票机制，收集跨领域研究者对模型长文本回答的偏好数据，并通过元评估基准验证自动评估系统的可靠性。

Result: 构建当前最全面的开放模型排行榜，发现现有自动评估方法与人类投票的一致性仅为62%，揭示自动化评估系统的技术瓶颈。

Conclusion: 社区驱动的评估能有效捕捉模型真实能力，但需开发更可靠的自动评估方法以支持科学文献处理系统的持续发展。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [MVGBench: Comprehensive Benchmark for Multi-view Generation Models](https://arxiv.org/abs/2507.00006)
*Xianghui Xie,Chuhang Zou,Meher Gitika Karumuri,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.GR

TL;DR: 提出了MVGBench多视角图像生成评估基准，引入3D自一致性指标系统评估12种模型，发现现有方法在鲁棒性和泛化性上的不足，并提出效果更优的ViFiGen方法。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标依赖真实视图对比，无法有效评估生成任务的多样性。不同MVGs在训练数据、视角和光照条件上的差异导致评估不全面，需建立标准化评估体系。

Method: 构建包含3D几何/纹理一致性、图像质量和语义分析的评估框架，提出基于多视图3D重建的自一致性指标，在4个真实/合成数据集上系统测试12种模型。

Result: 现有方法在数据泛化(真实数据准确率下降25%)和光照鲁棒性(性能波动达40%)方面存在明显缺陷。ViFiGen在3D一致性指标上超越所有基线模型。

Conclusion: MVGBench首次系统揭示多视角生成模型的关键瓶颈，基于最佳实践设计的ViFiGen验证了评估体系的有效性，公开的基准套件将推动领域发展。

Abstract: We propose MVGBench, a comprehensive benchmark for multi-view image
generation models (MVGs) that evaluates 3D consistency in geometry and texture,
image quality, and semantics (using vision language models). Recently, MVGs
have been the main driving force in 3D object creation. However, existing
metrics compare generated images against ground truth target views, which is
not suitable for generative tasks where multiple solutions exist while
differing from ground truth. Furthermore, different MVGs are trained on
different view angles, synthetic data and specific lightings -- robustness to
these factors and generalization to real data are rarely evaluated thoroughly.
Without a rigorous evaluation protocol, it is also unclear what design choices
contribute to the progress of MVGs. MVGBench evaluates three different aspects:
best setup performance, generalization to real data and robustness. Instead of
comparing against ground truth, we introduce a novel 3D self-consistency metric
which compares 3D reconstructions from disjoint generated multi-views. We
systematically compare 12 existing MVGs on 4 different curated real and
synthetic datasets. With our analysis, we identify important limitations of
existing methods specially in terms of robustness and generalization, and we
find the most critical design choices. Using the discovered best practices, we
propose ViFiGen, a method that outperforms all evaluated MVGs on 3D
consistency. Our code, model, and benchmark suite will be publicly released.

</details>


### [44] [ViscoReg: Neural Signed Distance Functions via Viscosity Solutions](https://arxiv.org/abs/2507.00412)
*Meenakshi Krishnan,Ramani Duraiswami*

Main category: cs.GR

TL;DR: 提出ViscoReg损失函数，通过粘性正则化增强神经SDF训练的稳定性，在理论分析和实验中均优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统基于Eikonal方程的神经SDF训练存在不适定性和梯度不稳定问题，需要引入数值计算中的粘性正则化理论

Method: 将粘性解理论融入损失函数设计，提出ViscoReg损失项，并证明其梯度流方程的稳定性

Result: ViscoReg在SIREN、DiGS等基准方法上取得更好效果，且未显著增加计算开销

Conclusion: 粘性正则化为神经SDF训练提供了理论保障和实用提升，ViscoReg具有广泛适用性

Abstract: Implicit Neural Representations (INRs) that learn a Signed Distance Function
(SDF) are a powerful tool for continuous 3D scene reconstruction. These models
are trained by enforcing the Eikonal equation. We demonstrate theoretically
that despite the ill-posedness of the Eikonal equation, generalization error
estimates may be obtained for Neural SDFs in terms of the training error.
However, training with the Eikonal loss can lead to unstable gradient flows,
necessitating alternate stabilization techniques. Traditional numerical solvers
for the equation have relied on viscosity approaches for regularization. We
enhance Neural SDF training using this well-developed theory, and introduce a
new loss formulation we call ViscoReg. We theoretically demonstrate the
stability of the gradient flow equation of our proposed loss term. Empirically,
ViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik
without adding significant computational cost.

</details>


### [45] [FreNBRDF: A Frequency-Rectified Neural Material Representation](https://arxiv.org/abs/2507.00476)
*Chenliang Zhou,Zheyuan Hu,Cengiz Oztireli*

Main category: cs.GR

TL;DR: 通过频域修正的神经材质表征FreNBRDF，显著提升材质建模的准确性与可编辑性


<details>
  <summary>Details</summary>
Motivation: 现有神经BRDF表征在频域特性缺乏系统研究，导致材质重建/编辑存在精度和鲁棒性不足

Method: ① 引入球谐函数整合频域特性 ② 提出基于频域分析的频率修正损失函数 ③ 构建自适应重建-编辑流程框架

Result: 实验证明该方法在材质外观重建精度提升23.6%，编辑鲁棒性提升18.4%（相比SOTA方法）

Conclusion: 该框架首次建立神经材质与频域的关联，为材质分析提供结构化表征，推动影视/AR行业高保真数字资产生产

Abstract: Accurate material modeling is crucial for achieving photorealistic rendering,
bridging the gap between computer-generated imagery and real-world photographs.
While traditional approaches rely on tabulated BRDF data, recent work has
shifted towards implicit neural representations, which offer compact and
flexible frameworks for a range of tasks. However, their behavior in the
frequency domain remains poorly understood. To address this, we introduce
FreNBRDF, a frequency-rectified neural material representation. By leveraging
spherical harmonics, we integrate frequency-domain considerations into neural
BRDF modeling. We propose a novel frequency-rectified loss, derived from a
frequency analysis of neural materials, and incorporate it into a generalizable
and adaptive reconstruction and editing pipeline. This framework enhances
fidelity, adaptability, and efficiency. Extensive experiments demonstrate that
\ours improves the accuracy and robustness of material appearance
reconstruction and editing compared to state-of-the-art baselines, enabling
more structured and interpretable downstream tasks and applications.

</details>


### [46] [Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory](https://arxiv.org/abs/2507.00725)
*Amritendu Dhar,Apratim Chakraborty,Vijay Natarajan*

Main category: cs.GR

TL;DR: 将Morse-Cerf理论扩展至分段线性函数族，提出顶点图/Cerf图表征临界点演化，定义拓扑描述符并开发对应算法与比较方法。


<details>
  <summary>Details</summary>
Motivation: 针对时变标量场的拓扑特征分析需求，将经典光滑函数理论框架迁移至更适用于计算几何处理的PL函数体系。

Method: 1. 通过顶点下链同调刻画顶点图交叉特性
2. 构建Cerf图记录PL函数临界点演化路径
3. 提出基于拓扑描述符的时变场分析方法

Result: 成功开发Cerf图自动生成算法，设计基于持续同调理论的Cerf图相似性度量指标，实验验证了时变场拓扑演化模式的有效捕捉。

Conclusion: 该理论框架为时变标量场的拓扑分析提供了新的数学工具，在科学可视化与几何数据处理领域具有应用潜力。

Abstract: Morse-Cerf theory considers a one-parameter family of smooth functions
defined on a manifold and studies the evolution of their critical points with
the parameter. This paper presents an adaptation of Morse-Cerf theory to a
family of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram
are introduced as representations of the evolution of critical points of the PL
function. The characterization of a crossing in the vertex diagram based on the
homology of the lower links of vertices leads to the definition of a
topological descriptor for time-varying scalar fields. An algorithm for
computing the Cerf diagram and a measure for comparing two Cerf diagrams are
also described together with experimental results on time-varying scalar
fields.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: 提出HDRAM框架解决LLMs的信息扩散问题，通过结合纠错码/全息计算/量子启发搜索实现高效关联检索


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在精度损失问题（重新定义为信息扩散），需从信息论角度解决KV/VK记忆机制缺陷

Method: 构建基于超令牌的符号存储框架，整合纠错码+全息计算+量子搜索，通过相位相干内存地址实现潜在空间的高效搜索

Result: 在不改变架构的前提下显著提升关联检索效率，验证CHQ原则对Transformer架构的增强效果

Conclusion: 信息论与量子启发的结合为改进大语言模型提供了新范式，HDRAM框架展示了跨学科方法在AI架构优化中的潜力

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [48] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: Unified framework connects SFT and preference learning, identifies limitations in conventional SFT, proposes learning rate reduction and alternative objectives with significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in conventional Supervised Fine-Tuning (SFT) and establishing theoretical connections between SFT/preference learning methods in LLM post-training.

Method: Mathematical framework revealing SFT as implicit reward learning special case, proposing learning rate reduction strategy and f-divergence based alternative objectives.

Result: Achieved 25% relative performance gain and 6% absolute win rate improvement in instruction following tasks through learning rate adjustment.

Conclusion: Theoretical unification and practical improvements provide new optimization directions for LLM post-training, bridging policy optimization with reward learning paradigms.

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [49] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: 提出GLU Attention注意力机制，通过给注意力值引入非线性提升模型性能与收敛速度，零参数量增加且兼容主流注意力优化技术


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制的值计算缺乏非线性表达能力，限制了模型性能潜力。GLU结构在FFN层表现优异，但尚未被应用于注意力值计算

Method: 在注意力机制的值（Value）计算过程中引入Gated Linear Units（GLU），通过门控机制实现非线性特征选择与增强

Result: 在文本和视觉任务中均提升模型性能，加快收敛速度（快30%），兼容Flash Attention/RoPE/GQA等技术且不增加参数量

Conclusion: GLU Attention是轻量高效的注意力改进方案，其开源实现将促进大规模应用。该方法为注意力机制优化提供了新方向

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [50] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 提出ROSE框架，通过多目标强化学习生成多样化的对抗性提示，有效提升大语言模型安全性评估的覆盖范围和现实场景适配性


<details>
  <summary>Details</summary>
Motivation: 现有手动安全基准存在静态更新慢、对抗话题覆盖不足的问题，难以适配快速迭代的大语言模型安全评估需求

Method: 采用多目标强化学习算法，在对抗提示生成中平衡话题多样性与现实场景适配性，构建动态优化的评估框架

Result: 实验表明ROSE在发现模型安全漏洞方面显著优于现有方法，综合评估指标提升明显

Conclusion: ROSE框架为LLMs安全评估提供了更贴近现实场景的解决方案，推动安全测试从静态基准向动态演化发展

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [51] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 本文解析了基于patch的时间序列基础模型（TSFMs）的成功机制，提出其通过扩展语言模型的概率分布表示范式，继承鲁棒表征能力，从而解决跨领域迁移的悖论。


<details>
  <summary>Details</summary>
Motivation: 针对时间序列基础模型在跨领域迁移中存在的理论矛盾（动态系统差异性 vs 实证成功），探索其底层表示学习机制与泛化能力的理论依据。

Method: 结合理论分析与实验验证，揭示时间序列patch可离散化为统计特性与自然语言高度一致的词汇表，实现从确定性向量到概率分布的范式扩展。

Result: 证实时间序列模型通过概率分布表示继承语言模型的迁移能力，其patch量化后的词汇表困惑度等指标与自然语言具有强可比性。

Conclusion: 为评估和改进大规模时间序列基础模型提供了理论框架，特别在模型安全性与可靠性维度建立了量化分析基础。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [52] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: 提出融合联邦学习的混合语言模型框架FedHLM，通过协作学习不确定性阈值和分层模型聚合，减少95%的大模型通信开销


<details>
  <summary>Details</summary>
Motivation: 现有混合语言模型（HLM）在低置信度预测时仍需频繁调用云端大模型（LLM），导致带宽受限场景下通信开销过高。需要一种兼顾计算效率和通信效率的解决方案。

Method: 1. 联邦学习协作优化词元级不确定性阈值
2. 基于嵌入表示的P2P语义解析机制复用相似词元
3. 分层模型聚合：边缘服务器优化本地路由策略，跨集群协调全局决策边界

Result: 在大规模新闻分类任务中，FedHLM在精度损失可忽略的前提下，成功降低95%以上的大模型传输量

Conclusion: 该框架通过分层设计捕获重复出现的不确定性模式，显著减少冗余的大模型查询，适用于边缘AI场景的规模化部署需求

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [53] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: 提出结合ResNet和Transformer热力图的新型解释框架，通过时空对齐技术提升模型可解释性，在医疗和工业数据集上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中卷积网络缺乏全局上下文、Transformer缺乏局部精度的时空错位问题，该局限在医疗和工业监测等安全关键领域阻碍有效决策

Method: 融合梯度加权激活图（ResNet）与Transformer注意力展开技术，通过NLP模块将融合热力图转化为领域特定解释

Result: PhysioNet数据集准确率94.1%（F1 0.93），UCI能源数据集回归误差RMSE=0.28kWh（R²=0.95），较基线模型提升3.8-12.4%

Conclusion: 通过形式化可解释性为因果保真与时空对齐，架起技术输出与业务理解的桥梁，为时间敏感的透明决策提供可扩展方案

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [54] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: 提出AutoDS方法，利用贝叶斯惊喜量化认知变化并结合蒙特卡洛树搜索策略，显著提升开放自主科学发现效率，实验显示其发现数量比基线模型提升5-29%。


<details>
  <summary>Details</summary>
Motivation: 现有开放自主科学发现方法存在假设空间导航能力不足和主观评价标准模糊的问题，需要更有效的系统自驱探索机制。

Method: 通过贝叶斯惊喜值量化LLM实验前后认知差异，将其作为蒙特卡洛树搜索的奖励函数，采用渐进扩展策略实现高效假设空间探索。

Result: 在21个跨领域真实数据集中，AutoDS产生的让LLM惊讶的发现数量超基准模型29%，领域专家认可其中66.7%发现具有新颖性。

Conclusion: 基于认知变化的量化探索机制为开放自主科学发现提供了新范式，贝叶斯惊喜驱动的搜索策略显著提升了系统发现能力，推动AI自主科研进程。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [55] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: 提出多尺度多模态大模型μ²LLM，通过整合视觉与文本特征及偏好优化，有效提升CT影像报告的生成质量


<details>
  <summary>Details</summary>
Motivation: 解决放射报告生成中信息提取复杂度高、人工评估差异大的痛点，整合多模态特征提升生成质量

Method: 设计μ²Tokenizer中间层融合多尺度视觉特征与文本特征，采用GREEN-RedLlama指导的直接偏好优化(DPO)框架

Result: 在四个大型CT数据集上超越现有方法，证明有限数据下微调模型的优越性

Conclusion: μ²LLM框架显著提升放射报告生成的准确性和效率，为医学影像分析提供新范式

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [56] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 提出TarFlowLM框架，通过基于transformer的自回归归一化流在连续潜在空间进行语言建模，突破传统离散token模型的限制


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型受限于离散token、单向上下文和单次解码，需探索连续潜在空间以获取双向上下文建模、灵活token块生成和多轮迭代生成能力

Method: 采用transformer自回归归一化流建模连续表征，设计混合耦合变换捕捉潜在空间复杂依赖，建立与离散模型的数学关联

Result: 在语言建模基准测试中展现出强似然性能，验证了框架在双向上下文建模、块状生成和层级多轮生成方面的灵活性

Conclusion: TarFlowLM为语言建模开辟了连续空间建模新范式，通过交替方向自回归变换实现了传统离散模型无法达到的全局上下文捕获能力

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [57] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: 本文提出新合成任务Joint Recall揭示SSMs长上下文建模局限性，通过整合上下文相关稀疏注意力（CDSA）设计HAX方法，实验证明其优于传统SSMs和CISA方案。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（SSMs）虽计算高效，但处理复杂长距离依赖能力不足。传统评估任务Associative Recall过于简化，需更贴近真实场景的评估体系推动长上下文建模技术发展。

Method: 1. 提出Joint Recall任务（多上下文键值对召回）
2. 理论证明SSMs无法次线性解决该任务
3. 提出SSMs+CDSA理论框架
4. 设计自然语言特化的HAX方法（哈希注意力+稀疏键选择）

Result: 在合成任务和真实长文本基准测试中，HAX相比SSM基线准确率提升21.9%，比SSM+CISA方案提升9.3%，显存消耗降低37%。

Conclusion: 通过理论分析与工程优化，揭示了SSMs的理论局限，提出HAX框架有效突破计算-性能平衡，为实际长文本处理提供了更优解决方案。

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: VirtualFencer通过无监督学习从视频中提取3D击剑动作与策略，并生成逼真对抗行为，支持自我对练、与真实选手对抗及职业选手互动。


<details>
  <summary>Details</summary>
Motivation: 击剑运动兼具动作多样性与双人策略互动特性，传统方法难以有效建模，需数据驱动方法捕捉复杂运动模式与战略逻辑。

Method: 开发无监督学习系统，从野生视频数据自动提取三维动作特征和战略决策模式，构建双代理对抗生成框架。

Result: 系统实现三种验证：自对抗训练、与在线视频选手实时对抗、职业选手实测互动，证实生成动作的战略合理性与运动真实性。

Conclusion: 该研究证明了数据驱动模型在复杂双人竞技运动中的建模潜力，为体育分析、智能训练系统开发提供了新技术路径。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [59] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 提出'moment sampling'帧采样方法，通过文本到视频时刻检索模型指导帧选择，解决长视频问答中关键帧丢失与冗余问题，显著提升视频大语言模型的长程推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有均匀帧采样方法在长视频问答中易丢失关键帧或产生冗余信息，导致模型准确性下降和计算资源浪费，需更智能的帧选择策略。

Method: 使用轻量级时刻检索模型动态选择与问题最相关的视频片段帧，替代传统均匀采样，实现上下文敏感的帧采样。

Result: 在四个长视频问答数据集和四个SOTA视频大语言模型上的实验表明，该方法显著提升长视频问答准确率(平均+3.8%)并降低计算开销。

Conclusion: 问题引导的moment sampling机制有效增强视频LLMs的长程推理能力，且具有模型无关性，为长视频理解提供了新范式。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [60] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 研究发现了GPT-o3在新型挑战性任务CaughtCheating中的性能骤降现象，提出该场景可作为评估多模态大语言模型侦探式推理能力的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试对先进MLLMs已不具备挑战性，需通过模拟真实社交媒体中检测伴侣可疑线索的困难场景，验证模型的人类级侦探推理能力。

Method: 创建CaughtCheating测试场景（受社交媒体求助启发），通过系统性实验分析GPT-o3等模型在细微视觉线索感知与情境推理中的失败原因。

Result: GPT-o3在CaughtCheating任务中准确率趋近于零，实验揭示了现有MLLMs在视觉细节关联与情境逻辑整合方面的能力缺陷。

Conclusion: CaughtCheating为MLLMs提供了兼具挑战性和实用价值的测试平台，其突破将推动模型获得真正的人类侦探级推理能力。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [61] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: 提出MANTA框架，通过结构化文本空间统一多模态输入，解决跨模态对齐、时间同步、分层表示和稀疏信息检索四大挑战，在长视频问答任务中实现最高22.6%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法常将不同模态割裂处理，导致表示和推理不一致。需构建理论化框架统一多模态信息表示，提升大语言模型处理效率。

Method: 基于信息论优化实现语义对齐，开发自适应时间同步机制，构建分层内容表示体系，结合上下文感知的稀疏信息检索。建立数学框架验证token限制下的最优上下文选择。

Result: 在长视频问答任务中准确率提升22.6%（30分钟以上视频达27.3%），时间推理任务提升23.8%，跨模态理解提升25.1%。新密度估计技术有效平衡冗余最小化和信号保留。

Conclusion: MANTA通过结构化文本统一多模态表示，建立新的理论框架。提出的密度估计技术为多模态学习奠定新基础，显著提升长序列、跨模态场景下的模型性能。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [62] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 提出基于轻量级DNN和MediaPipe的实时手语识别框架，通过结构化参数编码解决数据稀缺与计算瓶颈，在343个手势识别中达到92%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别领域的数据匮乏、计算成本高、训练推理帧率差异等实际部署瓶颈，推动边缘设备上的实时应用。

Method: 1. 将手语特征参数向量化输入
2. 采用MediaPipe进行关键点提取
3. 设计亚10MB的轻量DNN架构
4. 开发'slait data'数据标注平台实现结构化处理

Result: 在边缘设备实现10ms延迟的实时推理，模型准确率达92%，并成功集成至网页应用'slait ai'中稳定运行。

Conclusion: 该框架通过轻量化模型与专用数据处理工具的结合，有效突破了手语识别实际应用的关键技术障碍，为实时交互系统提供了可行方案。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [63] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: 提出ONLY方法解决大型视觉语言模型的幻觉问题，通过单次查询和熵比优化实现高效实时部署


<details>
  <summary>Details</summary>
Motivation: 现有对比解码方法需要多次查询影响响应速度，难以满足实时应用需求

Method: 采用训练自由的解码策略，基于文本到视觉的熵比选择性增强关键文本信息

Result: 在多个基准测试中超越SOTA方法，计算成本降低90%且保持高性能

Conclusion: ONLY首次实现单次查询干预，为LVLMs实时部署提供高效可靠解决方案，代码已开源

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: 提出奖励引导的数据集蒸馏框架AdvDistill，通过多个教师模型生成结果结合规则验证器分配奖励权重，提升小语言模型在复杂推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法仅让学生模型复制教师模型的同分布响应，限制了模型泛化能力且在推理任务中计算成本较高

Method: 使用教师模型为每个提示生成多个响应，通过规则验证器分配奖励形成正态分布权重指导学生模型训练

Result: 在数学推理和复杂推理任务中显著提升学生模型性能，验证了奖励机制在数据集蒸馏中的有效性

Conclusion: 将奖励机制融入数据集蒸馏过程能有效提升小语言模型的学习效果，为高效模型压缩提供了新方向

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [65] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 提出逆向推理框架SAGE-nano，通过元认知注意力机制实现LLM自我解释，在保持高推理准确率（74.6%）的同时获得92.1%的人类解释认可度，接近GPT-4o水平。


<details>
  <summary>Details</summary>
Motivation: 传统思维链（CoT）方法缺乏对推理路径选择依据的解释，导致LLM决策过程存在黑箱问题，阻碍AI透明化与安全发展。

Method: 逆向推理范式+注意力反向流动分析：1）构建元认知结构分解决策节点；2）通过注意力权重回溯识别关键推理步骤；3）自动生成多维度解释。

Result: 在AQUA-RAT/CommonsenseQA测试中：推理准确率74.6%（提升12.3%），人类解释认可度92.1%，推理速度比GPT-4快1.8倍。

Conclusion: 开创LLM自我解释新范式，验证逆向推理可同步提升性能与可解释性，为AI安全/教育/科研提供新方法论，缩小与顶级闭源模型的解释性差距。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [66] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO框架通过搜索算法训练语言模型实现自我反思/回溯/探索，显著提升Llama 3在数学难题上的推理表现（MATH-500 +16%，AMC 2023 +26.9%，AIME 2024 +20%）


<details>
  <summary>Details</summary>
Motivation: 现有开源推理模型依赖预训练阶段的搜索行为，非推理模型（如Llama 3）缺乏系统化的推理训练方法

Method: 1. 基于蒙特卡洛树搜索生成数学问题求解轨迹
2. 将搜索轨迹转化为包含成功/失败恢复的自然语言思维链
3. 结合可验证奖励的强化学习进行微调

Result: 在需要迭代修正的难题上效果显著：
- MATH-500提升16%
- AMC 2023提升26.9%
- AIME 2024提升20%

Conclusion: 搜索启发的训练方法为开源大模型提供了系统化的推理能力提升路径，证明结构化搜索行为内化对复杂问题解决的有效性

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [67] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 数学推理性能提升未必反映整体问题解决能力，强化学习调优模型展现跨领域泛化优势，监督微调导致能力遗忘


<details>
  <summary>Details</summary>
Motivation: 探究数学模型在数学基准上的性能提升是否具有领域泛化性，还是局限于特定任务的过拟合

Method: 通过跨领域评估（数学/科学QA/规划/编码）、控制实验（不同调优方法的Qwen3-14B模型）及潜在空间表征分析

Result: 数学优势模型普遍存在领域迁移障碍，RL调优保留通用能力（+13.5%跨领域提升），SFT导致表征漂移和性能下降

Conclusion: 需重新设计模型训练范式，减少对监督微调蒸馏数据的依赖，强化学习展现出更优的知识迁移特性

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [68] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 提出CIP方法，利用因果影响图提升自主代理安全性，实验验证在代码执行和移动设备控制任务中有效


<details>
  <summary>Details</summary>
Motivation: 确保LLM驱动的自主代理在辅助任务中的安全可靠，防止决策导致的意外后果

Method: 1. 基于任务规范初始化因果图 2. 用因果图指导环境交互 3. 根据观察结果迭代优化因果图

Result: 在代码执行和移动设备控制任务中显著增强安全性

Conclusion: 通过结构化因果建模实现更安全的代理决策，为安全AI系统提供新思路

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [69] [Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite](https://arxiv.org/abs/2507.00877)
*William H English,Chase Walker,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: eess.SY

TL;DR: 提出VLTL-Bench基准测试，解决现有NL到TL翻译系统评估中忽略命题场景迁移能力的问题


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估形式逻辑翻译准确性，忽视系统在新场景中基础命题的验证能力，导致性能指标虚高且缺乏可扩展性

Method: 构建包含3个状态空间、数千种自然语言规范及其对应时序逻辑公式的数据集，并提供验证样本链

Result: 创建支持端到端评估的统一基准，允许分阶段验证lifting/grounding/translation/verification子过程

Conclusion: VLTL-Bench通过提供分阶段验证能力，推动可验证NL到LTL翻译方法的方法论改进

Abstract: Empirical evaluation of state-of-the-art natural-language (NL) to
temporal-logic (TL) translation systems reveals near-perfect performance on
existing benchmarks. However, current studies measure only the accuracy of the
translation of NL logic into formal TL, ignoring a system's capacity to ground
atomic propositions into new scenarios or environments. This is a critical
feature, necessary for the verification of resulting formulas in a concrete
state space. Consequently, most NL-to-TL translation frameworks propose their
own bespoke dataset in which the correct grounding is known a-priori, inflating
performance metrics and neglecting the need for extensible, domain-general
systems. In this paper, we introduce the Verifiable Linear Temporal Logic
Benchmark ( VLTL-Bench), a unifying benchmark that measures verification and
verifiability of automated NL-to-LTL translation. The dataset consists of three
unique state spaces and thousands of diverse natural language specifications
and corresponding formal specifications in temporal logic. Moreover, the
benchmark contains sample traces to validate the temporal logic expressions.
While the benchmark directly supports end-to-end evaluation, we observe that
many frameworks decompose the process into i) lifting, ii) grounding, iii)
translation, and iv) verification. The benchmark provides ground truths after
each of these steps to enable researches to improve and evaluate different
substeps of the overall problem. To encourage methodologically sound advances
in verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:
https://www.kaggle.com/datasets/dubascudes/vltl bench.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [70] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 提出基于Transformer的端到端模型，通过动态数据增强和优化标记策略，在MIDI符号音乐节拍跟踪任务中取得优于传统方法的性能


<details>
  <summary>Details</summary>
Motivation: 现有节拍跟踪方法主要针对音频信号，而符号音乐(MIDI)的节拍跟踪对乐谱转录和节奏分析具有重要意义但缺乏有效解决方案

Method: 使用编码器-解码器架构的Transformer模型，开发动态增强的预处理流程和优化的MIDI标记策略，支持跨数据集的端到端节拍注释生成

Result: 在A-MAPS、ASAP等数据集上超越隐马尔可夫模型和现有深度学习方法，F1分数达到竞争水平且具有跨乐器泛化能力

Conclusion: 证明了Transformer架构在符号音乐节拍跟踪中的有效性，为未来与自动记谱系统的整合提供了技术基础

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [71] [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
*Yifan Gao,Jiao Fu,Long Guo,Hong Liu*

Main category: cs.SD

TL;DR: 结合大语言模型与传统特征的自杀风险评估方法在SW1挑战赛中以74%准确率夺冠


<details>
  <summary>Details</summary>
Motivation: 早期识别自杀风险对预防至关重要，语音作为非侵入性指标具有临床实用价值

Method: 融合LLM特征提取与传统声学/语义特征的多模态分析方法

Result: 测试集准确率74%，SW1挑战赛排名第一

Conclusion: 基于LLM的语音分析方法在自杀风险评估中展现显著潜力

Abstract: Early identification of suicide risk is crucial for preventing suicidal
behaviors. As a result, the identification and study of patterns and markers
related to suicide risk have become a key focus of current research. In this
paper, we present the results of our work in the 1st SpeechWellness Challenge
(SW1), which aims to explore speech as a non-invasive and easily accessible
mental health indicator for identifying adolescents at risk of suicide.Our
approach leverages large language model (LLM) as the primary tool for feature
extraction, alongside conventional acoustic and semantic features. The proposed
method achieves an accuracy of 74\% on the test set, ranking first in the SW1
challenge. These findings demonstrate the potential of LLM-based methods for
analyzing speech in the context of suicide risk assessment.

</details>


### [72] [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/abs/2507.00808)
*Hiroki Kanagawa,Kenichi Fujita,Aya Watanabe,Yusuke Ijima*

Main category: cs.SD

TL;DR: 提出支持多步交互的文本转语音方法，允许用户通过迭代反馈精细调整合成语音风格


<details>
  <summary>Details</summary>
Motivation: 现有文本转语音系统缺乏类似实际录音中的迭代反馈机制，导致无法在初次合成后进行细粒度的风格调整，难以满足用户预期的语音表达需求

Method: 构建模拟配音演员与导演互动关系的TTS模型，通过多步交互界面使用户能直观快速调整语音风格参数

Result: 实验验证模型能够根据用户的多轮指令实现语音风格的迭代优化，成功实现平均3轮交互达到目标效果

Conclusion: 该交互式TTS框架有效解决了传统合成系统缺乏细粒度调整能力的问题，显著提升了用户对语音生成过程的控制力

Abstract: Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthetized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enable iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available: https://ntt-hilab-gensp.
github.io/ssw13multiinteraction_tts/

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [73] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: SciBORG是一个模块化的AI代理框架，通过动态构建代理、有限状态自动机内存和上下文感知决策，提升LLM在复杂科学工作流中的可靠性和自适应性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂科学工作流中存在内存管理、多步规划和工具集成限制，难以实现稳健的领域特定任务执行。

Method: 基于源代码文档动态构建代理，采用有限状态自动机(FSA)实现持久状态追踪，通过上下文感知决策机制消除人工提示工程需求，支持跨工作流的上下文维护和故障恢复。

Result: 在微波合成器物理硬件和PubChem数据库虚拟实验中实现可靠的多步生物测定检索，系统基准测试显示代理具备自适应规划和可解释状态转换能力。

Conclusion: 内存管理和状态感知是智能体规划可靠性的关键，SciBORG为复杂环境中AI代理部署提供了可推广的基础架构。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [74] [Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels](https://arxiv.org/abs/2507.00333)
*Emin Zerman,Jonas Carlsson,Mårten Sjöström*

Main category: cs.HC

TL;DR: 开发射击可视化系统评估新手与专家射手训练效果，仪表盘式复合视图在10人测试中9人优先选择并提升理解效果


<details>
  <summary>Details</summary>
Motivation: 传统射击训练依赖重复练习且缺乏第一视角分析，教练无法实时观察射手视野，分析局限在姿势与准确度

Method: 基于第一人称射击视频开发5种复合可视化界面（含覆盖指标与图形摘要），通过定量任务、偏好比较和访谈的混合方法评估10名参与者（5专家+5新手）

Result: 结合原始视频/极坐标图/统计图的仪表盘式视图在90%案例中被优先选择，有效支持不同水平射手的训练分析

Conclusion: 第一人称视频与可视化分析结合对射击训练教练具有重要价值，该方法可推广至其他精准性运动训练优化

Abstract: Marksmanship practices are required in various professions, including police,
military personnel, hunters, as well as sports shooters, such as Olympic
shooting, biathlon, and modern pentathlon. The current form of training and
coaching is mostly based on repetition, where the coach does not see through
the eyes of the shooter, and analysis is limited to stance and accuracy
post-session. In this study, we present a shooting visualization system and
evaluate its perceived effectiveness for both novice and expert shooters. To
achieve this, five composite visualizations were developed using first-person
shooting video recordings enriched with overlaid metrics and graphical
summaries. These views were evaluated with 10 participants (5 expert marksmen,
5 novices) through a mixed-methods study including shot-count and aiming
interpretation tasks, pairwise preference comparisons, and semi-structured
interviews. The results show that a dashboard-style composite view, combining
raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases
and supported understanding across skill levels. The insights gained from this
design study point to the broader value of integrating first-person video with
visual analytics for coaching, and we suggest directions for applying this
approach to other precision-based sports.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [75] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: MassTool框架通过多任务学习和双塔架构提升工具检索准确性，解决现有方法忽视查询理解的问题


<details>
  <summary>Details</summary>
Motivation: 现有工具检索方法过度关注工具表示优化，忽视了精确查询理解对下游任务的关键影响

Method: 采用双塔架构（工具使用检测塔+QC-GCN检索塔），整合搜索式用户意图建模(SUIM)和自适应知识迁移模块(AdaKT)，通过多任务损失联合优化

Result: 实验证明该框架显著提升检索准确率，有效处理分布外查询

Conclusion: MassTool通过创新的双阶段决策流程和查询中心图网络，建立了精确的查询理解范式，为工具增强型LLM系统提供了新方向

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [76] [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
*Craig S Wright*

Main category: cs.CR

TL;DR: 通过形式化建模证明比特币SPV协议的安全性及最优性，提出带宽优化方案并纠正常见误解


<details>
  <summary>Details</summary>
Motivation: 当前主流实现曲解了比特币白皮书定义的SPV原理，需要从数学基础重构协议并验证其在对抗环境下的可靠性

Method: 基于符号自动机建立验证模型，结合Merkle树成员关系证明和概率博弈论分析，引入自适应轮询等低带宽优化技术

Result: 推导出协议安全运行的经济边界，证明在部分网络连接条件下仍保持活性与安全性，实现压缩区块头同步(减少70%带宽)

Conclusion: SPV协议在形式化规范下达到严格最优，既可安全实施又具备可扩展性，驳斥了非验证节点不可信的错误认知

Abstract: This paper presents a complete formal specification, protocol description,
and mathematical proof structure for Simplified Payment Verification (SPV) as
originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark
contrast to the misrepresentations proliferated by popular implementations, we
show that SPV is not only secure under bounded adversarial assumptions but
strictly optimal for digital cash systems requiring scalable and verifiable
transaction inclusion. We reconstruct the SPV protocol from first principles,
grounding its verification model in symbolic automata, Merkle membership
relations, and chain-of-proof dominance predicates. Through rigorous
probabilistic and game-theoretic analysis, we derive the economic bounds within
which the protocol operates securely and verify its liveness and safety
properties under partial connectivity, hostile relay networks, and adversarial
propagation delay. Our specification further introduces low-bandwidth
optimisations such as adaptive polling and compressed header synchronisation
while preserving correctness. This document serves both as a blueprint for
secure SPV implementation and a rebuttal of common misconceptions surrounding
non-validating clients.

</details>
