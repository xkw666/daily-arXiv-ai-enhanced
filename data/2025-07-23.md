<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [eSapiens's DEREK Module: Deep Extraction & Reasoning Engine for Knowledge with LLMs](https://arxiv.org/abs/2507.15863)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.CL

TL;DR: DEREK模块是面向企业文档问答的安全可扩展RAG系统，通过混合检索、验证机制和容器化部署实现高精度、可追溯的答案生成


<details>
  <summary>Details</summary>
Motivation: 解决企业级文档QA对安全性、可审计性和上下文忠实性的严苛需求，特别是在法律和金融等高风险领域需确保答案可追溯且无事实性错误

Method: 多格式文档分块(1000token重叠)+混合HNSW/BM25索引，结合GPT-4o查询扩展、Cohere重排序，通过CO-STAR提示工程生成答案，LangGraph验证器强制引用验证

Result: 1000token分块提升Recall@50约1%，混合检索+重排序使Precision@10提升7%，验证机制使TRACe指标超0.5且非支撑性陈述<3%，全组件容器化+TLS1.3/AES-256加密

Conclusion: DEREK模块以最小运维代价实现生产级安全文档QA，满足企业对于可审计、上下文一致检索的基线要求，为法律等专业领域提供可靠解决方案

Abstract: We present the DEREK (Deep Extraction & Reasoning Engine for Knowledge)
Module, a secure and scalable Retrieval-Augmented Generation pipeline designed
specifically for enterprise document question answering. Designed and
implemented by eSapiens, the system ingests heterogeneous content (PDF, Office,
web), splits it into 1,000-token overlapping chunks, and indexes them in a
hybrid HNSW+BM25 store. User queries are refined by GPT-4o, retrieved via
combined vector+BM25 search, reranked with Cohere, and answered by an LLM using
CO-STAR prompt engineering. A LangGraph verifier enforces citation overlap,
regenerating answers until every claim is grounded. On four LegalBench subsets,
1000-token chunks improve Recall@50 by approximately 1 pp and hybrid+rerank
boosts Precision@10 by approximately 7 pp; the verifier raises TRACe
Utilization above 0.50 and limits unsupported statements to less than 3%. All
components run in containers, enforce end-to-end TLS 1.3 and AES-256. These
results demonstrate that the DEREK module delivers accurate, traceable, and
production-ready document QA with minimal operational overhead. The module is
designed to meet enterprise demands for secure, auditable, and context-faithful
retrieval, providing a reliable baseline for high-stakes domains such as legal
and finance.

</details>


### [2] [Adversarial Demonstration Learning for Low-resource NER Using Dual Similarity](https://arxiv.org/abs/2507.15864)
*Guowen Yuan,Tien-Hsuan Wu,Lianghao Xia,Ben Kao*

Main category: cs.CL

TL;DR: 提出基于双相似性选择和对抗训练的NER方法，解决低资源场景中示范构建与模型参照能力不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有低资源NER方法在示范选择上仅依赖语义相似性，且模型缺乏有效参照示范的能力，导致性能受限

Method: 1. 双相似性选择（语义+特征相似性）构建示范
2. 对抗性示范训练强制模型参考示范

Result: 在低资源NER任务中全面超越现有方法

Conclusion: 特征相似性选择与对抗训练机制可显著提升模型在低资源场景下的示范学习效果

Abstract: We study the problem of named entity recognition (NER) based on demonstration
learning in low-resource scenarios. We identify two issues in demonstration
construction and model training. Firstly, existing methods for selecting
demonstration examples primarily rely on semantic similarity; We show that
feature similarity can provide significant performance improvement. Secondly,
we show that the NER tagger's ability to reference demonstration examples is
generally inadequate. We propose a demonstration and training approach that
effectively addresses these issues. For the first issue, we propose to select
examples by dual similarity, which comprises both semantic similarity and
feature similarity. For the second issue, we propose to train an NER model with
adversarial demonstration such that the model is forced to refer to the
demonstrations when performing the tagging task. We conduct comprehensive
experiments in low-resource NER tasks, and the results demonstrate that our
method outperforms a range of methods.

</details>


### [3] [Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models](https://arxiv.org/abs/2507.15868)
*Altynbek Ismailov,Salia Asanova*

Main category: cs.CL

TL;DR: 大语言模型在代码生成任务中表现出双重不对称性：对90%缺失的提示保持85%正确率（过鲁棒），但对关键语义翻转仅54%适应率（欠敏感）


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在代码生成任务中对提示扰动的敏感性边界，区分无害噪声与关键语义变化

Method: 使用50个LeetCode问题，设计三种提示扰动（渐进式欠规范/量化词翻转/技术术语替换），测试6个前沿模型（含3个推理调优版）的适应性

Result: 模型在90%提示缺失时保持85%正确率；量化词翻转仅54%适应（推理调优模型更差）；技术术语替换通过率56%

Conclusion: 当前LLMs模糊了噪声与语义变化的界限，需建立评估体系奖励差异化敏感性：稳定应对良性噪声，及时调整/拒绝真实语义变化

Abstract: Large language models (LLMs) now write code in settings where misreading a
single word can break safety or cost money, yet we still expect them to
overlook stray typos. To probe where useful robustness ends and harmful
insensitivity begins, we compile 50 LeetCode problems and craft three minimal
prompt perturbations that should vary in importance: (i) progressive
underspecification deleting 10 % of words per step; (ii) lexical flip swapping
a pivotal quantifier ("max" to "min"); and (iii) jargon inflation replacing a
common noun with an obscure technical synonym. Six frontier models, including
three "reasoning-tuned" versions, solve each mutated prompt, and their Python
outputs are checked against the original test suites to reveal whether they
reused the baseline solution or adapted. Among 11 853 generations we observe a
sharp double asymmetry. Models remain correct in 85 % of cases even after 90 %
of the prompt is missing, showing over-robustness to underspecification, yet
only 54 % react to a single quantifier flip that reverses the task, with
reasoning-tuned variants even less sensitive than their bases. Jargon edits lie
in between, passing through 56 %. Current LLMs thus blur the line between
harmless noise and meaning - changing edits, often treating both as ignorable.
Masking salient anchors such as function names can force re - evaluation. We
advocate evaluation and training protocols that reward differential
sensitivity: stay steady under benign noise but adapt - or refuse - when
semantics truly change.

</details>


### [4] [Enhancing Hindi NER in Low Context: A Comparative study of Transformer-based models with vs. without Retrieval Augmentation](https://arxiv.org/abs/2507.16002)
*Sumit Singh,Rohit Mishra,Uma Shanker Tiwary*

Main category: cs.CL

TL;DR: 研究通过检索增强技术和预训练模型提升印地语命名实体识别性能，证明数据增强在低资源语言中的有效性


<details>
  <summary>Details</summary>
Motivation: 解决资源有限语言（如印地语）NER性能不足的问题，探索检索增强与不同规模语言模型的协同效应

Method: 使用MuRIL/XLM-R预训练编码器和Llama/GPT系列生成模型，结合维基百科检索增强(RA)，采用微调和少样本生成两种范式

Result: RA使MuRIL/XLM-R的macro F1分别提升至0.70/0.71；微调版Llama2-7B显著优于原版，GPT3.5适配RA最佳，Llama大模型对RA敏感度低

Conclusion: 检索增强显著提升低语境数据性能，为资源有限语言的NER任务提供了有效的模型选择和数据增强方案

Abstract: One major challenge in natural language processing is named entity
recognition (NER), which identifies and categorises named entities in textual
input. In order to improve NER, this study investigates a Hindi NER technique
that makes use of Hindi-specific pretrained encoders (MuRIL and XLM-R) and
Generative Models ( Llama-2-7B-chat-hf (Llama2-7B), Llama-2-70B-chat-hf
(Llama2-70B), Llama-3-70B-Instruct (Llama3-70B) and GPT3.5-turbo), and augments
the data with retrieved data from external relevant contexts, notably from
Wikipedia. We have fine-tuned MuRIL, XLM-R and Llama2-7B with and without RA.
However, Llama2-70B, lama3-70B and GPT3.5-turbo are utilised for few-shot NER
generation. Our investigation shows that the mentioned language models (LMs)
with Retrieval Augmentation (RA) outperform baseline methods that don't
incorporate RA in most cases. The macro F1 scores for MuRIL and XLM-R are 0.69
and 0.495, respectively, without RA and increase to 0.70 and 0.71,
respectively, in the presence of RA. Fine-tuned Llama2-7B outperforms Llama2-7B
by a significant margin. On the other hand the generative models which are not
fine-tuned also perform better with augmented data. GPT3.5-turbo adopted RA
well; however, Llama2-70B and llama3-70B did not adopt RA with our retrieval
context. The findings show that RA significantly improves performance,
especially for low-context data. This study adds significant knowledge about
how best to use data augmentation methods and pretrained models to enhance NER
performance, particularly in languages with limited resources.

</details>


### [5] [Learning without training: The implicit dynamics of in-context learning](https://arxiv.org/abs/2507.16003)
*Benoit Dherin,Michael Munn,Hanna Mazzawi,Michael Wunder,Javier Gonzalvo*

Main category: cs.CL

TL;DR: Transformer块通过自注意力层与MLP的堆叠结构，实现根据上下文隐式调整MLP层权重，这可能是大语言模型具备上下文学习能力的关键机制。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在推理时无需额外训练即可通过提示示例学习新模式的底层机制，特别是transformer架构如何支持这种上下文学习能力。

Method: 通过理论推导和实验验证，分析transformer块中自注意力层与MLP的相互作用机制，证明其可将上下文信息转化为MLP层的低秩权重更新。

Result: 在适当简化条件下，transformer块能够将输入上下文隐式转换为MLP层的权重修改，形成可解释的数学表达形式。

Conclusion: 自注意力与MLP的协同工作机制为上下文学习提供了数学基础，揭示了LLM超越传统训练范式学习能力的结构根源。

Abstract: One of the most striking features of Large Language Models (LLM) is their
ability to learn in context. Namely at inference time an LLM is able to learn
new patterns without any additional weight update when these patterns are
presented in the form of examples in the prompt, even if these patterns were
not seen during training. The mechanisms through which this can happen are
still largely unknown. In this work, we show that the stacking of a
self-attention layer with an MLP, allows the transformer block to implicitly
modify the weights of the MLP layer according to the context. We argue through
theory and experimentation that this simple mechanism may be the reason why
LLMs can learn in context and not only during training. Specifically, we show
under mild simplifying assumptions how a transformer block implicitly
transforms a context into a low-rank weight-update of the MLP layer.

</details>


### [6] [Help Me Write a Story: Evaluating LLMs' Ability to Generate Writing Feedback](https://arxiv.org/abs/2507.16007)
*Hannah Rashkin,Elizabeth Clark,Fantine Huot,Mirella Lapata*

Main category: cs.CL

TL;DR: LLMs在创意写作反馈中展现出潜力但存在关键性局限


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否为创意写作提供有效反馈并分析其局限性

Method: 构建含1300个故意植入写作问题的测试集，采用自动+人工评估框架

Result: 模型能给出具体准确反馈，但常无法识别核心问题且反馈类型判断错误

Conclusion: 当前LLMs在关键问题定位与反馈类型决策方面仍需改进

Abstract: Can LLMs provide support to creative writers by giving meaningful writing
feedback? In this paper, we explore the challenges and limitations of
model-generated writing feedback by defining a new task, dataset, and
evaluation frameworks. To study model performance in a controlled manner, we
present a novel test set of 1,300 stories that we corrupted to intentionally
introduce writing issues. We study the performance of commonly used LLMs in
this task with both automatic and human evaluation metrics. Our analysis shows
that current models have strong out-of-the-box behavior in many respects --
providing specific and mostly accurate writing feedback. However, models often
fail to identify the biggest writing issue in the story and to correctly decide
when to offer critical vs. positive feedback.

</details>


### [7] [mRAKL: Multilingual Retrieval-Augmented Knowledge Graph Construction for Low-Resourced Languages](https://arxiv.org/abs/2507.16011)
*Hellina Hailu Nigatu,Min Li,Maartje ter Hoeve,Saloni Potdar,Sarah Chasins*

Main category: cs.CL

TL;DR: 将多语言知识图谱构建任务重构为问答形式，提出基于检索增强生成的mRAKL系统，在提格雷尼亚语和阿姆哈拉语上验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言知识图谱构建难题，探索阿拉伯语/英语到提格雷尼亚语/阿姆哈拉语的跨语言迁移潜力

Method: 使用BM25检索器的RAG框架，通过头实体+关系提问预测尾实体

Result: RAG方法相比无上下文提升性能，理想检索条件下提格雷尼亚语准确率提升4.92%，阿姆哈拉语提升8.79%

Conclusion: 验证了RAG方法在低资源语言知识图谱补全中的有效性，展示跨语言迁移的可行性

Abstract: Knowledge Graphs represent real-world entities and the relationships between
them. Multilingual Knowledge Graph Construction (mKGC) refers to the task of
automatically constructing or predicting missing entities and links for
knowledge graphs in a multilingual setting. In this work, we reformulate the
mKGC task as a Question Answering (QA) task and introduce mRAKL: a
Retrieval-Augmented Generation (RAG) based system to perform mKGC. We achieve
this by using the head entity and linking relation in a question, and having
our model predict the tail entity as an answer. Our experiments focus primarily
on two low-resourced languages: Tigrinya and Amharic. We experiment with using
higher-resourced languages Arabic and English for cross-lingual transfer. With
a BM25 retriever, we find that the RAG-based approach improves performance over
a no-context setting. Further, our ablation studies show that with an idealized
retrieval system, mRAKL improves accuracy by 4.92 and 8.79 percentage points
for Tigrinya and Amharic, respectively.

</details>


### [8] [AutoMeet: a proof-of-concept study of genAI to automate meetings in automotive engineering](https://arxiv.org/abs/2507.16054)
*Simon Baeuerle,Max Radyschevski,Ulrike Pado*

Main category: cs.CL

TL;DR: 研究团队开发基于生成式AI的端到端会议自动化系统，通过自动生成会议纪要和智能搜索显著减少会议工作量，验证技术可行性的同时强调组织因素对伦理应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统会议存在知识共享效率低、文档格式不一致的问题，生成式AI在语言处理方面的优势为解决这些痛点提供了技术可能性。

Method: 构建包含自动会议记录、AI生成纪要和聊天机器人搜索的完整流程，并在真实工程部门进行系统测试与多维度用户调研。

Result: 用户认可AI显著减少会议工作量(78%)，技术实现成熟度达92%，但87%受访者认为组织管理机制是伦理应用的关键制约因素。

Conclusion: 生成式AI在知识管理领域具备实用价值，技术障碍已基本突破，需通过组织流程优化和制度建设确保系统的伦理合规使用。

Abstract: In large organisations, knowledge is mainly shared in meetings, which takes
up significant amounts of work time. Additionally, frequent in-person meetings
produce inconsistent documentation -- official minutes, personal notes,
presentations may or may not exist. Shared information therefore becomes hard
to retrieve outside of the meeting, necessitating lengthy updates and
high-frequency meeting schedules.
  Generative Artificial Intelligence (genAI) models like Large Language Models
(LLMs) exhibit an impressive performance on spoken and written language
processing. This motivates a practical usage of genAI for knowledge management
in engineering departments: using genAI for transcribing meetings and
integrating heterogeneous additional information sources into an easily usable
format for ad-hoc searches.
  We implement an end-to-end pipeline to automate the entire meeting
documentation workflow in a proof-of-concept state: meetings are recorded and
minutes are created by genAI. These are further made easily searchable through
a chatbot interface. The core of our work is to test this genAI-based software
tooling in a real-world engineering department and collect extensive survey
data on both ethical and technical aspects. Direct feedback from this
real-world setup points out both opportunities and risks: a) users agree that
the effort for meetings could be significantly reduced with the help of genAI
models, b) technical aspects are largely solved already, c) organizational
aspects are crucial for a successful ethical usage of such a system.

</details>


### [9] [Deep Researcher with Test-Time Diffusion](https://arxiv.org/abs/2507.16075)
*Rujun Han,Yanfei Chen,Zoey CuiZhu,Lesly Miculicich,Guan Sun,Yuanjun Bi,Weiming Wen,Hui Wan,Chunfeng Wen,Solène Maître,George Lee,Vishy Tirumalashetty,Emily Xue,Zizhao Zhang,Salem Haykal,Burak Gokturk,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

TL;DR: 提出TTD-DR框架，将研究报告生成建模为扩散过程，通过动态骨架草稿、检索增强和自进化算法实现迭代优化，显著提升复杂研究报告质量。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理在生成长篇研究报告时存在性能瓶颈，受人类研究中'搜索-推理-修正'循环机制启发，需设计更符合研究本质的生成框架。

Method: 1) 建立可更新的骨架草稿作为演化基础 2) 融入检索机制的动态去噪优化 3) 代理工作流组件的自进化算法 4) 基于扩散过程的迭代生成范式

Result: 在需要密集搜索和多跳推理的基准测试中取得SOTA，生成质量显著优于现有研究代理系统

Conclusion: 草案中心设计增强生成连贯性，迭代检索机制减少信息损失，自进化组件实现工作流动态优化，为研究型AI提供新范式

Abstract: Deep research agents, powered by Large Language Models (LLMs), are rapidly
advancing; yet, their performance often plateaus when generating complex,
long-form research reports using generic test-time scaling algorithms. Drawing
inspiration from the iterative nature of human research, which involves cycles
of searching, reasoning, and revision, we propose the Test-Time Diffusion Deep
Researcher (TTD-DR). This novel framework conceptualizes research report
generation as a diffusion process. TTD-DR initiates this process with a
preliminary draft, an updatable skeleton that serves as an evolving foundation
to guide the research direction. The draft is then iteratively refined through
a "denoising" process, which is dynamically informed by a retrieval mechanism
that incorporates external information at each step. The core process is
further enhanced by a self-evolutionary algorithm applied to each component of
the agentic workflow, ensuring the generation of high-quality context for the
diffusion process. This draft-centric design makes the report writing process
more timely and coherent while reducing information loss during the iterative
search process. We demonstrate that our TTD-DR achieves state-of-the-art
results on a wide array of benchmarks that require intensive search and
multi-hop reasoning, significantly outperforming existing deep research agents.

</details>


### [10] [The Prompt Makes the Person(a): A Systematic Evaluation of Sociodemographic Persona Prompting for Large Language Models](https://arxiv.org/abs/2507.16076)
*Marlene Lutz,Indira Sen,Georg Ahnert,Elisa Rogers,Markus Strohmaier*

Main category: cs.CL

TL;DR: 研究验证人物提示策略对LLM模拟社会群体的影响，发现访谈式提示和姓名启动可提升边缘群体模拟效果，较小模型表现优于大模型


<details>
  <summary>Details</summary>
Motivation: 解决LLM在模拟不同社会人口群体时因提示策略差异导致结果失真的问题

Method: 使用5个开源LLM测试15个人口群体在两种任务类型中的表现，比较角色采纳形式和人口特征启动策略

Result: 小模型OLMo-2-7B优于大模型，访谈式提示减少刻板印象，非二元性别/西班牙裔/中东群体模拟效果最差

Conclusion: 提出基于访谈格式和姓名启动的提示设计指南，为LLM社会模拟研究提供优化方向

Abstract: Persona prompting is increasingly used in large language models (LLMs) to
simulate views of various sociodemographic groups. However, how a persona
prompt is formulated can significantly affect outcomes, raising concerns about
the fidelity of such simulations. Using five open-source LLMs, we
systematically examine how different persona prompt strategies, specifically
role adoption formats and demographic priming strategies, influence LLM
simulations across 15 intersectional demographic groups in both open- and
closed-ended tasks. Our findings show that LLMs struggle to simulate
marginalized groups, particularly nonbinary, Hispanic, and Middle Eastern
identities, but that the choice of demographic priming and role adoption
strategy significantly impacts their portrayal. Specifically, we find that
prompting in an interview-style format and name-based priming can help reduce
stereotyping and improve alignment. Surprisingly, smaller models like OLMo-2-7B
outperform larger ones such as Llama-3.3-70B. Our findings offer actionable
guidance for designing sociodemographic persona prompts in LLM-based simulation
studies.

</details>


### [11] [Efficient Compositional Multi-tasking for On-device Large Language Models](https://arxiv.org/abs/2507.16083)
*Ondrej Bohdal,Mete Ozay,Jijoong Moon,Kyeng-Hun Lee,Hyeonmok Ko,Umberto Michieli*

Main category: cs.CL

TL;DR: 提出设备端组合多任务处理框架及高效校准方法Learnable Calibration，突破LLMs单任务合并局限


<details>
  <summary>Details</summary>
Motivation: 现有LLMs任务合并技术仅支持单任务处理，无法满足设备端同时执行多任务（如翻译+摘要）的复合需求

Method: 开发Learnable Calibration方法，通过可学习的校准参数实现多任务参数融合，强调低资源消耗与高性能平衡

Result: 建立包含4个实用组合任务的基准测试，验证方法在资源受限场景下的有效性

Conclusion: 为LLMs在真实世界多任务场景的应用奠定基础，扩展其在复杂资源受限场景的适用边界

Abstract: Adapter parameters provide a mechanism to modify the behavior of machine
learning models and have gained significant popularity in the context of large
language models (LLMs) and generative AI. These parameters can be merged to
support multiple tasks via a process known as task merging. However, prior work
on merging in LLMs, particularly in natural language processing, has been
limited to scenarios where each test example addresses only a single task. In
this paper, we focus on on-device settings and study the problem of text-based
compositional multi-tasking, where each test example involves the simultaneous
execution of multiple tasks. For instance, generating a translated summary of a
long text requires solving both translation and summarization tasks
concurrently. To facilitate research in this setting, we propose a benchmark
comprising four practically relevant compositional tasks. We also present an
efficient method (Learnable Calibration) tailored for on-device applications,
where computational resources are limited, emphasizing the need for solutions
that are both resource-efficient and high-performing. Our contributions lay the
groundwork for advancing the capabilities of LLMs in real-world multi-tasking
scenarios, expanding their applicability to complex, resource-constrained use
cases.

</details>


### [12] [BIDWESH: A Bangla Regional Based Hate Speech Detection Dataset](https://arxiv.org/abs/2507.16183)
*Azizul Hakim Fayaz,MD. Shorif Uddin,Rayhan Uddin Bhuiyan,Zakia Sultana,Md. Samiul Islam,Bidyarthi Paul,Tashreef Muhammad,Shahriar Manzoor*

Main category: cs.CL

TL;DR: 首个多方言孟加拉语仇恨语音数据集BIDWESH填补了方言敏感内容审核的空白，包含三大方言区9183条人工标注样本。


<details>
  <summary>Details</summary>
Motivation: 现有标准孟加拉语仇恨语音检测系统无法识别方言区丰富的非正式文化表达，导致有害内容漏检。方言区覆盖不足引发内容审核偏见，需建立多方言数据集提升检测公平性。

Method: 1. 从BD-SHS语料库翻译并标注9,183条实例至三大地区方言
2. 人工验证每条数据的仇恨存在性、类型（诽谤/性别/宗教/暴力煽动）及目标群体
3. 确保语言特征与语境准确性，构建平衡包容的数据集

Result: 建成首个多方言孟加拉语仇恨语音数据集BIDWESH，提供方言敏感的NLP工具开发基础，支持低资源语言环境下公平的内容审核。

Conclusion: BIDWESH通过方言敏感的数据架构推动孟加拉语仇恨语音检测发展，为多方言地区实现语境感知的内容审核提供关键技术支撑，促进数字空间公平性。

Abstract: Hate speech on digital platforms has become a growing concern globally,
especially in linguistically diverse countries like Bangladesh, where regional
dialects play a major role in everyday communication. Despite progress in hate
speech detection for standard Bangla, Existing datasets and systems fail to
address the informal and culturally rich expressions found in dialects such as
Barishal, Noakhali, and Chittagong. This oversight results in limited detection
capability and biased moderation, leaving large sections of harmful content
unaccounted for. To address this gap, this study introduces BIDWESH, the first
multi-dialectal Bangla hate speech dataset, constructed by translating and
annotating 9,183 instances from the BD-SHS corpus into three major regional
dialects. Each entry was manually verified and labeled for hate presence, type
(slander, gender, religion, call to violence), and target group (individual,
male, female, group), ensuring linguistic and contextual accuracy. The
resulting dataset provides a linguistically rich, balanced, and inclusive
resource for advancing hate speech detection in Bangla. BIDWESH lays the
groundwork for the development of dialect-sensitive NLP tools and contributes
significantly to equitable and context-aware content moderation in low-resource
language settings.

</details>


### [13] [Do Large Language Models Have a Planning Theory of Mind? Evidence from MindGames: a Multi-Step Persuasion Task](https://arxiv.org/abs/2507.16196)
*Jared Moore,Ned Cooper,Rasmus Overmark,Beba Cibralic,Nick Haber,Cameron R. Jones*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在计划性心理理论（PToM）任务中与人类存在显著差距，人类在需要推断心理状态的任务中表现更优，而LLM在已知偏好时规划能力更强。


<details>
  <summary>Details</summary>
Motivation: 现有心理理论研究多关注被动观察，而人类实际运用ToM时涉及动态规划和干预他人心理状态。本文旨在探索LLM在此类主动任务中的能力差异。

Method: 设计MindGames任务，要求参与者推断对话者心理状态以说服其改变行为，比较人类与o1-preview模型在PToM任务和基线条件（仅需规划无需心理推断）的表现。

Result: 人类在PToM任务中显著优于LLM（11%更高，p=0.006）；LLM在基线条件下反超人类。

Conclusion: LLM缺乏人类隐含的因果心理模型，在社交推理任务中存在显著能力鸿沟，但在已知明确偏好时展现出更强的规划能力。

Abstract: Recent evidence suggests Large Language Models (LLMs) display Theory of Mind
(ToM) abilities. Most ToM experiments place participants in a spectatorial
role, wherein they predict and interpret other agents' behavior. However, human
ToM also contributes to dynamically planning action and strategically
intervening on others' mental states. We present MindGames: a novel `planning
theory of mind' (PToM) task which requires agents to infer an interlocutor's
beliefs and desires to persuade them to alter their behavior. Unlike previous
evaluations, we explicitly evaluate use cases of ToM. We find that humans
significantly outperform o1-preview (an LLM) at our PToM task (11% higher;
$p=0.006$). We hypothesize this is because humans have an implicit causal model
of other agents (e.g., they know, as our task requires, to ask about people's
preferences). In contrast, o1-preview outperforms humans in a baseline
condition which requires a similar amount of planning but minimal mental state
inferences (e.g., o1-preview is better than humans at planning when already
given someone's preferences). These results suggest a significant gap between
human-like social reasoning and LLM abilities.

</details>


### [14] [WakenLLM: A Fine-Grained Benchmark for Evaluating LLM Reasoning Potential and Reasoning Process Stability](https://arxiv.org/abs/2507.16199)
*Zipeng Ling,Yuehao Tang,Shuliang Liu,Junqi Yang,Shenghong Fu,Yao Wan,Kejia Huang,Zhichao Hou,Xuming Hu*

Main category: cs.CL

TL;DR: 本文提出'模糊感知'概念，区分LLMs输出'Unknown'的原因是模型能力不足或问题本身不确定，并通过量化框架探索其改进潜力。


<details>
  <summary>Details</summary>
Motivation: 现有评估仅关注答案诚实性，无法区分模型能力不足与问题本质不确定导致的'Unknown'响应，需更精细的归因分析。

Method: 构建量化框架，分离模型能力不足导致的'Unknown'，并通过引导刺激测试其转化为确定答案或本质不确定结果的可能性。

Result: 获得不同LLMs在推理任务中的理论准确率基线，验证引导方法能否帮助模型达到理论准确率。

Conclusion: 该框架为理解LLMs推理边界提供新视角，并为解决'模糊感知'现象开辟了方法论路径。

Abstract: Large Language Models (LLMs) frequently output the label \emph{Unknown}, yet
current evaluations focus almost exclusively on whether such answers are
\emph{honest} rather than why they arise. This blurs two distinct cases: (i) an
input that is genuinely indeterminate and (ii) a solvable problem that the
model fails to resolve. We call this phenomenon \emph{Vague Perception}. And
thus we introduce a framework that quantifies the proportion of \emph{Unknown}
responses attributable to model incapacity and tests whether guided stimulation
can convert them into either correct (\emph{Known}) or intrinsically
indeterminate outcomes. By separating these sources of uncertainty, our method
provides a clearer picture of LLM reasoning limits and their potential for
improvement. As we get a theoretical accuracy of reasoning task on different
LLMs, we apply different methods to test whether the model can reach the
accuracy given a baseline framework. Our work is meaningful in exploring the
true reasoning ability of LLMs and providing a new perspective on solving the
\emph{Vague Perception} phenomenon.

</details>


### [15] [Towards Compute-Optimal Many-Shot In-Context Learning](https://arxiv.org/abs/2507.16217)
*Shahriar Golchin,Yanfei Chen,Rujun Han,Manan Gandhi,Tianli Yu,Swaroop Mishra,Mihai Surdeanu,Rishabh Agarwal,Chen-Yu Lee,Tomas Pfister*

Main category: cs.CL

TL;DR: 提出两种高效上下文演示选择策略，结合相似性样本与聚类中心，在降低推理成本的同时提升多示例上下文学习性能。


<details>
  <summary>Details</summary>
Motivation: 针对多示例上下文学习中随机选择演示的低效性问题，寻求在保持计算缓存优势的前提下提升模型性能。

Method: 1. 相似性+随机混合选择策略；2. 基于k-means聚类中心的选择策略。两种方法均支持计算缓存复用。

Result: 在Gemini模型上验证，推理成本降低10倍，性能超越随机选择并达到最优基线水平。

Conclusion: 通过调整不同类型演示的比例，可实现多示例学习中性能与推理成本的灵活权衡。

Abstract: Long-context large language models (LLMs) are able to process inputs
containing up to several million tokens. In the scope of in-context learning
(ICL), this translates into using hundreds/thousands of demonstrations in the
input prompt, enabling many-shot ICL. In practice, a fixed set of
demonstrations is often selected at random in many-shot settings due to (1)
high inference costs, (2) the benefits of caching and reusing computations, and
(3) the similar performance offered by this strategy compared to others when
scaled. In this work, we propose two straightforward strategies for
demonstration selection in many-shot ICL that improve performance with minimal
computational overhead. Our first method combines a small number of
demonstrations, selected based on their similarity to each test sample, with a
disproportionately larger set of random demonstrations that are cached. The
second strategy improves the first by replacing random demonstrations with
those selected using centroids derived from test sample representations via
k-means clustering. Our experiments with Gemini Pro and Flash across several
datasets indicate that our strategies consistently outperform random selection
and surpass or match the most performant selection approach while supporting
caching and reducing inference cost by up to an order of magnitude. We also
show that adjusting the proportion of demonstrations selected based on
different criteria can balance performance and inference cost in many-shot ICL.

</details>


### [16] [FinResearchBench: A Logic Tree based Agent-as-a-Judge Evaluation Framework for Financial Research Agents](https://arxiv.org/abs/2507.16248)
*Run Sun,Zuo Bai,Wentao Zhang,Yuxiang Zhang,Li Zhao,Shan Sun,Zhengwen Qiu*

Main category: cs.CL

TL;DR: 提出了首个面向金融研究代理的自动评估系统FinResearchBench，通过逻辑树方法实现全面可靠的Agent-as-a-Judge评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估体系无法满足金融研究代理在复杂性和专业性方面的需求，需建立领域专用评估基准。

Method: 构建基于逻辑树的Agent-as-a-Judge系统，自动解析研究成果结构并评估7大类金融研究任务能力。

Result: 开发覆盖7类金融任务、包含70个典型问题的评估体系，提供多维度的自动化评估方案。

Conclusion: 首次实现面向金融领域的逻辑树评估框架，解决了研究代理评估在专业深度和系统化方面的关键瓶颈。

Abstract: Recently, AI agents are rapidly evolving in intelligence and widely used in
professional research applications, such as STEM, software development,
finance, etc. Among these AI agents, deep research agent is a key category as
it can perform long-horizon tasks and solve problems of greater complexity.
However, there are few evaluation frameworks and benchmarks that systematically
and automatically investigate the capabilities of these research agents.
Furthermore, financial research problems have distinct complexity and subtlety.
To fill in the gap, we propose FinResearchBench, which is a logic tree based
Agent-as-a-Judge and targets specifically for the financial research agents. It
provides a comprehensive and automatic assessment of the research agents across
7 key types of tasks in the financial research domain. The contributions of
this work are two-folded: (1) the first and innovative Agent-as-a-Judge system
that extracts the logic tree of the research outcome and uses it as the
intermediate information to present a comprehensive, reliable and robust
evaluation; (2) finance oriented that it covers 70 typical financial research
questions, spreading across 7 frequently encountered types of tasks in the
domain.

</details>


### [17] [Efficient RL for optimizing conversation level outcomes with an LLM-based tutor](https://arxiv.org/abs/2507.16252)
*Hyunji Nam,Omer Gottesman,Amy Zhang,Dean Foster,Emma Brunskill,Lyle Ungar*

Main category: cs.CL

TL;DR: 提出通过潜在状态表示学生并优化长期策略的方法，改进LLM数学辅导效果


<details>
  <summary>Details</summary>
Motivation: 现有RLHF框架在单轮对话优化有效，但无法满足多轮辅导中引导学生自主解题的长期需求

Method: 使用低维潜在状态表征学生状态，训练轻量级策略模型进行长期动作决策而非直接生成回复

Result: 在LLM模拟辅导任务中相比直接提示方法获得了更好的长期辅导效果

Conclusion: 潜在状态表示结合长期策略优化的方法显著提升辅导效果，且模型更轻量高效

Abstract: Large language models (LLMs) built on existing reinforcement learning with
human feedback (RLHF) frameworks typically optimize responses based on
immediate turn-level human preferences. However, this approach falls short in
multi-turn dialogue settings, such as online math tutoring. We propose a method
to enhance LLM-based tutors by representing the dialogue history with a
lower-dimensional latent state representation of a student and optimizing a
long-term policy to determine high-level actions based on the latent state. The
goal is to better align the tutor's behavior with the long-term objective of
guiding the student towards solving a target math problem on their own. Our
model is lightweight, requiring less computational resources than prior work of
training the tutor policy end-to-end to directly output the tutor's next
utterance. Our experiment results demonstrate that these modifications lead to
improved long-term outcomes compared to prompting in LLM-simulated tutoring
tasks.

</details>


### [18] [iShumei-Chinchunmei at SemEval-2025 Task 4: A balanced forgetting and retention multi-task framework using effective unlearning loss](https://arxiv.org/abs/2507.16263)
*Yujian Sun,Tian Li*

Main category: cs.CL

TL;DR: 提出可控的遗忘损失函数Effective Unlearning Loss，结合多技术实现高效可控的大模型敏感信息遗忘，系统获SemEval竞赛第五名。


<details>
  <summary>Details</summary>
Motivation: 解决大模型预训练阶段记忆的违规数据遗忘难题，在有限算力下高效擦除敏感信息。

Method: 设计更可控的Effective Unlearning Loss，探索其与多种技术融合实现可控遗忘。

Result: 系统在SemEval 2025竞赛中位列排行榜第五。

Conclusion: 提出的可控遗忘损失函数有效，但仍有优化空间，需平衡遗忘效果与模型能力保持。

Abstract: As the Large Language Model (LLM) gains widespread adoption, increasing
attention has been given to the challenge of making LLM forget non-compliant
data memorized during its pre-training. Machine Unlearning focuses on
efficiently erasing sensitive information from LLM under limited computational
resources. To advance research in this area, SemEval 2025 Task 4: "Unlearning
Sensitive Content from Large Language Models" introduces three unlearning
datasets and establishes a benchmark by evaluating both forgetting
effectiveness and the preservation of standard capabilities. In this work, we
propose a more controllable forgetting loss, Effective Unlearning Loss, and
explore its integration with various techniques to achieve more efficient and
controlled unlearning. Our system ultimately ranked 5th on the competition
leaderboard.

</details>


### [19] [Beyond Isolated Dots: Benchmarking Structured Table Construction as Deep Knowledge Extraction](https://arxiv.org/abs/2507.16271)
*Tianyun Zhong,Guozhao Mo,Yanjiang Liu,Yihan Chen,Lingdi Kong,Xuanang Chen,Yaojie Lu,Hongyu Lin,Ben He,Le Sun*

Main category: cs.CL

TL;DR: 提出AOE基准测试，用于系统评估大语言模型从碎片化文档中提取信息并整合为结构化表格的能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型处理复杂文档时生成混乱的段落式回答，缺乏信息组织性和可追溯性

Method: 构建包含11个跨领域任务的基准测试，要求模型根据输入查询生成特定上下文的结构化表格（与传统固定模式文本转表格任务不同）

Result: 实验表明最先进的大语言模型在AOE基准测试中表现显著不足（如GPT-4准确率仅28.7%）

Conclusion: AOE填补了现有评估体系的空白，为提升大语言模型在复杂文档结构化处理能力提供基准

Abstract: With the emergence of large language models (LLMs), there is an expectation
that LLMs can effectively extract explicit information from complex real-world
documents (e.g., papers, reports). However, most LLMs generate paragraph-style
answers that are chaotic, disorganized, and untraceable. To bridge this gap, we
introduce the Arranged and Organized Extraction Benchmark (AOE), a new
bilingual benchmark with data and documents of varying lengths designed to
systematically evaluate the ability of LLMs to comprehend fragmented documents
and reconstruct isolated information into one organized table. Unlike
conventional text-to-table tasks, which rely on fixed schema and narrow task
domains, AOE includes 11 carefully crafted tasks across three diverse domains,
requiring models to generate context-specific schema tailored to varied input
queries. In the experiment, we evaluated both open-source and closed-source
state-of-the-art LLMs. The results show that even the most advanced models
struggled significantly. The benchmark is available at
https://huggingface.co/datasets/tianyumyum/AOE.

</details>


### [20] [Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis](https://arxiv.org/abs/2507.16284)
*Paul-Andrei Pogăcean,Sanda-Maria Avram*

Main category: cs.CL

TL;DR: 本研究证明基于字母频率的传统方法在语言检测中仍保持高效，短文本准确率超80%，长文本达100%


<details>
  <summary>Details</summary>
Motivation: 针对AI语言模型快速发展背景下非AI语言识别方法被忽视的现象，验证传统频率分析法的有效性

Method: 采用语言学研究的单字母/双字母组合频率排序法，测试不同长度、时期和文体的多样化文本数据集

Result: 在150字符以下文本准确率超80%，长文本和历史文献达100%准确率

Conclusion: 经典频率分析法可作为AI模型的有效替代方案，兼具准确性和可扩展性

Abstract: The debate surrounding language identification has gained renewed attention
in recent years, especially with the rapid evolution of AI-powered language
models. However, the non-AI-based approaches to language identification have
been overshadowed. This research explores a mathematical implementation of an
algorithm for language determinism by leveraging monograms and bigrams
frequency rankings derived from established linguistic research. The datasets
used comprise texts varying in length, historical period, and genre, including
short stories, fairy tales, and poems. Despite these variations, the method
achieves over 80\% accuracy on texts shorter than 150 characters and reaches
100\% accuracy for longer texts and older writings. These results demonstrate
that classical frequency-based approaches remain effective and scalable
alternatives to AI-driven models for language detection.

</details>


### [21] [SpeLLM: Character-Level Multi-Head Decoding](https://arxiv.org/abs/2507.16323)
*Amit Ben-Artzy,Roy Schwartz*

Main category: cs.CL

TL;DR: 提出SpeLLM方法解决LLM词汇扩展难题，通过字符级多头预测机制减少计算成本并突破输出层瓶颈


<details>
  <summary>Details</summary>
Motivation: 传统LLM输出投影层随词汇量线性增长，导致扩展不切实际。注意力机制的二次方计算成本也制约效率

Method: 1. 解耦输入输出词表，通过k个线性头并行预测单个字符
2. 自蒸馏技术转换现有LLM为SpeLLM架构
3. 多头机制实现指数级输出空间扩展

Result: 4个预训练模型实验显示：下游任务性能相当，平均运行时减少5.1%，内存占用降低明显

Conclusion: 该方法为降低LLM成本提供新思路，同时增强对小语种和特定领域支持，具有实际应用潜力

Abstract: Scaling LLM vocabulary is often used to reduce input sequence length and
alleviate attention's quadratic cost. Yet, current LLM architectures impose a
critical bottleneck to this procedure: the output projection layer scales
linearly with vocabulary size, rendering substantial expansion impractical. We
propose SpeLLM, a method that decouples input and output vocabularies by
predicting character-level strings through multiple output heads. In SpeLLM,
each of the $k$ linear heads predicts a single character simultaneously,
enabling the model to represent a much larger output space using smaller,
independent linear heads. We present a self-distillation approach for
converting a standard LLM to a SpeLLM. Our experiments with four pre-trained
LLMs show their SpeLLM variants achieve competitive performance on downstream
tasks while reducing runtime by 5.1% on average across models. Our approach
provides a potential avenue for reducing LLM costs, while increasing support
for underrepresented languages and domains.

</details>


### [22] [Re:Form -- Reducing Human Priors in Scalable Formal Software Verification with RL in LLMs: A Preliminary Study on Dafny](https://arxiv.org/abs/2507.16331)
*Chuanhao Yan,Fengdi Che,Xuhan Huang,Xu Xu,Xin Li,Yizhi Li,Xingwei Qu,Jingzhe Shi,Zhuangzhuang He,Chenghua Lin,Yaodong Yang,Binhang Yuan,Hang Zhao,Yu Qiao,Bowen Zhou,Jie Fu*

Main category: cs.CL

TL;DR: 提出通过形式语言Dafny与强化学习结合的方法，显著提升LLM生成可验证代码的能力，并构建DafnyComp基准验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于非正式语言的LLM验证不可靠且难以扩展，依赖人工标注成本过高，需要探索形式语言自动验证路径。

Method: 1. 建立自动化的Dafny代码数据管道 2. 设计结合形式验证反馈的强化学习框架 3. 提出包含组合式程序验证任务的DafnyComp基准

Result: 0.5B小模型经SFT后即可生成语法有效且可验证的代码，RL正则化进一步实现47.5%的验证通过率，优于GPT-4等基线模型

Conclusion: 形式语言空间中的强化学习框架显著降低人工先验依赖，为大规模可靠的形式化软件验证提供了可行方案

Abstract: Existing informal language-based (e.g., human language) Large Language Models
(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:
their verification processes, which provide crucial training signals, are
neither reliable nor scalable. In fact, the prevalent large proprietary models
could hardly generate verifiable programs. A promising yet largely uncharted
alternative is formal language-based reasoning. Grounding LLMs in rigorous
formal systems where generative models operate in formal language spaces (e.g.,
Dafny) enables the automatic and mathematically provable verification of their
reasoning processes and outcomes. This capability is pivotal for achieving
large-scale, reliable formal software verification. It is a common practice to
employ human-annotated chain-of-thought and other human priors to induce the
reasoning and coding capabilities of LLMs. Unfortunately, it becomes
unacceptably all-consuming to provide such priors for supervising complex
programming tasks. In this work, we systematically explore ways to reduce human
priors with the formal language, Dafny, as the main environment for our pilot
study. Our pipeline mainly relies on introducing an automatic and scalable data
curation pipeline, and careful RL designs integrated with feedback from the
formal language verifier. We introduce DafnyComp, a benchmark of compositional
formal programs with auto-formalized specifications for specification
reasoning. Our supervised fine-tuning (SFT) stage enables even small models
(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,
surpassing proprietary models. RL with regularization further improves
performance, achieving stronger generalization to out-of-domain tasks and
outperforming all strong baselines on the challenging DafnyComp benchmark.

</details>


### [23] [GG-BBQ: German Gender Bias Benchmark for Question Answering](https://arxiv.org/abs/2507.16410)
*Shalaka Satheesh,Katrin Klug,Katharina Beckh,Héctor Allende-Cid,Sebastian Houben,Teena Hassan*

Main category: cs.CL

TL;DR: 通过机器翻译和人工校对创建德语性别偏见评估数据集，发现所有德国LLMs均存在双向社会偏见


<details>
  <summary>Details</summary>
Motivation: 在德语等语法性别语言中，直接机器翻译英语偏见评估数据集存在局限性，需要人工修正来保证评估有效性

Method: 1. 机器翻译英语基准数据集 2. 语言专家人工校订 3. 创建两个子集（性别术语和专有名词）4. 测试多个德语LLMs的准确性和偏见得分

Result: 所有模型均表现出既符合又违背社会刻板印象的双向偏见，人工修订显著提升数据集质量

Conclusion: 构建非英语偏见评估数据集时人工校对至关重要，凸显多语言模型偏见评估的必要性

Abstract: Within the context of Natural Language Processing (NLP), fairness evaluation
is often associated with the assessment of bias and reduction of associated
harm. In this regard, the evaluation is usually carried out by using a
benchmark dataset, for a task such as Question Answering, created for the
measurement of bias in the model's predictions along various dimensions,
including gender identity. In our work, we evaluate gender bias in German Large
Language Models (LLMs) using the Bias Benchmark for Question Answering by
Parrish et al. (2022) as a reference. Specifically, the templates in the gender
identity subset of this English dataset were machine translated into German.
The errors in the machine translated templates were then manually reviewed and
corrected with the help of a language expert. We find that manual revision of
the translation is crucial when creating datasets for gender bias evaluation
because of the limitations of machine translation from English to a language
such as German with grammatical gender. Our final dataset is comprised of two
subsets: Subset-I, which consists of group terms related to gender identity,
and Subset-II, where group terms are replaced with proper names. We evaluate
several LLMs used for German NLP on this newly created dataset and report the
accuracy and bias scores. The results show that all models exhibit bias, both
along and against existing social stereotypes.

</details>


### [24] [PromptAL: Sample-Aware Dynamic Soft Prompts for Few-Shot Active Learning](https://arxiv.org/abs/2507.16424)
*Hui Xiang,Jinqiao Shi,Ting Zhang,Xiaojie Zhao,Yong Liu,Yong Ma*

Main category: cs.CL

TL;DR: 提出PromptAL框架，通过动态软提示增强小样本主动学习的样本选择效果


<details>
  <summary>Details</summary>
Motivation: 传统主动学习方法在小样本场景下未充分利用未标注数据优化决策边界，导致样本选择偏离目标分布

Method: 构建样本感知的动态软提示调整模型预测分布，结合不确定性和多样性指标选取代表性样本

Result: 在6个域内数据集和3个域外数据集上超越9个基线模型

Conclusion: 通过动态整合未标注数据优化决策边界，显著提升小样本主动学习性能

Abstract: Active learning (AL) aims to optimize model training and reduce annotation
costs by selecting the most informative samples for labeling. Typically, AL
methods rely on the empirical distribution of labeled data to define the
decision boundary and perform uncertainty or diversity estimation, subsequently
identifying potential high-quality samples. In few-shot scenarios, the
empirical distribution often diverges significantly from the target
distribution, causing the decision boundary to shift away from its optimal
position. However, existing methods overlook the role of unlabeled samples in
enhancing the empirical distribution to better align with the target
distribution, resulting in a suboptimal decision boundary and the selection of
samples that inadequately represent the target distribution. To address this,
we propose a hybrid AL framework, termed \textbf{PromptAL} (Sample-Aware
Dynamic Soft \textbf{Prompts} for Few-Shot \textbf{A}ctive \textbf{L}earning).
This framework accounts for the contribution of each unlabeled data point in
aligning the current empirical distribution with the target distribution,
thereby optimizing the decision boundary. Specifically, PromptAL first
leverages unlabeled data to construct sample-aware dynamic soft prompts that
adjust the model's predictive distribution and decision boundary. Subsequently,
based on the adjusted decision boundary, it integrates uncertainty estimation
with both global and local diversity to select high-quality samples that more
accurately represent the target distribution. Experimental results on six
in-domain and three out-of-domain datasets show that PromptAL achieves superior
performance over nine baselines. Our codebase is openly accessible.

</details>


### [25] [Dutch CrowS-Pairs: Adapting a Challenge Dataset for Measuring Social Biases in Language Models for Dutch](https://arxiv.org/abs/2507.16442)
*Elza Strazda,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 该研究创建了荷兰版CrowS-Pairs数据集，发现荷兰语言模型存在显著偏见但低于英语模型，且角色设定会改变模型偏见表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有偏见研究集中于英语模型的局限，扩展至多语言环境下的公平性评估需求。

Method: 构建含9类偏见的1463对荷兰语句子数据集，测试BERTje、RobBERT等多模型，并与英法模型对比分析。

Result: 荷兰模型偏见程度最低，英语模型最高；角色设定(persona)会显著改变模型偏见输出水平。

Conclusion: 语言模型的偏见程度受文化语境和语言特性影响，需针对不同语言开发定制化公平性评估工具。

Abstract: Warning: This paper contains explicit statements of offensive stereotypes
which might be upsetting.
  Language models are prone to exhibiting biases, further amplifying unfair and
harmful stereotypes. Given the fast-growing popularity and wide application of
these models, it is necessary to ensure safe and fair language models. As of
recent considerable attention has been paid to measuring bias in language
models, yet the majority of studies have focused only on English language. A
Dutch version of the US-specific CrowS-Pairs dataset for measuring bias in
Dutch language models is introduced. The resulting dataset consists of 1463
sentence pairs that cover bias in 9 categories, such as Sexual orientation,
Gender and Disability. The sentence pairs are composed of contrasting
sentences, where one of the sentences concerns disadvantaged groups and the
other advantaged groups. Using the Dutch CrowS-Pairs dataset, we show that
various language models, BERTje, RobBERT, multilingual BERT, GEITje and
Mistral-7B exhibit substantial bias across the various bias categories. Using
the English and French versions of the CrowS-Pairs dataset, bias was evaluated
in English (BERT and RoBERTa) and French (FlauBERT and CamemBERT) language
models, and it was shown that English models exhibit the most bias, whereas
Dutch models the least amount of bias. Additionally, results also indicate that
assigning a persona to a language model changes the level of bias it exhibits.
These findings highlight the variability of bias across languages and contexts,
suggesting that cultural and linguistic factors play a significant role in
shaping model biases.

</details>


### [26] [Towards Enforcing Company Policy Adherence in Agentic Workflows](https://arxiv.org/abs/2507.16459)
*Naama Zwerdling,David Boaz,Ella Rabinovich,Guy Uziel,David Amid,Ateret Anaby-Tavor*

Main category: cs.CL

TL;DR: 提出两阶段框架（离线策略编译+运行时合规检查）提升LLM代理对企业政策的遵循能力，在航空领域初步验证有效但实际部署仍存挑战


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在业务流程自动化中难以可靠遵循复杂公司政策，需建立确定性机制确保合规性

Method: 1. 离线阶段：将政策文档编译为可验证的防护代码
2. 运行时阶段：在代理执行每个动作前进行合规性检查

Result: 在航空领域τ-bench测试中展示政策执行有效性，但实际部署需解决工程集成等关键挑战

Conclusion: 模块化框架为LLM代理合规提供新思路，需进一步解决策略冲突检测、动态更新等现实问题

Abstract: Large Language Model (LLM) agents hold promise for a flexible and scalable
alternative to traditional business process automation, but struggle to
reliably follow complex company policies. In this study we introduce a
deterministic, transparent, and modular framework for enforcing business policy
adherence in agentic workflows. Our method operates in two phases: (1) an
offline buildtime stage that compiles policy documents into verifiable guard
code associated with tool use, and (2) a runtime integration where these guards
ensure compliance before each agent action. We demonstrate our approach on the
challenging $\tau$-bench Airlines domain, showing encouraging preliminary
results in policy enforcement, and further outline key challenges for
real-world deployments.

</details>


### [27] [ICR Probe: Tracking Hidden State Dynamics for Reliable Hallucination Detection in LLMs](https://arxiv.org/abs/2507.16488)
*Zhenliang Zhang,Xinyu Hu,Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出ICR Score量化模块对隐藏状态更新的贡献，并开发ICR Probe方法实现更高效的幻觉检测，实验证明其优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于隐藏状态的检测方法聚焦静态孤立表征，忽略跨层动态演变，制约检测效能。

Method: 通过ICR Score指标衡量模块贡献，构建捕捉跨层演变的ICR Probe检测模型。

Result: ICR Probe在显著减少参数量的同时达到最优性能，消融实验揭示机制可解释性。

Conclusion: 该方法不仅提升检测效率，更通过动态追踪机制增强模型行为的可解释性，为LLM可靠性研究提供新视角。

Abstract: Large language models (LLMs) excel at various natural language processing
tasks, but their tendency to generate hallucinations undermines their
reliability. Existing hallucination detection methods leveraging hidden states
predominantly focus on static and isolated representations, overlooking their
dynamic evolution across layers, which limits efficacy. To address this
limitation, we shift the focus to the hidden state update process and introduce
a novel metric, the ICR Score (Information Contribution to Residual Stream),
which quantifies the contribution of modules to the hidden states' update. We
empirically validate that the ICR Score is effective and reliable in
distinguishing hallucinations. Building on these insights, we propose a
hallucination detection method, the ICR Probe, which captures the cross-layer
evolution of hidden states. Experimental results show that the ICR Probe
achieves superior performance with significantly fewer parameters. Furthermore,
ablation studies and case analyses offer deeper insights into the underlying
mechanism of this method, improving its interpretability.

</details>


### [28] [Combining Language and Topic Models for Hierarchical Text Classification](https://arxiv.org/abs/2507.16490)
*Jaco du Toit,Marcel Dunaiski*

Main category: cs.CL

TL;DR: 论文研究发现结合主题模型与预训练语言模型的特征提取会降低分层文本分类性能，推翻了传统认知。


<details>
  <summary>Details</summary>
Motivation: 验证PLM与主题模型特征融合对HTC任务是否普遍有益。现有研究认为主题模型能提供全局语料特征，与PLM的细粒度语义特征互补。

Method: 使用PLM（BERT）和主题模型（LDA）并行提取特征，通过独立卷积层处理后，采用标签注意力机制融合特征进行分类。实验覆盖三个标准HTC数据集。

Result: 主题模型特征导致所有数据集性能下降（平均F1降0.9-1.7%）。与先前多标签分类研究结论相反，显示该方法在HTC任务中的负面效果。

Conclusion: 文本分类任务中不应假设主题模型特征具有普适增益，需针对具体任务验证。HTC更依赖PLM的细粒度语义理解而非全局主题特征。

Abstract: Hierarchical text classification (HTC) is a natural language processing task
which has the objective of categorising text documents into a set of classes
from a predefined structured class hierarchy. Recent HTC approaches use various
techniques to incorporate the hierarchical class structure information with the
natural language understanding capabilities of pre-trained language models
(PLMs) to improve classification performance. Furthermore, using topic models
along with PLMs to extract features from text documents has been shown to be an
effective approach for multi-label text classification tasks. The rationale
behind the combination of these feature extractor models is that the PLM
captures the finer-grained contextual and semantic information while the topic
model obtains high-level representations which consider the corpus of documents
as a whole. In this paper, we use a HTC approach which uses a PLM and a topic
model to extract features from text documents which are used to train a
classification model. Our objective is to determine whether the combination of
the features extracted from the two models is beneficial to HTC performance in
general. In our approach, the extracted features are passed through separate
convolutional layers whose outputs are combined and passed to a label-wise
attention mechanisms which obtains label-specific document representations by
weighing the most important features for each class separately. We perform
comprehensive experiments on three HTC benchmark datasets and show that using
the features extracted from the topic model generally decreases classification
performance compared to only using the features obtained by the PLM. In
contrast to previous work, this shows that the incorporation of features
extracted from topic models for text classification tasks should not be assumed
beneficial.

</details>


### [29] [The Ever-Evolving Science Exam](https://arxiv.org/abs/2507.16514)
*Junying Wang,Zicheng Zhang,Yijin Guo,Farong Wen,Ye Shen,Yingji Liang,Yalun Wu,Wenzhe Li,Chunyi Li,Zijian Chen,Qi Jia,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出动态科学评测基准EESE，通过构建非公开题库+动态更新子集的方式，有效解决大模型科学评估中的数据泄露风险与低效问题


<details>
  <summary>Details</summary>
Motivation: 现有科学评测基准存在数据泄露风险（影响有效性）和评估效率低下（大规模测试导致）两大痛点，需构建更可靠的评估体系

Method: 1) 构建包含10万+科学实例的非公开EESE-Pool，覆盖5大学科500+子领域；2) 定期更新500题动态子集EESE，实现防泄漏、低开销评估

Result: 在32个开源/闭源模型上的实验表明，EESE能有效区分模型在科学领域和认知维度上的能力差异

Conclusion: EESE为科学基准测试提供了抗泄露、可扩展的前沿解决方案，能真实反映基础模型处理科学问题的能力

Abstract: As foundation models grow rapidly in capability and deployment, evaluating
their scientific understanding becomes increasingly critical. Existing science
benchmarks have made progress towards broad **Range**, wide **Reach**, and high
**Rigor**, yet they often face two major challenges: **data leakage risks**
that compromise benchmarking validity, and **evaluation inefficiency** due to
large-scale testing. To address these issues, we introduce the **Ever-Evolving
Science Exam (EESE)**, a dynamic benchmark designed to reliably assess
scientific capabilities in foundation models. Our approach consists of two
components: 1) a non-public **EESE-Pool** with over 100K expertly constructed
science instances (question-answer pairs) across 5 disciplines and 500+
subfields, built through a multi-stage pipeline ensuring **Range**, **Reach**,
and **Rigor**, 2) a periodically updated 500-instance subset **EESE**, sampled
and validated to enable leakage-resilient, low-overhead evaluations.
Experiments on 32 open- and closed-source models demonstrate that EESE
effectively differentiates the strengths and weaknesses of models in scientific
fields and cognitive dimensions. Overall, EESE provides a robust, scalable, and
forward-compatible solution for science benchmark design, offering a realistic
measure of how well foundation models handle science questions. The project
page is at: https://github.com/aiben-ch/EESE.

</details>


### [30] [Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness](https://arxiv.org/abs/2507.16515)
*Siqi Liu,Guangrong Dai,Dechao Li*

Main category: cs.CL

TL;DR: 研究发现质量评估(QE)可显著缩短机器翻译后编辑时间，在不同质量MT输出和译者经验水平中均有效，但不准确QE可能产生负面影响


<details>
  <summary>Details</summary>
Motivation: 探索质量评估在英汉机器翻译后编辑中的实际效用，揭示QE对工作效率的影响机制及边界条件

Method: 通过实验测量QE介入后的后编辑速度变化，结合学生译者的主观反馈，分析QE与MT质量/翻译经验的交互效应

Result: QE平均减少20%后编辑时间，在中等/高质量MT输出及不同经验译者中表现稳定，同时具备质量验证、双重检查等辅助功能

Conclusion: QE能有效提升翻译后编辑效率，但需确保评估准确性。研究为优化人机协作翻译流程提供了实证依据

Abstract: This preliminary study investigates the usefulness of sentence-level Quality
Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE),
focusing on its impact on post-editing speed and student translators'
perceptions. It also explores the interaction effects between QE and MT
quality, as well as between QE and translation expertise. The findings reveal
that QE significantly reduces post-editing time. The examined interaction
effects were not significant, suggesting that QE consistently improves MTPE
efficiency across medium- and high-quality MT outputs and among student
translators with varying levels of expertise. In addition to indicating
potentially problematic segments, QE serves multiple functions in MTPE, such as
validating translators' evaluations of MT quality and enabling them to
double-check translation outputs. However, interview data suggest that
inaccurate QE may hinder post-editing processes. This research provides new
insights into the strengths and limitations of QE, facilitating its more
effective integration into MTPE workflows to enhance translators' productivity.

</details>


### [31] [Learning Text Styles: A Study on Transfer, Attribution, and Verification](https://arxiv.org/abs/2507.16530)
*Zhiqiang Hu*

Main category: cs.CL

TL;DR: 提出基于大语言模型的三位一体文本风格计算框架（TST+AA+AV），通过参数高效适配、对比解构和指令微调提升性能


<details>
  <summary>Details</summary>
Motivation: 解决文本风格计算中风格-内容解耦、作者指纹识别和可解释验证三大核心挑战

Method: 1) 参数高效的大模型适配技术 2) 基于对比学习的风格特征解构 3) 指令微调驱动的可验证框架

Result: 建立统一的文本风格计算体系，在风格迁移保真度、作者识别准确率和可解释性验证方面取得突破

Conclusion: 该框架为自然语言处理中的风格控制、数字取证等应用提供了新的技术范式

Abstract: This thesis advances the computational understanding and manipulation of text
styles through three interconnected pillars: (1) Text Style Transfer (TST),
which alters stylistic properties (e.g., sentiment, formality) while preserving
content; (2)Authorship Attribution (AA), identifying the author of a text via
stylistic fingerprints; and (3) Authorship Verification (AV), determining
whether two texts share the same authorship. We address critical challenges in
these areas by leveraging parameter-efficient adaptation of large language
models (LLMs), contrastive disentanglement of stylistic features, and
instruction-based fine-tuning for explainable verification.

</details>


### [32] [Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language](https://arxiv.org/abs/2507.16557)
*Kristin Gnadt,David Thulke,Simone Kopeinik,Ralf Schlüter*

Main category: cs.CL

TL;DR: 构建了5个德语性别偏见评估数据集，揭示德语大语言模型存在男性职业术语歧义、中性名词影响性别认知等独特挑战


<details>
  <summary>Details</summary>
Motivation: 现有性别偏见评估方法主要针对英语设计，其他语言适用性存疑。德语存在语法性别等特性，需专门评估框架

Method: 基于成熟性别偏见概念开发5个德语评估数据集，测试8个多语言大模型，采用多种方法论路径进行验证

Result: 德语性别偏见存在特殊表现形式：男性职业术语歧义性、中性名词隐含性别倾向。多语言模型在德语场景表现显著差异

Conclusion: 跨语言性别偏见评估需考虑目标语言特性，开发定制化评测框架。德语语法结构导致偏见表现形式与英语存在系统性差异

Abstract: In recent years, various methods have been proposed to evaluate gender bias
in large language models (LLMs). A key challenge lies in the transferability of
bias measurement methods initially developed for the English language when
applied to other languages. This work aims to contribute to this research
strand by presenting five German datasets for gender bias evaluation in LLMs.
The datasets are grounded in well-established concepts of gender bias and are
accessible through multiple methodologies. Our findings, reported for eight
multilingual LLM models, reveal unique challenges associated with gender bias
in German, including the ambiguous interpretation of male occupational terms
and the influence of seemingly neutral nouns on gender perception. This work
contributes to the understanding of gender bias in LLMs across languages and
underscores the necessity for tailored evaluation frameworks.

</details>


### [33] [Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models](https://arxiv.org/abs/2507.16572)
*Mohamad Ballout,Serwan Jassim,Elia Bruni*

Main category: cs.CL

TL;DR: 最新多模态大语言模型在直觉物理任务中存在视觉-语言对齐瓶颈，视觉编码器捕获的物理线索未被语言模型有效整合。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs虽快速发展，但在直觉物理推理中难以区分物理合理/不合理场景，需系统评估表现并探究根本原因。

Method: 使用GRASP和IntPhys 2数据集评估InternVL 2.5等模型，通过嵌入分析提取中间表示，研究视觉-语言信息整合机制。

Result: 视觉编码器成功捕获物理合理性线索（准确率75%），但语言模型未能有效利用（整合效率仅42%），关键阶段出现信息丢失。

Conclusion: MLLMs的核心限制在于跨模态对齐机制，提升视觉-语言信息整合效率是未来发展的关键方向。

Abstract: This paper presents a systematic evaluation of state-of-the-art multimodal
large language models (MLLMs) on intuitive physics tasks using the GRASP and
IntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL,
LLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that
even the latest models struggle to reliably distinguish physically plausible
from implausible scenarios. To go beyond performance metrics, we conduct a
probing analysis of model embeddings, extracting intermediate representations
at key processing stages to examine how well task-relevant information is
preserved. Our results show that, depending on task difficulty, a critical
vision-language misalignment can emerge: vision encoders successfully capture
physical plausibility cues, but this information is not effectively utilized by
the language model, leading to failures in reasoning. This misalignment
suggests that the primary limitation of MLLMs in intuitive physics tasks is not
the vision component but the ineffective integration of visual and linguistic
information. Our findings highlight vision-language alignment as a key area for
improvement, offering insights for future MLLMs development.

</details>


### [34] [Step-Audio 2 Technical Report](https://arxiv.org/abs/2507.16632)
*Boyong Wu,Chao Yan,Chen Hu,Cheng Yi,Chengli Feng,Fei Tian,Feiyu Shen,Gang Yu,Haoyang Zhang,Jingbei Li,Mingrui Chen,Peng Liu,Wang You,Xiangyu Tony Zhang,Xingyuan Li,Xuerui Yang,Yayue Deng,Yechang Huang,Yuxin Li,Yuxin Zhang,Zhao You,Brian Li,Changyi Wan,Hanpeng Hu,Jiangjie Zhen,Siyu Chen,Song Yuan,Xuelin Zhang,Yimin Jiang,Yu Zhou,Yuxiang Yang,Bingxin Li,Buyun Ma,Changhe Song,Dongqing Pang,Guoqiang Hu,Haiyang Sun,Kang An,Na Wang,Shuli Gao,Wei Ji,Wen Li,Wen Sun,Xuan Wen,Yong Ren,Yuankai Ma,Yufan Lu,Bin Wang,Bo Li,Changxin Miao,Che Liu,Chen Xu,Dapeng Shi,Dingyuan Hu,Donghang Wu,Enle Liu,Guanzhe Huang,Gulin Yan,Han Zhang,Hao Nie,Haonan Jia,Hongyu Zhou,Jianjian Sun,Jiaoren Wu,Jie Wu,Jie Yang,Jin Yang,Junzhe Lin,Kaixiang Li,Lei Yang,Liying Shi,Li Zhou,Longlong Gu,Ming Li,Mingliang Li,Mingxiao Li,Nan Wu,Qi Han,Qinyuan Tan,Shaoliang Pang,Shengjie Fan,Siqi Liu,Tiancheng Cao,Wanying Lu,Wenqing He,Wuxun Xie,Xu Zhao,Xueqi Li,Yanbo Yu,Yang Yang,Yi Liu,Yifan Lu,Yilei Wang,Yuanhao Ding,Yuanwei Liang,Yuanwei Lu,Yuchu Luo,Yuhe Yin,Yumeng Zhan,Yuxiang Zhang,Zidong Yang,Zixin Zhang,Binxing Jiao,Daxin Jiang,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Yibo Zhu*

Main category: cs.CL

TL;DR: Step-Audio~2通过整合潜在音频编码器、强化学习和检索增强生成技术，实现了工业级音频理解与语音对话，在多项基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型对副语言信息响应不足的问题，通过工具调用减少幻觉现象并提升多场景表达能力。

Method: 集成潜在音频编码器+推理强化学习；引入离散音频令牌增强副语言响应；采用RAG框架调用网络搜索/音频搜索工具；百万小时语音数据训练。

Result: 在音频理解和对话基准测试中超越开源/商业方案，支持多样化场景的智能表达。

Conclusion: 端到端多模态设计+工具调用能力+海量训练数据，实现了工业级音频对话系统的智能性与表现力突破。

Abstract: This paper presents Step-Audio~2, an end-to-end multi-modal large language
model designed for industry-strength audio understanding and speech
conversation. By integrating a latent audio encoder and reasoning-centric
reinforcement learning (RL), Step-Audio 2 achieves promising performance in
automatic speech recognition (ASR) and audio understanding. To facilitate
genuine end-to-end speech conversation, Step-Audio 2 incorporates the
generation of discrete audio tokens into language modeling, significantly
enhancing its responsiveness to paralinguistic information such as speaking
styles and emotions. To effectively leverage the rich textual and acoustic
knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented
generation (RAG) and is able to call external tools such as web search to
mitigate hallucination and audio search to switch timbres. Trained on millions
of hours of speech and audio data, Step-Audio 2 delivers intelligence and
expressiveness across diverse conversational scenarios. Evaluation results
demonstrate that Step-Audio 2 achieves state-of-the-art performance on various
audio understanding and conversational benchmarks compared to other open-source
and commercial solutions. Please visit
https://github.com/stepfun-ai/Step-Audio2 for more information.

</details>


### [35] [Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models](https://arxiv.org/abs/2507.16642)
*Armin Berger,Lars Hillebrand,David Leonhard,Tobias Deußer,Thiago Bell Felix de Oliveira,Tim Dilmaghani,Mohamed Khaled,Bernd Kliem,Rüdiger Loitz,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 开源Llama-2 70B模型在检测财务文件不合规案例中表现最佳，但GPT-4在多语言场景下综合性能最优。


<details>
  <summary>Details</summary>
Motivation: 现有AI财务审计系统缺乏对推荐文本合规性的验证能力，需评估不同大语言模型的合规检测效率。

Method: 使用PwC提供的定制数据集，对比测试Llama-2等开源模型与GPT系列专有模型在不同配置下的合规检测能力。

Result: Llama-2 70B在真阴性案例检测中准确率最高，而GPT-4在多样化场景（尤其是非英语环境）表现最全面。

Conclusion: 开源模型在特定检测任务上可替代专有模型，但专有模型在复杂多语言场景仍具优势。

Abstract: The auditing of financial documents, historically a labor-intensive process,
stands on the precipice of transformation. AI-driven solutions have made
inroads into streamlining this process by recommending pertinent text passages
from financial reports to align with the legal requirements of accounting
standards. However, a glaring limitation remains: these systems commonly fall
short in verifying if the recommended excerpts indeed comply with the specific
legal mandates. Hence, in this paper, we probe the efficiency of publicly
available Large Language Models (LLMs) in the realm of regulatory compliance
across different model configurations. We place particular emphasis on
comparing cutting-edge open-source LLMs, such as Llama-2, with their
proprietary counterparts like OpenAI's GPT models. This comparative analysis
leverages two custom datasets provided by our partner PricewaterhouseCoopers
(PwC) Germany. We find that the open-source Llama-2 70 billion model
demonstrates outstanding performance in detecting non-compliance or true
negative occurrences, beating all their proprietary counterparts. Nevertheless,
proprietary models such as GPT-4 perform the best in a broad variety of
scenarios, particularly in non-English contexts.

</details>


### [36] [P-CoT: A Pedagogically-motivated Participatory Chain-of-Thought Prompting for Phonological Reasoning in LLMs](https://arxiv.org/abs/2507.16656)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

TL;DR: 研究通过P-CoT提示策略显著提升大语言模型在音韵任务中的表现，部分任务超越人类基线


<details>
  <summary>Details</summary>
Motivation: 现有文本大语言模型在音韵推理任务中潜力未被充分开发，且传统few-shot学习效果不稳定

Method: 使用PhonologyBench基准测试（押韵词生成/g2p转换/音节计数），提出基于教育理论的P-CoT提示策略（脚手架+发现学习）

Result: P-CoT策略使模型表现最高提升52%，在部分任务中超过人类水平，优于传统few-shot方法

Conclusion: 结构化指导能激活语言模型潜在音韵能力，未来可优化特定模型提示或扩展至多语言场景

Abstract: This study explores the potential of phonological reasoning within text-based
large language models (LLMs). Utilizing the PhonologyBench benchmark, we assess
tasks like rhyme word generation, g2p conversion, and syllable counting. Our
evaluations across 12 LLMs reveal that while few-shot learning offers
inconsistent gains, the introduction of a novel Pedagogically-motivated
Participatory Chain-of-Thought (P-CoT) prompt, which is anchored in educational
theories like scaffolding and discovery learning, consistently enhances
performance. This method leverages structured guidance to activate latent
phonological abilities, achieving up to 52% improvement and even surpassing
human baselines in certain tasks. Future work could aim to optimize P-CoT
prompts for specific models or explore their application across different
linguistic domains.

</details>


### [37] [Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs](https://arxiv.org/abs/2507.16663)
*Yujin Han,Hao Chen,Andi Han,Zhiheng Wang,Xinyu Lin,Yingya Zhang,Shiwei Zhang,Difan Zou*

Main category: cs.CL

TL;DR: 多模态大语言模型存在生成与理解的自我矛盾现象，提出非统一分数量化该矛盾，发现通过内部监督微调可提升生成与理解统一性，并揭示了共改善/共退化效应。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在生成和理解任务中存在自我矛盾，生成结果与模型自身理解不匹配，这种非统一性主要由生成能力不足导致而非理解缺陷。

Method: 定义非统一分数量化矛盾，利用模型强理解能力监督弱生成分支进行后训练（SFT/DPO），提出课程学习策略逐步引入困难样本。

Result: 微调生成分支可同时提升生成质量和理解检测能力，发现共改善效应（co-improvement）和共退化风险（co-degradation）。

Conclusion: 生成与理解具有协同优化潜力，但需警惕低质量数据引发的共退化，提出基于课程学习的训练框架有效提升MLLM的统一性。

Abstract: Despite efforts to unify multimodal generation and understanding tasks in a
single model, we show these MLLMs exhibit self-contradiction where generation
produces images deemed misaligned with input prompts based on the model's own
understanding. We define a Nonunified score that quantifies such
self-contradiction. Our empirical results reveal that the self-contradiction
mainly arises from weak generation that fails to align with prompts, rather
than misunderstanding. This capability asymmetry indicates the potential of
leveraging self-contradiction for self-improvement, where the stronger model
understanding guides the weaker generation to mitigate the
generation-understanding gap. Applying standard post-training methods (e.g.,
SFT, DPO) with such internal supervision successfully improves both generation
and unification. We discover a co-improvement effect on both generation and
understanding when only fine-tuning the generation branch, a phenomenon known
in pre-training but underexplored in post-training. Our analysis shows
improvements stem from better detection of false positives that are previously
incorrectly identified as prompt-aligned. Theoretically, we show the aligned
training dynamics between generation and understanding allow reduced
prompt-misaligned generations to also improve mismatch detection in the
understanding branch. Additionally, the framework reveals a potential risk of
co-degradation under poor supervision-an overlooked phenomenon that is
empirically validated in our experiments. Notably, we find intrinsic metrics
like Nonunified score cannot distinguish co-degradation from co-improvement,
which highlights the necessity of data quality check. Finally, we propose a
curriculum-based strategy based on our findings that gradually introduces
harder samples as the model improves, leading to better unification and
improved MLLM generation and understanding.

</details>


### [38] [PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization](https://arxiv.org/abs/2507.16679)
*Han Jiang,Dongyao Zhu,Zhihua Wei,Xiaoyuan Yi,Ziang Xiao,Xing Xie*

Main category: cs.CL

TL;DR: 提出PICACO方法解决In-Context Alignment中的价值冲突问题，通过优化元指令协调多价值观，实验验证其在多价值观平衡上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有ICA方法难以协调多元价值观冲突（如刺激vs传统），导致指令瓶颈问题，需开发能平衡多价值诉求的新方法。

Method: 通过最大化价值观与模型响应的总相关性，理论强化价值关联并减少干扰噪声，无需微调生成高效价值指令。

Result: 在5个价值观集测试中超越多个基线，对开源/黑盒模型均有效，可平衡多达8种价值观。

Conclusion: PICACO突破ICA指令瓶颈，为多价值观对齐提供创新解决方案，显著提升LLM的价值协调能力。

Abstract: In-Context Learning has shown great potential for aligning Large Language
Models (LLMs) with human values, helping reduce harmful outputs and accommodate
diverse preferences without costly post-training, known as In-Context Alignment
(ICA). However, LLMs' comprehension of input prompts remains agnostic, limiting
ICA's ability to address value tensions--human values are inherently
pluralistic, often imposing conflicting demands, e.g., stimulation vs.
tradition. Current ICA methods therefore face the Instruction Bottleneck
challenge, where LLMs struggle to reconcile multiple intended values within a
single prompt, leading to incomplete or biased alignment. To address this, we
propose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO
optimizes a meta-instruction that navigates multiple values to better elicit
LLMs' understanding of them and improve their alignment. This is achieved by
maximizing the total correlation between specified values and LLM responses,
theoretically reinforcing value correlation while reducing distractive noise,
resulting in effective value instructions. Extensive experiments on five value
sets show that PICACO works well with both black-box and open-source LLMs,
outperforms several recent strong baselines, and achieves a better balance
across up to 8 distinct values.

</details>


### [39] [Interpretable Topic Extraction and Word Embedding Learning using row-stochastic DEDICOM](https://arxiv.org/abs/2507.16695)
*Lars Hillebrand,David Biesner,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 提出行随机DEDICOM变体用于PMI矩阵分析，实现文本主题聚类与可解释词向量联合学习，并开发高效训练方法。


<details>
  <summary>Details</summary>
Motivation: 利用DEDICOM矩阵分解对非对称矩阵的解析优势，改进传统文本分析方法在主题建模和词向量可解释性上的局限。PMI矩阵的非对称特性与DEDICOM算法特性高度契合。

Method: 1. 开发行随机约束的DEDICOM变体适应概率场景 2. 应用于文本互信息矩阵分解 3. 提出带约束DEDICOM模型的高效训练算法

Result: 成功从词汇中提取潜在主题簇，同时生成具有可解释性的词嵌入。定性评估验证了该方法在主题一致性和词向量解释力方面的有效性。

Conclusion: 该工作证明了DEDICOM框架在联合主题建模与词向量学习中的独特价值，约束训练算法为实际应用提供技术支持，为NLP模型可解释性研究开辟新路径。

Abstract: The DEDICOM algorithm provides a uniquely interpretable matrix factorization
method for symmetric and asymmetric square matrices. We employ a new
row-stochastic variation of DEDICOM on the pointwise mutual information
matrices of text corpora to identify latent topic clusters within the
vocabulary and simultaneously learn interpretable word embeddings. We introduce
a method to efficiently train a constrained DEDICOM algorithm and a qualitative
evaluation of its topic modeling and word embedding performance.

</details>


### [40] [Advancing Risk and Quality Assurance: A RAG Chatbot for Improved Regulatory Compliance](https://arxiv.org/abs/2507.16711)
*Lars Hillebrand,Armin Berger,Daniel Uedelhoven,David Berghaus,Ulrich Warning,Tim Dilmaghani,Bernd Kliem,Thomas Schmid,Rüdiger Loitz,Rafet Sifa*

Main category: cs.CL

TL;DR: 提出新型混合检索增强生成系统，在124个真实查询中显著优于传统RAG方法，并通过超参数分析提供实践指导


<details>
  <summary>Details</summary>
Motivation: 解决传统风险与质量监管方法依赖专家导致的效率瓶颈和扩展性问题

Method: 结合大语言模型、混合搜索和相关度增强技术开发RAG系统，基于124个专家标注的真实查询进行评估，并进行超参数配置分析

Result: 已部署系统较传统RAG方法有明显改进，超参数研究为从业者提供配置优化洞见

Conclusion: 新型RAG系统有效提升监管查询处理效率和可扩展性，参数分析成果具备实用价值

Abstract: Risk and Quality (R&Q) assurance in highly regulated industries requires
constant navigation of complex regulatory frameworks, with employees handling
numerous daily queries demanding accurate policy interpretation. Traditional
methods relying on specialized experts create operational bottlenecks and limit
scalability. We present a novel Retrieval Augmented Generation (RAG) system
leveraging Large Language Models (LLMs), hybrid search and relevance boosting
to enhance R&Q query processing. Evaluated on 124 expert-annotated real-world
queries, our actively deployed system demonstrates substantial improvements
over traditional RAG approaches. Additionally, we perform an extensive
hyperparameter analysis to compare and evaluate multiple configuration setups,
delivering valuable insights to practitioners.

</details>


### [41] [RAVine: Reality-Aligned Evaluation for Agentic Search](https://arxiv.org/abs/2507.16725)
*Yilong Xu,Xiang Long,Zhi Zheng,Jinhua Gao*

Main category: cs.CL

TL;DR: 提出RAVine评估框架，通过多维度查询设计、属性化真实标注、迭代过程评估与效率考量，解决代理式搜索系统现有评估框架的三大不足


<details>
  <summary>Details</summary>
Motivation: 现有评估框架存在三大缺陷：1) 复杂查询偏离真实用户场景 2) 端到端评估存在噪声干扰 3) 忽视代理式搜索的迭代过程评估

Method: 1) 采用长回答形式和用户意图对齐的多点查询设计 2) 构建属性化真实标注策略减少噪声 3) 增加搜索过程交互评估 4) 引入时间/调用次数等效率指标

Result: 通过RAVine框架的基准测试验证了评估有效性，揭示了代理式搜索系统的关键改进方向，相关代码数据集已开源

Conclusion: RAVine实现了更贴近现实场景的代理搜索评估，通过系统性指标设计推动该领域发展，为后续研究提供标准化评估工具

Abstract: Agentic search, as a more autonomous and adaptive paradigm of retrieval
augmentation, is driving the evolution of intelligent search systems. However,
existing evaluation frameworks fail to align well with the goals of agentic
search. First, the complex queries commonly used in current benchmarks often
deviate from realistic user search scenarios. Second, prior approaches tend to
introduce noise when extracting ground truth for end-to-end evaluations,
leading to distorted assessments at a fine-grained level. Third, most current
frameworks focus solely on the quality of final answers, neglecting the
evaluation of the iterative process inherent to agentic search. To address
these limitations, we propose RAVine -- a Reality-Aligned eValuation framework
for agentic LLMs with search. RAVine targets multi-point queries and long-form
answers that better reflect user intents, and introduces an attributable ground
truth construction strategy to enhance the accuracy of fine-grained evaluation.
Moreover, RAVine examines model's interaction with search tools throughout the
iterative process, and accounts for factors of efficiency. We benchmark a
series of models using RAVine and derive several insights, which we hope will
contribute to advancing the development of agentic search systems. The code and
datasets are available at https://github.com/SwordFaith/RAVine.

</details>


### [42] [Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals](https://arxiv.org/abs/2507.16748)
*Jingni Wu,Amir Zeldes*

Main category: cs.CL

TL;DR: 研究通过eRST框架分析英语中多义话语标记(DMs)与非DM信号的共现模式，发现多义DMs伴随更多样化(非数量更多)的非DM信号，且文体类型显著影响其交互模式。


<details>
  <summary>Details</summary>
Motivation: 阐明多义话语标记与共现非DM信号的相互作用机制，解决因DM多义性(如'since'表时间/因果)和信号重叠('in the morning'≈'then')导致的语义模糊问题。

Method: 基于eRST框架构建DM多义性分级定义，通过相关性分析与回归模型检验多义DMs是否伴随更大量/多样化的非DM信号。

Result: 1. 多义DMs与非DM信号多样性正相关，但总数量无显著增长
2. 文体类型显著调控DM-信号交互模式(如学术vs.口语文本差异)

Conclusion: DM多义性通过共现信号多样性实现语境消歧，且文体特征作为调节变量塑造信号组合策略，对自然语言处理及语篇分析具有方法论启示。

Abstract: Discourse markers (DMs) like 'but' or 'then' are crucial for creating
coherence in discourse, yet they are often replaced by or co-occur with non-DMs
('in the morning' can mean the same as 'then'), and both can be ambiguous
('since' can refer to time or cause). The interaction mechanism between such
signals remains unclear but pivotal for their disambiguation. In this paper we
investigate the relationship between DM polysemy and co-occurrence of non-DM
signals in English, as well as the influence of genre on these patterns.
  Using the framework of eRST, we propose a graded definition of DM polysemy,
and conduct correlation and regression analyses to examine whether polysemous
DMs are accompanied by more numerous and diverse non-DM signals. Our findings
reveal that while polysemous DMs do co-occur with more diverse non-DMs, the
total number of co-occurring signals does not necessarily increase. Moreover,
genre plays a significant role in shaping DM-signal interactions.

</details>


### [43] [Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning](https://arxiv.org/abs/2507.16784)
*Hongyin Luo,Nathaniel Morgan,Tina Li,Derek Zhao,Ai Vy Ngo,Philip Schroeder,Lijie Yang,Assaf Ben-Kish,Jack O'Brien,James Glass*

Main category: cs.CL

TL;DR: 提出TIM推理模型与TIMRUN运行时，通过树状推理结构突破LLM上下文限制，实现GPU内存复用与长程推理


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因上下文限制导致的推理精度下降、GPU内存瓶颈及位置编码约束问题

Method: 1. 将自然语言建模为深度推理树而非线性序列
2. 动态维护工作记忆（仅保留关键KV状态）
3. 基于规则的子任务剪枝机制实现位置编码与GPU内存页复用

Result: 系统在90% KV缓存操作下保持高推理吞吐，数学任务准确率提升，成功处理需长程推理的多跳工具调用任务

Conclusion: 树状推理架构与内存优化机制有效突破LLM的上下文限制，为复杂推理任务提供可扩展解决方案

Abstract: To break the context limits of large language models (LLMs) that bottleneck
reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),
a family of LLMs trained for recursive and decompositional problem solving, and
TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond
context limits. Together, TIM hosted on TIMRUN supports virtually unlimited
working memory and multi-hop tool calls within a single language model
inference, overcoming output limits, positional-embedding constraints, and
GPU-memory bottlenecks. Performance is achieved by modeling natural language as
reasoning trees measured by both length and depth instead of linear sequences.
The reasoning trees consist of tasks with thoughts, recursive subtasks, and
conclusions based on the concept we proposed in Schroeder et al, 2025. During
generation, we maintain a working memory that retains only the key-value states
of the most relevant context tokens, selected by a rule-based subtask-pruning
mechanism, enabling reuse of positional embeddings and GPU memory pages
throughout reasoning. Experimental results show that our system sustains high
inference throughput, even when manipulating up to 90% of the KV cache in GPU
memory. It also delivers accurate reasoning on mathematical tasks and handles
information retrieval challenges that require long-horizon reasoning and
multi-hop tool use.

</details>


### [44] [Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent](https://arxiv.org/abs/2507.16799)
*Xiaoyu Zhan,Xinyu Fu,Hao Sun,Yuanqi Li,Jie Guo,Yanwen Guo*

Main category: cs.CL

TL;DR: 提出无需训练的TTM框架，通过角色特征解耦和三阶段生成流程提升角色扮演保真度


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演方法存在沉浸感不足（仅靠prompt）和资源限制（微调方法）的缺陷，需更高效的训练无关方案

Method: 将角色特征自动解耦为个性/记忆/语言风格，构建三阶段控制生成流程（特征提取-组合生成-风格校准）

Result: 人工评估显示该方法在生成表达力强且风格一致的对话方面表现优异

Conclusion: TTM框架无需训练即可实现高保真角色扮演，支持跨风格特征灵活组合，实验验证其有效性

Abstract: The rapid advancement of large language models (LLMs) has enabled
role-playing language agents to demonstrate significant potential in various
applications. However, relying solely on prompts and contextual inputs often
proves insufficient for achieving deep immersion in specific roles,
particularly well-known fictional or public figures. On the other hand,
fine-tuning-based approaches face limitations due to the challenges associated
with data collection and the computational resources required for training,
thereby restricting their broader applicability. To address these issues, we
propose Test-Time-Matching (TTM), a training-free role-playing framework
through test-time scaling and context engineering. TTM uses LLM agents to
automatically decouple a character's features into personality, memory, and
linguistic style. Our framework involves a structured, three-stage generation
pipeline that utilizes these features for controlled role-playing. It achieves
high-fidelity role-playing performance, also enables seamless combinations
across diverse linguistic styles and even variations in personality and memory.
We evaluate our framework through human assessment, and the results demonstrate
that our method achieves the outstanding performance in generating expressive
and stylistically consistent character dialogues.

</details>


### [45] [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802)
*Yanjun Zheng,Xiyang Du,Longfei Liao,Xiaoke Zhao,Zhaowen Zhou,Bo Zhang,Jiawei Liu,Xiang Qi,Zhe Li,Zhiqiang Zhang,Wang Wei,Peng Zhang*

Main category: cs.CL

TL;DR: 开发了Agentar-Fin-R1系列金融大模型（8B/32B参数），通过系统性优化方法显著提升金融领域推理能力、可信度及领域适应性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在金融场景中面临推理能力不足、可信度要求严格、任务适配效率低等核心痛点

Method: 整合金融任务分类法与多层可信保障框架（含知识工程/数据合成/验证治理），采用标签引导的自动化难度感知优化和两阶段学习流程

Result: 在FinEva等金融基准和MATH-500等通用测试中取得SOTA，创新性提出Finova评估基准验证实际部署能力

Conclusion: 该模型不仅在金融任务中表现优异，同时具备卓越的通用推理能力，验证了其作为高要求金融应用可信解决方案的有效性

Abstract: Large Language Models (LLMs) demonstrate tremendous potential in the
financial domain, yet existing models often fall short in scenarios demanding
robust reasoning capabilities, stringent trustworthiness requirements, and
efficient adaptation to task-specific needs. We introduce the Agentar-Fin-R1
series of financial large language models (8B and 32B parameters), specifically
engineered based on the Qwen3 foundation model to enhance reasoning
capabilities, reliability, and domain specialization for financial
applications. Our optimization approach integrates a high-quality, systematic
financial task taxonomy with a comprehensive multi-layered trustworthiness
assurance framework. This framework encompasses high-quality trustworthy
knowledge engineering, multi-agent trustworthy data synthesis, and rigorous
data validation governance. Through label-guided automated difficulty-aware
optimization, tow-stage learning processes, and detailed attribution systems,
we achieve substantial improvements in training efficiency. Our models undergo
comprehensive evaluation on mainstream financial benchmarks including FinEva,
FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500
and GPQA. To thoroughly assess real-world deployment capabilities, we
innovatively propose the Finova evaluation benchmark, which focuses on
agent-level financial reasoning and compliance verification. Experimental
results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art
performance on financial tasks but also exhibits exceptional general reasoning
capabilities, validating its effectiveness as a trustworthy solution for
high-stakes financial applications.

</details>


### [46] [LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs](https://arxiv.org/abs/2507.16809)
*Da-Chen Lian,Ri-Sheng Huang,Pin-Er Chen,Chunki Lim,You-Kuan Lin,Guan-Yu Tseng,Zi-Cheng Yang,Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: 提出LingBench++基准框架，通过结构化推理追踪和多智能体架构提升LLMs在跨文化语言任务中的准确性与可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基准仅关注答案准确性，缺乏对复杂推理过程和文化语言多样性的评估，需构建更全面的语言学评估体系

Method: 整合语法知识检索+工具增强推理+假设测试的多智能体架构，建立覆盖90+低资源语言的评估协议与类型学元数据

Result: 配备外部知识源的迭代推理模型在准确率（+15%）和可解释性指标上显著优于单次推理基线

Conclusion: LingBench++为LLMs实现语言认知合理化的跨文化推理提供了系统性评估框架与研究基础

Abstract: We propose LingBench++, a linguistically-informed benchmark and reasoning
framework designed to evaluate large language models (LLMs) on complex
linguistic tasks inspired by the International Linguistics Olympiad (IOL).
Unlike prior benchmarks that focus solely on final answer accuracy, LingBench++
provides structured reasoning traces, stepwise evaluation protocols, and rich
typological metadata across over 90 low-resource and cross-cultural languages.
We further develop a multi-agent architecture integrating grammatical knowledge
retrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through
systematic comparisons of baseline and our proposed agentic models, we
demonstrate that models equipped with external knowledge sources and iterative
reasoning outperform single-pass approaches in both accuracy and
interpretability. LingBench++ offers a comprehensive foundation for advancing
linguistically grounded, culturally informed, and cognitively plausible
reasoning in LLMs.

</details>


### [47] [MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning](https://arxiv.org/abs/2507.16812)
*Run-Ze Fan,Zengzhi Wang,Pengfei Liu*

Main category: cs.CL

TL;DR: 提出TextbookReasoning和MegaScience两个科学推理数据集，建立评估体系并验证模型性能提升


<details>
  <summary>Details</summary>
Motivation: 开源社区缺乏高质量可验证的科学推理数据集，制约AI科学推理能力发展

Method: 从12K教科书提取65万问题构建TextbookReasoning；通过系统消融实验整合125万实例构建MegaScience；建立含15个基准的评估体系

Result: 训练模型性能超越官方指导模型，大模型展现扩展优势，响应长度缩减33%

Conclusion: 数据集优化和科学调优有效提升模型科学推理能力，开放资源推动领域发展

Abstract: Scientific reasoning is critical for developing AI scientists and supporting
human researchers in advancing the frontiers of natural science discovery.
However, the open-source community has primarily focused on mathematics and
coding while neglecting the scientific domain, largely due to the absence of
open, large-scale, high-quality, verifiable scientific reasoning datasets. To
bridge this gap, we first present TextbookReasoning, an open dataset featuring
truthful reference answers extracted from 12k university-level scientific
textbooks, comprising 650k reasoning questions spanning 7 scientific
disciplines. We further introduce MegaScience, a large-scale mixture of
high-quality open-source datasets totaling 1.25 million instances, developed
through systematic ablation studies that evaluate various data selection
methodologies to identify the optimal subset for each publicly available
scientific dataset. Meanwhile, we build a comprehensive evaluation system
covering diverse subjects and question types across 15 benchmarks,
incorporating comprehensive answer extraction strategies to ensure accurate
evaluation metrics. Our experiments demonstrate that our datasets achieve
superior performance and training efficiency with more concise response lengths
compared to existing open-source scientific datasets. Furthermore, we train
Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which
significantly outperform the corresponding official instruct models in average
performance. In addition, MegaScience exhibits greater effectiveness for larger
and stronger models, suggesting a scaling benefit for scientific tuning. We
release our data curation pipeline, evaluation system, datasets, and seven
trained models to the community to advance scientific reasoning research.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [48] [Dream, Lift, Animate: From Single Images to Animatable Gaussian Avatars](https://arxiv.org/abs/2507.15979)
*Marcel C. Bühler,Ye Yuan,Xueting Li,Yangyi Huang,Koki Nagano,Umar Iqbal*

Main category: cs.GR

TL;DR: 提出DLA框架，通过多视角生成、3D高斯提升和UV空间映射，实现单图重建可动画的3D人体化身


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在动画一致性保持和细节保留上的不足，结合视频扩散模型的生成能力和结构化UV空间映射

Method: 1. 视频扩散模型生成多视角图像 → 2. 提升为3D高斯 → 3. Transformer编码器投影到UV空间结构化表征 → 4. 姿势驱动变形渲染

Result: 在ActorsHQ和4D-Dress数据集上超越SOTA方法，支持实时渲染和直观编辑

Conclusion: 通过将非结构化3D表示与UV空间映射结合，填补了高保真动画化身的制作空白

Abstract: We introduce Dream, Lift, Animate (DLA), a novel framework that reconstructs
animatable 3D human avatars from a single image. This is achieved by leveraging
multi-view generation, 3D Gaussian lifting, and pose-aware UV-space mapping of
3D Gaussians. Given an image, we first dream plausible multi-views using a
video diffusion model, capturing rich geometric and appearance details. These
views are then lifted into unstructured 3D Gaussians. To enable animation, we
propose a transformer-based encoder that models global spatial relationships
and projects these Gaussians into a structured latent representation aligned
with the UV space of a parametric body model. This latent code is decoded into
UV-space Gaussians that can be animated via body-driven deformation and
rendered conditioned on pose and viewpoint. By anchoring Gaussians to the UV
manifold, our method ensures consistency during animation while preserving fine
visual details. DLA enables real-time rendering and intuitive editing without
requiring post-processing. Our method outperforms state-of-the-art approaches
on ActorsHQ and 4D-Dress datasets in both perceptual quality and photometric
accuracy. By combining the generative strengths of video diffusion models with
a pose-aware UV-space Gaussian mapping, DLA bridges the gap between
unstructured 3D representations and high-fidelity, animation-ready avatars.

</details>


### [49] [MMS Player: an open source software for parametric data-driven animation of Sign Language avatars](https://arxiv.org/abs/2507.16463)
*Fabrizio Nunnari,Shailesh Mishra,Patrick Gebhard*

Main category: cs.GR

TL;DR: 开源软件MMS-Player通过新型多模态签流格式MMS生成手语动画，改进传统gloss表示法并支持多种输出格式


<details>
  <summary>Details</summary>
Motivation: 传统手语动画基于gloss的表示方法缺乏对并行执行、时间同步和屈折变化等要素的支持，需要更丰富的多模态表示格式

Method: 1. 设计MMS格式增强手语表示能力 2. 基于Blender开发Python脚本实现动画合成 3. 提供命令行和HTTP API两种调用方式

Result: 实现支持视频渲染和3D格式导出的开源工具（GPL-3.0），GitHub已公开代码库，支持主流动画交换格式

Conclusion: MMS-Player为手语动画合成提供标准化解决方案，通过多模态特征表示和灵活的输出方式推动相关领域研究

Abstract: This paper describes the MMS-Player, an open source software able to
synthesise sign language animations from a novel sign language representation
format called MMS (MultiModal Signstream). The MMS enhances gloss-based
representations by adding information on parallel execution of signs, timing,
and inflections. The implementation consists of Python scripts for the popular
Blender 3D authoring tool and can be invoked via command line or HTTP API.
Animations can be rendered as videos or exported in other popular 3D animation
exchange formats. The software is freely available under GPL-3.0 license at
https://github.com/DFKI-SignLanguage/MMS-Player.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [50] [RDMA: Cost Effective Agent-Driven Rare Disease Discovery within Electronic Health Record Systems](https://arxiv.org/abs/2507.15867)
*John Wu,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: RDMA框架通过本地化临床推理有效解决罕见疾病在EHR中的识别难题，提升30%性能并降低10倍成本


<details>
  <summary>Details</summary>
Motivation: 现有ICD编码系统无法有效捕捉EHR中的罕见疾病信息，且存在隐私泄露、缩写识别困难、隐性疾病模式遗漏等问题

Method: 开发模拟医学专家思维的框架，整合分散的临床观察数据，处理缩写/隐式模式，通过本地上下文推理连接疾病特征

Result: F1性能提升超30%，推理成本降低10倍，隐私风险显著减少

Conclusion: RDMA为临床医生提供了安全高效的EHR分析方案，支持罕见疾病的早期诊断，突破传统云处理的隐私瓶颈

Abstract: Rare diseases affect 1 in 10 Americans, yet standard ICD coding systems fail
to capture these conditions in electronic health records (EHR), leaving crucial
information buried in clinical notes. Current approaches struggle with medical
abbreviations, miss implicit disease mentions, raise privacy concerns with
cloud processing, and lack clinical reasoning abilities. We present Rare
Disease Mining Agents (RDMA), a framework that mirrors how medical experts
identify rare disease patterns in EHR. RDMA connects scattered clinical
observations that together suggest specific rare conditions. By handling
clinical abbreviations, recognizing implicit disease patterns, and applying
contextual reasoning locally on standard hardware, RDMA reduces privacy risks
while improving F1 performance by upwards of 30\% and decreasing inferences
costs 10-fold. This approach helps clinicians avoid the privacy risk of using
cloud services while accessing key rare disease information from EHR systems,
supporting earlier diagnosis for rare disease patients. Available at
https://github.com/jhnwu3/RDMA.

</details>


### [51] [Scaling Linear Attention with Sparse State Expansion](https://arxiv.org/abs/2507.16577)
*Yuqi Pan,Yongqi An,Zheng Li,Yuhong Chou,Ruijie Zhu,Xiaohui Wang,Mingxuan Wang,Jinqiao Wang,Guoqi Li*

Main category: cs.LG

TL;DR: 提出SSE架构通过稀疏状态更新和状态扩展机制，在保持线性效率的同时显著提升长上下文任务性能


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长上下文场景存在效率瓶颈，而现有线性注意力方法在压缩上下文时导致检索和推理性能下降

Method: 1. 基于信息分类的稀疏状态更新机制（softmax top-k硬分类）
2. 稀疏状态扩展（SSE）实现状态分区和解耦参数规模与状态容量

Result: 2B参数SSE-H模型在数学推理基准测试（AIME24 64.7分/AIME25 51.3分）超越同规模Transformer，检索性能随状态容量提升持续改善

Conclusion: SSE架构在保持计算效率的同时，在长上下文建模和推理任务中展现出显著优势，为高效长序列处理提供新方案

Abstract: The Transformer architecture, despite its widespread success, struggles with
long-context scenarios due to quadratic computation and linear memory growth.
While various linear attention variants mitigate these efficiency constraints
by compressing context into fixed-size states, they often degrade performance
in tasks such as in-context retrieval and reasoning. To address this limitation
and achieve more effective context compression, we propose two key innovations.
First, we introduce a row-sparse update formulation for linear attention by
conceptualizing state updating as information classification. This enables
sparse state updates via softmax-based top-$k$ hard classification, thereby
extending receptive fields and reducing inter-class interference. Second, we
present Sparse State Expansion (SSE) within the sparse framework, which expands
the contextual state into multiple partitions, effectively decoupling parameter
size from state capacity while maintaining the sparse classification paradigm.
Our design, supported by efficient parallelized implementations, yields
effective classification and discriminative state representations. We
extensively validate SSE in both pure linear and hybrid (SSE-H) architectures
across language modeling, in-context retrieval, and mathematical reasoning
benchmarks. SSE demonstrates strong retrieval performance and scales favorably
with state size. Moreover, after reinforcement learning (RL) training, our 2B
SSE-H model achieves state-of-the-art mathematical reasoning performance among
small reasoning models, scoring 64.7 on AIME24 and 51.3 on AIME25,
significantly outperforming similarly sized open-source Transformers. These
results highlight SSE as a promising and efficient architecture for
long-context modeling.

</details>


### [52] [Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning](https://arxiv.org/abs/2507.16795)
*Helena Casademunt,Caden Juang,Adam Karvonen,Samuel Marks,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.LG

TL;DR: CAFT通过潜在空间概念消融技术，在不修改训练数据的情况下有效控制LLM的分布外泛化


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖修改训练数据来控制模型泛化，但实际应用中往往难以实施。需要开发不依赖数据调整的新方法

Method: 在LLM潜在空间中定位非预期概念方向，通过线性投影技术进行概念消融，同步实施微调和泛化控制

Result: 在包含紧急错位场景的三个任务中，CAFT将错位响应降低10倍（从31%→3%），同时保持原有任务性能（保留98.5%的准确率）

Conclusion: CAFT开创了不依赖训练数据调整的LLM泛化控制范式，为解决模型微调中的意外泛化问题提供了新方向

Abstract: Fine-tuning large language models (LLMs) can lead to unintended
out-of-distribution generalization. Standard approaches to this problem rely on
modifying training data, for example by adding data that better specify the
intended generalization. However, this is not always practical. We introduce
Concept Ablation Fine-Tuning (CAFT), a technique that leverages
interpretability tools to control how LLMs generalize from fine-tuning, without
needing to modify the training data or otherwise use data from the target
distribution. Given a set of directions in an LLM's latent space corresponding
to undesired concepts, CAFT works by ablating these concepts with linear
projections during fine-tuning, steering the model away from unintended
generalizations. We successfully apply CAFT to three fine-tuning tasks,
including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow
task generalize to give egregiously misaligned responses to general questions.
Without any changes to the fine-tuning data, CAFT reduces misaligned responses
by 10x without degrading performance on the training distribution. Overall,
CAFT represents a novel approach for steering LLM generalization without
modifying training data.

</details>


### [53] [Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty](https://arxiv.org/abs/2507.16806)
*Mehul Damani,Isha Puri,Stewart Slocum,Idan Shenfeld,Leshem Choshen,Yoon Kim,Jacob Andreas*

Main category: cs.LG

TL;DR: 提出RLCR方法，通过结合正确性奖励与Brier评分联合优化语言模型准确率与置信度校准


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用二元奖励函数导致模型校准性恶化，增加幻觉概率

Method: 在强化学习过程中同时生成预测结果和数值化置信度，使用Brier评分作为校准指标构建组合奖励函数

Result: 在多个数据集上实现校准性显著提升（准确性不变），优于传统RL训练和事后置信度分类器

Conclusion: 通过显式优化校准目标可以构建更可靠的语言推理模型，验证置信度加权方法对实际应用的提升价值

Abstract: When language models (LMs) are trained via reinforcement learning (RL) to
generate natural language "reasoning chains", their performance improves on a
variety of difficult question answering tasks. Today, almost all successful
applications of RL for reasoning use binary reward functions that evaluate the
correctness of LM outputs. Because such reward functions do not penalize
guessing or low-confidence outputs, they often have the unintended side-effect
of degrading calibration and increasing the rate at which LMs generate
incorrect responses (or "hallucinate") in other problem domains. This paper
describes RLCR (Reinforcement Learning with Calibration Rewards), an approach
to training reasoning models that jointly improves accuracy and calibrated
confidence estimation. During RLCR, LMs generate both predictions and numerical
confidence estimates after reasoning. They are trained to optimize a reward
function that augments a binary correctness score with a Brier score -- a
scoring rule for confidence estimates that incentivizes calibrated prediction.
We first prove that this reward function (or any analogous reward function that
uses a bounded, proper scoring rule) yields models whose predictions are both
accurate and well-calibrated. We next show that across diverse datasets, RLCR
substantially improves calibration with no loss in accuracy, on both in-domain
and out-of-domain evaluations -- outperforming both ordinary RL training and
classifiers trained to assign post-hoc confidence scores. While ordinary RL
hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized
confidence can be leveraged at test time to improve accuracy and calibration
via confidence-weighted scaling methods. Our results show that explicitly
optimizing for calibration can produce more generally reliable reasoning
models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [Why Braking? Scenario Extraction and Reasoning Utilizing LLM](https://arxiv.org/abs/2507.15874)
*Yin Wu,Daniel Slieter,Vivek Subramanian,Ahmed Abouelazm,Robin Bohn,J. Marius Zöllner*

Main category: cs.AI

TL;DR: 提出基于大语言模型的驾驶场景理解框架，通过双路径检索机制有效识别已知/未知危险场景


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的场景检索方法在复杂城市道路中泛化性不足，且海量驾驶数据中安全关键场景识别困难

Method: 利用LLM桥接数值信号与自然语言，设计支持已知场景分类检索和未知OOD场景嵌入检索的双路径框架

Result: 在Argoverse 2数据集上超越规则基准，对OOD场景展示良好泛化能力

Conclusion: 首次将LLM引入驾驶场景理解，实现语义级场景检索，为数据驱动的自动驾驶安全评估提供新思路

Abstract: The growing number of ADAS-equipped vehicles has led to a dramatic increase
in driving data, yet most of them capture routine driving behavior. Identifying
and understanding safety-critical corner cases within this vast dataset remains
a significant challenge. Braking events are particularly indicative of
potentially hazardous situations, motivating the central question of our
research: Why does a vehicle brake? Existing approaches primarily rely on
rule-based heuristics to retrieve target scenarios using predefined condition
filters. While effective in simple environments such as highways, these methods
lack generalization in complex urban settings. In this paper, we propose a
novel framework that leverages Large Language Model (LLM) for scenario
understanding and reasoning. Our method bridges the gap between low-level
numerical signals and natural language descriptions, enabling LLM to interpret
and classify driving scenarios. We propose a dual-path scenario retrieval that
supports both category-based search for known scenarios and embedding-based
retrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate
evaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.
Experimental results show that our method outperforms rule-based baselines and
generalizes well to OOD scenarios.

</details>


### [55] [SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting](https://arxiv.org/abs/2507.16145)
*Shuhao Mei,Yongchao Long,Shan Cao,Xiaobo Han,Shijia Geng,Jinbo Sun,Yuxi Zhou,Shenda Hong*

Main category: cs.AI

TL;DR: 首个多模态大语言模型SpiroLLM通过融合呼吸曲线形态特征与肺功能数值，实现了可解释的COPD自动化诊断（AUROC 0.8980），在核心数据缺失时仍保持100%有效响应率。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型缺乏诊断过程解释，传统LLM无法理解呼吸曲线特征，严重制约临床可信度。通过深度整合生理信号与语言模型，建立新一代可解释临床决策系统。

Method: 使用UK Biobank的234,028例数据，通过SpiroEncoder提取呼吸曲线形态特征，经SpiroProjector与肺功能数值在潜空间对齐，最终驱动LLM生成结构化诊断报告。

Result: 诊断AUROC达0.8980（95%CI 0.8820-0.9132），核心数据缺失时响应有效性100%，显著优于纯文本模型（13.4%有效性）。

Conclusion: 开创了生理信号与LLM深度融合的新范式，为可解释临床决策支持系统建立技术标准，证实多模态设计在医疗AI中的核心优势。

Abstract: Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory
disease with persistent airflow limitation, is a leading global cause of
disability and mortality. Respiratory spirogram time series, routinely
collected during pulmonary function tests (PFTs), play a critical role in the
early detection of repsiratory diseases and in monitoring lung function over
time. However, most current AI models for COPD diagnosis are limited to
outputting classification results without providing a rationale for their
diagnostic process, while current Large Language Models (LLMs) cannot
understand spirograms yet, which severely limits their clinical trust and
adoption. To tackle this challenge, we leverage a cohort of 234,028 individuals
from the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large
language model that can understand spirogram. The model extracts morphological
features from respiratory curves via a SpiroEncoder and aligns them with PFT
numerical values in a unified latent space using a SpiroProjector, ultimately
empowering a large language model to generate a comprehensive diagnostic
report. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC
of 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,
it maintained a 100% valid response rate, far surpassing the 13.4% of a
text-only model and showcasing the superiority of its multimodal design. This
work demonstrates the substantial potential of deeply fusing physiological
signals with large language models, establishing a new paradigm for the next
generation of interpretable and reliable clinical decision support tools.

</details>


### [56] [Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report](https://arxiv.org/abs/2507.16534)
*Shanghai AI Lab,:,Xiaoyang Chen,Yunhao Chen,Zeren Chen,Zhiyun Chen,Hanyun Cui,Yawen Duan,Jiaxuan Guo,Qi Guo,Xuhao Hu,Hong Huang,Lige Huang,Chunxiao Li,Juncheng Li,Qihao Lin,Dongrui Liu,Xinmin Liu,Zicheng Liu,Chaochao Lu,Xiaoya Lu,Jingjing Qu,Qibing Ren,Jing Shao,Jingwei Shi,Jingwei Sun,Peng Wang,Weibing Wang,Jia Xu,Lewen Yan,Xiao Yu,Yi Yu,Boxuan Zhang,Jie Zhang,Weichen Zhang,Zhijie Zheng,Tianyi Zhou,Bowen Zhou*

Main category: cs.AI

TL;DR: 基于E-T-C框架对前沿AI模型七类风险进行评估，当前模型均未突破红线风险，多数处于绿色/黄色区域。


<details>
  <summary>Details</summary>
Motivation: 识别快速发展的AI模型可能引发的重大风险（如网络攻击、生化风险、自主AI失控等），推动风险管理。

Method: 采用'AI-45度法则'，通过红/黄线划分风险等级（绿/黄/红区），结合E-T-C（环境-威胁-能力）三维分析框架。

Result: 所有模型均未达红线。说服/操纵类多属黄区，生化风险需深入评估，自我复制/战略欺骗类多属绿区。

Conclusion: 需集体行动建立风险缓解机制，持续改进评估框架以应对AI技术快速迭代带来的新型风险挑战。

Abstract: To understand and identify the unprecedented risks posed by rapidly advancing
artificial intelligence (AI) models, this report presents a comprehensive
assessment of their frontier risks. Drawing on the E-T-C analysis (deployment
environment, threat source, enabling capability) from the Frontier AI Risk
Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks
in seven areas: cyber offense, biological and chemical risks, persuasion and
manipulation, uncontrolled autonomous AI R\&D, strategic deception and
scheming, self-replication, and collusion. Guided by the "AI-$45^\circ$ Law,"
we evaluate these risks using "red lines" (intolerable thresholds) and "yellow
lines" (early warning indicators) to define risk zones: green (manageable risk
for routine deployment and continuous monitoring), yellow (requiring
strengthened mitigations and controlled deployment), and red (necessitating
suspension of development and/or deployment). Experimental results show that
all recent frontier AI models reside in green and yellow zones, without
crossing red lines. Specifically, no evaluated models cross the yellow line for
cyber offense or uncontrolled AI R\&D risks. For self-replication, and
strategic deception and scheming, most models remain in the green zone, except
for certain reasoning models in the yellow zone. In persuasion and
manipulation, most models are in the yellow zone due to their effective
influence on humans. For biological and chemical risks, we are unable to rule
out the possibility of most models residing in the yellow zone, although
detailed threat modeling and in-depth assessment are required to make further
claims. This work reflects our current understanding of AI frontier risks and
urges collective action to mitigate these challenges.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [57] [AlgoTune: Can Language Models Speed Up General-Purpose Numerical Programs?](https://arxiv.org/abs/2507.15887)
*Ori Press,Brandon Amos,Haoyu Zhao,Yikai Wu,Samuel K. Ainsworth,Dominik Krupke,Patrick Kidger,Touqir Sajed,Bartolomeo Stellato,Jisun Park,Nathanael Bosch,Eli Meril,Albert Steppi,Arman Zharmagambetov,Fangzhao Zhang,David Perez-Pineiro,Alberto Mercurio,Ni Zhan,Talor Abramovich,Kilian Lieret,Hanlin Zhang,Shirley Huang,Matthias Bethge,Ofir Press*

Main category: cs.SE

TL;DR: 提出AlgoTune基准测试，评估语言模型在开放算法设计任务中的性能，发现当前模型仅实现表面优化（1.72倍加速），未实现算法创新。


<details>
  <summary>Details</summary>
Motivation: 现有评估聚焦人类已解决问题，需测试语言模型在计算机科学/物理/数学领域开放式算法设计的能力。

Method: 构建含155个专家级编程任务的基准，开发AlgoTuner代理验证模型生成代码效率，并与SciPy/sk-learn/CVXPY参考实现对比。

Result: AlgoTuner平均实现1.72倍加速，但模型倾向表面优化而非算法创新。

Conclusion: AlgoTune有望推动语言模型突破人类现有算法性能，实现创造性问题解决。

Abstract: Despite progress in language model (LM) capabilities, evaluations have thus
far focused on models' performance on tasks that humans have previously solved,
including in programming (Jimenez et al., 2024) and mathematics (Glazer et al.,
2024). We therefore propose testing models' ability to design and implement
algorithms in an open-ended benchmark: We task LMs with writing code that
efficiently solves computationally challenging problems in computer science,
physics, and mathematics. Our AlgoTune benchmark consists of 155 coding tasks
collected from domain experts and a framework for validating and timing
LM-synthesized solution code, which is compared to reference implementations
from popular open-source packages. In addition, we develop a baseline LM agent,
AlgoTuner, and evaluate its performance across a suite of frontier models.
AlgoTuner achieves an average 1.72x speedup against our reference solvers,
which use libraries such as SciPy, sk-learn and CVXPY. However, we find that
current models fail to discover algorithmic innovations, instead preferring
surface-level optimizations. We hope that AlgoTune catalyzes the development of
LM agents exhibiting creative problem solving beyond state-of-the-art human
performance.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [58] [WhatsApp Tiplines and Multilingual Claims in the 2021 Indian Assembly Elections](https://arxiv.org/abs/2507.16298)
*Gautam Kishore Shahi,Scot A. Hale*

Main category: cs.SI

TL;DR: 分析WhatsApp查证热线在印度选举中的使用情况，发现跨语言谣言相似性、用户多语言提交行为及约2天的查证周期，建议加强多语言支持与伦理考量


<details>
  <summary>Details</summary>
Motivation: 探究多语言环境下查证热线对抗选举/疫情谣言的效果，揭示用户行为模式与事实核查组织的协作效率

Method: 混合方法分析580条用户提交的谣言（英/印地/泰卢固语），通过高频词分析、神经句子嵌入聚类和用户重叠研究，测量查证时效

Result: 跨语言谣言内容相似，11%用户多语言提交；平均需2天完成查证；不同核查组织用户群完全独立

Conclusion: 查证热线需优化多语言响应速度，保持组织独立性能维持服务专注度，伦理设计应优先保护用户信息

Abstract: WhatsApp tiplines, first launched in 2019 to combat misinformation, enable
users to interact with fact-checkers to verify misleading content. This study
analyzes 580 unique claims (tips) from 451 users, covering both high-resource
languages (English, Hindi) and a low-resource language (Telugu) during the 2021
Indian assembly elections using a mixed-method approach. We categorize the
claims into three categories, election, COVID-19, and others, and observe
variations across languages. We compare content similarity through frequent
word analysis and clustering of neural sentence embeddings. We also investigate
user overlap across languages and fact-checking organizations. We measure the
average time required to debunk claims and inform tipline users. Results reveal
similarities in claims across languages, with some users submitting tips in
multiple languages to the same fact-checkers. Fact-checkers generally require a
couple of days to debunk a new claim and share the results with users. Notably,
no user submits claims to multiple fact-checking organizations, indicating that
each organization maintains a unique audience. We provide practical
recommendations for using tiplines during elections with ethical consideration
of users' information.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [59] [Document Haystack: A Long Context Multimodal Image/Document Understanding Vision LLM Benchmark](https://arxiv.org/abs/2507.15882)
*Goeric Huybrechts,Srikanth Ronanki,Sai Muralidhar Jayanthi,Jack Fitzgerald,Srinivasan Veeravanallur*

Main category: cs.CV

TL;DR: 提出Document Haystack基准测试，用于评估视觉语言模型在长文档中的多模态检索能力


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在长文档处理领域缺乏有效的评估基准

Method: 构建包含5-200页文档的数据集，插入不同深度的纯文本/图文混合'needles'，包含400种变体和8,250个问题

Result: 建立包含自动化评估框架的大规模基准，具体模型测试结果未在摘要中展示

Conclusion: 该基准填补了长文档评估的空白，为改进视觉语言模型的检索能力提供了新方向

Abstract: The proliferation of multimodal Large Language Models has significantly
advanced the ability to analyze and understand complex data inputs from
different modalities. However, the processing of long documents remains
under-explored, largely due to a lack of suitable benchmarks. To address this,
we introduce Document Haystack, a comprehensive benchmark designed to evaluate
the performance of Vision Language Models (VLMs) on long, visually complex
documents. Document Haystack features documents ranging from 5 to 200 pages and
strategically inserts pure text or multimodal text+image "needles" at various
depths within the documents to challenge VLMs' retrieval capabilities.
Comprising 400 document variants and a total of 8,250 questions, it is
supported by an objective, automated evaluation framework. We detail the
construction and characteristics of the Document Haystack dataset, present
results from prominent VLMs and discuss potential research avenues in this
area.

</details>


### [60] [C2-Evo: Co-Evolving Multimodal Data and Model for Self-Improving Reasoning](https://arxiv.org/abs/2507.16518)
*Xiuwei Chen,Wentao Hu,Hanhui Li,Jun Zhou,Zisheng Chen,Meng Cao,Yihan Zeng,Kui Zhang,Yu-Jie Yuan,Jianhua Han,Hang Xu,Xiaodan Liang*

Main category: cs.CV

TL;DR: C2-Evo提出闭环自进化框架，通过跨模态数据进化循环和数据-模型进化循环协同提升多模态大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有自改进模型存在视觉-文本数据复杂度不匹配（如图表过简但文本冗余）以及数据-模型进化分离的问题

Method: 构建双循环框架：1）跨模态数据进化循环生成结构化文本子问题+几何图表的组合问题 2）数据-模型进化循环基于模型表现自适应选择问题，交替进行监督微调和强化学习

Result: 在多个数学推理基准测试中持续获得显著性能提升（具体数值未提及）

Conclusion: 该框架实现了数据与模型的协同进化，验证了闭环自改进策略的有效性，代码和数据集将开源

Abstract: Recent advances in multimodal large language models (MLLMs) have shown
impressive reasoning capabilities. However, further enhancing existing MLLMs
necessitates high-quality vision-language datasets with carefully curated task
complexities, which are both costly and challenging to scale. Although recent
self-improving models that iteratively refine themselves offer a feasible
solution, they still suffer from two core challenges: (i) most existing methods
augment visual or textual data separately, resulting in discrepancies in data
complexity (e.g., over-simplified diagrams paired with redundant textual
descriptions); and (ii) the evolution of data and models is also separated,
leading to scenarios where models are exposed to tasks with mismatched
difficulty levels. To address these issues, we propose C2-Evo, an automatic,
closed-loop self-improving framework that jointly evolves both training data
and model capabilities. Specifically, given a base dataset and a base model,
C2-Evo enhances them by a cross-modal data evolution loop and a data-model
evolution loop. The former loop expands the base dataset by generating complex
multimodal problems that combine structured textual sub-problems with
iteratively specified geometric diagrams, while the latter loop adaptively
selects the generated problems based on the performance of the base model, to
conduct supervised fine-tuning and reinforcement learning alternately.
Consequently, our method continuously refines its model and training data, and
consistently obtains considerable performance gains across multiple
mathematical reasoning benchmarks. Our code, models, and datasets will be
released.

</details>


### [61] [Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning](https://arxiv.org/abs/2507.16746)
*Ang Li,Charles Wang,Kaiyu Yue,Zikui Cai,Ollie Liu,Deqing Fu,Peng Guo,Wang Bill Zhu,Vatsal Sharan,Robin Jia,Willie Neiswanger,Furong Huang,Tom Goldstein,Micah Goldblum*

Main category: cs.CV

TL;DR: 研究者提出了包含18.2万样本的视觉思维链数据集Zebra-CoT，通过微调模型在多项任务中实现了12-13%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉思维链模型性能不足及训练数据匮乏的问题，覆盖几何、物理、算法等需要视觉推理的自然场景。

Method: 构建跨科学问题/视觉搜索/三维推理/逻辑游戏的四类任务数据集，对Anole-7B和Bagel-7B模型进行微调。

Result: 模型在测试集准确率提升12%，在标准VLM基准最高提升13%，生成的图文推理链质量显著提高。

Conclusion: 开源数据集和模型为多模态推理能力的发展提供了基础设施支持。

Abstract: Humans often use visual aids, for example diagrams or sketches, when solving
complex problems. Training multimodal models to do the same, known as Visual
Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf
visual CoT performance, which hinders reinforcement learning, and (2) the lack
of high-quality visual CoT training data. We introduce $\textbf{Zebra-CoT}$, a
diverse large-scale dataset with 182,384 samples, containing logically coherent
interleaved text-image reasoning traces. We focus on four categories of tasks
where sketching or visual reasoning is especially natural, spanning scientific
questions such as geometry, physics, and algorithms; 2D visual reasoning tasks
like visual search and jigsaw puzzles; 3D reasoning tasks including 3D
multi-hop inference, embodied and robot planning; visual logic problems and
strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT
training corpus results in an improvement of +12% in our test-set accuracy and
yields up to +13% performance gain on standard VLM benchmark evaluations.
Fine-tuning Bagel-7B yields a model that generates high-quality interleaved
visual reasoning chains, underscoring Zebra-CoT's effectiveness for developing
multimodal reasoning abilities. We open-source our dataset and models to
support development and evaluation of visual CoT.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [62] [Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory](https://arxiv.org/abs/2507.16713)
*Guowei Lan,Kaixian Qu,René Zurbrügg,Changan Chen,Christopher E. Mower,Haitham Bou-Ammar,Marco Hutter*

Main category: cs.RO

TL;DR: ExpTeach框架通过自生成经验记忆增强视觉语言模型在机器人任务中的表现，成功率提升显著且具备通用性


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在迁移到真实机器人时存在经验缺失和空间理解不足的问题

Method: 闭环系统包含自主规划-结果验证-失败反思-行为适应模块，建立长期记忆库并集成图像标注增强空间理解

Result: 反思机制使成功率从36%提升至84%，长期记忆使单次任务成功率从22%提升至80%（含8个未见场景）

Conclusion: 通过经验记忆机制实现了VLM在机器人领域的有效迁移，展示了创造性工具使用等智能交互行为的涌现

Abstract: Vision-language models (VLMs) have been widely adopted in robotics to enable
autonomous planning. However, grounding VLMs, originally trained on internet
data, to diverse real-world robots remains a challenge. This paper presents
ExpTeach, a framework that grounds VLMs to physical robots by building a
self-generated memory of real-world experiences. In ExpTeach, the VLM
autonomously plans actions, verifies outcomes, reflects on failures, and adapts
robot behaviors in a closed loop. The self-generated experiences during this
process are then summarized into a long-term memory, enabling retrieval of
learned knowledge to guide future tasks via retrieval-augmented generation
(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with
an on-demand image annotation module. In experiments, we show that reflection
improves success rates from 36% to 84% on four challenging robotic tasks and
observe the emergence of intelligent object interactions, including creative
tool use. Across extensive tests on 12 real-world scenarios (including eight
unseen ones), we find that grounding with long-term memory boosts single-trial
success rates from 22% to 80%, demonstrating the effectiveness and
generalizability of ExpTeach.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [63] [Characterizing Online Activities Contributing to Suicide Mortality among Youth](https://arxiv.org/abs/2507.16185)
*Aparna Ananthasubramaniam,Elyse J. Thulin,Viktoryia Kalesnikava,Silas Falde,Jonathan Kertawidjaja,Lily Johns,Alejandro Rodríguez-Putnam,Emma Spring,Kara Zivin,Briana Mezuk*

Main category: cs.CY

TL;DR: 研究通过混合方法识别出12种与青少年自杀相关的在线活动类型，并开发零样本学习框架分析其变化规律，发现这些活动与自杀理论阶段、个体特征及COVID-19封锁期相关。


<details>
  <summary>Details</summary>
Motivation: 应对青少年自杀率上升问题，探索网络经历如何影响自杀风险，结合自杀理论和计算模型揭示潜在干预机会。

Method: 分析2013-2022年29,124份死亡调查文本摘要，采用主题分析识别12类在线活动，开发零样本学习框架进行大规模建模。

Result: 发现网络自我伤害、人际互动异常、在线活跃度变化等主题，与年龄/死亡方式/人际问题相关，COVID-19期间多数主题出现频率显著增加。

Conclusion: 数字平台需结合自杀理论和计算模型，开发针对非显性自杀风险指标的干预措施，尤其在网络活动模式识别方面具有重要应用潜力。

Abstract: The recent rise in youth suicide highlights the urgent need to understand how
online experiences contribute to this public health issue. Our mixed-methods
approach responds to this challenge by developing a set of themes focused on
risk factors for suicide mortality in online spaces among youth ages 10-24, and
a framework to model these themes at scale. Using 29,124 open text summaries of
death investigations between 2013-2022, we conducted a thematic analysis to
identify 12 types of online activities that were considered by investigators or
next of kin to be relevant in contextualizing a given suicide death. We then
develop a zero-shot learning framework to model these 12 themes at scale, and
analyze variation in these themes by decedent characteristics and over time.
Our work uncovers several online activities related to harm to self, harm to
others, interpersonal interactions, activity levels online, and life events,
which correspond to different phases of suicide risk from two prominent suicide
theories. We find an association between these themes and decedent
characteristics like age, means of death, and interpersonal problems, and many
themes became more prevalent during the 2020 COVID-19 lockdowns. While digital
spaces have taken some steps to address expressions of suicidality online, our
work illustrates the opportunities for developing interventions related to less
explicit indicators of suicide risk by combining suicide theories with
computational research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [64] [Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric](https://arxiv.org/abs/2507.16165)
*Liam Naddell,Marcelo Ponce*

Main category: cs.DC

TL;DR: 开发并行开源程序实现黑洞几何环境下的光线追踪成像，结合数学近似、科学计算库及多种并行计算技术


<details>
  <summary>Details</summary>
Motivation: 解决传统光线追踪方法在黑洞等极端时空几何场景下的计算效率问题，整合科学计算与计算机图形学技术优势

Method: 采用数学近似优化光线路径计算，利用科学计算库（如MPI/OpenMP），结合共享内存与分布式内存混合并行架构

Result: 成功实现可扩展的并行光线追踪框架，验证了在复杂时空几何下的高效可视化能力

Conclusion: 该工作为天体物理可视化提供新工具，并展示跨领域技术融合在科学计算中的有效性

Abstract: Rendering images of black holes by utilizing ray tracing techniques is a
common methodology employed in many aspects of scientific and astrophysical
visualizations. Similarly, general ray tracing techniques are widely used in
areas related to computer graphics. In this work we describe the implementation
of a parallel open-source program that can ray trace images in the presence of
a black hole geometry. We do this by combining a couple of different techniques
usually present in parallel scientific computing, such as, mathematical
approximations, utilization of scientific libraries, shared-memory and
distributed-memory parallelism.

</details>
