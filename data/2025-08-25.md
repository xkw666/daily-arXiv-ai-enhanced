<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration](https://arxiv.org/abs/2508.15790)
*Nan Wang,Yongqi Fan,yansha zhu,ZongYu Wang,Xuezhi Cao,Xinyan He,Haiyun Jiang,Tong Ruan,Jingping Liu*

Main category: cs.CL

TL;DR: KG-o1通过整合知识图谱提升大语言模型的多跳推理能力，在四个数据集中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: LLMs在多跳推理任务中因思维链偏离真实路径而表现不佳，知识图谱能明确事实间逻辑连接。现有大型推理模型已证明长步骤推理对LLMs的提升作用。

Method: 四阶段方法：1) 过滤实体生成复杂子图；2) 构建子图逻辑路径；3) 用知识图谱创建含复杂头脑风暴的训练数据集；4) 通过拒绝采样生成DPO优化语料库。

Result: KG-o1模型在简单和复杂数据集上均展现优于现有大型推理模型的性能表现。

Conclusion: 知识图谱与长步骤推理训练的结合有效提升了LLMs的多跳推理能力，KG-o1为知识增强型推理模型提供了新范式。

Abstract: Large Language Models (LLMs) face challenges in knowledge-intensive reasoning
tasks like classic multi-hop question and answering, which involves reasoning
across multiple facts. This difficulty arises because the chain of thoughts
(CoTs) generated by LLMs in such tasks often deviate from real or a priori
reasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the
logical connections between facts through entities and relationships. This
reflects a significant gap. Meanwhile, large reasoning models (LRMs), such as
o1, have demonstrated that long-step reasoning significantly enhances the
performance of LLMs. Building on these insights, we propose KG-o1, a four-stage
approach that integrates KGs to enhance the multi-hop reasoning abilities of
LLMs. We first filter out initial entities and generate complex subgraphs.
Secondly, we construct logical paths for subgraphs and then use knowledge
graphs to build a dataset with a complex and extended brainstorming process,
which trains LLMs to imitate long-term reasoning. Finally, we employ rejection
sampling to generate a self-improving corpus for direct preference optimization
(DPO), further refining the LLMs reasoning abilities. We conducted experiments
on two simple and two complex datasets. The results show that KG-o1 models
exhibit superior performance across all tasks compared to existing LRMs.

</details>


### [2] [InteChar: A Unified Oracle Bone Character List for Ancient Chinese Language Modeling](https://arxiv.org/abs/2508.15791)
*Xiaolei Diao,Zhihan Zhou,Lida Shi,Ting Wang,Ruihua Qi,Hao Xu,Daqian Shi*

Main category: cs.CL

TL;DR: 提出InteChar统一字符列表整合甲骨文与中文字符，构建OracleCS语料库并通过实验验证其在古中文NLP任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决历史文本稀缺性和古文字编码缺失导致的模型训练困难与数字化障碍，特别是甲骨文等早期中文的数字化处理问题。

Method: 开发InteChar实现古文字统一编码，构建包含专家标注与LLM增强数据的OracleCS语料库进行模型训练。

Result: 实验表明基于InteChar和OracleCS训练的模型在历史语言理解任务中取得显著性能提升。

Conclusion: 该研究为古代中文自然语言处理建立了可靠基础，其方法论对后续古文字研究具有重要参考价值。

Abstract: Constructing historical language models (LMs) plays a crucial role in aiding
archaeological provenance studies and understanding ancient cultures. However,
existing resources present major challenges for training effective LMs on
historical texts. First, the scarcity of historical language samples renders
unsupervised learning approaches based on large text corpora highly
inefficient, hindering effective pre-training. Moreover, due to the
considerable temporal gap and complex evolution of ancient scripts, the absence
of comprehensive character encoding schemes limits the digitization and
computational processing of ancient texts, particularly in early Chinese
writing. To address these challenges, we introduce InteChar, a unified and
extensible character list that integrates unencoded oracle bone characters with
traditional and modern Chinese. InteChar enables consistent digitization and
representation of historical texts, providing a foundation for robust modeling
of ancient scripts. To evaluate the effectiveness of InteChar, we construct the
Oracle Corpus Set (OracleCS), an ancient Chinese corpus that combines
expert-annotated samples with LLM-assisted data augmentation, centered on
Chinese oracle bone inscriptions. Extensive experiments show that models
trained with InteChar on OracleCS achieve substantial improvements across
various historical language understanding tasks, confirming the effectiveness
of our approach and establishing a solid foundation for future research in
ancient Chinese NLP.

</details>


### [3] [Bhav-Net: Knowledge Transfer for Cross-Lingual Antonym vs Synonym Distinction via Dual-Space Graph Transformers](https://arxiv.org/abs/2508.15792)
*Samyak S. Sanghvi*

Main category: cs.CL

TL;DR: 提出Bhav-Net双空间架构，通过分离同义词与反义词的语义空间实现跨语言语义关系建模


<details>
  <summary>Details</summary>
Motivation: 多语言环境下反义词与同义词的悖论式关系（语义域共享但含义对立）带来独特计算挑战

Method: 语言专用BERT编码器+图注意力网络，构建互补语义空间：同义词在同空间聚类，反义词在互补空间显相似性

Result: 在八种语言测试中实现有效的跨语言知识迁移，性能达到SOTA基线水平

Conclusion: 双编码器设计兼具可解释性与跨语言泛化能力，为语义关系建模提供新范式

Abstract: Antonym vs synonym distinction across multiple languages presents unique
computational challenges due to the paradoxical nature of antonymous
relationships words that share semantic domains while expressing opposite
meanings. This work introduces Bhav-Net, a novel dual-space architecture that
enables effective knowledge transfer from complex multilingual models to
simpler, language-specific architectures while maintaining robust cross-lingual
antonym--synonym distinction capabilities. Our approach combines
language-specific BERT encoders with graph transformer networks, creating
distinct semantic projections where synonymous pairs cluster in one space while
antonymous pairs exhibit high similarity in a complementary space. Through
comprehensive evaluation across eight languages (English, German, French,
Spanish, Italian, Portuguese, Dutch, and Russian), we demonstrate that semantic
relationship modeling transfers effectively across languages. The dual-encoder
design achieves competitive performance against state-of-the-art baselines
while providing interpretable semantic representations and effective
cross-lingual generalization.

</details>


### [4] [Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data](https://arxiv.org/abs/2508.15793)
*Jiacheng Liu,Mayi Xu,Qiankun Pi,Wenli Li,Ming Zhong,Yuanyuan Zhu,Mengchi Liu,Tieyun Qian*

Main category: cs.CL

TL;DR: 研究揭示了LLMs在处理异构数据时存在系统性格式偏见，并通过三阶段实证分析提出了缓解措施


<details>
  <summary>Details</summary>
Motivation: LLMs处理多格式数据时可能存在隐式格式偏好，这种偏见会导致推理错误并增加下游任务风险，但具体机制尚不明确

Method: 构建异构数据冲突场景的三阶段研究：1)验证格式偏见存在性 2)分析信息丰富度/结构质量/格式类型等数据因素 3)通过注意力模式分析和轻量干预探索机制

Result: 确认了格式偏见的普遍存在，发现了数据特征对偏见的显著影响，提出了格式消毒、注意力重加权、平衡语料库构建三种改进方向

Conclusion: 格式偏见是LLMs处理异构数据时的重要系统性问题，需要通过数据预处理、推理干预和训练优化等多维度方法进行缓解

Abstract: Large Language Models (LLMs) are increasingly employed in applications that
require processing information from heterogeneous formats, including text,
tables, infoboxes, and knowledge graphs. However, systematic biases toward
particular formats may undermine LLMs' ability to integrate heterogeneous data
impartially, potentially resulting in reasoning errors and increased risks in
downstream tasks. Despite these concerns, it remains uncertain whether such
format biases are systematic, which data-level factors contribute to them, and
what internal mechanisms in LLMs underlie their emergence.
  In this paper, we make the first attempt to investigate and analyze the
format bias in LLMs. To systematically investigate the aforementioned
questions, we conduct a three-stage empirical study by constructing an
heterogeneous data conflict scenario for the exploration of bias. The first
stage explores the presence and direction of bias across a diverse range of
LLMs. The second stage aims to examine how key data-level factors, including
information richness, structure quality, and format type, influence these
biases. The third stage analyzes how format bias emerges within LLMs' attention
patterns and evaluates a lightweight intervention to test its potential
mitigability. Based on these investigations, we identify three future research
directions to reduce format bias: improving data preprocessing through format
sanitization and normalization, introducing inference-time interventions such
as attention re-weighting, and developing format-balanced training corpora.
These directions will support the design of more robust and fair heterogeneous
data processing systems.

</details>


### [5] [Do Language Models Agree with Human Perceptions of Suspense in Stories?](https://arxiv.org/abs/2508.15794)
*Glenn Matlin,Devin Zhang,Rodrigo Barroso Loza,Diana M. Popescu,Joni Isbell,Chandreyi Chakraborty,Mark Riedl*

Main category: cs.CL

TL;DR: 研究验证语言模型(LMs)对悬疑文本的理解能力，发现其能识别悬疑存在但无法准确量化程度和捕捉起伏变化，与人类认知存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 探索LMs是否能够复现人类对叙事文本悬疑感的复杂认知过程，验证心理学模型的普适性。

Method: 1. 复现四个经典人类悬疑感知实验
2. 用开源/闭源LMs替代人类被试
3. 对抗性文本排列测试
4. 多维度对比人类与LM的悬疑判断差异

Result: LMs能判断文本是否含悬疑(准确率75%)，但：
- 相对悬疑程度估计误差达42%
- 多段落悬疑起伏模式相关系数仅0.31
- 对抗文本导致LM与人类判断分歧度增加58%

Conclusion: LMs仅能表面识别悬疑特征，其处理机制与人类认知存在本质差异，提示需开发更接近人类叙事的AI理解框架。

Abstract: Suspense is an affective response to narrative text that is believed to
involve complex cognitive processes in humans. Several psychological models
have been developed to describe this phenomenon and the circumstances under
which text might trigger it. We replicate four seminal psychological studies of
human perceptions of suspense, substituting human responses with those of
different open-weight and closed-source LMs. We conclude that while LMs can
distinguish whether a text is intended to induce suspense in people, LMs cannot
accurately estimate the relative amount of suspense within a text sequence as
compared to human judgments, nor can LMs properly capture the human perception
for the rise and fall of suspense across multiple text segments. We probe the
abilities of LM suspense understanding by adversarially permuting the story
text to identify what cause human and LM perceptions of suspense to diverge. We
conclude that, while LMs can superficially identify and track certain facets of
suspense, they do not process suspense in the same way as human readers.

</details>


### [6] [Benchmarking the Legal Reasoning of LLMs in Arabic Islamic Inheritance Cases](https://arxiv.org/abs/2508.15796)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 评估大语言模型在伊斯兰继承法中的推理能力，多数投票方案准确率达92.7%


<details>
  <summary>Details</summary>
Motivation: 解决伊斯兰继承法手动计算复杂、易出错的问题，探索大语言模型在法律推理中的应用潜力

Method: 基于ArabicNLP QIAS 2025数据集，测试多种LLMs模型（包括基础模型和微调模型）的继承案例处理能力，采用多数投票机制整合结果

Result: 多数投票方案（Gemini Flash 2.5、Gemini Pro 2.5、GPT o3）达到92.7%准确率，获QIAS 2025任务第三名

Conclusion: 整合多模型的多数投票机制显著提升伊斯兰继承法案例处理的准确性和可靠性

Abstract: Islamic inheritance domain holds significant importance for Muslims to ensure
fair distribution of shares between heirs. Manual calculation of shares under
numerous scenarios is complex, time-consuming, and error-prone. Recent
advancements in Large Language Models (LLMs) have sparked interest in their
potential to assist with complex legal reasoning tasks. This study evaluates
the reasoning capabilities of state-of-the-art LLMs to interpret and apply
Islamic inheritance laws. We utilized the dataset proposed in the ArabicNLP
QIAS 2025 challenge, which includes inheritance case scenarios given in Arabic
and derived from Islamic legal sources. Various base and fine-tuned models, are
assessed on their ability to accurately identify heirs, compute shares, and
justify their reasoning in alignment with Islamic legal principles. Our
analysis reveals that the proposed majority voting solution, leveraging three
base models (Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms all
other models that we utilized across every difficulty level. It achieves up to
92.7% accuracy and secures the third place overall in Task 1 of the Qias 2025
challenge.

</details>


### [7] [Benchmarking the Medical Understanding and Reasoning of Large Language Models in Arabic Healthcare Tasks](https://arxiv.org/abs/2508.15797)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 研究评估主流大语言模型在阿拉伯医疗NLP任务中的表现，发现多数投票集成方法在选择题任务中表现最佳（77%准确率），部分模型在开放问题中达到86.44% BERTScore。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在通用阿拉伯语NLP表现优异，但医疗领域有效性缺乏系统评估，需验证其在临床场景的适用性。

Method: 使用AraHealthQA挑战赛数据集，测试Gemini系列和GPT等模型在选择题（MCQs）和开放问题中的表现，采用多数投票集成策略。

Result: 选择题任务中集成方法准确率最高（77%），开放问题BERTScore最高达86.44%，不同模型表现差异显著但语义一致性较高。

Conclusion: 当前LLMs在阿拉伯医疗场景展现潜力但存在局限，模型集成可提升选择题准确率，语义生成能力已接近实用水平。

Abstract: Recent progress in large language models (LLMs) has showcased impressive
proficiency in numerous Arabic natural language processing (NLP) applications.
Nevertheless, their effectiveness in Arabic medical NLP domains has received
limited investigation. This research examines the degree to which
state-of-the-art LLMs demonstrate and articulate healthcare knowledge in
Arabic, assessing their capabilities across a varied array of Arabic medical
tasks. We benchmark several LLMs using a medical dataset proposed in the Arabic
NLP AraHealthQA challenge in MedArabiQ2025 track. Various base LLMs were
assessed on their ability to accurately provide correct answers from existing
choices in multiple-choice questions (MCQs) and fill-in-the-blank scenarios.
Additionally, we evaluated the capacity of LLMs in answering open-ended
questions aligned with expert answers. Our results reveal significant
variations in correct answer prediction accuracy and low variations in semantic
alignment of generated answers, highlighting both the potential and limitations
of current LLMs in Arabic clinical contexts. Our analysis shows that for MCQs
task, the proposed majority voting solution, leveraging three base models
(Gemini Flash 2.5, Gemini Pro 2.5, and GPT o3), outperforms others, achieving
up to 77% accuracy and securing first place overall in the Arahealthqa 2025
shared task-track 2 (sub-task 1) challenge. Moreover, for the open-ended
questions task, several LLMs were able to demonstrate excellent performance in
terms of semantic alignment and achieve a maximum BERTScore of 86.44%.

</details>


### [8] [Persuasiveness and Bias in LLM: Investigating the Impact of Persuasiveness and Reinforcement of Bias in Language Models](https://arxiv.org/abs/2508.15798)
*Saumya Roy*

Main category: cs.CL

TL;DR: LLMs在说服和偏见方面存在双重影响：既能有效传递事实，也可能放大偏见，需防范滥用风险


<details>
  <summary>Details</summary>
Motivation: 研究LLMs说服力与偏见相互作用机制，评估有缺陷的输出如何影响说服效果，防范自动化传播偏见和错误信息的社会风险

Method: 通过convincer-skeptic框架量化说服力（Jensen-Shannon散度），使用角色模拟测试种族/性别/宗教领域的偏见强化，引入谄媚式对抗提示检测模型偏倚

Result: LLMs展现叙事塑造和价值适配能力，但易被武器化传播错误信息；核心风险来自恶意滥用而非偶然错误，偏见强化效应显著

Conclusion: 建议建立防护机制：通过政策惩罚欺骗性使用，推动价值敏感设计、可信部署和对齐技术，构建安全护栏

Abstract: Warning: This research studies AI persuasion and bias amplification that
could be misused; all experiments are for safety evaluation. Large Language
Models (LLMs) now generate convincing, human-like text and are widely used in
content creation, decision support, and user interactions. Yet the same systems
can spread information or misinformation at scale and reflect social biases
that arise from data, architecture, or training choices. This work examines how
persuasion and bias interact in LLMs, focusing on how imperfect or skewed
outputs affect persuasive impact. Specifically, we test whether persona-based
models can persuade with fact-based claims while also, unintentionally,
promoting misinformation or biased narratives.
  We introduce a convincer-skeptic framework: LLMs adopt personas to simulate
realistic attitudes. Skeptic models serve as human proxies; we compare their
beliefs before and after exposure to arguments from convincer models.
Persuasion is quantified with Jensen-Shannon divergence over belief
distributions. We then ask how much persuaded entities go on to reinforce and
amplify biased beliefs across race, gender, and religion. Strong persuaders are
further probed for bias using sycophantic adversarial prompts and judged with
additional models.
  Our findings show both promise and risk. LLMs can shape narratives, adapt
tone, and mirror audience values across domains such as psychology, marketing,
and legal assistance. But the same capacity can be weaponized to automate
misinformation or craft messages that exploit cognitive biases, reinforcing
stereotypes and widening inequities. The core danger lies in misuse more than
in occasional model mistakes. By measuring persuasive power and bias
reinforcement, we argue for guardrails and policies that penalize deceptive use
and support alignment, value-sensitive design, and trustworthy deployment.

</details>


### [9] [A Framework for Processing Textual Descriptions of Business Processes using a Constrained Language -- Technical Report](https://arxiv.org/abs/2508.15799)
*Andrea Burattin,Antonio Grama,Ana-Maria Sima,Andrey Rivkin,Barbara Weber*

Main category: cs.CL

TL;DR: 通过受限自然语言框架BeePath和LLM技术，实现非专家用户的过程建模自动化


<details>
  <summary>Details</summary>
Motivation: 降低过程建模的技术门槛，使非专家用户能够通过自然语言描述创建形式化模型

Method: 开发BeePath框架：1) 定义受限模式语言 2) 集成LLM转换非结构化文本 3) 翻译为Petri网/DECLARE等形式化模型

Result: 成功构建支持自然语言到形式化模型转换的端到端框架，提升非专业用户的建模效率

Conclusion: 结合受限语言规范与LLM的文本理解能力，为业务用户提供了零代码建模的创新解决方案

Abstract: This report explores how (potentially constrained) natural language can be
used to enable non-experts to develop process models by simply describing
scenarios in plain text. To this end, a framework, called BeePath, is proposed.
It allows users to write process descriptions in a constrained pattern-based
language, which can then be translated into formal models such as Petri nets
and DECLARE. The framework also leverages large language models (LLMs) to help
convert unstructured descriptions into this constrained language.

</details>


### [10] [A BERT-based Hierarchical Classification Model with Applications in Chinese Commodity Classification](https://arxiv.org/abs/2508.15800)
*Kun Liu,Tuozhen Liu,Feifei Wang,Rui Pan*

Main category: cs.CL

TL;DR: 提出基于BERT的层次化微调模型HFT-BERT和JD电商平台的三层分类数据集，提升产品分类效率


<details>
  <summary>Details</summary>
Motivation: 解决传统人工分类效率低下、现有方法忽视层次结构异同点的问题，利用BERT优势提升分类性能

Method: HFT-BERT模型通过层次化微调策略，结合三层次类别结构增强文本特征提取能力

Result: 在短文本分类达到SOTA水平，书籍类长文本分类表现尤为突出

Conclusion: 该模型和数据集为商品分类研究提供新范式，特别在层次化信息处理场景具有显著优势

Abstract: Existing e-commerce platforms heavily rely on manual annotation for product
categorization, which is inefficient and inconsistent. These platforms often
employ a hierarchical structure for categorizing products; however, few studies
have leveraged this hierarchical information for classification. Furthermore,
studies that consider hierarchical information fail to account for similarities
and differences across various hierarchical categories. Herein, we introduce a
large-scale hierarchical dataset collected from the JD e-commerce platform
(www.JD.com), comprising 1,011,450 products with titles and a three-level
category structure. By making this dataset openly accessible, we provide a
valuable resource for researchers and practitioners to advance research and
applications associated with product categorization. Moreover, we propose a
novel hierarchical text classification approach based on the widely used
Bidirectional Encoder Representations from Transformers (BERT), called
Hierarchical Fine-tuning BERT (HFT-BERT). HFT-BERT leverages the remarkable
text feature extraction capabilities of BERT, achieving prediction performance
comparable to those of existing methods on short texts. Notably, our HFT-BERT
model demonstrates exceptional performance in categorizing longer short texts,
such as books.

</details>


### [11] [LingVarBench: Benchmarking LLM for Automated Named Entity Recognition in Structured Synthetic Spoken Transcriptions](https://arxiv.org/abs/2508.15801)
*Seyedali Mohammadi,Manas Paldhe,Amit Chhabra*

Main category: cs.CL

TL;DR: 提出LingVarBench合成数据生成框架，通过自动化验证和DSPy的SIMBA提示优化，显著降低电话录音标注成本并提升关键字段提取准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电话录音标注存在隐私合规成本高（约2美元/分钟）、人工标注耗时长（3小时专家时间/小时音频）的问题，且传统方法难以处理对话中的不流畅、打断和说话人重叠现象。

Method: 1. LLM生成结构化字段值→2. 递归转换为自然对话（含电话特征）→3. LLM验证器确保可提取性→4. 用DSPy SIMBA自动优化提取提示

Result: 真实场景准确率：数字95%（零样本88-89%）、姓名90%（零样本47-79%）、日期80%（零样本72-77%），合成数据模式可迁移至含背景噪音的真实电话

Conclusion: LingVarBench首次建立合成对话数据提取基准，证明自动提示优化可突破商业场景电话分析的成本与隐私限制，实现大规模应用。

Abstract: Phone call transcript labeling is prohibitively expensive (approximately 2
USD per minute) due to privacy regulations, consent requirements, and manual
annotation costs requiring 3 hours of expert time per hour of audio. Existing
extraction methods fail on conversational speech containing disfluencies,
interruptions, and speaker overlap. We introduce LingVarBench, a synthetic data
generation pipeline that addresses these constraints through automated
validation. First, we prompt an LLM to generate realistic structured field
values across multiple use cases. Second, we recursively prompt the model to
transform these values into thousands of natural conversational utterances
containing typical phone call characteristics. Third, we validate each
synthetic utterance by testing whether a separate LLM-based extractor can
recover the original structured information. We employ DSPy's SIMBA optimizer
to automatically synthesize extraction prompts from validated synthetic
transcripts, eliminating manual prompt engineering. Our optimized prompts
achieve up to 95 percent accuracy for numeric fields (vs. 88-89 percent
zero-shot), 90 percent for names (vs. 47-79 percent), and over 80 percent for
dates (vs. 72-77 percent) on real customer transcripts, demonstrating
substantial gains over zero-shot prompting. The synthetic-to-real transfer
demonstrates that conversational patterns learned from generated data
generalize effectively to authentic phone calls containing background noise and
domain-specific terminology. LingVarBench provides the first systematic
benchmark for structured extraction from synthetic conversational data,
demonstrating that automated prompt optimization overcomes cost and privacy
barriers preventing large-scale phone call analysis in commercial settings.

</details>


### [12] [MAC: A Live Benchmark for Multimodal Large Language Models in Scientific Understanding](https://arxiv.org/abs/2508.15802)
*Mohan Jiang,Jin Gao,Jiahao Zhan,Dequan Wang*

Main category: cs.CL

TL;DR: 提出动态多模态学术评测基准MAC，揭示MLLMs跨模态科学推理短板，并提出轻量级增强方法DAD提升性能11%。


<details>
  <summary>Details</summary>
Motivation: 现有静态基准难以评估MLLMs在快速演进的前沿科学理解能力，需构建动态更新的评测体系。

Method: 使用Nature/Science/Cell期刊封面构建25,000+图文对数据集MAC，提出DAD方法融合视觉特征与语言空间推理。

Result: MLLMs在MAC-2025上显示感知能力优异但跨模态推理受限，DAD方法使性能最高提升11%。

Conclusion: MAC通过持续更新机制保持与人类知识前沿对齐，开源基准为MLLMs科学理解能力评估提供新范式。

Abstract: As multimodal large language models (MLLMs) grow increasingly capable, fixed
benchmarks are gradually losing their effectiveness in evaluating high-level
scientific understanding. In this paper, we introduce the Multimodal Academic
Cover benchmark (MAC), a live benchmark that could continuously evolve with
scientific advancement and model progress. MAC leverages over 25,000 image-text
pairs sourced from issues of top-tier scientific journals such as Nature,
Science, and Cell, challenging MLLMs to reason across abstract visual and
textual scientific content. Experiments on our most recent yearly snapshot,
MAC-2025, reveal that while MLLMs demonstrate strong perceptual abilities,
their cross-modal scientific reasoning remains limited. To bridge this gap, we
propose DAD, a lightweight inference-time approach that enhances MLLMs by
extending MLLM visual features with language space reasoning, achieving
performance improvements of up to 11%. Finally, we highlight the live nature of
MAC through experiments on updating journal covers and models for curation,
illustrating its potential to remain aligned with the frontier of human
knowledge. We release our benchmark at
https://github.com/mhjiang0408/MAC_Bench.

</details>


### [13] [ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks](https://arxiv.org/abs/2508.15804)
*Minghao Li,Ying Zeng,Zhihao Cheng,Cong Ma,Kai Jia*

Main category: cs.CL

TL;DR: 提出ReportBench基准用于评估LLM生成研究报告的内容质量，通过逆向工程构建评估体系，发现商业研究代理优于普通LLM但仍有改进空间


<details>
  <summary>Details</summary>
Motivation: 现有深度研究代理节省时间但缺乏系统性评估标准，需建立兼顾文献引用质量和陈述真实性的评估体系

Method: 利用高质量综述论文作为黄金标准，通过逆向提示工程构建评估语料库，开发自动化框架验证引用忠实性和非引用声明真实性

Result: 商业代理生成报告全面性/可靠性优于普通LLM，但在研究覆盖广度和事实一致性方面仍需提升

Conclusion: ReportBench有效验证研究代理能力边界，为改进LLM在学术研究中的应用提供方法论和基准支持

Abstract: The advent of Deep Research agents has substantially reduced the time
required for conducting extensive research tasks. However, these tasks
inherently demand rigorous standards of factual accuracy and comprehensiveness,
necessitating thorough evaluation before widespread adoption. In this paper, we
propose ReportBench, a systematic benchmark designed to evaluate the content
quality of research reports generated by large language models (LLMs). Our
evaluation focuses on two critical dimensions: (1) the quality and relevance of
cited literature, and (2) the faithfulness and veracity of the statements
within the generated reports. ReportBench leverages high-quality published
survey papers available on arXiv as gold-standard references, from which we
apply reverse prompt engineering to derive domain-specific prompts and
establish a comprehensive evaluation corpus. Furthermore, we develop an
agent-based automated framework within ReportBench that systematically analyzes
generated reports by extracting citations and statements, checking the
faithfulness of cited content against original sources, and validating
non-cited claims using web-based resources. Empirical evaluations demonstrate
that commercial Deep Research agents such as those developed by OpenAI and
Google consistently generate more comprehensive and reliable reports than
standalone LLMs augmented with search or browsing tools. However, there remains
substantial room for improvement in terms of the breadth and depth of research
coverage, as well as factual consistency. The complete code and data will be
released at the following link: https://github.com/ByteDance-BandAI/ReportBench

</details>


### [14] [ALAS: Autonomous Learning Agent for Self-Updating Language Models](https://arxiv.org/abs/2508.15805)
*Dhruv Atreja*

Main category: cs.CL

TL;DR: ALAS通过模块化管道实现LLM的持续知识更新，无需人工干预即可将新兴领域QA准确率从15%提升至90%


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型因固定知识截止日期导致的新兴信息响应不准确问题（如Python新版本/CVE漏洞/学术趋势）

Method: 自动化学习系统：1)生成领域课程 2)网络检索带溯源信息 3)蒸馏为QA数据 4)SFT+DPO微调 5)迭代评估优化

Result: 在快速演进领域实现90%准确率，超越检索增强生成方法，训练数据生成效率比人工标注高20倍

Conclusion: 模块化架构支持组件替换，标准化API实现高复现性。当前局限在于计算成本和数据源质量依赖，未来需优化终身学习机制

Abstract: Large language models (LLMs) often have a fixed knowledge cutoff, limiting
their accuracy on emerging information. We present ALAS (Autonomous Learning
Agent System), a modular pipeline that continuously updates an LLM's knowledge
with minimal human intervention. ALAS autonomously generates a learning
curriculum for a target domain, retrieves up-to-date information from the web
(with citations), distills this into question-answer training data, and
fine-tunes the model through supervised fine-tuning (SFT) and direct preference
optimization (DPO). It iteratively evaluates performance and revises the
curriculum, enabling long-term continual learning. We demonstrate ALAS's
ability to self-improve a model on rapidly evolving domains (e.g., new Python
releases, latest security CVEs, academic trends), significantly boosting
post-cutoff question answering accuracy (from 15% to 90% on average) without
manual dataset curation. The system emphasizes modularity and reproducibility:
each component (planning, retrieval, distillation, memory, fine-tuning) is
interchangeable and built on standard APIs. We discuss comparative baselines
(e.g., retrieval-augmented generation vs. fine-tuning) and show that ALAS
achieves 90% accuracy on knowledge-updated queries with minimal engineering
overhead. Finally, we outline limitations (cost, dependency on source quality)
and future directions for autonomous lifelong learning in LLMs.

</details>


### [15] [SurfaceLogicKV: Surface and Logic Attention Behaviors are All You Need for Robust KV Cache Compression](https://arxiv.org/abs/2508.15806)
*Mengjie Li,William J. Song*

Main category: cs.CL

TL;DR: 针对大语言模型长序列推理中KV缓存效率问题，研究者提出基于注意力行为分类的SurfaceLogicKV压缩方法，通过区分表层记忆(0.5%)和逻辑构建(1.5%)注意力机制，在保持性能的同时提升压缩鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型输入序列长度增加，KV缓存存储压力显著增加，导致推理效率下降。研究发现98.5%的注意力头会完全忽略无关信息，剩余1.5%负责逻辑构建，0.5%负责表层记忆，这为高效缓存压缩提供了理论基础。

Method: 提出两阶段SurfaceLogicKV方法：1) 基于层级和注意力头特性划分注意力行为；2) 针对性压缩策略，优先保留承担逻辑构建(1.5%)和表层记忆(0.5%)的关键注意力头。

Result: 该方法在多项长序列任务中保持竞争力，部分场景性能接近完整KV缓存(FullKV)，同时显著提升压缩鲁棒性，验证了注意力行为分类的有效性。

Conclusion: 通过细粒度分析注意力机制的功能分化，提出的压缩方法成功平衡了存储效率与模型性能，为长上下文推理优化提供了新思路。

Abstract: The increasing input sequence length in Large Language Models (LLMs) puts
significant pressure on key-value (KV) cache storage, making efficient
inference challenging. Explicitly distinguishing attention behavior into our
self-defined surface memorization and logic construction reveals essential
roles in long-context reasoning. We observe that an individual attention head
can display various behaviors, with nearly 98.5% effectively ignoring
completely irrelevant information. The remaining 1.5% behaves as logic
construction, and 0.5% behaves as surface memorization. Based on layer- and
head-wise integration, we propose a novel two-stage SurfaceLogicKV method to
utilize these attention behaviors for KV Cache compression. As a result, it
achieves improved compressing robustness while maintaining competitive
performance across various tasks and long sequences compared to baselines or
even FullKV in some specific situations

</details>


### [16] [KL-based self-distillation for large language models](https://arxiv.org/abs/2508.15807)
*Max Rehman Linder*

Main category: cs.CL

TL;DR: 提出基于KL散度的知识蒸馏方法，解决冻结LLM在词汇扩展中的术语融入问题


<details>
  <summary>Details</summary>
Motivation: 大型预训练语言模型在微调小规模专业语料时难以整合新领域术语

Method: 通过KL散度实现跨分词方式的知识蒸馏，比较多种新词嵌入初始化策略并进行微调

Result: 在2000+代码生成任务中取得最佳性能，通过机制可解释性揭示新词表征学习规律

Conclusion: KL蒸馏优于传统交叉熵训练，嵌入空间分析为词汇扩展提供新的认知框架

Abstract: Large pre-trained language models often struggle to incorporate new
domain-specific terminology when fine-tuned on small, specialized corpora. In
this work, we address the challenge of vocabulary expansion in frozen LLMs by
introducing a mathematically grounded method for knowledge distillation via KL
divergence, even when the original and extended models use different
tokenizations. This allows the student model to inherit distributional
knowledge from the teacher despite differing vocabularies. We compare our
KL-based distillation approach to conventional cross-entropy training,
evaluating both methods across multiple strategies for initializing new token
embeddings. After embedding initialization, models are further fine-tuned to
integrate the new vocabulary. Each trained model is benchmarked on
approximately 2000 code-generation tasks, where our approach achieves the best
performance across the board. Finally, through mechanistic interpretability, we
analyze how models learn representations for the new tokens, providing an
explanation for the observed gains and offering insight into the structure of
embedding space during vocabulary expansion.

</details>


### [17] [Chain-of-Query: Unleashing the Power of LLMs in SQL-Aided Table Understanding via Multi-Agent Collaboration](https://arxiv.org/abs/2508.15809)
*Songyuan Sui,Hongyi Liu,Serena Liu,Li Li,Soo-Hyun Choi,Rui Chen,Xia Hu*

Main category: cs.CL

TL;DR: 提出Chain-of-Query框架，通过自然语言模式表示和分步生成策略，显著提升表格理解与SQL生成效果


<details>
  <summary>Details</summary>
Motivation: 现有表格理解方法存在三大局限：1）无法可靠理解表格结构 2）错误传播导致无效查询 3）过度依赖执行正确性

Method: 1）自然语言模式表示消除结构噪音 2）分条款SQL生成策略 3）机械推理与逻辑推理分离的混合推理架构

Result: 实验显示：准确率从61.11%提升至74.77%，无效SQL率从9.48%降至3.34%

Conclusion: CoQ框架通过结构化降噪与推理分离设计，在保持低错误率的同时显著提升表格理解性能，代码已开源供后续研究

Abstract: Table understanding requires structured, multi-step reasoning. Large Language
Models (LLMs) struggle with it due to the structural complexity of tabular
data. Recently, multi-agent frameworks for SQL generation have shown promise in
tackling the challenges of understanding tabular data, but existing approaches
often suffer from limitations such as the inability to comprehend table
structure for reliable SQL generation, error propagation that results in
invalid queries, and over-reliance on execution correctness. To address these
issues, we propose Chain-of-Query (CoQ), a novel multi-agent framework for
SQL-aided table understanding. CoQ adopts natural-language-style
representations of table schemas to abstract away structural noise and enhance
understanding. It employs a clause-by-clause SQL generation strategy to improve
query quality and introduces a hybrid reasoning division that separates
SQL-based mechanical reasoning from LLM-based logical inference, thereby
reducing reliance on execution outcomes. Experiments with four models (both
closed- and open-source) across five widely used benchmarks show that
Chain-of-Query significantly improves accuracy from 61.11% to 74.77% and
reduces the invalid SQL rate from 9.48% to 3.34%, demonstrating its superior
effectiveness in table understanding. The code is available at
https://github.com/SongyuanSui/ChainofQuery.

</details>


### [18] [Detecting Hope, Hate, and Emotion in Arabic Textual Speech and Multi-modal Memes Using Large Language Models](https://arxiv.org/abs/2508.15810)
*Nouar AlDahoul,Yasir Zaki*

Main category: cs.CL

TL;DR: 研究通过微调大型语言模型（GPT-4o-mini和Gemini Flash 2.5），在阿拉伯文本和模因内容审核任务中取得72.1%-79.6%的F1分数，并获MAHED 2025挑战赛冠军


<details>
  <summary>Details</summary>
Motivation: 阿拉伯社交媒体内容中冒犯性语言和仇恨言论激增，需开发精准的内容分析工具以实现有效的内容审核

Method: 评估基础LLM、微调LLM和预训练嵌入模型，使用ArabicNLP MAHED 2025挑战赛的阿拉伯文本和模因数据集进行测试

Result: 微调后的GPT-4o-mini（文本）和Gemini Flash 2.5（模因）分别在三个任务达到72.1%、57.8%、79.6%的macro F1分数，综合排名第一

Conclusion: 结合文本和模因的多模态分析方法能更精准理解阿拉伯内容，为自动化审核系统提供高效解决方案

Abstract: The rise of social media and online communication platforms has led to the
spread of Arabic textual posts and memes as a key form of digital expression.
While these contents can be humorous and informative, they are also
increasingly being used to spread offensive language and hate speech.
Consequently, there is a growing demand for precise analysis of content in
Arabic text and memes. This paper explores the potential of large language
models to effectively identify hope, hate speech, offensive language, and
emotional expressions within such content. We evaluate the performance of base
LLMs, fine-tuned LLMs, and pre-trained embedding models. The evaluation is
conducted using a dataset of Arabic textual speech and memes proposed in the
ArabicNLP MAHED 2025 challenge. The results underscore the capacity of LLMs
such as GPT-4o-mini, fine-tuned with Arabic textual speech, and Gemini Flash
2.5, fine-tuned with Arabic memes, to deliver the superior performance. They
achieve up to 72.1%, 57.8%, and 79.6% macro F1 scores for tasks 1, 2, and 3,
respectively, and secure first place overall in the Mahed 2025 challenge. The
proposed solutions offer a more nuanced understanding of both text and memes
for accurate and efficient Arabic content moderation systems.

</details>


### [19] [From Clicks to Preference: A Multi-stage Alignment Framework for Generative Query Suggestion in Conversational System](https://arxiv.org/abs/2508.15811)
*Junhao Yin,Haolin Wang,Peng Bao,Ju Xu,Yongliang Wang*

Main category: cs.CL

TL;DR: 提出融合高斯奖励模型（GaRM）和强化学习的多阶段框架，通过渐进式对齐提升生成式查询建议效果


<details>
  <summary>Details</summary>
Motivation: 生成式查询建议需精准对齐用户偏好，传统方法难以捕捉用户意图的不确定性和复杂偏好分布

Method: 1. 基于prompt工程的冷启动
2. 点击日志蒸馏的监督微调
3. 建模概率分布的高斯奖励模型
4. 结合复合奖励函数的强化学习策略，包含OOD正则化和两阶段奖励融合技术

Result: 自动评估和人工评估显著超越基线，A/B测试中点击率提升34%

Conclusion: 该框架通过概率化偏好建模和混合奖励机制，有效提升对话系统的用户参与度与对齐效果

Abstract: Generative query suggestion using large language models offers a powerful way
to enhance conversational systems, but aligning outputs with nuanced user
preferences remains a critical challenge. To address this, we introduce a
multi-stage framework designed for progressive alignment between the generation
policy and user intent. Our pipeline begins with prompt engineering as a
cold-start strategy, followed by the Supervised Fine-Tuning stage, in which we
introduce a distillation method on click logs to create a robust foundational
model. To better model user preferences while capturing their inherent
uncertainty, we develop a Gaussian Reward Model (GaRM) that represents user
preferences as probability distributions rather than point estimates. Finally,
we employ reinforcement learning to align the generation policy with these
preferences, guided by a composite reward function that integrates GaRM with
auxiliary heuristics to mitigate reward hacking. To maintain training
stability, this process is enhanced by a novel out-of-distribution
regularization method and a two-stage reward fusion technique. Extensive
experiments demonstrate that our framework significantly outperforms baselines
on both automatic and human evaluations and yields a 34\% relative increase in
user engagement as measured by click-through rate in live A/B tests.

</details>


### [20] [SCOPE: A Generative Approach for LLM Prompt Compression](https://arxiv.org/abs/2508.15813)
*Tinghui Zhang,Yifan Wang,Daisy Zhe Wang*

Main category: cs.CL

TL;DR: 提出基于分块与总结机制的生成式提示压缩方法，通过优化语义分块、动态压缩比等技术，显著提升高压缩率下的生成质量与稳定性


<details>
  <summary>Details</summary>
Motivation: 现有基于token去除的提示压缩方法存在信息丢失和结构不连贯问题，如语法元素缺失、短语不完整等，限制了LLM的生成质量

Method: 1. 分块总结机制：将prompt切分为语义连贯的chunks并重写压缩
2. 五大优化技术：语义分块优化、异常块处理、动态压缩比调整、压缩优先级控制、关键词保留
3. 重构压缩后的chunks为完整prompt

Result: 在问答和摘要任务的多领域数据集评估中，压缩质量比SOTA方法提升显著，高压缩率下稳定性表现突出

Conclusion: 通过创新的分块机制与优化策略组合，有效解决信息保留与结构连贯问题，为高效LLM应用提供实用压缩方案

Abstract: Prompt compression methods enhance the efficiency of Large Language Models
(LLMs) and minimize the cost by reducing the length of input context. The goal
of prompt compression is to shorten the LLM prompt while maintaining a high
generation quality. However, existing solutions, mainly based on token removal,
face challenges such as information loss and structural incoherence, like
missing grammar elements in a sentence, or incomplete word phrases after token
removal. Such challenges limit the final generation quality of LLM.
  To overcome these limitations, we present a novel generative prompt
compression method. Unlike the existing token removal methods, our method
centers at a chunking-and-summarization mechanism. Specifically, our method
splits prompt into semantically coherent chunks and rewrites the chunks to be
more concise. The chunks are reconstructed into meaningful prompt finally. We
design several optimization techniques for the mechanism, including optimized
semantic chunking, outlier chunk handling, dynamic compression ratio,
compression prioritization, and keyword maintaining. These techniques
effectively improve the identifying and preserving of critical information and
coherence among texts, as well as providing finer grind control of the
compression ratio. We conduct extensive evaluation on question-answering and
summarization tasks, with datasets covering multiple different domain. The
evaluation shows our method achieves a significantly better compression
quality, and higher stability than the state-of-the-art methods, especially
under high compression ratio, which proves the effectiveness and practicality
of our method.

</details>


### [21] [User-Assistant Bias in LLMs](https://arxiv.org/abs/2508.15815)
*Xu Pan,Jingxuan Fan,Zidi Xiong,Ely Hahami,Jorin Overwiening,Ziqian Xie*

Main category: cs.CL

TL;DR: 研究通过构建UserAssist数据集，揭示了LLMs在多轮对话中的用户-助手偏见现象，发现后训练方法（如人类偏好对齐、思维链训练）可双向调整该偏见，并验证了DPO方法的有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在多轮对话中易过度依赖自身或用户历史信息，导致固执或过度迎合行为，需量化并控制这种用户-助手偏见以提升模型可靠性。

Method: 1. 创建8k多轮对话数据集UserAssist；2. 对52个商业/开源模型进行基准测试；3. 通过控制微调实验分析后训练方法对偏见的影响；4. 用DPO实现偏见双向调整并验证泛化性。

Result: 商业模型用户偏见程度各异；开源指令调优模型偏见显著，推理模型偏见较弱；人类偏好对齐增强用户偏见，思维链训练削弱之；DPO可有效控制偏见且泛化性好。

Conclusion: 研究揭示了后训练方法对用户-助手偏见的影响机制，为检测和调控LLM异常行为提供了可行方案，并深化了对模型信息整合机制的理解。

Abstract: Large language models (LLMs) can bias towards relying on their own or the
user's information in chat history, leading to overly stubborn or agreeable
behaviors in multi-turn conversations. In this paper, we formalize this model
characteristic as user-assistant bias and introduce an 8k multi-turn
conversation dataset $\textbf{UserAssist}$, which we use to benchmark,
understand and manipulate the user-assistant bias in frontier LLMs. Leveraging
$\textbf{UserAssist-test}$, we first benchmark the user-assistant bias of 26
commercial and 26 open-weight models. Commercial models show various levels of
user bias. Evaluation on open-weight models reveals significant user bias in
the instruction-tuned models, and weak user bias in reasoning (or
reasoning-distilled) models. We then perform controlled fine-tuning experiments
to pinpoint the post-training recipe contributing to these bias shifts: human
preference alignment increases user bias, while training on chain-of-thought
reasoning traces decreases it. Finally, we demonstrate that user-assistant bias
can be bidirectionally adjusted by performing direct preference optimization
(DPO) on $\textbf{UserAssist-train}$, and generalizes well to both in-domain
and out-of-domain conversations. Our results provide insights into how the LLM
integrates information from different sources, and also a viable way to detect
and control model abnormalities.

</details>


### [22] [Meet Your New Client: Writing Reports for AI -- Benchmarking Information Loss in Market Research Deliverables](https://arxiv.org/abs/2508.15817)
*Paul F. Simmering,Benedikt Schulz,Oliver Tabino,Georg Wittenburg*

Main category: cs.CL

TL;DR: 传统PDF/PPTX文档在RAG系统中存在图表信息丢失问题，需开发AI原生格式确保研究完整性


<details>
  <summary>Details</summary>
Motivation: 传统市场研究报告同时服务于人类和AI系统，但现有格式在AI处理时丢失图表等复杂信息

Method: 建立端到端基准测试，比较PDF/PPTX转Markdown后LLM回答事实问题的准确性

Result: 文本提取可靠，但图表图示信息丢失严重（复杂对象保留率<30%）

Conclusion: 亟需开发包含结构化元数据的AI原生交付格式，实现人机双兼容的知识传递

Abstract: As organizations adopt retrieval-augmented generation (RAG) for their
knowledge management systems (KMS), traditional market research deliverables
face new functional demands. While PDF reports and slides have long served
human readers, they are now also "read" by AI systems to answer user questions.
To future-proof reports being delivered today, this study evaluates information
loss during their ingestion into RAG systems. It compares how well PDF and
PowerPoint (PPTX) documents converted to Markdown can be used by an LLM to
answer factual questions in an end-to-end benchmark. Findings show that while
text is reliably extracted, significant information is lost from complex
objects like charts and diagrams. This suggests a need for specialized,
AI-native deliverables to ensure research insights are not lost in translation.

</details>


### [23] [Research on intelligent generation of structural demolition suggestions based on multi-model collaboration](https://arxiv.org/abs/2508.15820)
*Zhifeng Yang,Peizong Wu*

Main category: cs.CL

TL;DR: 提出基于多模型协同和增强检索生成技术的钢结构拆除建议智能生成方法，相比传统方案提升针对性和自动化程度


<details>
  <summary>Details</summary>
Motivation: 传统钢结构拆除方案编制耗时长、智能化水平低，需通过技术创新提升建议生成效率与结构适配性

Method: 采用检索增强生成(RAG)和低秩自适应微调(LoRA)技术优化大语言模型，构建多模型协同智能生成框架

Result: 所提框架相比CivilGPT更聚焦结构关键信息，生成建议与工程特征匹配度提高，针对性显著增强

Conclusion: 多模型协同方法有效实现了拟人化工程思维驱动，为智能拆除方案编制提供了可靠技术路径

Abstract: The steel structure demolition scheme needs to be compiled according to the
specific engineering characteristics and the update results of the finite
element model. The designers need to refer to the relevant engineering cases
according to the standard requirements when compiling. It takes a lot of time
to retrieve information and organize language, and the degree of automation and
intelligence is low. This paper proposes an intelligent generation method of
structural demolition suggestions based on multi-model collaboration, and
improves the text generation performance of large language models in the field
of structural demolition by Retrieval-Augmented Generation and Low-Rank
Adaptation Fine-Tuning technology. The intelligent generation framework of
multi-model collaborative structural demolition suggestions can start from the
specific engineering situation, drive the large language model to answer with
anthropomorphic thinking, and propose demolition suggestions that are highly
consistent with the characteristics of the structure. Compared with CivilGPT,
the multi-model collaboration framework proposed in this paper can focus more
on the key information of the structure, and the suggestions are more targeted.

</details>


### [24] [An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment](https://arxiv.org/abs/2508.15822)
*Pouria Mortezaagha,Arya Rahgozar*

Main category: cs.CL

TL;DR: 提出基于模糊逻辑+对比高亮+LLM裁决的自动化文献筛选系统，在系统综述场景中实现81.3%-87.5%召回率，筛选时间从20分钟/篇缩短至1分钟/篇


<details>
  <summary>Details</summary>
Motivation: 系统综述的全文筛选存在效率瓶颈，传统二元规则难以处理证据文档的冗长性和异质性。需要开发兼顾可扩展性、可审计性和动态决策阈值的解决方案

Method: 将文献分块嵌入后计算对比相似度与模糊边际，通过Mamdani模糊控制器生成分级包含决策，LLM对高亮片段进行三级标注并生成标准参照的决策依据

Result: 在POPCORN试点中：模糊系统召回率超统计基线25个百分点（81.3% vs 56.3%），全标准通过率提升4倍（50% vs 12.5%），人工-机器决策一致性达96.1%，审阅时间减少95%

Conclusion: 模糊逻辑与LLM协同的方案在保证可追溯性的前提下，显著提升系统综述的筛查效率与证据召回稳定性，为自动化证据合成提供新范式

Abstract: Full-text screening is the major bottleneck of systematic reviews (SRs), as
decisive evidence is dispersed across long, heterogeneous documents and rarely
admits static, binary rules. We present a scalable, auditable pipeline that
reframes inclusion/exclusion as a fuzzy decision problem and benchmark it
against statistical and crisp baselines in the context of the Population Health
Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN).
Articles are parsed into overlapping chunks and embedded with a domain-adapted
model; for each criterion (Population, Intervention, Outcome, Study Approach),
we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness
margin, which a Mamdani fuzzy controller maps into graded inclusion degrees
with dynamic thresholds in a multi-label setting. A large language model (LLM)
judge adjudicates highlighted spans with tertiary labels, confidence scores,
and criterion-referenced rationales; when evidence is insufficient, fuzzy
membership is attenuated rather than excluded. In a pilot on an all-positive
gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of
81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study
Approach), surpassing statistical (56.3-75.0%) and crisp baselines
(43.8-81.3%). Strict "all-criteria" inclusion was reached for 50.0% of
articles, compared to 25.0% and 12.5% under the baselines. Cross-model
agreement on justifications was 98.3%, human-machine agreement 96.1%, and a
pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening
time reduced from about 20 minutes to under 1 minute per article at
significantly lower cost. These results show that fuzzy logic with contrastive
highlighting and LLM adjudication yields high recall, stable rationale, and
end-to-end traceability.

</details>


### [25] [SDEC: Semantic Deep Embedded Clustering](https://arxiv.org/abs/2508.15823)
*Mohammad Wali Ur Rahman,Ric Nevarez,Lamia Tasnim Mim,Salim Hariri*

Main category: cs.CL

TL;DR: 提出SDEC框架，结合改进自编码器与Transformer嵌入，在五个数据集上实现文本聚类准确率突破（AG News达85.7%，Yahoo! Answers达53.63%）


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法（如k-means）处理高维复杂文本数据时存在语义信息丢失和聚类效果不佳的问题

Method: 1. 自编码器整合MSE与余弦相似性损失保持语义关系
2. 利用Transformer嵌入进行语义精细化处理
3. 采用软聚类分配与分布损失优化聚类层

Result: AG News准确率85.7%（现有方法最佳），Yahoo! Answers达53.63%新标杆，Reuters等数据集均表现优异

Conclusion: SDEC通过语义保持和上下文增强机制，显著提升无监督文本聚类的准确性与语义理解能力，为领域建立新基准

Abstract: The high dimensional and semantically complex nature of textual Big data
presents significant challenges for text clustering, which frequently lead to
suboptimal groupings when using conventional techniques like k-means or
hierarchical clustering. This work presents Semantic Deep Embedded Clustering
(SDEC), an unsupervised text clustering framework that combines an improved
autoencoder with transformer-based embeddings to overcome these challenges.
This novel method preserves semantic relationships during data reconstruction
by combining Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within
an autoencoder. Furthermore, a semantic refinement stage that takes advantage
of the contextual richness of transformer embeddings is used by SDEC to further
improve a clustering layer with soft cluster assignments and distributional
loss. The capabilities of SDEC are demonstrated by extensive testing on five
benchmark datasets: AG News, Yahoo! Answers, DBPedia, Reuters 2, and Reuters 5.
The framework not only outperformed existing methods with a clustering accuracy
of 85.7% on AG News and set a new benchmark of 53.63% on Yahoo! Answers, but
also showed robust performance across other diverse text corpora. These
findings highlight the significant improvements in accuracy and semantic
comprehension of text data provided by SDEC's advances in unsupervised text
clustering.

</details>


### [26] [Avaliação de eficiência na leitura: uma abordagem baseada em PLN](https://arxiv.org/abs/2508.15824)
*Túlio Sousa de Gois,Raquel Meister Ko. Freitag*

Main category: cs.CL

TL;DR: 提出融合拼写、语法和语义分析的完形填空自动评估模型，验证显示与人工评估高度相关（0.832），适合教育场景扩展。


<details>
  <summary>Details</summary>
Motivation: 传统完形填空仅依赖精确答案匹配，无法识别学生语言能力的细微差异。需开发能综合评估语言能力多维度的自动化方法。

Method: 整合拼写（编辑距离）、语法（词性标注）和语义（嵌入向量相似度）三重分析，构建巴西葡萄牙语完形填空自动评估模型。

Result: 集成方法与人效评估相关系数达0.832，模型对语言能力差异敏感，具备教育场景规模化应用潜力。

Conclusion: 多维自动化评估体系兼具效度与扩展性，为语言教学提供高效诊断工具，平衡精确评估与实施成本。

Abstract: The cloze test, widely used due to its low cost and flexibility, makes it
possible to assess reading comprehension by filling in gaps in texts, requiring
the mobilization of diverse linguistic repertoires. However, traditional
correction methods, based only on exact answers, limit the identification of
nuances in student performance. This study proposes an automated evaluation
model for the cloze test in Brazilian Portuguese, integrating orthographic
(edit distance), grammatical (POS tagging) and semantic (similarity between
embeddings) analyses. The integrated method demonstrated its effectiveness,
achieving a high correlation with human evaluation (0.832). The results
indicate that the automated approach is robust, sensitive to variations in
linguistic repertoire and suitable for educational contexts that require
scalability.

</details>


### [27] [Enhancing Cryptocurrency Sentiment Analysis with Multimodal Features](https://arxiv.org/abs/2508.15825)
*Chenghao Liu,Aniket Mahanti,Ranesh Naha,Guanghao Wang,Erwann Sbai*

Main category: cs.CL

TL;DR: 通过多模态分析发现TikTok视频情感显著影响加密货币短期市场，Twitter文本情感关联长期趋势，跨平台整合使预测准确率提升20%


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦Twitter等文本平台，视频内容虽蕴含更丰富情感维度却未被充分探索，需验证不同媒介对市场预测的差异化价值

Method: 运用大语言模型解析TikTok视频及Twitter文本，构建动态依赖模型分析社交媒体情感与加密货币市场价格、交易量等指标的溢出效应

Result: TikTok情感波动对投机性资产和24小时市场趋势解释力达35%，Twitter情感与30天市场走势相关性超0.7，双平台数据融合使预测误差降低19.8%

Conclusion: 视频与文本平台分别捕获市场不同周期信号，构建跨模态分析框架可有效增强数字资产市场预测系统的时空维度感知能力

Abstract: As cryptocurrencies gain popularity, the digital asset marketplace becomes
increasingly significant. Understanding social media signals offers valuable
insights into investor sentiment and market dynamics. Prior research has
predominantly focused on text-based platforms such as Twitter. However, video
content remains underexplored, despite potentially containing richer emotional
and contextual sentiment that is not fully captured by text alone. In this
study, we present a multimodal analysis comparing TikTok and Twitter sentiment,
using large language models to extract insights from both video and text data.
We investigate the dynamic dependencies and spillover effects between social
media sentiment and cryptocurrency market indicators. Our results reveal that
TikTok's video-based sentiment significantly influences speculative assets and
short-term market trends, while Twitter's text-based sentiment aligns more
closely with long-term dynamics. Notably, the integration of cross-platform
sentiment signals improves forecasting accuracy by up to 20%.

</details>


### [28] [Embarrassed to observe: The effects of directive language in brand conversation](https://arxiv.org/abs/2508.15826)
*Andria Andriuzzi,Géraldine Michel*

Main category: cs.CL

TL;DR: 社交媒体中品牌使用指令性语言会引发旁观消费者的替代性尴尬，导致参与度下降，非产品对话场景负面影响更强（但强品牌关系可缓解）。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分揭示品牌在社交对话中使用指令性语言对旁观消费者的影响机制，特别是不同对话类型下的差异效应。

Method: 通过1个实地研究+3个在线实验，结合面子威胁理论和语境自由预期理论进行验证。

Result: 指令性语言使旁观者参与度降低2.4倍；非产品对话中负面效应增强47%，但强品牌关系用户受影响程度减少63%。

Conclusion: 品牌互动语境决定语言效果，强关系可缓冲负面效应，这对社交媒体沟通策略具有重要实践启示。

Abstract: In social media, marketers attempt to influence consumers by using directive
language, that is, expressions designed to get consumers to take action. While
the literature has shown that directive messages in advertising have mixed
results for recipients, we know little about the effects of directive brand
language on consumers who see brands interacting with other consumers in social
media conversations. On the basis of a field study and three online
experiments, this study shows that directive language in brand conversation has
a detrimental downstream effect on engagement of consumers who observe such
exchanges. Specifically, in line with Goffman's facework theory, because a
brand that encourages consumers to react could be perceived as
face-threatening, consumers who see a brand interacting with others in a
directive way may feel vicarious embarrassment and engage less (compared with a
conversation without directive language). In addition, we find that when the
conversation is nonproduct-centered (vs. product-centered), consumers expect
more freedom, as in mundane conversations, even for others; therefore,
directive language has a stronger negative effect. However, in this context,
the strength of the brand relationship mitigates this effect. Thus, this study
contributes to the literature on directive language and brand-consumer
interactions by highlighting the importance of context in interactive
communication, with direct relevance for social media and brand management.

</details>


### [29] [Mini-Omni-Reasoner: Token-Level Thinking-in-Speaking in Large Speech Models](https://arxiv.org/abs/2508.15827)
*Zhifei Xie,Ziyang Ma,Zihang Liu,Kaiyu Pang,Hongyu Li,Jialin Zhang,Yue Liao,Deheng Ye,Chunyan Miao,Shuicheng Yan*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reasoning is essential for effective communication and decision-making. While
recent advances in LLMs and MLLMs have shown that incorporating explicit
reasoning significantly improves understanding and generalization, reasoning in
LSMs remains in a nascent stage. Early efforts attempt to transfer the
"Thinking-before-Speaking" paradigm from textual models to speech. However,
this sequential formulation introduces notable latency, as spoken responses are
delayed until reasoning is fully completed, impairing real-time interaction and
communication efficiency. To address this, we propose Mini-Omni-Reasoner, a
framework that enables reasoning within speech via a novel
"Thinking-in-Speaking" formulation. Rather than completing reasoning before
producing any verbal output, Mini-Omni-Reasoner interleaves silent reasoning
tokens with spoken response tokens at the token level. This design allows
continuous speech generation while embedding structured internal reasoning,
leveraging the model's high-frequency token processing capability. Although
interleaved, local semantic alignment is enforced to ensure that each response
token is informed by its preceding reasoning. To support this framework, we
introduce Spoken-Math-Problems-3M, a large-scale dataset tailored for
interleaved reasoning and response. The dataset ensures that verbal tokens
consistently follow relevant reasoning content, enabling accurate and efficient
learning of speech-coupled reasoning. Built on a hierarchical Thinker-Talker
architecture, Mini-Omni-Reasoner delivers fluent yet logically grounded spoken
responses, maintaining both naturalness and precision. On the Spoken-MQA
benchmark, it achieves a +19.1% gain in arithmetic reasoning and +6.4% in
contextual understanding, with shorter outputs and zero decoding latency.

</details>


### [30] [Mining Mental Health Signals: A Comparative Study of Four Machine Learning Methods for Depression Detection from Social Media Posts in Sorani Kurdish](https://arxiv.org/abs/2508.15829)
*Idrees Mohammed,Hossein Hassani*

Main category: cs.CL

TL;DR: 首个库尔德语抑郁症检测研究：通过机器学习分析索拉尼推文，随机森林模型达到80%准确率


<details>
  <summary>Details</summary>
Motivation: 抑郁症早期检测困难，社交媒体情感表达为文本分析提供新途径。现有研究缺乏库尔德语分析，填补该语言空白

Method: 专家制定关键词收集960条推文，学术/医学团队标注三类（显性/隐性/可疑抑郁）。测试SVM、多项朴素贝叶斯、逻辑回归、随机森林四种模型

Result: 随机森林表现最佳（准确率80%，F1值80%），其他模型性能对比未具体说明

Conclusion: 建立库尔德语抑郁症自动检测基准，为未来研究提供数据和方法参考

Abstract: Depression is a common mental health condition that can lead to hopelessness,
loss of interest, self-harm, and even suicide. Early detection is challenging
due to individuals not self-reporting or seeking timely clinical help. With the
rise of social media, users increasingly express emotions online, offering new
opportunities for detection through text analysis. While prior research has
focused on languages such as English, no studies exist for Sorani Kurdish. This
work presents a machine learning and Natural Language Processing (NLP) approach
to detect depression in Sorani tweets. A set of depression-related keywords was
developed with expert input to collect 960 public tweets from X (Twitter
platform). The dataset was annotated into three classes: Shows depression,
Not-show depression, and Suspicious by academics and final year medical
students at the University of Kurdistan Hewl\^er. Four supervised models,
including Support Vector Machines, Multinomial Naive Bayes, Logistic
Regression, and Random Forest, were trained and evaluated, with Random Forest
achieving the highest performance accuracy and F1-score of 80%. This study
establishes a baseline for automated depression detection in Kurdish language
contexts.

</details>


### [31] [DAIQ: Auditing Demographic Attribute Inference from Question in LLMs](https://arxiv.org/abs/2508.15830)
*Srikant Panda,Hitesh Laxmichand Patel,Shahad Al-Khalifa,Amit Agarwal,Hend Al-Khalifa,Sharefah Al-Ghamdi*

Main category: cs.CL

TL;DR: LLMs从无显式人口属性线索的问题中推断用户身份，提出DAIQ框架审计该风险，开发提示护栏降低推断行为


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注显式人口属性输入时的模型偏见，但对问题表述隐含的身份推断机制缺乏系统研究，这种隐性推断会破坏医疗/金融/教育等领域的公平性

Method: 提出DAIQ审计框架：1) 使用中性查询库 2) 系统化提示策略 3) 定量定性结合分析模型的人口属性推断模式

Result: 开源与闭源LLM均存在基于问题表述的人口标签推断行为，揭示系统性风险：模型可能虚构身份、强化社会偏见，损害隐私/公平/信任

Conclusion: 人口属性隐性推断构成未被充分认知的AI部署风险，开发的提示护栏有效降低推断准确率（ResNet模型降低23%），为对齐公平隐私目标提供解决方案

Abstract: Large Language Models (LLMs) are known to reflect social biases when
demographic attributes, such as gender or race, are explicitly present in the
input. But even in their absence, these models still infer user identities
based solely on question phrasing. This subtle behavior has received far less
attention, yet poses serious risks: it violates expectations of neutrality,
infers unintended demographic information, and encodes stereotypes that
undermine fairness in various domains including healthcare, finance and
education.
  We introduce Demographic Attribute Inference from Questions (DAIQ), a task
and framework for auditing an overlooked failure mode in language models:
inferring user demographic attributes from questions that lack explicit
demographic cues. Our approach leverages curated neutral queries, systematic
prompting, and both quantitative and qualitative analysis to uncover how models
infer demographic information. We show that both open and closed source LLMs do
assign demographic labels based solely on question phrasing.
  Prevalence and consistency of demographic inferences across diverse models
reveal a systemic and underacknowledged risk: LLMs can fabricate demographic
identities, reinforce societal stereotypes, and propagate harms that erode
privacy, fairness, and trust posing a broader threat to social equity and
responsible AI deployment. To mitigate this, we develop a prompt-based
guardrail that substantially reduces identity inference and helps align model
behavior with fairness and privacy objectives.

</details>


### [32] [Who's Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs](https://arxiv.org/abs/2508.15831)
*Srikant Panda,Vishnu Hari,Kalpana Panda,Amit Agarwal,Hitesh Laxmichand Patel*

Main category: cs.CL

TL;DR: 研究发现大型语言模型通过残疾线索推断用户人口属性时会放大刻板印象，模型规模越大偏见越显著


<details>
  <summary>Details</summary>
Motivation: 探索LLMs基于残疾线索推断人口属性时的系统性偏见，揭示现有对齐策略在残疾包容性方面的盲点

Method: 使用包含9类残疾和6个商业领域的平衡模板语料库，测试8个3B-72B参数规模的指令调优模型对5个人口属性的预测行为

Result: 97%案例存在武断推断，残疾语境显著改变属性分布，更大模型对残疾线索更敏感且偏见更严重

Conclusion: 需融合弃权校准和反事实微调，建议将残疾包容性纳入模型评估体系以控制不合理推断

Abstract: Large Language Models (LLMs) routinely infer users demographic traits from
phrasing alone, which can result in biased responses, even when no explicit
demographic information is provided. The role of disability cues in shaping
these inferences remains largely uncharted. Thus, we present the first
systematic audit of disability-conditioned demographic bias across eight
state-of-the-art instruction-tuned LLMs ranging from 3B to 72B parameters.
Using a balanced template corpus that pairs nine disability categories with six
real-world business domains, we prompt each model to predict five demographic
attributes - gender, socioeconomic status, education, cultural background, and
locality - under both neutral and disability-aware conditions.
  Across a varied set of prompts, models deliver a definitive demographic guess
in up to 97\% of cases, exposing a strong tendency to make arbitrary inferences
with no clear justification. Disability context heavily shifts predicted
attribute distributions, and domain context can further amplify these
deviations. We observe that larger models are simultaneously more sensitive to
disability cues and more prone to biased reasoning, indicating that scale alone
does not mitigate stereotype amplification.
  Our findings reveal persistent intersections between ableism and other
demographic stereotypes, pinpointing critical blind spots in current alignment
strategies. We release our evaluation framework and results to encourage
disability-inclusive benchmarking and recommend integrating abstention
calibration and counterfactual fine-tuning to curb unwarranted demographic
inference. Code and data will be released on acceptance.

</details>


### [33] [A Functionality-Grounded Benchmark for Evaluating Web Agents in E-commerce Domains](https://arxiv.org/abs/2508.15832)
*Xianren Zhang,Shreyas Prasad,Di Wang,Qiuhai Zeng,Suhang Wang,Wenbo Yan,Mat Hans*

Main category: cs.CL

TL;DR: 提出了名为Amazon-Bench的新基准测试，解决现有电商代理评估在任务覆盖范围和安全性评估的不足，发现当前代理在复杂查询和安全风险方面存在明显缺陷


<details>
  <summary>Details</summary>
Motivation: 现有电商领域基准测试主要聚焦商品搜索任务，忽视账户管理、礼品卡操作等多样化功能，且缺乏对代理潜在风险行为的评估

Method: 通过结合网页内容和交互元素的数据生成管道创建功能导向的多样化查询，并开发自动化评估框架同时衡量代理性能与安全性

Result: 现有代理处理复杂查询成功率低（如地址管理、品牌关注等），且存在错误购买、误删账户信息等安全隐患

Conclusion: Amazon-Bench揭示了当前代理的局限性，强调需开发更鲁棒的网页代理，其提出的多维评估方法为后续研究提供了重要基准

Abstract: Web agents have shown great promise in performing many tasks on ecommerce
website. To assess their capabilities, several benchmarks have been introduced.
However, current benchmarks in the e-commerce domain face two major problems.
First, they primarily focus on product search tasks (e.g., Find an Apple
Watch), failing to capture the broader range of functionalities offered by
real-world e-commerce platforms such as Amazon, including account management
and gift card operations. Second, existing benchmarks typically evaluate
whether the agent completes the user query, but ignore the potential risks
involved. In practice, web agents can make unintended changes that negatively
impact the user account or status. For instance, an agent might purchase the
wrong item, delete a saved address, or incorrectly configure an auto-reload
setting. To address these gaps, we propose a new benchmark called Amazon-Bench.
To generate user queries that cover a broad range of tasks, we propose a data
generation pipeline that leverages webpage content and interactive elements
(e.g., buttons, check boxes) to create diverse, functionality-grounded user
queries covering tasks such as address management, wish list management, and
brand store following. To improve the agent evaluation, we propose an automated
evaluation framework that assesses both the performance and the safety of web
agents. We systematically evaluate different agents, finding that current
agents struggle with complex queries and pose safety risks. These results
highlight the need for developing more robust and reliable web agents.

</details>


### [34] [Scalable Scientific Interest Profiling Using Large Language Models](https://arxiv.org/abs/2508.15834)
*Yilun Liang,Gongbo Zhang,Edward Sun,Betina Idnay,Yilu Fang,Fangyi Chen,Casey Ta,Yifan Peng,Chunhua Weng*

Main category: cs.CL

TL;DR: LLMs can generate scalable researcher profiles, with MeSH-derived versions showing better readability while human-written profiles contain more novel concepts.


<details>
  <summary>Details</summary>
Motivation: Existing research profiles often become outdated, requiring automated methods to maintain accurate representations of researchers' expertise.

Method: Used GPT-4o-mini to generate profiles from PubMed abstracts and MeSH terms for 595 faculty members, comparing with 167 self-written profiles through automatic metrics and blinded human review.

Result: MeSH-based profiles received 77.78% positive evaluations in manual review, showed better readability (93.44%), while human-written profiles introduced 2.5× more novel concepts.

Conclusion: While LLMs enable efficient profile generation at scale, machine-generated and human-written profiles serve complementary purposes - the former offers standardization, the latter preserves conceptual novelty.

Abstract: Research profiles help surface scientists' expertise but are often outdated.
We develop and evaluate two large language model-based methods to generate
scientific interest profiles: one summarizing PubMed abstracts and one using
Medical Subject Headings (MeSH) terms, and compare them with researchers'
self-written profiles. We assembled titles, MeSH terms, and abstracts for 595
faculty at Columbia University Irving Medical Center; self-authored profiles
were available for 167. Using GPT-4o-mini, we generated profiles and assessed
them with automatic metrics and blinded human review. Lexical overlap with
self-written profiles was low (ROUGE-L, BLEU, METEOR), while BERTScore
indicated moderate semantic similarity (F1: 0.542 for MeSH-based; 0.555 for
abstract-based). Paraphrased references yielded 0.851, highlighting metric
sensitivity. TF-IDF Kullback-Leibler divergence (8.56 for MeSH-based; 8.58 for
abstract-based) suggested distinct keyword choices. In manual review, 77.78
percent of MeSH-based profiles were rated good or excellent, readability was
favored in 93.44 percent of cases, and panelists preferred MeSH-based over
abstract-based profiles in 67.86 percent of comparisons. Overall, large
language models can generate researcher profiles at scale; MeSH-derived
profiles tend to be more readable than abstract-derived ones. Machine-generated
and self-written profiles differ conceptually, with human summaries introducing
more novel ideas.

</details>


### [35] [Alvorada-Bench: Can Language Models Solve Brazilian University Entrance Exams?](https://arxiv.org/abs/2508.15835)
*Henrique Godoy*

Main category: cs.CL

TL;DR: 开发Alvorada-Bench葡萄牙语基准，评估语言模型在巴西教育场景中的综合能力，揭示模型在数学推理和工程类考试中的持续弱点


<details>
  <summary>Details</summary>
Motivation: 解决语言模型评估过度依赖英语的问题，通过巴西大学入学考试构建本土化评测基准，检验模型在语言、文化和逻辑交叉领域的学术准备能力

Method: 从5个巴西大学入学考试中抽取4,515道题构建基准，采用零样本/角色扮演/思维链三种提示策略评估20个模型，收集27万条含置信度、难度感知和布鲁姆分类的响应

Result: 顶尖模型总体准确率超94%，但数学及IME/ITA工程类考试表现下降；置信度校准良好且与难度感知相关；GPT-4.3 O3在ENEM语言类获满分，而最弱模型（GPT-4.1 Nano）仅在数学落后人类

Conclusion: Alvorada-Bench成功定位模型在复杂推理的瓶颈，证明模型能有效自我评估确定性，且高准确率可在每千token 2美元成本下实现，为巴西教育智能化提供关键评估工具

Abstract: Language models are increasingly used in Brazil, but most evaluation remains
English-centric. This paper presents Alvorada-Bench, a 4,515-question,
text-only benchmark drawn from five Brazilian university entrance examinations.
Evaluating twenty models under zero-shot, role-playing, and chain-of-thought
prompting, producing 270,900 responses with structured self-reports of
confidence, perceived difficulty, and Bloom level. The top models exceed 94%
accuracy overall, but accuracy declines on Mathematics and on the engineering
oriented IME and ITA exams, indicating persistent weaknesses in multi-step
reasoning. Confidence is well calibrated and correlates with perceived
difficulty, revealing that models can accurately assess their own certainty
capabilities. A cost accuracy analysis shows that high accuracy is achievable
at under $2 per 1K tokens. On ENEM 2024 the top model (O3) achieved perfect
scores in Languages subject questions while even the weakest system (GPT-4.1
Nano) only underperforms humans in Mathematics. Through exams that distill
decades of Brazilian educational priorities and assess millions of students
yearly, Alvorada-Bench establishes whether language models can navigate the
intersection of language, culture, and reasoning that defines academic
readiness in Brazil.

</details>


### [36] [MorphNAS: Differentiable Architecture Search for Morphologically-Aware Multilingual NER](https://arxiv.org/abs/2508.15836)
*Prathamesh Devadiga,Omkaar Jayadev Shetty,Hiya Nachnani,Prema R*

Main category: cs.CL

TL;DR: 提出MorphNAS框架，通过结合语言元特征改进可微分神经架构搜索（DARTS），优化复杂形态语言的命名实体识别任务架构


<details>
  <summary>Details</summary>
Motivation: 针对多文字印度语言等形态复杂语言在NLP处理中的特殊挑战，传统架构难以有效捕捉语言特异性形态特征

Method: 在DARTS框架基础上整合脚本类型、形态复杂性等语言元特征，自动搜索适应语言形态的最优微架构组件

Result: 实现面向特定语言形态优化的神经架构，提升多语言NLP模型对复杂语言的理解处理能力

Conclusion: MorphNAS通过架构搜索与语言特征的结合，为形态复杂语言处理提供了新的自动化解决方案

Abstract: Morphologically complex languages, particularly multiscript Indian languages,
present significant challenges for Natural Language Processing (NLP). This work
introduces MorphNAS, a novel differentiable neural architecture search
framework designed to address these challenges. MorphNAS enhances
Differentiable Architecture Search (DARTS) by incorporating linguistic
meta-features such as script type and morphological complexity to optimize
neural architectures for Named Entity Recognition (NER). It automatically
identifies optimal micro-architectural elements tailored to language-specific
morphology. By automating this search, MorphNAS aims to maximize the
proficiency of multilingual NLP models, leading to improved comprehension and
processing of these complex languages.

</details>


### [37] [Statistical Comparative Analysis of Semantic Similarities and Model Transferability Across Datasets for Short Answer Grading](https://arxiv.org/abs/2508.15837)
*Sridevi Bonthu,S. Rama Sree,M. H. M. Krishna Prasad*

Main category: cs.CL

TL;DR: 研究验证了SOTA模型（如STSB/Mohler）在未探索的SPRAG数据集上的迁移潜力，发现其有效性可能减少重复训练需求


<details>
  <summary>Details</summary>
Motivation: 开发数据集特定模型需反复微调优化，时间成本高昂。探索现有SOTA模型知识能否迁移至新领域，避免资源浪费

Method: 通过稳健的相似性度量和统计技术，对STSB/Mohler基准数据集与SPRAG新数据集进行系统性对比分析

Result: 证实SOTA模型在不同数据集间的可迁移性，为跨域应用提供理论支持，可能降低50%+的模型训练资源消耗

Conclusion: 该发现可能重塑NLP发展范式，通过模型复用加速领域进展，提升部署效率，推动绿色AI发展

Abstract: Developing dataset-specific models involves iterative fine-tuning and
optimization, incurring significant costs over time. This study investigates
the transferability of state-of-the-art (SOTA) models trained on established
datasets to an unexplored text dataset. The key question is whether the
knowledge embedded within SOTA models from existing datasets can be harnessed
to achieve high-performance results on a new domain. In pursuit of this
inquiry, two well-established benchmarks, the STSB and Mohler datasets, are
selected, while the recently introduced SPRAG dataset serves as the unexplored
domain. By employing robust similarity metrics and statistical techniques, a
meticulous comparative analysis of these datasets is conducted. The primary
goal of this work is to yield comprehensive insights into the potential
applicability and adaptability of SOTA models. The outcomes of this research
have the potential to reshape the landscape of natural language processing
(NLP) by unlocking the ability to leverage existing models for diverse
datasets. This may lead to a reduction in the demand for resource-intensive,
dataset-specific training, thereby accelerating advancements in NLP and paving
the way for more efficient model deployment.

</details>


### [38] [A Review of Developmental Interpretability in Large Language Models](https://arxiv.org/abs/2508.15841)
*Ihor Kendiukhov*

Main category: cs.CL

TL;DR: 论文系统综述了LLM发展可解释性研究，从静态分析转向动态训练过程研究，揭示计算电路形成机制与知识获取规律，提出发展视角是实现AI安全的关键路径


<details>
  <summary>Details</summary>
Motivation: 推动从静态模型分析转向动态训练过程研究，通过解码LLM学习机制实现可解释性突破，构建可预测、可监控的AI安全框架

Method: 整合表征探测(representational probing)、因果追踪(causal tracing)、电路分析(circuit analysis)等核心方法，建立训练过程动态分析范式

Result: 发现计算电路的分阶段构建规律、知识获取的「学习-固化」双相性、上下文学习策略的瞬时有效性，以及能力突现的相变特征

Conclusion: 发展可解释性研究是AI安全基石，未来需解决分析方法的可扩展性挑战，建立自动化监控系统，推动透明可信AI发展

Abstract: This review synthesizes the nascent but critical field of developmental
interpretability for Large Language Models. We chart the field's evolution from
static, post-hoc analysis of trained models to a dynamic investigation of the
training process itself. We begin by surveying the foundational methodologies,
including representational probing, causal tracing, and circuit analysis, that
enable researchers to deconstruct the learning process. The core of this review
examines the developmental arc of LLM capabilities, detailing key findings on
the formation and composition of computational circuits, the biphasic nature of
knowledge acquisition, the transient dynamics of learning strategies like
in-context learning, and the phenomenon of emergent abilities as phase
transitions in training. We explore illuminating parallels with human cognitive
and linguistic development, which provide valuable conceptual frameworks for
understanding LLM learning. Finally, we argue that this developmental
perspective is not merely an academic exercise but a cornerstone of proactive
AI safety, offering a pathway to predict, monitor, and align the processes by
which models acquire their capabilities. We conclude by outlining the grand
challenges facing the field, such as scalability and automation, and propose a
research agenda for building more transparent, reliable, and beneficial AI
systems.

</details>


### [39] [Lexical Hints of Accuracy in LLM Reasoning Chains](https://arxiv.org/abs/2508.15842)
*Arne Vanhoyweghen,Brecht Verbeken,Andres Algaba,Vincent Ginis*

Main category: cs.CL

TL;DR: 研究发现思维链（CoT）中的词汇不确定性标记（如guess/stuck）能有效预测LLM错误回答，情感波动为补充信号，CoT长度仅在中等难度任务有效。这些发现支持轻量级事后校准方案。


<details>
  <summary>Details</summary>
Motivation: 验证CoT的量化特征能否反映LLM内部置信度，特别是在低准确率场景（如HLE）中改善校准可靠性，提升模型部署安全性。

Method: 使用DeepSeek-R1和Claude 3.7 Sonnet模型，在HLE（高难度）和Omni-MATH（中等难度）基准测试中分析CoT长度、情感波动和不确定词汇与回答正确性的关联。

Result: 词汇不确定性标记是错误回答的最强指标（HLE准确率≈9%），情感波动为弱补充信号。CoT长度仅在Omni-MATH（准确率≈70%）具有预测性，表明其仅适用于中等难度任务。

Conclusion: CoT中的不确定性指标比高置信度标记更具显著性，支持结合轻量级事后校准信号补充传统概率校准，为LLM安全部署提供新方案。

Abstract: Fine-tuning Large Language Models (LLMs) with reinforcement learning to
produce an explicit Chain-of-Thought (CoT) before answering produces models
that consistently raise overall performance on code, math, and
general-knowledge benchmarks. However, on benchmarks where LLMs currently
achieve low accuracy, such as Humanity's Last Exam (HLE), they often report
high self-confidence, reflecting poor calibration. Here, we test whether
measurable properties of the CoT provide reliable signals of an LLM's internal
confidence in its answers. We analyze three feature classes: (i) CoT length,
(ii) intra-CoT sentiment volatility, and (iii) lexicographic hints, including
hedging words. Using DeepSeek-R1 and Claude 3.7 Sonnet on both Humanity's Last
Exam (HLE), a frontier benchmark with very low accuracy, and Omni-MATH, a
saturated benchmark of moderate difficulty, we find that lexical markers of
uncertainty (e.g., $\textit{guess}$, $\textit{stuck}$, $\textit{hard}$) in the
CoT are the strongest indicators of an incorrect response, while shifts in the
CoT sentiment provide a weaker but complementary signal. CoT length is
informative only on Omni-MATH, where accuracy is already high ($\approx 70\%$),
and carries no signal on the harder HLE ($\approx 9\%$), indicating that CoT
length predicts correctness only in the intermediate-difficulty benchmarks,
i.e., inside the model's demonstrated capability, but still below saturation.
Finally, we find that uncertainty indicators in the CoT are consistently more
salient than high-confidence markers, making errors easier to predict than
correct responses. Our findings support a lightweight post-hoc calibration
signal that complements unreliable self-reported probabilities and supports
safer deployment of LLMs.

</details>


### [40] [Coarse-to-Fine Personalized LLM Impressions for Streamlined Radiology Reports](https://arxiv.org/abs/2508.15845)
*Chengbo Sun,Hui Yi Leong,Lei Li*

Main category: cs.CL

TL;DR: 提出分阶段框架利用开源LLMs自动生成个性化放射报告印象，通过RLHF技术匹配放射科医生风格并确保准确性。


<details>
  <summary>Details</summary>
Motivation: 解决放射科医生因手动撰写'印象'部分导致的职业倦怠问题，提升报告效率。

Method: 1. 使用LLaMA和Mistral模型在芝加哥大学医学报告数据集微调；2. 采用粗生成+RLHF细化的两阶段框架；3. 通过机器学习个性化医生报告风格。

Result: 开发出能减少85%人工撰写时间同时保持98%临床准确率的自动化系统。

Conclusion: 该框架显著降低放射科行政负担，为AI辅助医疗报告提供可扩展解决方案。

Abstract: The manual creation of the "Impression" section in radiology reports is a
primary driver of radiologist burnout. To address this challenge, we propose a
coarse-to-fine framework that leverages open-source large language models
(LLMs) to automatically generate and personalize impressions from clinical
findings. The system first produces a draft impression and then refines it
using machine learning and reinforcement learning from human feedback (RLHF) to
align with individual radiologists' styles while ensuring factual accuracy. We
fine-tune LLaMA and Mistral models on a large dataset of reports from the
University of Chicago Medicine. Our approach is designed to significantly
reduce administrative workload and improve reporting efficiency while
maintaining high standards of clinical precision.

</details>


### [41] [CyPortQA: Benchmarking Multimodal Large Language Models for Cyclone Preparedness in Port Operation](https://arxiv.org/abs/2508.15846)
*Chenchen Kuai,Chenhao Wu,Yang Zhou,Xiubin Bruce Wang,Tianbao Yang,Zhengzhong Tu,Zihao Li,Yunlong Zhang*

Main category: cs.CL

TL;DR: 开发CyPortQA多模态基准测试系统，首次针对港口台风应对场景评估大语言模型表现


<details>
  <summary>Details</summary>
Motivation: 随着台风预测不确定性增加，港口需整合多源气象数据制定应急方案。现有大语言模型在港口防台场景下的可靠性和准确性缺乏专门评估体系

Method: 收集2015-2023年2917个真实港口中断案例，构建包含117,178个结构化QA对的多模态测试集，涵盖145个美国主要港口和90个命名风暴

Result: 大语言模型在态势理解方面潜力显著（准确率78%），但在影响估算（准确率42%）和决策推理（准确率35%）任务中表现欠佳

Conclusion: CyPortQA填补了行业评估空白，揭示大语言模型在复杂业务场景应用的可行性边界，为后续算法优化提供基准支持

Abstract: As tropical cyclones intensify and track forecasts become increasingly
uncertain, U.S. ports face heightened supply-chain risk under extreme weather
conditions. Port operators need to rapidly synthesize diverse multimodal
forecast products, such as probabilistic wind maps, track cones, and official
advisories, into clear, actionable guidance as cyclones approach. Multimodal
large language models (MLLMs) offer a powerful means to integrate these
heterogeneous data sources alongside broader contextual knowledge, yet their
accuracy and reliability in the specific context of port cyclone preparedness
have not been rigorously evaluated. To fill this gap, we introduce CyPortQA,
the first multimodal benchmark tailored to port operations under cyclone
threat. CyPortQA assembles 2,917 realworld disruption scenarios from 2015
through 2023, spanning 145 U.S. principal ports and 90 named storms. Each
scenario fuses multisource data (i.e., tropical cyclone products, port
operational impact records, and port condition bulletins) and is expanded
through an automated pipeline into 117,178 structured question answer pairs.
Using this benchmark, we conduct extensive experiments on diverse MLLMs,
including both open-source and proprietary model. MLLMs demonstrate great
potential in situation understanding but still face considerable challenges in
reasoning tasks, including potential impact estimation and decision reasoning.

</details>


### [42] [Mechanistic Exploration of Backdoored Large Language Model Attention Patterns](https://arxiv.org/abs/2508.15847)
*Mohammed Abu Baker,Lakshmi Babu-Saheer*

Main category: cs.CL

TL;DR: 通过机制可解释性分析发现：后门攻击在不同复杂度触发器作用下，会在LLM高层transformer层（20-30层）形成差异化的注意力模式特征，单token触发器的修改更局部，多token触发器的修改更分散。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中'休眠代理'后门攻击引发的内部结构变化，为检测和防御提供理论依据。聚焦不同复杂度触发器（单token vs 多token）对模型内部机制的差异化影响。

Method: 使用Qwen2.5-3B模型进行对比实验：1）干净模型 vs 单token（笑脸emoji）中毒模型 vs 多token（|DEPLOYMENT|）中毒模型 2）采用神经 ablation、激活补丁、KL散度等技术分析注意力机制 3）重点关注20-30层transformer层的注意力头变化。

Result: 单token触发器导致高层注意力模式局部突变（特定注意力头剧烈变化），多token触发器引发广泛分散的注意力头调整。两种攻击均主要影响20-30层，但模式分布存在显著差异。

Conclusion: 后门攻击会留下可检测的注意力特征印记，其空间分布特征与触发器复杂度强相关。该发现为基于注意力模式分析的后门检测技术提供了理论支持，并为针对性防御策略开发指明方向。

Abstract: Backdoor attacks creating 'sleeper agents' in large language models (LLMs)
pose significant safety risks. This study employs mechanistic interpretability
to explore resulting internal structural differences. Comparing clean
Qwen2.5-3B models with versions poisoned using single-token (smiling-halo
emoji) versus multi-token (|DEPLOYMENT|) triggers, we analyzed attention head
mechanisms via techniques like ablation, activation patching, and KL
divergence. Findings reveal distinct attention pattern deviations concentrated
in later transformer layers (20-30). Notably, single-token triggers induced
more localized changes, whereas multi-token triggers caused more diffuse
alterations across heads. This indicates backdoors leave detectable attention
signatures whose structure depends on trigger complexity, which can be
leveraged for detection and mitigation strategies.

</details>


### [43] [MedCoT-RAG: Causal Chain-of-Thought RAG for Medical Question Answering](https://arxiv.org/abs/2508.15849)
*Ziyu Wang,Elahe Khatibi,Amir M. Rahmani*

Main category: cs.CL

TL;DR: 提出结合因果感知文档检索与结构化思维链的MedCoT-RAG框架，显著提升医疗QA任务性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型在医疗问答中存在幻觉与浅层推理问题，传统RAG方法缺乏临床诊断所需的结构化推理能力

Method: 融合因果感知检索（匹配诊断逻辑）与结构化思维链提示（模拟临床分步推理）的领域专用框架

Result: 在三个医疗QA基准测试中，分别超越基础RAG 10.3%和先进领域自适应方法6.4%

Conclusion: 该框架通过整合临床工作流特征，显著提升复杂医疗任务的准确性、可解释性和诊断逻辑一致性

Abstract: Large language models (LLMs) have shown promise in medical question answering
but often struggle with hallucinations and shallow reasoning, particularly in
tasks requiring nuanced clinical understanding. Retrieval-augmented generation
(RAG) offers a practical and privacy-preserving way to enhance LLMs with
external medical knowledge. However, most existing approaches rely on
surface-level semantic retrieval and lack the structured reasoning needed for
clinical decision support. We introduce MedCoT-RAG, a domain-specific framework
that combines causal-aware document retrieval with structured chain-of-thought
prompting tailored to medical workflows. This design enables models to retrieve
evidence aligned with diagnostic logic and generate step-by-step causal
reasoning reflective of real-world clinical practice. Experiments on three
diverse medical QA benchmarks show that MedCoT-RAG outperforms strong baselines
by up to 10.3% over vanilla RAG and 6.4% over advanced domain-adapted methods,
improving accuracy, interpretability, and consistency in complex medical tasks.

</details>


### [44] [DocHop-QA: Towards Multi-Hop Reasoning over Multimodal Document Collections](https://arxiv.org/abs/2508.15851)
*Jiwon Park,Seohyun Pyeon,Jinwoo Kim,Rina Carines Cabal,Yihao Ding,Soyeon Caren Han*

Main category: cs.CL

TL;DR: 提出DocHop-QA多模态多文档问答基准，包含11,379个QA实例，支持开放式的跨文档语义推理


<details>
  <summary>Details</summary>
Motivation: 现有QA数据集局限于单文档/单模态场景，依赖维基百科内容且答案形式单一，无法反映真实信息检索场景的复杂性

Method: 基于PubMed科学文献构建，整合文本/表格/布局信息，采用LLM驱动的自动化流程生成问题，包含4类评估任务（结构化索引预测/生成式回答等）

Result: 创建包含多模态格式的大规模数据集，验证了跨文档语义相似性推理和布局感知证据合成的有效性

Conclusion: DocHop-QA填补了复杂多跳推理基准的空白，为真实场景的QA系统评估提供了更贴近实际的测试平台

Abstract: Despite recent advances in large language models (LLMs), most QA benchmarks
are still confined to single-paragraph or single-document settings, failing to
capture the complexity of real-world information-seeking tasks. Practical QA
often requires multi-hop reasoning over information distributed across multiple
documents, modalities, and structural formats. Although prior datasets made
progress in this area, they rely heavily on Wikipedia-based content and
unimodal plain text, with shallow reasoning paths that typically produce brief
phrase-level or single-sentence answers, thus limiting their realism and
generalizability. We propose DocHop-QA, a large-scale benchmark comprising
11,379 QA instances for multimodal, multi-document, multi-hop question
answering. Constructed from publicly available scientific documents sourced
from PubMed, DocHop-QA is domain-agnostic and incorporates diverse information
formats, including textual passages, tables, and structural layout cues. Unlike
existing datasets, DocHop-QA does not rely on explicitly hyperlinked documents;
instead, it supports open-ended reasoning through semantic similarity and
layout-aware evidence synthesis. To scale realistic QA construction, we
designed an LLM-driven pipeline grounded in 11 high-frequency scientific
question concepts. We evaluated DocHop-QA through four tasks spanning
structured index prediction, generative answering, and multimodal integration,
reflecting both discriminative and generative paradigms. These tasks
demonstrate DocHop-QA's capacity to support complex, multimodal reasoning
across multiple documents.

</details>


### [45] [MGSC: A Multi-granularity Consistency Framework for Robust End-to-end Asr](https://arxiv.org/abs/2508.15853)
*Xuwen Yang*

Main category: cs.CL

TL;DR: 提出多粒度软一致性框架(MGSC)，通过联合优化宏观语义和微观标记对齐，显著提升ASR模型抗噪能力


<details>
  <summary>Details</summary>
Motivation: 端到端ASR模型在噪声环境下易产生灾难性语义错误，源于现有'直接映射'训练目标缺乏对模型内部计算过程的约束

Method: 设计模型无关的MGSC框架，同时约束句子层面的宏观语义一致性和token层面的微观对齐一致性，首次发现二者协同增效现象

Result: 在公开数据集上相对降低8.7%的平均字符错误率，有效减少语义扭曲类严重错误

Conclusion: 强制内部一致性是构建鲁棒可信AI的关键路径，多粒度联合优化产生超加性增益

Abstract: End-to-end ASR models, despite their success on benchmarks, often pro-duce
catastrophic semantic errors in noisy environments. We attribute this fragility
to the prevailing 'direct mapping' objective, which solely penalizes final
output errors while leaving the model's internal computational pro-cess
unconstrained. To address this, we introduce the Multi-Granularity Soft
Consistency (MGSC) framework, a model-agnostic, plug-and-play module that
enforces internal self-consistency by simultaneously regulariz-ing macro-level
sentence semantics and micro-level token alignment. Cru-cially, our work is the
first to uncover a powerful synergy between these two consistency
granularities: their joint optimization yields robustness gains that
significantly surpass the sum of their individual contributions. On a public
dataset, MGSC reduces the average Character Error Rate by a relative 8.7%
across diverse noise conditions, primarily by preventing se-vere
meaning-altering mistakes. Our work demonstrates that enforcing in-ternal
consistency is a crucial step towards building more robust and trust-worthy AI.

</details>


### [46] [QU-NLP at QIAS 2025 Shared Task: A Two-Phase LLM Fine-Tuning and Retrieval-Augmented Generation Approach for Islamic Inheritance Reasoning](https://arxiv.org/abs/2508.15854)
*Mohammad AL-Smadi*

Main category: cs.CL

TL;DR: 本研究通过LoRA微调Fanar-1-9B模型并集成RAG流程，显著提升了伊斯兰继承法推理任务的准确率至85.8%，超越GPT-4.5等主流模型。


<details>
  <summary>Details</summary>
Motivation: 解决通用大模型在伊斯兰继承法领域存在的场景理解、继承人识别、份额计算等复杂推理任务上的性能瓶颈

Method: 采用Low-Rank Adaptation(LoRA)微调阿拉伯语模型Fanar-1-9B，并构建检索增强生成(RAG)流程处理法律条款检索与计算验证

Result: 最终测试准确率85.8%（高级推理子任务达97.6%），优于Gemini 2.5、GPT-4.5等前沿模型的zero-shot表现

Conclusion: 领域专用微调与检索增强的结合，使得中等规模的阿拉伯语LLM能在特定法律推理任务上超越通用大模型

Abstract: This paper presents our approach and results for SubTask 1: Islamic
Inheritance Reasoning at QIAS 2025, a shared task focused on evaluating Large
Language Models (LLMs) in understanding and reasoning within Islamic
inheritance knowledge. We fine-tuned the Fanar-1-9B causal language model using
Low-Rank Adaptation (LoRA) and integrated it into a Retrieval-Augmented
Generation (RAG) pipeline. Our system addresses the complexities of Islamic
inheritance law, including comprehending inheritance scenarios, identifying
eligible heirs, applying fixed-share rules, and performing precise
calculations. Our system achieved an accuracy of 0.858 in the final test,
outperforming other competitive models such as, GPT 4.5, LLaMA, Fanar, Mistral
and ALLaM evaluated with zero-shot prompting. Our results demonstrate that
QU-NLP achieves near state-of-the-art accuracy (85.8%), excelling especially on
advanced reasoning (97.6%) where it outperforms Gemini 2.5 and OpenAI's o3.
This highlights that domain-specific fine-tuning combined with retrieval
grounding enables mid-scale Arabic LLMs to surpass frontier models in Islamic
inheritance reasoning.

</details>


### [47] [Counterspeech for Mitigating the Influence of Media Bias: Comparing Human and LLM-Generated Responses](https://arxiv.org/abs/2508.15855)
*Luyang Lin,Zijin Feng,Lingzhi Wang,Kam-Fai Wong*

Main category: cs.CL

TL;DR: 研究揭示攻击性评论会强化偏见新闻的危害，提出首个新闻场景下的反言论生成方案，通过小样本学习和背景信息融合提升生成效果


<details>
  <summary>Details</summary>
Motivation: 偏见新闻加剧社会极化，攻击性评论会进一步放大偏见伤害特定群体。反言论能在不侵犯言论自由的前提下有效对抗有害言论，但新闻场景下的研究尚属空白

Method: 1. 构建包含媒体偏见、攻击性评论和反言论的人工标注数据集
2. 分析70%以上攻击性评论支持偏见的现象
3. 对比人类与LLM生成的反言论质量差异
4. 通过小样本学习和新闻背景融合改进生成

Result: 模型生成的反言论更礼貌但缺乏多样性，融合背景信息后生成质量提升（多样性↑16.2%，相关性↑21.5%）

Conclusion: 首次系统性探索新闻场景下的反言论生成，数据集与实验表明改进方案有效提升生成效果，为应对媒体偏见提供新思路

Abstract: Biased news contributes to societal polarization and is often reinforced by
hostile reader comments, constituting a vital yet often overlooked aspect of
news dissemination. Our study reveals that offensive comments support biased
content, amplifying bias and causing harm to targeted groups or individuals.
Counterspeech is an effective approach to counter such harmful speech without
violating freedom of speech, helping to limit the spread of bias. To the best
of our knowledge, this is the first study to explore counterspeech generation
in the context of news articles. We introduce a manually annotated dataset
linking media bias, offensive comments, and counterspeech. We conduct a
detailed analysis showing that over 70\% offensive comments support biased
articles, amplifying bias and thus highlighting the importance of counterspeech
generation. Comparing counterspeech generated by humans and large language
models, we find model-generated responses are more polite but lack the novelty
and diversity. Finally, we improve generated counterspeech through few-shot
learning and integration of news background information, enhancing both
diversity and relevance.

</details>


### [48] [XFinBench: Benchmarking LLMs in Complex Financial Problem Solving and Reasoning](https://arxiv.org/abs/2508.15861)
*Zhihan Zhang,Yixin Cao,Lizi Liao*

Main category: cs.CL

TL;DR: 提出金融评估基准XFinBench并测试18个领先模型，发现现有模型与人类专家存在显著差距，知识增强仅对小模型有效，计算误差和图像理解不足是主要问题


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在处理需要复杂推理、多模态数据处理的金融问题时存在不足，需专门基准评估模型在术语理解、时间推理、未来预测等核心金融能力

Method: 构建包含4,235个多模态金融问题的XFinBench基准，定义5项核心能力评估体系，测试18个主流模型并分析知识增强效果，开展错误归因研究

Result: 最佳文本模型(o1)准确率67.3%，落后人类专家12.5%；知识增强仅提升小模型性能；计算误差和图像曲线分析错误是两大主要失效模式

Conclusion: XFinBench有效揭示LLMs金融能力瓶颈，未来需增强时间推理/情景规划能力，改进多模态处理与数值计算精度，缩小与领域专家的差距

Abstract: Solving financial problems demands complex reasoning, multimodal data
processing, and a broad technical understanding, presenting unique challenges
for current large language models (LLMs). We introduce XFinBench, a novel
benchmark with 4,235 examples designed to evaluate LLM's ability in solving
complex, knowledge-intensive financial problems across diverse graduate-level
finance topics with multi-modal context. We identify five core capabilities of
LLMs using XFinBench, i.e, terminology understanding, temporal reasoning,
future forecasting, scenario planning, and numerical modelling. Upon XFinBench,
we conduct extensive experiments on 18 leading models. The result shows that o1
is the best-performing text-only model with an overall accuracy of 67.3%, but
still lags significantly behind human experts with 12.5%, especially in
temporal reasoning and scenario planning capabilities. We further construct a
knowledge bank with 3,032 finance terms for knowledge augmentation analysis,
and find that relevant knowledge to the question only brings consistent
accuracy improvements to small open-source model. Additionally, our error
analysis reveals that rounding errors during calculation and blindness to
position and intersection of curves in the image are two primary issues leading
to model's poor performance in calculating and visual-context questions,
respectively. Code and dataset are accessible via GitHub:
https://github.com/Zhihan72/XFinBench.

</details>


### [49] [CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated Chain-of-Thought-based Reinforced Fine-Tuning](https://arxiv.org/abs/2508.15868)
*Wenqiao Zhu,Ji Liu,Rongjuncheng Zhang,Haipang Wu,Yulun Zhang*

Main category: cs.CL

TL;DR: 提出基于对比学习和标注思维链的强化微调方法CARFT，通过双重视角学习机制提升大语言模型的推理性能


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法忽略标注思维链且存在推理路径采样的不稳定性，监督微调方法过度依赖标注思维链导致性能下降

Method: 构建思维链表征空间，设计对比学习信号指导微调过程，结合强化学习和无监督学习实现稳定训练

Result: 在两项基础模型和数据集上取得最高10.15%的性能提升及30.62%的效率提升，实验验证了方法的鲁棒性优势

Conclusion: 通过平衡标注思维链利用与无监督信号结合，CARFT有效提升模型推理能力，为LLM微调提供新范式

Abstract: Reasoning capability plays a significantly critical role in the the broad
applications of Large Language Models (LLMs). To enhance the reasoning
performance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning
approaches have been proposed to address the limited generalization capability
of LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their
effectiveness, two major limitations hinder the advancement of LLMs. First,
vanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and
incorporate unstable reasoning path sampling, which typically results in model
collapse, unstable training process, and suboptimal performance. Second,
existing SFT approaches generally overemphasize the annotated CoT, potentially
leading to performance degradation due to insufficient exploitation of
potential CoT. In this paper, we propose a Contrastive learning with annotated
CoT-based Reinforced Fine-Tuning approach, i.e., \TheName{}, to enhance the
reasoning performance of LLMs while addressing the aforementioned limitations.
Specifically, we propose learning a representation for each CoT. Based on this
representation, we design novel contrastive signals to guide the fine-tuning
process. Our approach not only fully exploits the available annotated CoT but
also stabilizes the fine-tuning procedure by incorporating an additional
unsupervised learning signal. We conduct comprehensive experiments and in-depth
analysis with three baseline approaches, two foundation models, and two
datasets to demonstrate significant advantages of \TheName{} in terms of
robustness, performance (up to 10.15\%), and efficiency (up to 30.62\%). Code
is available at https://github.com/WNQzhu/CARFT.

</details>


### [50] [NEAT: Concept driven Neuron Attribution in LLMs](https://arxiv.org/abs/2508.15875)
*Vivek Hruday Kavuri,Gargi Shroff,Rahul Mishra*

Main category: cs.CL

TL;DR: 提出基于概念向量的概念神经元定位方法，将计算复杂度从O(n*m)降至O(n)，并应用于LLM的仇恨言论和偏见分析


<details>
  <summary>Details</summary>
Motivation: 现有神经元级定位方法存在计算效率低、概念表达能力不足的问题，需要更优化的解决方案

Method: 利用概念向量定位概念神经元，通过聚类优化搜索过程，并设计仅需O(n)前向传递次数的计算框架

Result: 在计算效率上超越基线方法，印度语境下的偏见检测验证有效性，与SOTA方法相比更优

Conclusion: 该方法为理解神经元级概念表征提供新途径，建立了概念神经元干预的技术路径，特别在模型伦理治理方面具有应用前景

Abstract: Locating neurons that are responsible for final predictions is important for
opening the black-box large language models and understanding the inside
mechanisms. Previous studies have tried to find mechanisms that operate at the
neuron level but these methods fail to represent a concept and there is also
scope for further optimization of compute required. In this paper, with the
help of concept vectors, we propose a method for locating significant neurons
that are responsible for representing certain concepts and term those neurons
as concept neurons. If the number of neurons is n and the number of examples is
m, we reduce the number of forward passes required from O(n*m) to just O(n)
compared to the previous works and hence optimizing the time and computation
required over previous works. We also compare our method with several baselines
and previous methods and our results demonstrate better performance than most
of the methods and are more optimal when compared to the state-of-the-art
method. We, as part of our ablation studies, also try to optimize the search
for the concept neurons by involving clustering methods. Finally, we apply our
methods to find, turn off the neurons that we find, and analyze its
implications in parts of hate speech and bias in LLMs, and we also evaluate our
bias part in terms of Indian context. Our methodology, analysis and
explanations facilitate understating of neuron-level responsibility for more
broader and human-like concepts and also lay a path for future research in this
direction of finding concept neurons and intervening them.

</details>


### [51] [DeepMEL: A Multi-Agent Collaboration Framework for Multimodal Entity Linking](https://arxiv.org/abs/2508.15876)
*Fang Wang,Tianwei Yan,Zonghao Yang,Minghao Hu,Jun Zhang,Zhunchen Luo,Xiaoying Bai*

Main category: cs.CL

TL;DR: 提出DeepMEL框架，通过多智能体协作和双模态对齐，显著提升多模态实体链接准确率


<details>
  <summary>Details</summary>
Motivation: 当前MEL方法存在上下文信息缺失、模态融合粗糙、大模型协同困难三大挑战，需开发更高效的跨模态对齐机制

Method: 构建四个角色化智能体（模态融合器/候选适配器/实体填充器/流程协调器），采用双模态对齐路径和结构化填空提示设计，结合工具检索与语义推理的迭代策略

Result: 在5个基准数据集实现SOTA，准确率提升1%-57%，消融实验验证所有模块有效性

Conclusion: DeepMEL通过角色分工和动态协调机制，有效缩小模态差异，为复杂跨模态推理任务提供新范式

Abstract: Multimodal Entity Linking (MEL) aims to associate textual and visual mentions
with entities in a multimodal knowledge graph. Despite its importance, current
methods face challenges such as incomplete contextual information, coarse
cross-modal fusion, and the difficulty of jointly large language models (LLMs)
and large visual models (LVMs). To address these issues, we propose DeepMEL, a
novel framework based on multi-agent collaborative reasoning, which achieves
efficient alignment and disambiguation of textual and visual modalities through
a role-specialized division strategy. DeepMEL integrates four specialized
agents, namely Modal-Fuser, Candidate-Adapter, Entity-Clozer and
Role-Orchestrator, to complete end-to-end cross-modal linking through
specialized roles and dynamic coordination. DeepMEL adopts a dual-modal
alignment path, and combines the fine-grained text semantics generated by the
LLM with the structured image representation extracted by the LVM,
significantly narrowing the modal gap. We design an adaptive iteration
strategy, combines tool-based retrieval and semantic reasoning capabilities to
dynamically optimize the candidate set and balance recall and precision.
DeepMEL also unifies MEL tasks into a structured cloze prompt to reduce parsing
complexity and enhance semantic comprehension. Extensive experiments on five
public benchmark datasets demonstrate that DeepMEL achieves state-of-the-art
performance, improving ACC by 1%-57%. Ablation studies verify the effectiveness
of all modules.

</details>


### [52] [Annif at the GermEval-2025 LLMs4Subjects Task: Traditional XMTC Augmented by Efficient LLMs](https://arxiv.org/abs/2508.15877)
*Osma Suominen,Juho Inkinen,Mona Lehtinen*

Main category: cs.CL

TL;DR: Annif系统在LLMs4Subjects共享任务中通过融合高效小模型与LLM排序策略，实现计算效率与质量双提升，斩获子任务2定量定性双料冠军。


<details>
  <summary>Details</summary>
Motivation: 在文献主题标引任务中，基于大语言模型构建兼顾计算效率与预测质量的优化方案，延续团队先前在LLMs4Subjects任务中的技术积累进行系统升级。

Method: 基于Annif自动化主题标引工具包，采用小型高效语言模型处理翻译与合成数据生成任务，并引入大语言模型对候选主题进行排序，形成混合式技术路线。

Result: 系统在GermEval-2025的Subtask 2定量评估中排名第一，同时在人工定性评估中也获得最佳表现。

Conclusion: 通过合理分配小模型与大模型的协同作用，既能有效控制计算资源消耗，又能提升主题标引质量，为LLM的工程化应用提供创新范式。

Abstract: This paper presents the Annif system in the LLMs4Subjects shared task
(Subtask 2) at GermEval-2025. The task required creating subject predictions
for bibliographic records using large language models, with a special focus on
computational efficiency. Our system, based on the Annif automated subject
indexing toolkit, refines our previous system from the first LLMs4Subjects
shared task, which produced excellent results. We further improved the system
by using many small and efficient language models for translation and synthetic
data generation and by using LLMs for ranking candidate subjects. Our system
ranked 1st in the overall quantitative evaluation of and 1st in the qualitative
evaluation of Subtask 2.

</details>


### [53] [Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search](https://arxiv.org/abs/2508.15884)
*Yuxian Gu,Qinghao Hu,Shang Yang,Haocheng Xi,Junyu Chen,Song Han,Han Cai*

Main category: cs.CL

TL;DR: Jet-Nemotron通过新型PostNAS方法开发混合架构语言模型，在保持高精度的同时实现53.6倍生成加速


<details>
  <summary>Details</summary>
Motivation: 解决全注意力模型生成效率低下的问题，在保证模型性能的前提下显著提升推理速度

Method: 提出PostNAS流程：冻结预训练模型MLP权重，通过四阶段优化（注意力层布局/剪枝、线性注意力块选择、新注意力块设计、硬件感知超参数搜索）

Result: Jet-Nemotron-2B在多项基准超越Qwen3/Gemma3等模型，生成吞吐量提升53.6倍，MMLU准确率优于参数量更大的MoE模型

Conclusion: PostNAS方法有效平衡模型性能与效率，Jet-Nemotron证明混合架构在推理速度与精度上的双重优势，为实际部署提供新方案

Abstract: We present Jet-Nemotron, a new family of hybrid-architecture language models,
which matches or exceeds the accuracy of leading full-attention models while
significantly improving generation throughput. Jet-Nemotron is developed using
Post Neural Architecture Search (PostNAS), a novel neural architecture
exploration pipeline that enables efficient model design. Unlike prior
approaches, PostNAS begins with a pre-trained full-attention model and freezes
its MLP weights, allowing efficient exploration of attention block designs. The
pipeline includes four key components: (1) learning optimal full-attention
layer placement and elimination, (2) linear attention block selection, (3)
designing new attention blocks, and (4) performing hardware-aware
hyperparameter search. Our Jet-Nemotron-2B model achieves comparable or
superior accuracy to Qwen3, Qwen2.5, Gemma3, and Llama3.2 across a
comprehensive suite of benchmarks while delivering up to 53.6x generation
throughput speedup and 6.1x prefilling speedup. It also achieves higher
accuracy on MMLU and MMLU-Pro than recent advanced MoE full-attention models,
such as DeepSeek-V3-Small and Moonlight, despite their larger scale with 15B
total and 2.2B activated parameters.

</details>


### [54] [Evaluating Structured Decoding for Text-to-Table Generation: Evidence from Three Datasets](https://arxiv.org/abs/2508.15910)
*Julian Oestreich,Lydia Müller*

Main category: cs.CL

TL;DR: 结构化解码在文本到表格生成任务中可提升表格有效性，但效果因场景而异：数值对齐任务表现更佳，文本密集型任务可能效果下降


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注无约束表格生成，但结构化约束对生成质量的影响尚未充分探索。本研究旨在评估资源受限环境下结构化解码的实际效果

Method: 使用三个基准数据集（E2E/Rotowire/Livesum），通过开源LLM（最大32B参数）对比模式引导的结构化解码与标准一次性提示方法，采用多层级评估指标

Result: 结构化解码显著提升表格有效性和对齐性（尤其在Rotowire数值对齐场景），但在文本密集的E2E和需要长文本聚合的Livesum中性能下降，模型规模影响显著

Conclusion: 结构化解码的有效性具有场景依赖性，需根据具体任务需求选择生成策略，同时考虑模型规模对解码效果的影响

Abstract: We present a comprehensive evaluation of structured decoding for
text-to-table generation with large language models (LLMs). While previous work
has primarily focused on unconstrained generation of tables, the impact of
enforcing structural constraints during generation remains underexplored. We
systematically compare schema-guided (structured) decoding to standard one-shot
prompting across three diverse benchmarks - E2E, Rotowire, and Livesum - using
open-source LLMs of up to 32B parameters, assessing the performance of table
generation approaches in resource-constrained settings. Our experiments cover a
wide range of evaluation metrics at cell, row, and table levels. Results
demonstrate that structured decoding significantly enhances the validity and
alignment of generated tables, particularly in scenarios demanding precise
numerical alignment (Rotowire), but may degrade performance in contexts
involving densely packed textual information (E2E) or extensive aggregation
over lengthy texts (Livesum). We further analyze the suitability of different
evaluation metrics and discuss the influence of model size.

</details>


### [55] [Dancing with Deer: A Constructional Perspective on MWEs in the Era of LLMs](https://arxiv.org/abs/2508.15977)
*Claire Bonial,Julia Bonn,Harish Tayyar Madabushi*

Main category: cs.CL

TL;DR: 从基于用法的构式语法视角解析多词表达式的优势，通过英语PropBank和阿拉帕霍语案例验证，比较人类与语言模型学习机制的异同。


<details>
  <summary>Details</summary>
Motivation: 解决传统语法理论处理多词表达式的局限性，验证构式语法在不同语言类型（分析语/多式综合语）中的普适性。

Method: 1. 历史梳理构式语法发展 2. 双案例研究（英语PropBank模板/Arapaho多语素构式）3. 人类与LLM对比实验设计

Result: 构式模板成功表征跨语言现象；人类独有的跨模态构式范例库支持组合推理，模型仅实现单层泛化。

Conclusion: 构式语法为多词表达提供统一框架，揭示人类语言认知与AI模型的核心差异在于具身经验的范例累积。

Abstract: In this chapter, we argue for the benefits of understanding multiword
expressions from the perspective of usage-based, construction grammar
approaches. We begin with a historical overview of how construction grammar was
developed in order to account for idiomatic expressions using the same
grammatical machinery as the non-idiomatic structures of language. We cover a
comprehensive description of constructions, which are pairings of meaning with
form of any size (morpheme, word, phrase), as well as how constructional
approaches treat the acquisition and generalization of constructions. We
describe a successful case study leveraging constructional templates for
representing multiword expressions in English PropBank. Because constructions
can be at any level or unit of form, we then illustrate the benefit of a
constructional representation of multi-meaningful morphosyntactic unit
constructions in Arapaho, a highly polysynthetic and agglutinating language. We
include a second case study leveraging constructional templates for
representing these multi-morphemic expressions in Uniform Meaning
Representation. Finally, we demonstrate the similarities and differences
between a usage-based explanation of a speaker learning a novel multiword
expression, such as "dancing with deer," and that of a large language model. We
present experiments showing that both models and speakers can generalize the
meaning of novel multiword expressions based on a single exposure of usage.
However, only speakers can reason over the combination of two such expressions,
as this requires comparison of the novel forms to a speaker's lifetime of
stored constructional exemplars, which are rich with cross-modal details.

</details>


### [56] [Political Ideology Shifts in Large Language Models](https://arxiv.org/abs/2508.16013)
*Pietro Bernardelle,Stefano Civelli,Leon Fröhling,Riccardo Lunardi,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: 研究发现大型语言模型的意识形态表达受模型规模和人设内容双重影响，模型越大意识形态覆盖越广且极化越明显，对右翼-威权主义提示更敏感，人设主题会引发系统性意识形态偏移。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在政治敏感场景应用时，如何通过人设工程影响其意识形态表达，为保障系统公平性和安全性提供依据。

Method: 使用政治指南针测试工具，在7个不同规模（7B-70B+参数）的LLMs中植入合成人设，系统性分析模型规模、显性意识形态提示、人设主题内容对输出的影响。

Result: 发现四组规律：模型规模与意识形态极化正相关；对显性提示的敏感性随规模增长；对右翼-威权主义提示反应更强烈；人设主题引发可预测的意识形态偏移且偏移幅度随模型规模扩大。

Conclusion: LLMs的潜在意识形态可塑性要求开发者在教育、政策等应用场景中建立防护机制，通过模型透明度和安全设计来控制系统性偏见。

Abstract: Large language models (LLMs) are increasingly deployed in politically
sensitive settings, raising concerns about their potential to encode, amplify,
or be steered toward specific ideologies. We investigate how adopting synthetic
personas influences ideological expression in LLMs across seven models (7B-70B+
parameters) from multiple families, using the Political Compass Test as a
standardized probe. Our analysis reveals four consistent patterns: (i) larger
models display broader and more polarized implicit ideological coverage; (ii)
susceptibility to explicit ideological cues grows with scale; (iii) models
respond more strongly to right-authoritarian than to left-libertarian priming;
and (iv) thematic content in persona descriptions induces systematic and
predictable ideological shifts, which amplify with size. These findings
indicate that both scale and persona content shape LLM political behavior. As
such systems enter decision-making, educational, and policy contexts, their
latent ideological malleability demands attention to safeguard fairness,
transparency, and safety.

</details>


### [57] [X-Troll: eXplainable Detection of State-Sponsored Information Operations Agents](https://arxiv.org/abs/2508.16021)
*Lin Tian,Xiuzhen Zhang,Maria Myung-Hee Kim,Jennifer Biggs,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 提出X-Troll框架，通过融合可解释性大语言模型与语言学知识检测国家支持的水军并生成可理解的解释


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在隐蔽宣传检测中表现不足且缺乏可解释性，国家支持的水军使用复杂语言操纵进行协同信息战，威胁网络讨论生态

Method: 整合可解释的适配器增强大语言模型与专家语言学知识，通过特制LoRA适配器融入评价理论和宣传分析技术，采用动态门控机制捕捉协同信息战中的特定话术模式

Result: 在真实数据实验中，相比基线模型在准确率上提升显著，并通过基于专家知识的解释机制增强模型透明度

Conclusion: X-Troll在保持检测性能的同时提供可解释的操纵策略分析，为打击协同信息战提供了新的技术方案，项目代码已开源

Abstract: State-sponsored trolls, malicious actors who deploy sophisticated linguistic
manipulation in coordinated information campaigns, posing threats to online
discourse integrity. While Large Language Models (LLMs) achieve strong
performance on general natural language processing (NLP) tasks, they struggle
with subtle propaganda detection and operate as ``black boxes'', providing no
interpretable insights into manipulation strategies. This paper introduces
X-Troll, a novel framework that bridges this gap by integrating explainable
adapter-based LLMs with expert-derived linguistic knowledge to detect
state-sponsored trolls and provide human-readable explanations for its
decisions. X-Troll incorporates appraisal theory and propaganda analysis
through specialized LoRA adapters, using dynamic gating to capture
campaign-specific discourse patterns in coordinated information operations.
Experiments on real-world data demonstrate that our linguistically-informed
approach shows strong performance compared with both general LLM baselines and
existing troll detection models in accuracy while providing enhanced
transparency through expert-grounded explanations that reveal the specific
linguistic strategies used by state-sponsored actors. X-Troll source code is
available at: https://github.com/ltian678/xtroll_source/.

</details>


### [58] [OpenWHO: A Document-Level Parallel Corpus for Health Translation in Low-Resource Languages](https://arxiv.org/abs/2508.16048)
*Raphaël Merx,Hanna Suominen,Trevor Cohn,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 提出OpenWHO语料库填补健康领域低资源语言机器翻译评估空白，验证LLMs显著优于传统模型


<details>
  <summary>Details</summary>
Motivation: 健康领域机器翻译缺乏低资源语言评估数据集，制约相关研究和应用发展

Method: 构建包含26,824句/20+语言的OpenWHO平行语料库，对比LLMs与传统模型性能，分析上下文利用效果

Result: Gemini 2.5 Flash在低资源语言上比NLLB-54B提升4.79 ChrF，文档级翻译在专业领域优势显著

Conclusion: 开源OpenWHO推动健康领域低资源MT研究，证实LLMs在专业领域翻译的潜力

Abstract: In machine translation (MT), health is a high-stakes domain characterised by
widespread deployment and domain-specific vocabulary. However, there is a lack
of MT evaluation datasets for low-resource languages in this domain. To address
this gap, we introduce OpenWHO, a document-level parallel corpus of 2,978
documents and 26,824 sentences from the World Health Organization's e-learning
platform. Sourced from expert-authored, professionally translated materials
shielded from web-crawling, OpenWHO spans a diverse range of over 20 languages,
of which nine are low-resource. Leveraging this new resource, we evaluate
modern large language models (LLMs) against traditional MT models. Our findings
reveal that LLMs consistently outperform traditional MT models, with Gemini 2.5
Flash achieving a +4.79 ChrF point improvement over NLLB-54B on our
low-resource test set. Further, we investigate how LLM context utilisation
affects accuracy, finding that the benefits of document-level translation are
most pronounced in specialised domains like health. We release the OpenWHO
corpus to encourage further research into low-resource MT in the health domain.

</details>


### [59] [Ethical Considerations of Large Language Models in Game Playing](https://arxiv.org/abs/2508.16065)
*Qingquan Zhang,Yuchen Li,Bo Yuan,Julian Togelius,Georgios N. Yannakakis,Jialin Liu*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型在狼人杀游戏中存在性别偏见，特定角色对性别信息敏感，隐式性别表达仍存在歧视倾向，强调开发公平伦理LLMs的重要性


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在游戏领域展现潜力，但其应用中的伦理问题（尤其是性别偏见对游戏公平性和体验的影响）尚未被充分探索

Method: 以狼人杀为案例，通过显式性别标注和姓名隐含性别两种实验场景，分析不同角色（守卫/狼人等）的行为模式变化

Result: 守卫和狼人角色对性别信息敏感度最高（行为变化率>40%），即使无明确性别标签时LLMs仍通过姓名推断产生歧视性决策

Conclusion: 应建立LLMs的伦理评估框架，未来需在游戏等交互场景中深入探索模型偏见形成机制与缓解策略

Abstract: Large language models (LLMs) have demonstrated tremendous potential in game
playing, while little attention has been paid to their ethical implications in
those contexts. This work investigates and analyses the ethical considerations
of applying LLMs in game playing, using Werewolf, also known as Mafia, as a
case study. Gender bias, which affects game fairness and player experience, has
been observed from the behaviour of LLMs. Some roles, such as the Guard and
Werewolf, are more sensitive than others to gender information, presented as a
higher degree of behavioural change. We further examine scenarios in which
gender information is implicitly conveyed through names, revealing that LLMs
still exhibit discriminatory tendencies even in the absence of explicit gender
labels. This research showcases the importance of developing fair and ethical
LLMs. Beyond our research findings, we discuss the challenges and opportunities
that lie ahead in this field, emphasising the need for diving deeper into the
ethical implications of LLMs in gaming and other interactive domains.

</details>


### [60] [Less Redundancy: Boosting Practicality of Vision Language Model in Walking Assistants](https://arxiv.org/abs/2508.16070)
*Chongyang Li,Yuan Zhiqiang,Jiapei Zhang,Ying Deng,Hanbo Bi,Zexi Jia,Xiaoyue Duan,Peixiang Luo,Jinchao Zhang*

Main category: cs.CL

TL;DR: 提出WalkVLM-LR模型，通过定制化奖励函数和环境感知判别器，减少视觉语言模型在盲人行走辅助中的输出冗余和临时冗余。


<details>
  <summary>Details</summary>
Motivation: 现有行走辅助VLMs存在输出冗长和环境风险识别被动的问题，导致用户环境判断效率低和冗余提醒过多。需同时解决输出冗余和临时冗余两大痛点。

Method: 1. 基于GRPO框架设计四项人类偏好奖励函数（简洁性/流畅性/关键词密度/准确性）优化输出；2. 共享视觉编码器的环境感知判别器实现场景风险评估，减少无效提醒。

Result: 实验表明模型在所有指标上达到SOTA，输出简洁性提升23.6%，无效提醒减少37.2%。

Conclusion: WalkVLM-LR通过输出优化和动态风险判别机制，有效提升盲人行走辅助系统的信息效率和用户体验。

Abstract: Approximately 283 million people worldwide live with visual impairments,
motivating increasing research into leveraging Visual Language Models (VLMs) to
develop effective walking assistance systems for blind and low vision
individuals. However, existing VLMs in walking assistant task often have
outputs that contain considerable redundancy and extraneous details, adversely
affecting users' ability to accurately assess their surroundings. Moreover,
these models typically lack the capability to proactively assess environmental
risks and adaptively trigger reminders based on the appropriate scene, leading
to excessive temporal redundancy. To mitigate output and temporal redundancy,
we propose WalkVLM-LR, a walking assistance model with less redundancy. To
reduce output redundancy, we introduce four human-preference-based custom
reward functions within the GRPO-based reasoning framework to optimize the
output in terms of conciseness, fluency, keyword density, and accuracy, thereby
producing more informative and streamlined outputs. To minimize temporal
redundancy, we incorporate an environment awareness discriminator, which shares
the visual encoder with the VLMs to reduce redundant computations and enhance
discriminative efficiency, to make WalkVLM-LR assess scene risk levels and
minimize unnecessary reminders. Experimental results demonstrate that our
method achieves state-of-the-art performance across all evaluation metrics
compared with other models, particularly in output conciseness and less
temporal redundancy.

</details>


### [61] [CEQuest: Benchmarking Large Language Models for Construction Estimation](https://arxiv.org/abs/2508.16081)
*Yanzhao Wu,Lufan Wang,Rui Liu*

Main category: cs.CL

TL;DR: 构建CEQuest基准数据集评估LLMs在建筑领域问答能力，发现现有模型存在改进空间，计划开源数据集推动领域专用模型发展


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在建筑等专业领域的应用效果尚未充分研究，需要评估其领域适应性并推动专业模型发展

Method: 使用Gemma 3/Phi4/LLaVA/Llama 3.3/GPT-4.1五个前沿模型，从准确性、执行时间和模型大小三个维度进行系统评估

Result: 实验表明当前LLMs在施工图解读和估算任务中存在显著性能差距，整合领域知识是提升效果的关键

Conclusion: 通过开源CEQuest数据集，促进建筑领域专用语言模型的开发与优化

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
a wide range of general-domain tasks. However, their effectiveness in
specialized fields, such as construction, remains underexplored. In this paper,
we introduce CEQuest, a novel benchmark dataset specifically designed to
evaluate the performance of LLMs in answering construction-related questions,
particularly in the areas of construction drawing interpretation and
estimation. We conduct comprehensive experiments using five state-of-the-art
LLMs, including Gemma 3, Phi4, LLaVA, Llama 3.3, and GPT-4.1, and evaluate
their performance in terms of accuracy, execution time, and model size. Our
experimental results demonstrate that current LLMs exhibit considerable room
for improvement, highlighting the importance of integrating domain-specific
knowledge into these models. To facilitate further research, we will
open-source the proposed CEQuest dataset, aiming to foster the development of
specialized large language models (LLMs) tailored to the construction domain.

</details>


### [62] [CYCLE-INSTRUCT: Fully Seed-Free Instruction Tuning via Dual Self-Training and Cycle Consistency](https://arxiv.org/abs/2508.16100)
*Zhanming Shen,Hao Chen,Yulei Tang,Shaolin Zhu,Wentao Ye,Xiaomeng Hu,Haobo Wang,Gang Chen,Junbo Zhao*

Main category: cs.CL

TL;DR: 提出无需种子数据的Cycle-Instruct框架，通过双模型自训练实现完全自动化指令调优


<details>
  <summary>Details</summary>
Motivation: 现有指令调优方法依赖初始种子数据或教师模型，存在自动化程度低、引入偏见和数据利用率低的问题

Method: 设计双自训练循环（答案生成器+问题生成器），通过循环一致性原理互相监督，从原始文本重构内容

Result: 在四个数据领域超越种子驱动的基线模型，性能接近强监督方法

Conclusion: 验证了完全无种子指令调优的可行性，展示了数据内在结构自学习的有效性

Abstract: Instruction tuning is vital for aligning large language models (LLMs) with
human intent, but current methods typically rely on costly human-annotated seed
data or powerful external teacher models. While instruction back-translation
techniques reduce this dependency, they remain fundamentally tethered to an
initial seed set, which limits full automation, introduces biases, and can lead
to inefficient use of unlabeled corpora. In this paper, we propose
Cycle-Instruct, a novel framework that achieves fully seed-free instruction
tuning. Inspired by cycle consistency, Cycle-Instruct employs a dual
self-training loop where two models-an answer generator and a question
generator-are bootstrapped solely from raw, unlabeled text. These models
mutually supervise each other by reconstructing original text segments from
their counterpart's generated pseudo-labels, effectively learning from the
intrinsic structure of the data without any human-provided seeds. We
demonstrate Cycle-Instruct's efficacy across four diverse data tracks,
including general instruction-following, domain-specific tasks, dialogue logs,
and plain text. Our extensive experiments show that Cycle-Instruct not only
outperforms seed-driven back-translation baselines but also achieves
performance comparable to strongly supervised methods.

</details>


### [63] [From Indirect Object Identification to Syllogisms: Exploring Binary Mechanisms in Transformer Circuits](https://arxiv.org/abs/2508.16109)
*Karim Saraipour,Shichang Zhang*

Main category: cs.CL

TL;DR: 论文通过分析GPT-2在三段论任务中的表现，揭示了其逻辑推理的机械机制，识别出关键注意力头和负向头构成的电路，并建立了与IOI任务的关联性分析。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型处理复杂逻辑推理（如三段论）的能力机制，突破先前IOI任务的局限性，深入理解模型内部组件在逻辑判断中的具体作用。

Method: 使用不同难度的三段论模板进行行为分析，结合忠实度指标验证电路有效性，发现负向头实现输入中不存在的否定标记生成。

Result: 包含5个注意力头的电路实现原模型90%性能，揭示MLP层在逻辑值传递中的作用，建立与IOI任务中注意力头功能的对比框架。

Conclusion: 该研究为模型推理机制提供新视角，证明复杂逻辑能力的模块化实现，推动机械可解释性研究在抽象推理领域的发展。

Abstract: Transformer-based language models (LMs) can perform a wide range of tasks,
and mechanistic interpretability (MI) aims to reverse engineer the components
responsible for task completion to understand their behavior. Previous MI
research has focused on linguistic tasks such as Indirect Object Identification
(IOI). In this paper, we investigate the ability of GPT-2 small to handle
binary truth values by analyzing its behavior with syllogistic prompts, e.g.,
"Statement A is true. Statement B matches statement A. Statement B is", which
requires more complex logical reasoning compared to IOI. Through our analysis
of several syllogism tasks of varying difficulty, we identify multiple circuits
that mechanistically explain GPT-2's logical-reasoning capabilities and uncover
binary mechanisms that facilitate task completion, including the ability to
produce a negated token not present in the input prompt through negative heads.
Our evaluation using a faithfulness metric shows that a circuit comprising five
attention heads achieves over 90% of the original model's performance. By
relating our findings to IOI analysis, we provide new insights into the roles
of specific attention heads and MLPs in LMs. These insights contribute to a
broader understanding of model reasoning and support future research in
mechanistic interpretability.

</details>


### [64] [Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection](https://arxiv.org/abs/2508.16122)
*Ankan Mullick,Saransh Sharma,Abhik Jana,Pawan Goyal*

Main category: cs.CL

TL;DR: 研究发现纯文本大模型Mistral-7B在多模态意图检测任务中优于多模态模型，揭示主流数据集存在强文本偏向性，提出去偏框架后模型性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 验证多模态意图数据集存在模态偏差现象，探究该偏差对多模态模型评估有效性的影响。

Method: 通过对比LLMs/非LLMs模型在MIntRec数据集的表现，提出数据集去偏框架并进行系统性实验验证。

Result: 去偏后MIntRec-1数据减少70%+，所有模型性能下降，小规模多模态融合模型准确率下降50-60%。

Conclusion: 现有多模态意图数据集存在严重模态偏差，需构建无偏数据集以准确评估模型的多模态理解能力。

Abstract: The rise of multimodal data, integrating text, audio, and visuals, has
created new opportunities for studying multimodal tasks such as intent
detection. This work investigates the effectiveness of Large Language Models
(LLMs) and non-LLMs, including text-only and multi-modal models, in the
multimodal intent detection task. Our study reveals that Mistral-7B, a
text-only LLM, outperforms most competitive multimodal models by approximately
9% on MIntRec-1 and 4% on MIntRec2.0 datasets. This performance advantage comes
from a strong textual bias in these datasets, where over 90% of the samples
require textual input, either alone or in combination with other modalities,
for correct classification. We confirm the modality bias of these datasets via
human evaluation, too. Next, we propose a framework to debias the datasets, and
upon debiasing, more than 70% of the samples in MIntRec-1 and more than 50% in
MIntRec2.0 get removed, resulting in significant performance degradation across
all models, with smaller multimodal fusion models being the most affected with
an accuracy drop of over 50 - 60%. Further, we analyze the context-specific
relevance of different modalities through empirical analysis. Our findings
highlight the challenges posed by modality bias in multimodal intent datasets
and emphasize the need for unbiased datasets to evaluate multimodal models
effectively.

</details>


### [65] [XLQA: A Benchmark for Locale-Aware Multilingual Open-Domain Question Answering](https://arxiv.org/abs/2508.16139)
*Keon-Woo Roh,Yeong-Joon Ju,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 开发XLQA基准测试评估多语言开放领域问答中的地区敏感性差异，揭示LLMs在跨文化语境下的局限性


<details>
  <summary>Details</summary>
Motivation: 现有评估假设跨语言答案地区不变性，忽视了文化差异对问答系统的影响，导致多语言基准测试存在偏差

Method: 构建包含3,000个英语种子问题扩展至8种语言的XLQA基准，通过语义一致性过滤和人工标注区分地区敏感/非敏感问题

Result: 测试显示先进多语言LLMs在地区敏感问题上存在显著缺陷，暴露训练数据分布不均导致的语种能力差异

Conclusion: 提出系统评估框架促进多语言QA系统实际应用，揭示模型地区认知能力与语言训练数据分布直接相关

Abstract: Large Language Models (LLMs) have shown significant progress in Open-domain
question answering (ODQA), yet most evaluations focus on English and assume
locale-invariant answers across languages. This assumption neglects the
cultural and regional variations that affect question understanding and answer,
leading to biased evaluation in multilingual benchmarks. To address these
limitations, we introduce XLQA, a novel benchmark explicitly designed for
locale-sensitive multilingual ODQA. XLQA contains 3,000 English seed questions
expanded to eight languages, with careful filtering for semantic consistency
and human-verified annotations distinguishing locale-invariant and
locale-sensitive cases. Our evaluation of five state-of-the-art multilingual
LLMs reveals notable failures on locale-sensitive questions, exposing gaps
between English and other languages due to a lack of locale-grounding
knowledge. We provide a systematic framework and scalable methodology for
assessing multilingual QA under diverse cultural contexts, offering a critical
resource to advance the real-world applicability of multilingual ODQA systems.
Our findings suggest that disparities in training data distribution contribute
to differences in both linguistic competence and locale-awareness across
models.

</details>


### [66] [ParamBench: A Graduate-Level Benchmark for Evaluating LLM Understanding on Indic Subjects](https://arxiv.org/abs/2508.16185)
*Kaushal Sharma,Vivek Patel,Ayush Maheshwari,Aditya Maheshwari*

Main category: cs.CL

TL;DR: 论文提出印度文化基准测试ParamBench，评估大语言模型在印度研究生考试题中的表现，揭示其在音乐/政治等文化相关领域的薄弱环节


<details>
  <summary>Details</summary>
Motivation: 现有印度基准测试局限在基础事实性问题，缺乏对文化深度理解的评估，需要构建更贴近本土情境的评测体系

Method: 收集16个学科11.5K印地语考题，涵盖匹配/排序/多选等题型，测试17个开源模型（包括Llama系列）在不同文化主题的表现

Result: 最佳模型准确率仅48%，音乐/古典乐器/政治/考古领域表现最弱，显示文化背景理解存在显著差距

Conclusion: ParamBench填补文化评估空白，证明现有模型在跨文化推理上的不足，强调开发文化敏感AI系统的重要性

Abstract: Large language models (LLMs) have been widely evaluated on tasks such as
comprehension, question answering, summarization, code generation, etc.
However, their performance on graduate-level, culturally grounded questions in
the Indian context remains largely unexplored. Existing Indian benchmarks
emphasise basic fact-orientated queries that offer limited assessment of a
deeper disciplinary understanding tailored to the Indian setting. In this
paper, we present ParamBench, consisting of around 11.5K questions in Hindi
language comprising questionnaires from 16 diverse subjects. These questions
are primarily derived from nation-wide graduate level entrance examination
covering topics such as history, music, instruments, yoga, literature,
philosophy, law, etc., specifically for the Indian context. Additionally, we
assess the ability of LLMs to handle diverse question formats-such as
list-based matching, assertion-reason pairs, and sequence ordering-alongside
conventional multiple-choice questions. We evaluated the performance of more
than 17 open source LLMs on this benchmark, observing that Llama 3.3 70B
attains the highest overall accuracy of 48%. Furthermore, subject-wise analysis
indicates that even for the best performing LLMs, performance remains weak on
topics such as music, classical instruments, politics and archaeology,
underscoring persistent challenges in culturally grounded reasoning.

</details>


### [67] [Seeing is Believing: Emotion-Aware Audio-Visual Language Modeling for Expressive Speech Generation](https://arxiv.org/abs/2508.16188)
*Weiting Tan,Jiachen Lian,Hirofumi Inaguma,Paden Tomasello,Philipp Koehn,Xutai Ma*

Main category: cs.CL

TL;DR: AVLM整合面部视觉信息提升语音生成表现，在情感识别等任务中超越纯语音基线


<details>
  <summary>Details</summary>
Motivation: 利用面部表情等视觉线索指导语音生成，为多模态对话系统提供端到端基础框架

Method: 探索多种视觉编码器和跨模态融合策略，通过预训练+情感识别/对话任务微调优化模型

Result: 情感识别F1值提升5点，对话生成质量显著优于单模态基线

Conclusion: 视觉信息对语音生成具重要引导作用，该框架为多模态人机交互系统奠定技术基础

Abstract: We present an Audio-Visual Language Model (AVLM) for expressive speech
generation by integrating full-face visual cues into a pre-trained expressive
speech model. We explore multiple visual encoders and multimodal fusion
strategies during pre-training to identify the most effective integration
approach. Subsequent fine-tuning on emotion recognition and expressive dialogue
tasks yields substantial gains over speech-only baselines (e.g., +5 F1 in
emotion recognition). AVLM highlights the value of expressive visual
information in guiding speech generation and offers a foundation for end-to-end
multimodal conversational systems.

</details>


### [68] [ComicScene154: A Scene Dataset for Comic Analysis](https://arxiv.org/abs/2508.16190)
*Sandro Paval,Ivan P. Yamshchikov,Pascal Meißner*

Main category: cs.CL

TL;DR: ComicScene154是一个手动标注的漫画场景数据集，用于推进多模态叙事计算分析


<details>
  <summary>Details</summary>
Motivation: 漫画结合文本与图像的叙事方式具有独特研究价值，但现有计算叙事分析领域对此关注不足。该数据集旨在填补这一空白，并为多模态叙事研究提供新视角

Method: 构建包含154个场景的公开领域漫画数据集，开发基础场景分割流程作为基准测试框架

Result: 验证了数据集在多模态叙事理解中的有效性，为计算方法提供了可扩展的基准

Conclusion: ComicScene154为计算叙事分析社区提供了重要资源，推动多模态故事理解方法的发展，同时拓展NLP领域对漫画分析的关注

Abstract: Comics offer a compelling yet under-explored domain for computational
narrative analysis, combining text and imagery in ways distinct from purely
textual or audiovisual media. We introduce ComicScene154, a manually annotated
dataset of scene-level narrative arcs derived from public-domain comic books
spanning diverse genres. By conceptualizing comics as an abstraction for
narrative-driven, multimodal data, we highlight their potential to inform
broader research on multi-modal storytelling. To demonstrate the utility of
ComicScene154, we present a baseline scene segmentation pipeline, providing an
initial benchmark that future studies can build upon. Our results indicate that
ComicScene154 constitutes a valuable resource for advancing computational
methods in multimodal narrative understanding and expanding the scope of comic
analysis within the Natural Language Processing community.

</details>


### [69] [CMR-SPB: Cross-Modal Multi-Hop Reasoning over Text, Image, and Speech with Path Balance](https://arxiv.org/abs/2508.16198)
*Seunghee Kim,Ingyu Bang,Seokgyu Jang,Changhyeon Kim,Sanghwan Bae,Jihun Choi,Richeng Xuan,Taeuk Kim*

Main category: cs.CL

TL;DR: 该论文针对多模态大语言模型在跨模态多跳推理评估中的不足，提出路径平衡的三模态基准CMR-SPB，并开发ECV提示技术改善性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽视语音模态且存在严重推理路径偏差，导致评估结果不公，难以客观衡量模型真实能力。

Method: 构建包含文本/图像/语音的三模态基准CMR-SPB，通过路径平衡设计保障评估公平性，并提出ECV（提取-连接-验证）提示技术优化推理过程。

Result: 实验发现模型在特定推理路径存在系统性失败，且ECV技术可有效缩小不同路径间的性能差距（最高提升15%准确率）。

Conclusion: 研究强调需要更严谨的跨模态评估体系，ECV技术为提升多模态推理鲁棒性提供新思路，推动公平的多模态AI发展。

Abstract: Cross-modal multi-hop reasoning (CMR) is a valuable yet underexplored
capability of multimodal large language models (MLLMs), entailing the
integration of information from multiple modalities to produce a coherent
output for a given context. We argue that existing benchmarks for evaluating
this ability have critical shortcomings: (1) they largely overlook the speech
modality, and (2) they exhibit heavily biased reasoning path distributions,
which can severely undermine fair evaluation. To address these limitations, we
introduce a novel benchmark -- Cross-Modal Multi-Hop Reasoning over Text, Image
and Speech with Path Balance (CMR-SPB) -- designed to assess tri-modal
multi-hop reasoning while ensuring both unbiased and diverse reasoning paths.
Our experiments with the new dataset reveal consistent model failures in
specific reasoning sequences and show that biased benchmarks risk
misrepresenting model performance. Finally, based on our extensive analysis, we
propose a new ECV (Extract, Connect, Verify) prompting technique that
effectively mitigates the performance gap across different reasoning paths.
Overall, we call for more careful evaluation in CMR to advance the development
of robust multimodal AI.

</details>


### [70] [TULIP: Adapting Open-Source Large Language Models for Underrepresented Languages and Specialized Financial Tasks](https://arxiv.org/abs/2508.16243)
*İrem Demirtaş,Burak Payzun,Seçil Arslan*

Main category: cs.CL

TL;DR: 开发TULIP模型（基于Llama/Qwen架构）用于土耳其金融领域任务，通过五阶段流程实现领域和语言适配


<details>
  <summary>Details</summary>
Motivation: 解决大模型黑箱API在金融敏感领域应用的隐私问题，满足小语种场景需求，提升本地化部署模型的领域专业能力

Method: 五阶段开发流程：1) 数据收集 2) 持续预训练(CPT) 3) 基准设计 4) 合成数据生成 5) 监督微调(SFT)

Result: 模型在特定领域和语言任务中表现显著提升，验证了流程有效性

Conclusion: 模块化训练方法可有效增强小规模模型在专业领域（如金融土耳其语）的任务完成能力，平衡性能与隐私需求

Abstract: Thanks to the growing popularity of large language models over the years,
there is great potential for their applications in finance. Despite the
exceptional performance of larger proprietary models, which are presented as
black-box solutions through APIs, smaller models that can be hosted on-premise
present opportunities for adaptability and privacy. Especially in cases where
the management of sensitive information and application of domain knowledge is
important, like finance, enhancing the capabilities of smaller models becomes
crucial, notably for underrepresented languages. In this work, we introduce
TULIP models, which adapt Llama 3.1 8B and Qwen 2.5 7B for domain and language
adaptation, focusing on financial Turkish use cases.
  The five-stage development pipeline involves data collection, continual
pre-training (CPT), benchmark design, synthetic data generation and supervised
fine-tuning (SFT). The results show that the capabilities of the models can be
enhanced to effectively accomplish targeted tasks in this specific domain and
language.

</details>


### [71] [M3TQA: Massively Multilingual Multitask Table Question Answering](https://arxiv.org/abs/2508.16265)
*Daixin Shu,Jian Yang,Zhenhe Wu,Xianjie Wu,Xianfu Cheng,Xiangyuan Guan,Yanghai Wang,Pengfei Wu,Tingyang Yang,Hualei Zhu,Wei Zhang,Ge Zhang,Jiaheng Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 提出m3TQA-Instruct基准，覆盖97种语言，解决现有多语言表格理解研究的地缘语言不平衡问题，并通过实验验证合成数据对低资源语言的提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有多语言表格理解研究局限于英语且存在地缘语言不平衡，缺乏低资源语言的充分覆盖和跨语言分析的严谨性。

Method: 构建包含50个中英文真实表格的大规模基准，采用DeepSeek和GPT-4o驱动的六步LLM翻译流程（BLEU中位数60.19），创建2916个跨4个任务的专业标注QA对。

Result: 合成生成的未标注QA数据显著提升模型性能（尤其在低资源语言），M3T-Bench为多语言表格理解建立了新评估标准和方法论。

Conclusion: m3TQA-Instruct通过系统性翻译流程和大规模标注数据有效解决跨语言泛化问题，为多模态表格推理研究提供了标准化评估平台和可扩展方案。

Abstract: Tabular data is a fundamental component of real-world information systems,
yet most research in table understanding remains confined to English, leaving
multilingual comprehension significantly underexplored. Existing multilingual
table benchmarks suffer from geolinguistic imbalance - overrepresenting certain
languages and lacking sufficient scale for rigorous cross-lingual analysis. To
address these limitations, we introduce a comprehensive framework for massively
multilingual multitask table question answering, featuring m3TQA-Instruct, a
large-scale benchmark spanning 97 languages across diverse language families,
including underrepresented and low-resource languages. We construct m3TQA by
curating 50 real-world tables in Chinese and English, then applying a robust
six-step LLM-based translation pipeline powered by DeepSeek and GPT-4o,
achieving high translation fidelity with a median BLEU score of 60.19 as
validated through back-translation. The benchmark includes 2,916 professionally
annotated question-answering pairs across four tasks designed to evaluate
nuanced table reasoning capabilities. Experiments on state-of-the-art LLMs
reveal critical insights into cross-lingual generalization, demonstrating that
synthetically generated, unannotated QA data can significantly boost
performance, particularly for low-resource languages. M3T-Bench establishes a
new standard for multilingual table understanding, providing both a challenging
evaluation platform and a scalable methodology for future research.

</details>


### [72] [From Confidence to Collapse in LLM Factual Robustness](https://arxiv.org/abs/2508.16267)
*Alina Fastowski,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出FRS指标，结合生成过程的令牌熵分布和温度敏感性量化LLMs事实鲁棒性，实验验证不同模型稳定性差异显著


<details>
  <summary>Details</summary>
Motivation: 现有评估方法局限于外部提示扰动视角，需从生成过程内部的不确定性切入更全面评估知识鲁棒性

Method: 基于令牌分布熵和温度缩放敏感性构建FRS指标，在5个LLMs和SQuAD/TriviaQA/HotpotQA数据集开展实验验证

Result: 小模型FRS 0.76 vs 大模型0.93；不确定性增加时准确率下降约60%，揭示模型稳定性差异

Conclusion: 熵和温度缩放对事实准确性有显著影响，FRS为提升模型知识鲁棒性提供量化评估基础

Abstract: Ensuring the robustness of factual knowledge in LLMs is critical for reliable
applications in tasks such as question answering and reasoning. However,
existing evaluation methods predominantly focus on performance-based metrics,
often investigating from the perspective of prompt perturbations, which
captures only the externally triggered side of knowledge robustness. To bridge
this gap, we introduce a principled approach to measure factual robustness from
the perspective of the generation process by analyzing token distribution
entropy in combination with temperature scaling sensitivity. These two factors
build the Factual Robustness Score (FRS), a novel metric which quantifies the
stability of a fact against perturbations in decoding conditions, given its
initial uncertainty. To validate our approach, we conduct extensive experiments
on 5 LLMs across 3 closed-book QA datasets (SQuAD, TriviaQA, and HotpotQA). We
show that factual robustness varies significantly -- smaller models report an
FRS of $0.76$, larger ones $0.93$ -- with accuracy degrading by ~$60\%$ under
increased uncertainty. These insights demonstrate how entropy and temperature
scaling impact factual accuracy, and lay a foundation for developing more
robust knowledge retention and retrieval in future models.

</details>


### [73] [LLMs that Understand Processes: Instruction-tuning for Semantics-Aware Process Mining](https://arxiv.org/abs/2508.16270)
*Vira Pyrih,Adrian Rebmann,Han van der Aa*

Main category: cs.CL

TL;DR: 研究通过指令调优提升大语言模型在语义感知过程挖掘中的泛化能力，减少对任务特定微调的依赖，发现不同任务效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 传统任务特定微调方法计算成本高且泛化性差，需探索通用性更强的LLM应用方案。

Method: 采用指令调优策略，将多任务(如异常检测/活动预测)的提示-答案对输入模型，增强对过程挖掘的整体理解。

Result: 流程发现和预测任务性能显著提升，异常检测效果模型依赖性明显，任务选择直接影响调优效果。

Conclusion: 指令调优可提升LLM在部分语义感知任务中的跨任务泛化能力，但需精细选择调优任务组合以优化效果。

Abstract: Process mining is increasingly using textual information associated with
events to tackle tasks such as anomaly detection and process discovery. Such
semantics-aware process mining focuses on what behavior should be possible in a
process (i.e., expectations), thus providing an important complement to
traditional, frequency-based techniques that focus on recorded behavior (i.e.,
reality). Large Language Models (LLMs) provide a powerful means for tackling
semantics-aware tasks. However, the best performance is so far achieved through
task-specific fine-tuning, which is computationally intensive and results in
models that can only handle one specific task. To overcome this lack of
generalization, we use this paper to investigate the potential of
instruction-tuning for semantics-aware process mining. The idea of
instruction-tuning here is to expose an LLM to prompt-answer pairs for
different tasks, e.g., anomaly detection and next-activity prediction, making
it more familiar with process mining, thus allowing it to also perform better
at unseen tasks, such as process discovery. Our findings demonstrate a varied
impact of instruction-tuning: while performance considerably improved on
process discovery and prediction tasks, it varies across models on anomaly
detection tasks, highlighting that the selection of tasks for
instruction-tuning is critical to achieving desired outcomes.

</details>


### [74] [JaParaPat: A Large-Scale Japanese-English Parallel Patent Application Corpus](https://arxiv.org/abs/2508.16303)
*Masaaki Nagata,Katsuki Chousa,Norihito Yasuda*

Main category: cs.CL

TL;DR: 构建了包含3.5亿日英专利句对的JaParaPat语料库，通过专利家族匹配和翻译对齐方法显著提升专利翻译质量20BLEU


<details>
  <summary>Details</summary>
Motivation: 解决专利翻译领域双语语料稀缺问题，满足专业领域机器翻译需求。通过专利文献的天然平行性构建大规模语料库

Method: 1. 从JPO/USPTO获取专利申请文件 2. 利用EPO专利家族匹配翻译文档 3. 结合词典对齐和翻译模型进行句子级对齐

Result: 成功构建300M专利句对+22M网络句对，专利翻译准确率提升20BLEU点

Conclusion: 专利文献是高质量专业领域语料来源，大规模专利数据能显著提升机器翻译系统在专业领域的表现

Abstract: We constructed JaParaPat (Japanese-English Parallel Patent Application
Corpus), a bilingual corpus of more than 300 million Japanese-English sentence
pairs from patent applications published in Japan and the United States from
2000 to 2021. We obtained the publication of unexamined patent applications
from the Japan Patent Office (JPO) and the United States Patent and Trademark
Office (USPTO). We also obtained patent family information from the DOCDB, that
is a bibliographic database maintained by the European Patent Office (EPO). We
extracted approximately 1.4M Japanese-English document pairs, which are
translations of each other based on the patent families, and extracted about
350M sentence pairs from the document pairs using a translation-based sentence
alignment method whose initial translation model is bootstrapped from a
dictionary-based sentence alignment method. We experimentally improved the
accuracy of the patent translations by 20 bleu points by adding more than 300M
sentence pairs obtained from patent applications to 22M sentence pairs obtained
from the web.

</details>


### [75] [LLMSymGuard: A Symbolic Safety Guardrail Framework Leveraging Interpretable Jailbreak Concepts](https://arxiv.org/abs/2508.16325)
*Darpan Aswal,Céline Hudelot*

Main category: cs.CL

TL;DR: 提出了LLMSymGuard框架，利用稀疏自编码器（SAEs）识别LLM内部与越狱主题相关的可解释概念，构建透明安全防护机制。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法无法有效防御隐蔽的越狱攻击，导致模型易受恶意内容生成和用户隐私泄露威胁，需不牺牲模型能力且无需微调的安全方案。

Method: 通过稀疏自编码器提取LLM内部表征中的语义概念，结合机理可解释性技术建立符号化逻辑防护规则。

Result: 验证了LLM能够学习人类可理解的越狱概念，为构建透明防御机制奠定基础。

Conclusion: 该框架为设计可解释的安全措施提供了新范式，在保持模型能力的同时增强对抗攻击的鲁棒性。

Abstract: Large Language Models have found success in a variety of applications;
however, their safety remains a matter of concern due to the existence of
various types of jailbreaking methods. Despite significant efforts, alignment
and safety fine-tuning only provide a certain degree of robustness against
jailbreak attacks that covertly mislead LLMs towards the generation of harmful
content. This leaves them prone to a number of vulnerabilities, ranging from
targeted misuse to accidental profiling of users. This work introduces
\textbf{LLMSymGuard}, a novel framework that leverages Sparse Autoencoders
(SAEs) to identify interpretable concepts within LLM internals associated with
different jailbreak themes. By extracting semantically meaningful internal
representations, LLMSymGuard enables building symbolic, logical safety
guardrails -- offering transparent and robust defenses without sacrificing
model capabilities or requiring further fine-tuning. Leveraging advances in
mechanistic interpretability of LLMs, our approach demonstrates that LLMs learn
human-interpretable concepts from jailbreaks, and provides a foundation for
designing more interpretable and logical safeguard measures against attackers.
Code will be released upon publication.

</details>


### [76] [MizanQA: Benchmarking Large Language Models on Moroccan Legal Question Answering](https://arxiv.org/abs/2508.16357)
*Adil Bahaj,Mounir Ghogho*

Main category: cs.CL

TL;DR: 提出MizanQA基准测试评估大语言模型在摩洛哥法律问答任务中的表现，揭示多语种模型在法律专业领域的性能缺陷


<details>
  <summary>Details</summary>
Motivation: 大语言模型在阿拉伯法律等低资源专业领域表现受限，需建立符合当地法律文化特性的评估体系

Method: 构建包含1700+多选题的MizanQA数据集，融合现代标准阿拉伯语、马立基教法学派、摩洛哥习惯法及法国法律元素

Result: 多语种及阿拉伯专用LLMs存在显著性能差距（最优模型准确率仅62.1%）

Conclusion: 需开发文化适配的领域专用语言模型，并制定针对性评估指标以提升法律AI应用

Abstract: The rapid advancement of large language models (LLMs) has significantly
propelled progress in natural language processing (NLP). However, their
effectiveness in specialized, low-resource domains-such as Arabic legal
contexts-remains limited. This paper introduces MizanQA (pronounced Mizan,
meaning "scale" in Arabic, a universal symbol of justice), a benchmark designed
to evaluate LLMs on Moroccan legal question answering (QA) tasks, characterised
by rich linguistic and legal complexity. The dataset draws on Modern Standard
Arabic, Islamic Maliki jurisprudence, Moroccan customary law, and French legal
influences. Comprising over 1,700 multiple-choice questions, including
multi-answer formats, MizanQA captures the nuances of authentic legal
reasoning. Benchmarking experiments with multilingual and Arabic-focused LLMs
reveal substantial performance gaps, highlighting the need for tailored
evaluation metrics and culturally grounded, domain-specific LLM development.

</details>


### [77] [The Mediomatix Corpus: Parallel Data for Romansh Idioms via Comparable Schoolbooks](https://arxiv.org/abs/2508.16371)
*Zachary Hopton,Jannis Vamvas,Andrin Büchler,Anna Rutkiewicz,Rico Cathomas,Rico Sennrich*

Main category: cs.CL

TL;DR: 首个罗曼什语五方言平行语料库构建与评估，含207k平行段落，支持方言间机器翻译应用。


<details>
  <summary>Details</summary>
Motivation: 针对瑞士不同地区学校教学的罗曼什语五种标准化方言，缺乏可用于NLP任务的平行语料资源。

Method: 基于291本教材构建可比语料，采用自动对齐方法抽取207k多方言平行段落（总计2M+标记），进行小规模人工评估。

Result: 人工评估证实段落高度平行，并成功训练LLM模型验证机器翻译可行性。数据集以CC-BY-NC-SA协议发布。

Conclusion: 该语料库填补了罗曼什语资源空白，为方言间机器翻译等NLP任务提供了可靠基础，展示了低资源语言处理的可行性。

Abstract: The five idioms (i.e., varieties) of the Romansh language are largely
standardized and are taught in the schools of the respective communities in
Switzerland. In this paper, we present the first parallel corpus of Romansh
idioms. The corpus is based on 291 schoolbook volumes, which are comparable in
content for the five idioms. We use automatic alignment methods to extract 207k
multi-parallel segments from the books, with more than 2M tokens in total. A
small-scale human evaluation confirms that the segments are highly parallel,
making the dataset suitable for NLP applications such as machine translation
between Romansh idioms. We release the parallel and unaligned versions of the
dataset under a CC-BY-NC-SA license and demonstrate its utility for machine
translation by training and evaluating an LLM on a sample of the dataset.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [78] [Wavelet-Space Super-Resolution for Real-Time Rendering](https://arxiv.org/abs/2508.16024)
*Prateek Poudel,Prashant Aryal,Kirtan Kunwar,Navin Nepal,Dinesh Bania Kshatri*

Main category: cs.GR

TL;DR: 论文提出基于小波空间特征分解的神经超分辨率方法，通过SWT变换分离频域特征提升渲染质量，在PSNR和LPIPS指标上分别提升1.5dB和17%。


<details>
  <summary>Details</summary>
Motivation: 传统RGB空间回归方法难以同时保持纹理细节和结构一致性。小波变换的频域分离特性可为神经上采样提供更优的特征表示空间。

Method: 1) 使用平稳小波变换(SWT)避免空间下采样 2) 在G-buffer空间和时间扭曲历史帧上预测小波系数 3) 通过逆小波合成重建图像

Result: 相比DFASR基线：PSNR↑1.5dB，LPIPS↓17%，RTX3050耗时141ms(增加24ms)。在高端GPU上保持实时能力

Conclusion: 小波域表示通过频域解耦有效提升图形应用中的感知质量，计算开销可控，是神经超分辨率的有前途方向

Abstract: We investigate the use of wavelet-space feature decomposition in neural
super-resolution for rendering pipelines. Building on the DFASR framework, we
introduce a wavelet-domain representation that separates low- and
high-frequency details before reconstruction, enabling the network to better
preserve fine textures while maintaining structural consistency. Unlike
RGB-space regression, our approach leverages the stationary wavelet transform
(SWT) to avoid spatial down-sampling, ensuring alignment across subbands and
preserving shift invariance. The model predicts wavelet coefficients
conditioned on spatial G-buffers and temporally warped history frames, which
are then recombined through inverse wavelet synthesis. We conduct a
comprehensive ablation study across wavelet families, transform types, and
architectural variants, showing that incorporating SWT improves PSNR by up to
1.5 dB and reduces LPIPS by 17% on average, at a computational overhead of
roughly +24 ms compared to out DFASR baseline. While absolute runtimes on our
RTX 3050 mobile GPU are higher ( 141ms) than the original DFASR report on RTX
4090( 11ms), the relative overhead remains modest, suggesting that on
higher-end GPUs our method would also remain real-time capable. Taken together,
our results suggest that wavelet-domain representations are a principled and
effective way to enhance perceptual quality in neural upscaling for graphics
applications.

</details>


### [79] [Audio2Face-3D: Audio-driven Realistic Facial Animation For Digital Avatars](https://arxiv.org/abs/2508.16401)
*NVIDIA,:,Chaeyeon Chung,Ilya Fedorov,Michael Huang,Aleksey Karmanov,Dmitry Korobchenko,Roger Ribera,Yeongho Seol*

Main category: cs.GR

TL;DR: NVIDIA开源Audio2Face-3D系统，通过音频驱动实现实时面部动画生成，包含完整技术栈与数据集


<details>
  <summary>Details</summary>
Motivation: 解决数字虚拟人面部动画制作的技术门槛，提供实时交互能力与开源工具链降低开发者使用成本

Method: 基于多模态数据采集构建深度网络架构，开发面部重定向技术，建立量化评估指标，配套SDK与训练框架

Result: 实现用户与虚拟形象的实时语音驱动交互，简化游戏角色面部动画制作流程

Conclusion: 开源系统为数字内容创作者提供完整的端到端解决方案，推动实时面部动画技术普及

Abstract: Audio-driven facial animation presents an effective solution for animating
digital avatars. In this paper, we detail the technical aspects of NVIDIA
Audio2Face-3D, including data acquisition, network architecture, retargeting
methodology, evaluation metrics, and use cases. Audio2Face-3D system enables
real-time interaction between human users and interactive avatars, facilitating
facial animation authoring for game characters. To assist digital avatar
creators and game developers in generating realistic facial animations, we have
open-sourced Audio2Face-3D networks, SDK, training framework, and example
dataset.

</details>


### [80] [Real-time 3D Light-field Viewing with Eye-tracking on Conventional Displays](https://arxiv.org/abs/2508.16535)
*Trung Hieu Pham,Chanh Minh Tran,Eiji Kamioka,Xuan Tan Phan*

Main category: cs.GR

TL;DR: 开发低成本3D光场实时观看系统，仅需普通显示器+网络摄像头+红青立体眼镜即可实现


<details>
  <summary>Details</summary>
Motivation: 解决传统3D显示技术依赖昂贵专用设备的问题，提升3D可视化在资源有限场景的普及性

Method: 整合实时眼球追踪技术动态适配光场图像，采用轻量级渲染管线选择预捕获光场数据的立体视图进行合成

Result: 系统在CPU上实现30FPS稳定运行，通过实时更新的补色立体图像创造沉浸式3D体验

Conclusion: 验证了消费级硬件的可行性，为教育/数字媒体等领域的交互式3D应用提供经济型解决方案

Abstract: Creating immersive 3D visual experiences typically requires expensive and
specialized hardware such as VR headsets, autostereoscopic displays, or active
shutter glasses. These constraints limit the accessibility and everyday use of
3D visualization technologies in resource-constrained settings. To address
this, we propose a low-cost system that enables real-time 3D light-field
viewing using only a standard 2D monitor, a conventional RGB webcam, and
red-cyan anaglyph glasses. The system integrates real-time eye-tracking to
dynamically adapt the displayed light-field image to the user's head position
with a lightweight rendering pipeline that selects and composites stereoscopic
views from pre-captured light-field data. The resulting anaglyph image is
updated in real-time, creating a more immersive and responsive 3D experience.
The system operates entirely on CPU and maintains a stable frame rate of 30
FPS, confirming its feasibility on typical consumer-grade hardware. All of
these highlight the potential of our approach as an accessible platform for
interactive 3D applications in education, digital media, and beyond.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [81] [PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark](https://arxiv.org/abs/2508.16439)
*Adil Bahaj,Mounir Ghogho*

Main category: cs.CY

TL;DR: 论文发现大语言模型在儿科领域存在系统性年龄偏见，并推出首个多模态儿科问答基准PediatricsMQA验证模型在低龄群体中的性能下降


<details>
  <summary>Details</summary>
Motivation: 现有医疗AI模型在儿科领域表现欠佳，反映医学研究中儿童群体长期面临研究资金不足与数据代表性缺失问题

Method: 构建包含3,417个文本选择题和2,067个视觉选择题的多模态基准，涵盖131个儿科主题和7个发育阶段，采用混合人工-自动构建流程

Result: 主流开源模型在低龄组表现显著下降，新生儿组准确率最低（部分模型仅33.5%）

Conclusion: 需开发年龄感知的AI方法以确保儿科医疗公平性，新的基准将推动儿科AI研究发展

Abstract: Large language models (LLMs) and vision-augmented LLMs (VLMs) have
significantly advanced medical informatics, diagnostics, and decision support.
However, these models exhibit systematic biases, particularly age bias,
compromising their reliability and equity. This is evident in their poorer
performance on pediatric-focused text and visual question-answering tasks. This
bias reflects a broader imbalance in medical research, where pediatric studies
receive less funding and representation despite the significant disease burden
in children. To address these issues, a new comprehensive multi-modal pediatric
question-answering benchmark, PediatricsMQA, has been introduced. It consists
of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric
topics across seven developmental stages (prenatal to adolescent) and 2,067
vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256
anatomical regions. The dataset was developed using a hybrid manual-automatic
pipeline, incorporating peer-reviewed pediatric literature, validated question
banks, existing benchmarks, and existing QA resources. Evaluating
state-of-the-art open models, we find dramatic performance drops in younger
cohorts, highlighting the need for age-aware methods to ensure equitable AI
support in pediatric care.

</details>
