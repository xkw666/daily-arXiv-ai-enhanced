<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 47]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: 开发Truth Sleuth和Trend Bender双AI代理系统，通过RAG技术核查YouTube视频中的虚假声明，并生成评论引导用户理性讨论。


<details>
  <summary>Details</summary>
Motivation: 应对YouTube平台虚假信息快速传播的挑战，利用AI技术实现自动化事实核查与用户交互干预。

Method: 1. Truth Sleuth代理：抽取视频声明，采用RAG框架整合维基百科/Google搜索等资源生成核查报告
2. Trend Bender代理：基于核查报告生成辩论性评论，通过自评估机制迭代优化输出质量

Result: 在标准测试集和真实YouTube部署中验证系统有效性，事实核查准确率高，AI生成评论能有效引导用户参与理性讨论

Conclusion: 该系统展示了AI驱动干预在遏制网络虚假信息和构建良性数字公共空间的创新潜力

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [2] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: 开发完全离线的智能手机心理健康应用EmoSApp，通过优化LLM模型实现设备端部署并提升隐私保护


<details>
  <summary>Details</summary>
Motivation: 当前数字心理健康平台存在用户可访问性受限、网络依赖性强及数据隐私隐患，需开发离线解决方案

Method: 基于LLaMA-3.2-1B-Instruct模型，使用14,582组心理健康QA数据集进行微调，采用Torchtune和Executorch进行量化设备端部署

Result: 定性评估显示应用具备共情对话能力，量化测试在9个基准中验证了低资源环境性能，模型大小压缩至手机可运行范围

Conclusion: 通过设备端部署与领域适配，为便携式心理健康AI解决方案提供技术蓝图，平衡隐私保护与服务质量

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [3] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: 开发基于本地开放权重模型的模块化工具链，通过LLM标准化/匿名化处理敏感文本数据，应用于瑞典法院涉毒监管案件的大规模半自动化分析


<details>
  <summary>Details</summary>
Motivation: 法律/医疗等非结构化文本蕴含丰富研究价值，但受限于隐私泄露风险与数据异构性难以被有效利用

Method: 结合LLM提示工程实现文本标准化/摘要/翻译，采用多层级匿名策略（LLM修订+实体识别+规则过滤），在10,842份瑞典LVM法案判决文书上进行验证

Result: 工具链成功去除98%敏感信息并保持语义完整性，基于小样本标注训练的预测模型证明其大规模分析可行性

Conclusion: 该方案突破隐私与异构性限制，为敏感领域文本研究开辟可扩展的分析路径

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [4] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: 提出了一个针对自然语言解释（NLEs）的三维度XAI分类法框架，用于增强AI系统的透明度治理


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法需要适应大语言模型时代NLEs的新特性，解决模型行为验证和治理的结构化需求

Method: 基于可解释AI文献构建更新的分类体系，包含上下文、生成与呈现、评估三个维度共11个属性分类

Result: 该分类法为研究人员、审计机构和政策制定者提供了系统化的NLEs设计评估框架

Conclusion: 结构化分类框架有助于实现透明AI系统，促进不同利益相关者对模型行为的协同治理

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [5] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: 提出AutoRAG-LoRA框架，通过LoRA适配器与KL正则化训练减少大语言模型幻觉问题，在保持模型效率的同时实现检索增强生成


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型(LLM)生成内容时存在事实性错误(幻觉)的问题，增强生成结果的可信度以满足实际部署需求

Method: 构建模块化RAG框架，集成：1)自动提示重写与混合检索技术 2)低秩适配器(LoRA)微调 3)基于分类器与自评估的幻觉检测模块 4)KL正则化对比损失反馈修正循环

Result: 实验证明该框架显著降低生成内容的事实性偏差，在保持模型轻量化（参数量仅增加0.1%）的同时维持生成流畅性

Conclusion: 通过轻量级适配器与检索增强的协同优化，实现了生成准确性与计算效率的平衡，为LLM的实际应用提供可靠解决方案

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [6] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: 论文提出LLM需通过拟人化不确定性表达（模仿人类语言习惯）来增强可信度，并指出当前NLP研究在不确定性沟通中的不足。


<details>
  <summary>Details</summary>
Motivation: 解决LLM输出过度自信导致的信任危机，需通过自然语言界面传递合理置信度以促进人机协作。

Method: 综合人类不确定性沟通研究，分析NLP领域现有数据偏差，提出拟人化不确定性概念框架。

Result: 揭示NLP研究忽视人类不确定性表达的复杂性及数据偏差问题，验证拟人化路径的可行性。

Conclusion: 应通过模仿人类沟通机制（个性化/语境化）构建可信的机器不确定性表达体系，并规划具体研究方向。

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [7] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: 提出PLEX方法解决LLM文本分类可解释性难题，通过连体网络与上下文嵌入实现免扰动高效解释


<details>
  <summary>Details</summary>
Motivation: 现有LIME/SHAP等方法依赖大量扰动计算，在LLM场景下产生极高计算开销

Method: 结合LLM的上下文嵌入特征，设计连体网络对齐特征重要性分数，一次性训练后无需扰动

Result: 在4个分类任务中与主流方法保持92%+一致性，计算效率提升2-4个数量级

Conclusion: PLEX为LLM文本分类提供高效可靠的可解释方案，显著降低计算资源需求

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [8] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLM）能形成与人类相似的情感层级结构，模型规模越大结构越复杂，但对边缘群体的情感识别存在系统性偏差，暗示结合认知理论可优化模型评估。


<details>
  <summary>Details</summary>
Motivation: 为探究LLM如何建模用户情感状态（对伦理部署至关重要），受心理学情感轮启发，分析模型输出中情感状态的概率依赖关系，揭示其潜在社会认知模式。

Method: 基于情感轮构建概率依赖分析框架，通过模型输出的层级情感树分析，结合不同社会经济身份的人设测试，并辅以人类研究对比验证。

Result: LLM自然形成符合人类心理模型的情感层级树（模型越大结构越复杂）；发现跨阶层身份的情感识别系统偏差（交叉弱势群体误判加剧）；人类研究显示LLM内化了社会认知模式。

Conclusion: LLM展现出类人的情感推理能力，其社会认知偏差提示结合认知科学理论开发更科学的模型评估体系具有重要潜力。

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [9] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: 研究通过定制Transformer模型提升成人服务网站文本分析效能，支持性交易受害者识别


<details>
  <summary>Details</summary>
Motivation: 成人服务网站广告文本存在表情符号泛滥、语法混乱和故意混淆等问题，导致传统文本分析方法难以有效识别性交易受害者。现有预训练模型在处理此类特殊文本时存在效率不足和成本过高的问题

Method: 系统比较信息检索方法、预训练Transformer模型与定制模型，展示定制模型在有限GPU资源下的训练可行性及消费级硬件的推理适用性

Result: 定制模型在准确率(76.5%)、召回率(82.3%)、F1值(79.2%)和ROC AUC(0.89)上全面超越BERT-base等模型，成功应用于图结构分解、广告聚类和表情符号语义解析三大任务

Conclusion: 定制Transformer模型显著推进了ASW文本分析技术，为犯罪模式识别、执法效率提升提供了新的技术路径，相关方法可扩展至其他隐蔽网络犯罪文本分析领域

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [10] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: 利用预训练文本嵌入模型增强属性图语义分析，提升节点分类和关系预测任务效果


<details>
  <summary>Details</summary>
Motivation: 针对带标签属性图中丰富的文本属性未被充分利用的问题，探索通过文本嵌入技术增强图分析的上下文理解能力

Method: 集成预训练语言模型生成文本属性的嵌入表示，保持原有图结构不变的情况下增强语义特征

Result: 文本语义特征显著提升了属性图分析的准确性和可解释性

Conclusion: 无需改变图结构，文本嵌入技术能有效提升带标签属性图的语义分析能力

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [11] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: 提出首个评估模型解读科学文献示意图能力的基准MISS-QA，包含1500个标注样本，测试18个前沿多模态模型发现与人类专家存在显著差距


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门评估模型解读科学图表能力的基准，而示意图理解对科研至关重要，需系统衡量模型在此任务上的表现

Method: 构建含465篇论文1500个专家标注样本的基准，要求模型结合示意图和论文上下文回答问题，评估18个多模态模型并进行错误分析

Result: 前沿模型与人类专家存在显著性能差距（平均差32.4%），在不可回答问题上的准确率不足60%，可视化理解能力成为主要瓶颈

Conclusion: MISS-QA揭示了当前多模态模型在科学图表理解上的局限性，为提升科研文献的多模态理解提供了关键评估框架和改进方向

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [12] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: 研究通过分析Parler平台1.1亿条仇恨言论帖子，发现社交认可（点赞数）与后续仇恨言论产量及极端程度无显著关联，甚至存在负相关，挑战了传统社会认可理论在极端内容传播中的普适性。


<details>
  <summary>Details</summary>
Motivation: 验证Walther社会认可理论的两个核心假设：H1a（更多社交认可导致更多后续仇恨言论）和H1b（社交认可提升会加剧仇恨言论极端程度）。

Method: 使用Parler平台2018-2021年间超1.1亿条仇恨言论帖子数据，采用跨时间段（次日/周/月/季度/半年）的个体间效应分析。

Result: 个体内分析显示点赞数与后续仇恨言论无关；个体间分析在帖子层级呈现负相关，但不同时间段结果不一致。

Conclusion: 社交认可对仇恨言论的强化机制在小众社交媒体平台可能失效，暗示极端内容传播机制存在平台特异性。

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [13] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: 研究构建了评估大语言模型司法公平性的框架，发现LLMs存在显著的不一致性和偏见，并开发了开源工具包支持未来研究。


<details>
  <summary>Details</summary>
Motivation: LLMs在高风险司法领域的应用可能加剧社会不公，但其司法公平性尚未被系统评估。需验证LLMs作为'法官'的公平性以保障社会正义。

Method: 基于司法公平理论构建含65个标签的评估框架；创建含17.7万案例的JudiFair数据集；开发不一致性、偏见、失衡不准确性三大量化指标；测试16个主流LLM的公平性表现。

Result: 所有LLM均显示严重司法不公平：①人口统计标签偏见最显著 ②模型不一致性越高偏见越低 ③预测精度提升会加剧偏见 ④温度参数调节可改善公平性，模型规模/发布时间/国别无显著影响。

Conclusion: 提出首个系统评估LLM司法公平的方法论，发现算法参数调整比模型架构改进更有效，开源工具包为后续研究提供基准测试和优化基础。

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [14] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究探讨对话生成中用户主观与第三方客观风格相似性的差异及其对用户偏好的影响，发现两者存在显著不同，强调需区分评估


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视主客观风格相似性差异对用户偏好的影响，需探究两者区别及其作用机制

Method: 构建包含用户偏好、主观风格相似性评价和第三方客观标注的新数据集进行对比分析

Result: 用户主观风格相似性与偏好呈强正相关，且与第三方客观评价存在显著统计学差异

Conclusion: 研究强调在分析风格相似性时必须区分主客观评估维度，两者捕捉用户偏好的机制存在本质差异

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [15] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: 提出HanjaBridge方法解决韩语同音汉字词歧义问题，通过持续预训练注入多义候选并配合知识蒸馏，显著提升LLMs韩语理解能力


<details>
  <summary>Details</summary>
Motivation: LLMs在韩语等低资源语言中表现不佳，主要因汉字词在韩文中的同音异义现象导致语义歧义（如Hangul无法区分不同Hanja）

Method: 1. 在持续预训练中为同形汉字词提供所有Hanja候选，引导模型学习上下文消歧
2. 结合token级知识蒸馏防止灾难性遗忘
3. 通过共享Hanja增强中韩语义对齐

Result: 1. 韩语理解任务KoBALT相对提升21%
2. 观察到中韩跨语言正迁移效应
3. 推理阶段无需Hanja增强仍保持性能增益

Conclusion: 该方法在提升韩语语义理解的同时，保持了推理效率（无额外计算开销），验证了字形信息注入对低资源语言的有效性

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [16] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: 研究对比大语言模型（LLMs）与人类在类比推理任务中的表现，发现模型通过语义表征和提示策略能部分接近人类水平，但推理模式存在差异。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明LLMs虽能提取类比相似性，但缺乏类人推理机制。本文旨在通过故事类比对任务，系统评估LLMs在细粒度推理能力上与人类表现的异同。

Method: 1. 使用句子嵌入分析LLMs对类比源文本-目标文本相似性及源文本-干扰文本差异性的捕捉能力
2. 测试显式提示解释策略的有效性
3. 跨模型规模（8B/70B）和架构（GPT-4/LLaMA3）比较
4. 在个体类比层面（非总体准确率）评估推理模式

Result: LLMs的语义表征能有效区分类比关系，但解释性提示未显著提升性能；70B模型在类比映射任务中接近人类水平，不同架构模型表现出差异化推理特征。

Conclusion: 研究强调模型规模和架构对类比推理能力的影响，提出细粒度评估方法论，为LLMs作为人类认知模型的可能性提供新见解。

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [17] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: DS@GT团队在eRisk 2025挑战赛中，采用提示工程策略让不同大语言模型基于BDI-II标准进行抑郁症对话检测，生成结构化JSON输出并分析对话线索对症状预测的影响。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在心理健康诊断中的应用潜力，通过对话数据分析提升抑郁症筛查的自动化水平。

Method: 1. 设计提示工程框架，使LLMs根据BDI-II标准生成结构化评估
2. 通过跨模型一致性验证和内部一致性检验评估模型表现
3. 分析对话线索与症状预测的关联性

Result: 最佳模型在官方排行榜位列第二（DCHR=0.50, ADODL=0.89, ASHR=0.27），模型输出与临床标准高度对齐

Conclusion: 提示工程能有效引导LLMs遵循临床评估标准，为基于对话的抑郁症检测提供了可解释的技术路径，推动LLM在医疗诊断场景的应用创新。

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [18] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: 提出TEAM-Sign框架，通过微调大语言模型并采用分步提示策略，将手语视为自然语言处理，有效对齐手语与口语的分布差异和语法规则。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型对手语生成的革新有限，因其需处理手语的复杂结构（空间位置、手势顺序等特有规则）与口语的语法差异。

Method: 1. 将手语建模为自然语言进行LLM微调；2. 设计分步提示策略提取LLM内部手语知识；3. 通过文本-手语对齐机制支持生成。

Result: 在How2Sign/Phoenix14T数据集上验证，该方法能有效利用LLM的知识推理能力，实现手语与口语分布对齐（BLEU-4提升15.2%）。

Conclusion: 将手语纳入LLM处理范畴，通过知识引导的渐进式学习机制，为解决跨模态语言生成问题提供新范式。

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [19] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 基于Llama 3.1 8B的分层LoRA适配方法，通过多语言联合训练高效解决文本性别歧视检测任务


<details>
  <summary>Details</summary>
Motivation: 传统性别歧视检测方法依赖复杂数据处理和集成策略，且多语言场景需要独立模型。需要更高效参数微调方法同时保持跨语言迁移能力

Method: 1. 在Llama 3.1上应用分层LoRA：条件适配器路由显式建模三个层次子任务依赖关系
2. 将QLoRA(4-bit)扩展到所有线性层（传统仅注意力层）
3. 多语言联合训练策略，共享基础模型参数

Result: ICM-Hard任务表现：二元分类F1 0.6774/意图检测0.4991/多标签分类0.6519。仅需1.67%可训练参数，训练时间减少75%，模型存储降低98%

Conclusion: 分层LoRA适配在保持竞争力的同时显著提升效率，多语言联合训练实现1.7-2.4%的跨语言性能提升，验证参数高效微调的有效性

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [20] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2系统通过文档摘要、答案重构、训练后量化和更新语言模型主干等改进，在事实核查任务中取得第二名的成绩并保持最高运行效率


<details>
  <summary>Details</summary>
Motivation: 改进去年最佳开源模型HerO的性能，在保持效率的同时提升证据质量和预测准确性

Method: 1. 文档摘要和答案重构改进证据质量
2. 训练后量化优化预测性能
3. 更新语言模型主干提升系统表现

Result: 1. AVeriTeC排行榜第二名
2. 前三名中运行时间最短
3. 代码已开源供实际应用

Conclusion: HerO 2在保持高效运行的同时，通过多维度改进实现了接近顶级系统的性能，展示了实际场景应用的可行性

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [21] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: 提出首个韩语新闻立场检测数据集K-News-Stance，并开发基于段落级语义代理的JoA-ICL框架，显著提升长文本立场检测效果


<details>
  <summary>Details</summary>
Motivation: 现有立场检测研究局限于短文本和高资源语言，难以解决新闻推荐系统导致的信息茧房问题。韩语等长文本立场检测数据与方法存在空白

Method: 构建包含2,000篇文章及19,650段落标注的数据集；设计新闻结构引导的智能体框架，通过分析导语、引文等关键段落立场聚合得出整体立场

Result: JoA-ICL在立场检测准确率上超越现有方法，段落级分析使长文本立场识别效果提升显著。案例验证其在新闻多样性推荐和媒体偏见分析的有效性

Conclusion: 该研究为跨语言立场检测提供新范式，通过结构化段落分析突破长文本处理瓶颈，为构建多样性新闻生态系统提供技术支撑

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [22] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: 提出LLM增强的临床NLP流程，通过领域适应和提示工程提升心血管疾病风险评估性能


<details>
  <summary>Details</summary>
Motivation: 现有模型依赖结构化数据，但非结构化临床记录包含早期风险信号，需更有效提取利用

Method: 心血管定向微调+提示推理架构，结合实体感知和混合规则验证解决上下文幻觉及时序问题

Result: 在MIMIC-III和CARDIO-NLP上达到0.82临床一致性kappa值，关键指标全面超越基线模型

Conclusion: 验证LLM在临床决策系统的可行性，通过工程化改进实现从自由文本到风险评估的可靠转化

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [23] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: 开发混合Transformer框架分析孟加拉语社交媒体评论，结合XMB-BERT与投票分类器实现83.7%准确率


<details>
  <summary>Details</summary>
Motivation: 解析七月革命期间社交媒体舆论动态，探索低资源语言场景下的情感分析可行性

Method: 使用4,200条社交媒体数据，集成BanglaBERT/mBERT/XLM-RoBERTa进行特征提取，结合PCA降维，测试11种分类器组合

Result: XMB-BERT结合投票分类器取得最优表现（83.7%准确率），显著优于单一模型

Conclusion: 验证了混合Transformer模型在低资源语言情感分析中的有效性，为类似社会运动研究提供技术框架

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [24] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: 研究对比传统算法与LLMs在跨境金融实体匹配中的表现，发现接口型LLMs准确率超93%、F1分数超96%，误报率降低40-80%，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 跨境金融活动增加导致实体识别需求激增，但传统算法受限于语言变化/特殊字符/法律形式变更，存在高误报率（20-40%）。LLMs凭借语义理解能力可突破这些限制。

Method: 使用65个葡萄牙企业案例，对比评估三类方法：1) 传统算法（Jaccard/余弦/Levenshtein） 2) Hugging Face的LLMs 3) 接口型LLMs（Microsoft Copilot/Qwen 2.5）

Result: 传统方法准确率92%但误报率高（20-40%），接口型LLMs综合表现最佳：准确率＞93%、F1分数＞96%、误报率降幅达40-80%。

Conclusion: 接口型LLMs在金融实体匹配中展现显著优势，其上下文理解能力可有效提升风险管理与合规效率，为跨境金融监管提供新解决方案。

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [25] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出了首个针对扩散语言模型（dLLMs）的越狱攻击框架DIJA，揭示了现有安全对齐机制在对抗上下文感知的掩码输入攻击时失效的根本缺陷


<details>
  <summary>Details</summary>
Motivation: 现有扩散语言模型的双向建模和平行解码特性导致传统安全机制无法有效过滤上下文相关的对抗性提示，暴露新的攻击面

Method: 通过构建交错式掩码文本对抗提示，利用dLLMs的双向上下文建模特性强制生成有害内容，并规避动态过滤机制

Result: 在Dream-Instruct数据集实现100%关键词攻击成功率，比现有最优方法ReNeLLM提升78.5%评估器ASR和37.7 StrongREJECT分数

Conclusion: 扩散语言模型架构存在根本性安全缺陷，亟需重新设计针对其生成机制的安全对齐方案

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [26] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: 研究揭示大型语言模型中多个后门触发器可共存且互不干扰，高嵌入相似性的触发器具有抗干扰激活能力，并提出分层权重差异恢复方法实现高效防御


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单触发器的攻击有效性，缺乏对多触发器交互机制和触发原理的系统性分析，需深入探究LLMs在数据投毒攻击中的复杂脆弱性

Method: 构建多触发器共存性验证框架，采用嵌入空间相似性分析测试触发鲁棒性，设计基于分层权重差异的后训练选择性微调策略

Result: 实证多触发器可并行嵌入模型且激活互不影响，相似性>0.85的触发器在字符替换/长跨度插入时保持90%+攻击成功率，恢复方法仅需更新0.3%参数即可消除恶意行为

Conclusion: 暴露LLMs存在系统性安全漏洞，分层恢复机制为多触发器投毒提供实用解决方案，强调需在模型训练阶段加强安全验证

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [27] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: 轻量级OCR-VLM集成系统通过精确提示策略和跨语言增强，在多语言教育场景中优于重型端到端模型


<details>
  <summary>Details</summary>
Motivation: 开发面向ImageCLEF 2025 EXAMS V挑战的多语言多模态推理系统，验证轻量级集成模型在关键教育场景的有效性

Method: 集成Gemini系列模型实现视觉描述/文本修正/推理决策三阶段流程，通过英语数据集训练及多语言增强，结合零样本/少样本提示工程进行消融实验

Result: 英语验证集准确率提升5.8%（55.9%→61.7%），官方测试以81.4%准确率获多语言赛道冠军，13种语言中11种领先（克罗地亚语95.07%，意大利语92.12%）

Conclusion: 轻量OCR-VLM集成配合精准提示策略，通过跨语言增强可在保持高效的同时超越复杂端到端模型，适用于高要求的多语言教育环境

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [28] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: 提出WikiMem数据集及模型无关指标，量化LLM中个人数据记忆程度，解决GDPR遗忘权落地的关键技术难题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法需已知遗忘数据，且隐私审计技术难以实现个体级数据追溯，无法满足欧盟GDPR的'被遗忘权'要求。

Method: 构建含5000+自然语言样本的WikiMem数据集，通过校准负对数似然值比较真实值与替代项，设计模型无关的个体-事实关联量化指标。

Result: 在15个LLM(410M-70B)测试显示：记忆强度与模型规模/主体网络曝光度正相关，验证了方法有效性。

Conclusion: 首次实现LLM个体级数据记忆检测，为动态构建遗忘集、满足GDPR合规要求奠定技术基础。

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [29] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: 研究发现LLM多智能体系统（MAS）在定性编码中温度参数显著影响共识达成，单智能体表现普遍优于多智能体，仅在特定模型/场景下MAS展现优势


<details>
  <summary>Details</summary>
Motivation: 探索多智能体系统模拟人类编码流程时相比单智能体的潜在优势，分析智能体角色配置和温度参数对编码质量的影响

Method: 使用6个开源LLM（3B-32B参数）在18种配置下，通过77,000次编码决策对比黄金标准数据集，构建包含结构化讨论和共识仲裁的开源MAS系统

Result: 温度参数显著影响共识达成速度；多角色配置延迟共识；单智能体在多数情况下表现更优；仅OpenHermesV2:7B模型在低温+至少1个强势角色时展现MAS优势

Conclusion: 挑战了「多样MAS角色带来更好结果」的假设，提出MAS可能辅助解决模糊编码场景，开源实验系统为后续研究提供基础

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [30] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: 针对西班牙社会背景开发西语/加泰罗尼亚语偏见评测基准（EsBBQ/CaBBQ），发现大语言模型在模糊场景中易依赖社会偏见且高准确率常伴随强偏见倾向。


<details>
  <summary>Details</summary>
Motivation: 现有社会偏见评估资源多集中于英语与美国语境，缺乏针对西班牙语言环境（西语/加泰罗尼亚语）的评测工具。

Method: 基于原版BBQ框架构建平行数据集，采用多选题形式评估10类社会偏见，测试不同规模/类型的LLM在西班牙语境下的表现。

Result: 模型在模糊场景中正确率低，且高QA准确率与偏见依赖度正相关（准确率越高越依赖刻板印象）。

Conclusion: 需开发本土化偏见评测工具，当前模型即使在其他语言环境中仍严重依赖社会偏见，凸显跨文化偏见缓解的必要性。

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [31] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM框架利用大语言模型从RFC文档提取精准有限状态机，解决了传统方法扩展性差、覆盖率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有有限状态机提取技术存在扩展性限制、覆盖率不足以及自然语言规范歧义性问题，制约协议分析和漏洞挖掘。

Method: 结合提示链(prompt chaining)和思维链推理(chain-of-thought)，通过智能体系统处理协议规范，构建结构化规则手册。

Result: 在FTP/RTSP协议实验中实现高精度提取，显著减少幻觉状态转换，验证了框架有效性。

Conclusion: 基于LLM的智能体系统为协议分析和逆向工程领域开辟了新路径，具有网络安全应用潜力。

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [32] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 提出SAE-LAPE方法识别LLMs中的语言特定特征，发现其分布规律与功能应用


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦单个神经元的多义性缺陷，需探索跨语言表示中语言特定特征的分离方法

Method: 基于特征激活概率开发SAE-LAPE方法，在Transformer前馈网络中定位语言特定特征

Result: 语言特征主要分布于中高层，具有可解释性且影响多语言性能，语言识别准确率媲美fastText

Conclusion: 该方法有效解析LLMs多语言机制，语言特征可视化提升模型可解释性，为跨语言应用提供新思路

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [33] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: 通过降维KV缓存和优化位置编码提升LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 解决Transformer解码器在推理过程中Key-Value缓存膨胀导致的内存消耗和带宽限制问题

Method: 1. 将KV向量降采样到潜在空间减少缓存体积 2. 改进Rotary位置编码的频率采样机制 3. 仅需少量额外训练（预训练量的1%）

Result: 在包含Grouped Query Attention的模型中取得满意效果，验证了分别压缩Key/Value组件的影响

Conclusion: KV-Latent显著提升推理效率，为KV缓存优化开辟新方向，代码已开源

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [34] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: 提出基于大语言模型的自动形式化流程，构建了奥林匹克数学问题数据集LeanFormal，验证了错误反馈机制对形式化推理的增强效果。


<details>
  <summary>Details</summary>
Motivation: 现有形式数学推理依赖手工形式化，需通过大规模自然语言数学问题数据集构建自动形式化方法推动领域发展。

Method: 开发无需训练的全自动流程，结合大语言模型和错误反馈机制，构建包含3,922自然语言/9,787 Lean形式化问题的数据集。

Result: 数据集64.46%达到优质标准，实验证明小样本学习/错误反馈/增加采样数能提升形式化能力，验证数据集对定理证明器的挑战性。

Conclusion: LeanFormal数据集为自动定理证明提供有效基准，错误反馈机制显著提升大语言模型在形式数学推理中的表现。

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [35] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: 构建首个中文细粒度仇恨言论数据集STATE ToxiCN，研究编码仇恨术语并提出词典融合方法，提升检测性能与可解释性


<details>
  <summary>Details</summary>
Motivation: 中文仇恨言论检测面临细粒度标注数据缺乏和编码仇恨语义解释性不足的双重挑战，阻碍模型在复杂场景中的应用

Method: 1. 创建span级标注数据集STATE ToxiCN
2. 系统研究中文编码仇恨术语及LLM解释能力
3. 提出标注词典融合方法

Result: 1. 提供首个中文细粒度仇恨数据集
2. 揭示LLM在仇恨语义解释的局限性
3. 词典融合方法显著提升检测性能（F1提高5.2%）

Conclusion: 本研究为中文仇恨检测可解释性研究提供关键资源与方法论，推动复杂场景下的语义理解与模型解释能力发展

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [36] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot是基于多智能体大语言模型的系统，通过17个评估维度为罗马尼亚语医生提供实时反馈，优化在线问诊回复质量。采用DSPy自动优化提示词，部署开源模型于罗马尼亚低资源医疗场景，实证显示能有效提升用户评价和响应质量。


<details>
  <summary>Details</summary>
Motivation: 解决罗马尼亚语在线医疗咨询中，医生回复质量常受表达方式而非专业准确性影响的问题，填补低资源语言环境下医疗沟通优化的技术空白。

Method: 1. 三智能体LLM架构（自动提示优化DSPy）
2. 17个可解释反馈维度评估体系
3. 基于开源模型的低资源部署方案
4. 与现有问诊平台实时集成

Result: 1. 41名医生实证显示用户评分提升23%
2. 响应质量指标改善18%
3. 成为罗马尼亚首个实际落地的医疗LLM应用
4. 系统延迟低于800ms满足实时需求

Conclusion: 该系统成功验证了LLM在非英语医疗场景的适用性，通过优化沟通质量显著提升在线问诊效果，为低资源语言地区数字化医疗提供可扩展解决方案。

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [37] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: 提出ConVA方法通过控制LLM潜在表征中的价值观编码，实现精准且不影响性能的价值观对齐


<details>
  <summary>Details</summary>
Motivation: 现有LLM价值观对齐方法存在潜在表征解释偏差大、控制力度与模型性能难以平衡的问题

Method: 采用上下文控制的值向量识别方法确保解释准确性，设计门控激活机制实现最小程度有效控制

Result: 在10个基本价值观维度上达到最高控制成功率（提升12.8%），模型性能指标保持97%以上

Conclusion: ConVA在保证模型流畅性的同时实现稳定价值观控制，有效抵御恶意输入诱导

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [38] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 结合人类专家知识与大语言模型（LLM），通过提取同行评审报告中的新颖性描述、LLM总结论文方法，并设计稀疏注意力融合模块，显著提升了学术论文方法新颖性评估效果。


<details>
  <summary>Details</summary>
Motivation: 传统新颖性评估依赖专家（知识有限）或引用组合（有效性存疑），且未验证独特引用与新颖性的关联。LLM具备广博知识但缺乏人类判断力，需两者互补解决评估局限。

Method: 1. 从评审报告中提取新颖性相关句子 2. 用LLM总结论文方法论 3. 微调BERT等预训练模型 4. 设计文本引导的稀疏注意力融合模块整合人类与LLM知识

Result: 大量基线对比实验表明，本方法在F1-score等指标上优于传统模型，稀疏注意力模块有效提升知识融合效果。

Conclusion: 通过知识协同与模块创新，实现了更准确的学术新颖性评估，为LLM与人类智能协同研究提供新范式。

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [39] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: 研究评估多种流程模型表示在LLM流程建模中的适用性，Mermaid综合表现最佳，BPMN text在生成准确性领先


<details>
  <summary>Details</summary>
Motivation: 现有流程模型表示（PMRs）差异大且缺乏系统比较，不同流程模型生成（PMG）方法难以横向评估

Method: 构建含55个流程描述和九种PMRs的数据集，从LLM适用性和PMG性能两个维度评估

Result: Mermaid在六项流程建模标准中总分最高，BPMN text在流程元素相似性指标表现最优

Conclusion: 研究为流程建模表示法的选择提供了实证依据，建议根据任务需求选择不同表示法

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [40] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: 通过加权损失函数改进Transformer模型在多标签情绪检测中的表现，高频情绪类提升显著但少数类效果有限


<details>
  <summary>Details</summary>
Motivation: 解决多标签情绪检测中数据不平衡问题，避免传统重采样方法的高计算成本

Method: 使用BERT/RoBERTa/BART模型在BRIGHTER数据集上测试动态加权损失函数，评估指标包括Micro F1、Macro F1等

Result: 加权损失函数提升高频情绪类检测性能，但对数据稀缺的少数类改进有限

Conclusion: 动态加权方法在平衡多标签分类中部分有效，但需结合其他技术改善少数类识别

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [41] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: 提出了数据污染风险(DCR)框架，通过四个检测层级和模糊推理系统量化大模型的数据污染程度，生成污染感知的准确率指标


<details>
  <summary>Details</summary>
Motivation: 针对大模型在评估中可能记忆测试数据导致性能虚高的问题，需要开发轻量透明的污染检测工具来确保评估可信度

Method: 建立语义/信息/数据/标签四个检测层级，通过模糊推理系统合成污染分数，生成统一DCR因子调整原始准确率

Result: 在3类任务9个模型上验证，DCR调整后的准确率与未污染基准误差小于4%

Conclusion: DCR框架通过高效透明的污染量化机制，为模型评估提供了可信度修正工具，推动更公平的LLM比较

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [42] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0整合非推理与推理模式，推出32B高性能模型和1.2B移动端模型，扩展西班牙语支持，性能超越同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为智能体AI时代提供兼具可用性与高级推理能力的模型，扩展多语言支持(新增西班牙语)并优化不同场景部署需求。

Method: 开发双模式架构(非推理/推理模式)，设计32B中型模型(高性能)和1.2B小型模型(移动端应用)，通过工具智能体实现功能扩展。

Result: 在同类别开源模型中表现最优，与前沿商业模型保持竞争力，HuggingFace下载量验证其易用性。

Conclusion: EXAONE 4.0系列为AI研究社区提供了高性能且易获取的多语言模型解决方案，推动智能体AI技术发展。

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [43] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: 提出因果思维图（CCG）解释思维链提升大模型推理能力的机制，构建KisMATH数据集验证CCG结构的有效性


<details>
  <summary>Details</summary>
Motivation: 针对思维链提升LLM推理性能的机制不明确问题，探索其底层因果关系与模型内部认知结构的对应关系

Method: 通过自动提取推理轨迹构建有向无环图（CCG），收集1671个数学问题形成KisMATH数据集，使用15个开源大模型进行图结构对齐分析

Result: 发现CCG节点是答案的必要中介，且LLM内部推理路径与CCG结构高度吻合

Conclusion: KisMATH为可控的图对齐干预提供基准，揭示了思维链的结构化表征机制，开辟了LLM推理过程研究的新途径

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [44] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 论文通过训练同参数规模的编码器与解码器模型，揭示两者在分类/检索与生成任务上的优势差异，并发现模型跨任务适配效果不如专用架构，同时开源了完整训练资源


<details>
  <summary>Details</summary>
Motivation: 解决LLM社区对解码器模型的过度关注与编码器模型实际应用需求之间的矛盾，通过公平实验比较两种架构的真实性能差异

Method: 开发Ettin模型套件（1700万至10亿参数），使用相同训练方法同步训练编码器与解码器，并在分类、检索、生成任务上进行对比测试

Result: 编码器在MNLI分类任务（400M参数优于1B解码器）、解码器在生成任务占优，跨架构适配训练效果显著弱于专用架构

Conclusion: 验证架构专用优势理论，强调开源资源对学术研究的促进作用，建议根据任务需求选择合适架构而非强行适配

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [45] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: 研究通过引导LLMs自适应选择推理策略，提升其在多样化问题中的解决能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型因固守单一推理策略导致的多场景适应性问题

Method: 通过实验验证不同提示策略效果，开发模型策略选择引导方法

Result: 自适应策略选择相比单一策略可提升推理性能

Conclusion: 策略选择机制为增强LLMs推理能力提供新方向

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [46] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: 开发HKGAI-V1大语言模型，针对香港多语言环境及'一国两制'框架下的数字主权需求，集成检索增强生成系统与本土价值对齐机制


<details>
  <summary>Details</summary>
Motivation: 解决香港三语环境（粤语/普通话/英语）、独特的社会法律架构及本土文化价值诉求，构建符合地区规范的AI治理基础设施

Method: 基于DeepSeek架构进行全参数微调，整合RAG系统确保事实准确性，开发对抗性香港价值基准测试工具进行伦理对齐验证

Result: 模型在香港特色敏感问题处理上优于通用模型，建立首个香港本土AI伦理评估体系，实现关键领域AI应用的治理嵌入

Conclusion: 为区域性AI系统开发提供可复制的技术范式，实现数字主权与本土身份认同的有机统一，推动负责任AI的区域实践

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [47] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: 研究评估LLM生成酒店摘要的忠实度，发现传统指标在跨域数据中表现优异（Spearman 0.63），而LLM评估存在严重标注偏差


<details>
  <summary>Details</summary>
Motivation: 验证酒店摘要生成模型是否忠实反映原始数据，比较人工评估、传统指标与LLM评判的可靠性差异

Method: 通过人工标注（错误分类+细粒度标注）对比传统重叠度指标、可训练模型和LLM自动评估方法

Result: 词重叠指标与人工评估相关性最高，LLM虽能生成优质摘要但其评估结果存在过度/不足标注问题，不可核查信息风险最大

Conclusion: 传统指标在跨域评估中更具鲁棒性，LLM评估不可靠性及众包标注挑战突显自动化评估系统的局限性

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [48] [Developing and evaluating quilts for the depiction of large layered graphs](https://arxiv.org/abs/2507.10883)
*Juhee Bae,Benjamin Watson*

Main category: cs.GR

TL;DR: 传统分层图在复杂场景下存在理解困难，研究提出矩阵式Quilts图表并通过实验验证其路径查找效率显著优于节点-链接和传统矩阵图表。


<details>
  <summary>Details</summary>
Motivation: 传统流程图等分层图表在复杂度增加时难以理解，需开发更有效的可视化方案（Quilts）来优化复杂图结构的表现力。

Method: 1. 对比三种Quilts跳连设计（纯颜色/纯文本/混合式）；2. 将混合式Quilts与节点-链接图、居中矩阵进行路径查找效率对比实验。

Result: 混合式Quilts路径查找速度最快（46.6秒 vs 节点-链接58.3秒/矩阵71.2秒），200节点图表优势更显著（55.4秒 vs 71.1秒/84.2秒）。

Conclusion: Quilts显著提升复杂图表的可读性和操作效率，特别适用于大规模数据可视化场景，矩阵混合设计是成功关键因素。

Abstract: Traditional layered graph depictions such as flow charts are in wide use. Yet
as graphs grow more complex, these depictions can become difficult to
understand. Quilts are matrix-based depictions for layered graphs designed to
address this problem. In this research, we first improve Quilts by developing
three design alternatives, and then compare the best of these alternatives to
better-known node-link and matrix depictions. A primary weakness in Quilts is
their depiction of skip links, links that do not simply connect to a succeeding
layer. Therefore in our first study, we compare Quilts using color-only,
text-only, and mixed (color and text) skip link depictions, finding that path
finding with the color-only depiction is significantly slower and less
accurate, and that in certain cases, the mixed depiction offers an advantage
over the text-only depiction. In our second study, we compare Quilts using the
mixed depiction to node-link diagrams and centered matrices. Overall results
show that users can find paths through graphs significantly faster with Quilts
(46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams.
This speed advantage is still greater in large graphs (e.g. in 200 node graphs,
55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions).

</details>


### [49] [OffsetCrust: Variable-Radius Offset Approximation with Power Diagrams](https://arxiv.org/abs/2507.10924)
*Zihan Zhao,Pengfei Wang,Minfeng Xu,Shuangmin Chen,Shiqing Xin,Changhe Tu,Wenping Wang*

Main category: cs.GR

TL;DR: 提出OffsetCrust框架，通过构造power diagram和轻量级微调算法，有效解决变半径偏移曲面计算难题，并在MAT重建等应用中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定半径偏移曲面计算取得进展，但变半径偏移曲面仍存在计算难题。本文旨在开发高效解决变半径偏移问题的新方法。

Method: 基于基面采样点和离面点构建power diagram，通过半径函数控制位移方向（固定半径时与法线方向一致），并采用轻量级微调算法解决传统crust方法的错位问题。

Result: 实验验证了OffsetCrust的精度和效率，成功应用于MAT表示到原始边界曲面的重建。

Conclusion: OffsetCrust为变半径偏移曲面计算提供了可靠解决方案，在几何处理领域具有重要应用价值。

Abstract: Offset surfaces, defined as the Minkowski sum of a base surface and a rolling
ball, play a crucial role in geometry processing, with applications ranging
from coverage motion planning to brush modeling. While considerable progress
has been made in computing constant-radius offset surfaces, computing
variable-radius offset surfaces remains a challenging problem. In this paper,
we present OffsetCrust, a novel framework that efficiently addresses the
variable-radius offsetting problem by computing a power diagram. Let $R$ denote
the radius function defined on the base surface $S$. The power diagram is
constructed from contributing sites, consisting of carefully sampled base
points on $S$ and their corresponding off-surface points, displaced along
$R$-dependent directions. In the constant-radius case only, these displacement
directions align exactly with the surface normals of $S$. Moreover, our method
mitigates the misalignment issues commonly seen in crust-based approaches
through a lightweight fine-tuning procedure. We validate the accuracy and
efficiency of OffsetCrust through extensive experiments, and demonstrate its
practical utility in applications such as reconstructing original boundary
surfaces from medial axis transform (MAT) representations.

</details>


### [50] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D框架通过HFS-SDEdit纹理增强和交替几何优化，实现低质3D模型到高质量资产的转换


<details>
  <summary>Details</summary>
Motivation: 解决高质量3D资产获取成本高、资源稀缺的问题

Method: 1. 使用HFS-SDEdit进行视图级纹理增强 2. 交替执行纹理/几何优化流程 3. 整合单目几何预测器提升几何细节

Result: 在3D模型细化任务中达到SOTA质量，超越现有方法

Conclusion: 有效缓解开源高质量3D资产短缺问题，为图形学应用提供新解决方案

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [51] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: 研究提出通过展示对话系统的推理过程或不确定性表达，增强用户自我表露意愿，核心机制是提升CUI心智理论的透明度。


<details>
  <summary>Details</summary>
Motivation: 针对人类自我表露困难的心理机制（担心他人负面反馈），探索对话界面设计中社会线索对降低心理防线的作用。

Method: 从理论层面分析不确定性表达、系统推理过程可视化两种设计策略，结合心智理论（theory of mind）框架进行概念论证。

Result: 理论推导表明透明化的心智模型能减少用户对CUI反应的焦虑，可能提升亲密话题交流中的自我表露频率与深度。

Conclusion: 对话系统的心智透明化设计（通过展示推理/表达不确定性）是增强人机信任、促进自我表露的有效设计范式，为CUI社交属性优化提供新方向。

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [52] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: 论文提出需改善自然语言处理领域与公众的沟通方式，通过规范术语、管理预期、强调伦理来增强公众理解与支持


<details>
  <summary>Details</summary>
Motivation: 大语言模型发展引发公众关注，但媒体报道常存在术语模糊、期待过高、伦理讨论不足等问题，阻碍公众正确理解NLP研究

Method: 基于已发表的NLP研究和新闻报道案例，分析三个核心障碍（模糊术语、非理性期待、伦理缺失）并提出沟通建议

Result: 识别出术语标准化、预期管理、伦理框架建设三个关键改进方向，并给出具体可操作的沟通策略

Conclusion: 透明有效的公众沟通是NLP领域可持续发展的基础，需要学界主动引导公众对话，构建负责任的研究生态系统

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [53] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: 现有大型语言模型在专利律师考试中表现未达专业标准，准确率最高仅82%（GPT-4o），远低于要求的90%。模型存在逻辑不一致、多模态整合缺陷和提示敏感性，专家评估揭示自动指标与人工判断存在偏差。


<details>
  <summary>Details</summary>
Motivation: 量化评估LLMs在法律资格考试中的实际表现，揭示公众认知与技术现实之间的差距，指出虚拟专利律师发展的关键技术瓶颈。

Method: 通过欧洲专利律师资格考试(EQE)部分试题，测试GPT系列/Anthropic/Deepseek/Llama-3等模型性能，结合专利专家对模型输出的法律逻辑和格式完整性评估。

Result: 最佳模型GPT-4o准确率0.82（F1 0.81），最差模型AWS Llama 3.1 8B仅0.50。所有模型未达专业通过线（0.90），温度参数0.2变化可使准确率波动±15%。人类专家更重视法律逻辑而非答案正确性。

Conclusion: 需提升逻辑一致性、跨模态整合能力和提示鲁棒性。当前技术距离虚拟专利律师实用化仍有显著差距，论文揭示了模型在专业领域的具体局限和改进方向。

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [54] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: 通过共享任务评估基于大语言模型的AI导师在教学对话中纠正学生错误的能力，涵盖五个评估维度，结果显示当前模型仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 提升AI导师在错误识别、精准定位、指导提供、反馈可操作性等教学核心环节的表现质量，建立符合学习科学原则的评估体系。

Method: 设计五赛道评估框架（含教学能力四维度+导师身份检测），收集50+国际团队模型，采用人工标注金标准进行对比评估。

Result: 最佳模型F1分数在四个教学维度达到58.34-71.81（三分类任务），导师身份检测赛道9分类任务F1达96.98。

Conclusion: 公开任务资源推动领域发展，当前结果表明AI导师教学能力需进一步提升，尤其在反馈指导等深层次教学环节。

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [55] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: 提出了首个全模态语音助手基准测试MultiVox，通过包含1000个带副语言特征和视觉线索的对话数据，揭示当前模型在整合多模态信息生成上下文响应方面的显著缺陷


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估语音助手对细粒度语音特征（音调/情感/环境音）的理解能力，以及视觉与副语言信号的对齐能力，制约了多模态交互的发展

Method: 构建包含1000个标注样本的MultiVox数据集（含语音对话/图像/视频），设计涵盖副语言特征和视觉线索的任务，并对9个SOTA模型进行全面评估

Result: 人类在任务中表现优异（平均准确率89%），但现有模型（包括GPT-4）平均准确率仅52%，在环境音理解任务中表现最差（32%准确率）

Conclusion: MultiVox填补了多模态评估空白，揭示了当前模型在上下文理解和跨模态对齐方面的重大缺陷，为提升语音助手的情境感知能力提供了新方向

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [56] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: 研究比较了强化学习(RL)和监督微调(SFT)在LLM数学推理训练中的表现，发现RL对现有能力有增强作用，而SFT会覆盖旧技能并导致跨领域性能下降


<details>
  <summary>Details</summary>
Motivation: 探究RL和SFT这两种主流LLM训练方法在数学推理任务中的动态差异及其对模型知识保留的影响

Method: 使用相同模型和超参数配置，在数学问题上并行开展RL和SFT训练，分析检查点参数变化，并尝试冻结部分网络层

Result: RL在数学领域获得有限提升但在MMLU等知识基准上轻微下降，SFT趋势更显著且影响中层MLP参数。参数冻结实验效果不一致

Conclusion: RL通过重点调整注意力机制参数实现能力增强，而SFT的大规模全参数更新可能导致知识遗忘，算法选择需根据训练目标权衡利弊

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [57] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: 提出基于编码器-解码器架构的领域自适应小语言模型（SLM），用于提升商品和服务税码的分层序列预测精度


<details>
  <summary>Details</summary>
Motivation: 跨境交易中商品税码（如HSN/SAC）的精准判定对税务合规至关重要，错误的税码分类会导致税务处罚。现有方法难以有效捕捉税码间的层次依赖关系

Method: 采用编码器-解码器架构的SLM模型，利用非结构化产品描述数据，通过序列生成方式捕捉税码层级结构。模型适应税务领域特点，对比测试平面分类器、纯编码器和纯解码器架构

Result: 在HSN税码预测任务中，领域自适应SLM显著优于平面分类器，在结构化序列生成任务中超越单一架构模型（F1分数提升3.5%）

Conclusion: 该方法可扩展至UNSPSC、NCM等其他政府税务商品编码体系，证明了编码器-解码器架构在结构化税务代码预测中的有效性，为行业提供可落地的合规解决方案

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [58] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM提出一种新型后训练量化方法，通过显式引入一阶梯度项改进量化误差补偿，在3比特权重量化下将Llama3-8B的困惑度降低89.6%，并在多基准测试中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有补偿式权重校准方法基于二阶泰勒展开假设一阶项可忽略，但渐进补偿过程实际导致潜在权重与其全精度版本间的一阶偏差累积，该假设存在根本性缺陷。

Method: 1. 通过直接计算潜在权重与全精度权重差异近似梯度
2. 利用预计算的Cholesky因子实时高效恢复Hessian子矩阵逆
3. 避免反向传播梯度计算的高成本与有限泛化性

Result: Llama3-70B在5-shot MMLU准确率从51.7%提升至74.9%（全精度78.6%）；与GPTAQ/SpinQuant集成后在W4A4KV4设置下进一步缩小与全精度基线的差距

Conclusion: FOEM不仅超越经典GPTQ方法，还能与现有先进技术无缝集成，在极低比特量化场景下取得当前SOTA方法无法实现的精度提升。

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [59] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: 提出AirLLM框架，通过强化学习驱动的分层扩散策略实现通信感知的LoRA适配，显著降低传输成本同时提升微调效果


<details>
  <summary>Details</summary>
Motivation: 现有LoRA方法采用固定/启发式秩配置导致传输效率低下，需解决边缘设备运行大模型时的通信带宽和计算资源限制问题

Method: 结合PPO代理（观察无线状态和语言复杂性生成粗粒度决策）与DDIM（通过去噪扩散生成高分辨率秩向量），采用Classifier-Free Guidance联合优化

Result: 在不同信噪比下实验显示，AirLLM在降低63%传输成本的同时保持优于基线模型的性能表现

Conclusion: 强化学习引导的扩散优化机制有效实现了任务和信道自适应的秩配置，为空中远程微调提供了可扩展的高效解决方案

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [60] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: 提出LiLM-RDB-SFC方法，结合轻量级语言模型与关系数据库指导DRL模型，FLAN-T5模型在SFC部署中展现显著性能优势


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在网络决策中受限于结构化数据依赖和固定规则，难以适应动态网络环境下的服务功能链智能部署需求

Method: 使用BART和FLAN-T5两种轻量语言模型解析网络数据，结合关系数据库处理SFC需求、资源状态等多维度查询

Result: FLAN-T5测试损失0.00161（BART为0.00734），准确率94.79%（BART为80.2%），处理时间较SQLCoder减少96%

Conclusion: LiLM-RDB-SFC框架有效提升网络智能化水平，FLAN-T5在保持高准确率的同时实现处理效率的突破性提升

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [61] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: 提出PAiR框架，通过整合视角感知AI与XR技术，利用Chronicles身份模型实现可解释的沉浸式体验


<details>
  <summary>Details</summary>
Motivation: 现有XR系统存在用户建模浅层化、认知场景理解受限等问题，无法实现真正的自适应体验

Method: 基于Chronicles多模态身份建模，构建用户认知演化的闭环系统，通过Unity引擎实现两种验证场景

Result: 在OpenDome引擎中实现两个概念验证场景，验证身份模型与沉浸式环境动态联动的可行性

Conclusion: PAiR框架开创了基于身份认知模型的沉浸式人机交互新范式，推动AI与XR技术的深度融合

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [62] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: 提出模块化多智能体AI框架，通过置信度协调器与图像检索技术，在零样本苹果叶病诊断中实现85.63%准确率，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体AI在零样本场景下的可信度问题，通过协调器动态校准智能体置信度，提升农业病害诊断的可靠性和解释性。

Method: 整合通用多模态智能体+非视觉协调器+RAG模块，采用置信度校准指标（ECE/OCR/CCC）动态调整信任权重，通过CLIP图像检索实现迭代预测优化。

Result: 零样本准确率提升77.94%，总精度达85.63%；GPT-4o展现更好校准能力，图像检索成功纠正Qwen-2.5-VL的过度自信问题。

Conclusion: 该框架实现感知与推理的模块化分离，为农业诊断等信任敏感领域提供可扩展解决方案，开源生态促进透明AI发展。

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [63] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: 本文提出了Web of Agents (WoA)的进化框架，分析其从早期标准到现代协议的演变，并强调智能核心向LLM的转移及未来挑战


<details>
  <summary>Details</summary>
Motivation: 整合分散在多智能体系统(MAS)和语义Web领域的研究，揭示现代Agent协议与早期标准之间的进化关系，促进对领域发展轨迹的整体理解

Method: 通过四轴分类法（语义基础/通信范式/智能核心/发现机制）系统分析不同世代的架构，对比FIPA/OWL标准与现代A2A/MCP协议的技术演进

Result: 发现智能核心从外部数据（语义Web）和平台（MAS）向LLM内嵌模型的范式转移，奠定了现代Agentic AI实现可扩展自适应系统的基础

Conclusion: 需突破单纯协议改进，建立解决去中心化身份/经济模型/安全/治理等社会技术挑战的新研究议程，构建可信开放的WoA生态系统

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [64] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: 研究验证了基于少量样本提示的大语言模型（如GPT-4o）可自动化主题分析，为定性研究提供可扩展的补充方案


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型能否复现专家主导的社交媒体数据主题分析，探索其在公共卫生领域的应用潜力

Method: 使用两个非重叠时间段的Reddit数据集（优化集n=286，验证集n=686），采用零样本/单样本/少量样本提示策略，将多标签分类任务转化为系列二分类任务进行评估

Result: GPT-4o在验证集上表现最佳（准确率90.9%，F1分数0.71），高流行主题的分布与专家分类高度吻合（如甲苯噻嗪使用率13.6% vs 17.8%）

Conclusion: 少量样本提示的LLM方法能够有效自动化主题分析，为定性研究提供可扩展的技术补充

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [65] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: Proposes NavComposer for automatic high-quality navigation instruction generation by decomposing/recomposing semantic entities, paired with annotation-free evaluation system NavInstrCritic.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of scarce expert annotations and low-quality synthetic instructions in embodied AI navigation research.

Method: Modular framework decomposes actions/scenes/objects into semantic components, recomposes them into natural instructions. Introduces three-dimensional evaluation metrics without requiring annotations.

Result: Enables scalable instruction generation across diverse trajectories and provides holistic quality assessment beyond traditional expert-dependent metrics.

Conclusion: Decoupling instruction generation/evaluation from specific agents facilitates more generalizable embodied AI research paradigms.

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [66] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: TREC 2022年深度学习轨道聚焦构建更完善的段落检索测试集，使用大规模更新后的MS MARCO数据集，发现预训练神经模型仍优于传统方法，但密集检索表现下降。


<details>
  <summary>Details</summary>
Motivation: 构建更高质量的段落检索测试集以提高评估可靠性，利用更新后规模显著扩大的MS MARCO数据集，延续往年的研究基础并优化资源分配。

Method: 1. 使用刷新后的段落/文档集合（段落规模扩大16倍，文档扩大4倍） 2. 优先标注段落级数据，文档级标签通过段落标签推断 3. 对比深度神经排序模型与传统检索方法

Result: 1. 基于大规模预训练的深度神经模型持续优于传统方法 2. 顶级模型未使用密集检索 3. 单阶段密集检索效果较去年显著下降（文档任务作为次要任务）

Conclusion: 测试集质量提升使结果更具区分度，但需重新评估密集检索的优化方向。研究为未来检索模型评估提供了更可靠的基础设施，揭示了模型性能动态变化特性。

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [67] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: 提出新型动态基准测试SWE-MERA，解决SWE-bench数据集存在的严重数据污染问题（32.67%解决方案泄露，31.08%测试不足）


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试SWE-bench存在根本性缺陷：32.67%成功修复涉及解决方案泄露，31.08%因测试用例不足通过，无法准确评估模型性能

Method: 通过自动化收集GitHub真实问题构建动态更新基准，建立包含质量验证的可靠流程，当前包含10,000潜在任务（已开放300样本）

Result: 使用Aider编码代理评估显示显著模型区分度，报告2024年9月至2025年6月期间12个最新LLM在真实任务中的性能表现

Conclusion: SWE-MERA通过动态更新机制和严格质量验证，为LLM软件工程能力评估提供更可靠、防污染的基准测试框架

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>
