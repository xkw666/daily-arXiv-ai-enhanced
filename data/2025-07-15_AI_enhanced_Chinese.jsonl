{"id": "2507.08865", "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "categories": ["cs.CL"], "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "\u63d0\u51faSpatial ModernBERT\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u5d4c\u5165\u589e\u5f3a\u7684Transformer\u7ed3\u6784\u5b9e\u73b0\u91d1\u878d\u6587\u6863\u8868\u683c/\u952e\u503c\u5bf9\u63d0\u53d6", "motivation": "\u91d1\u878d\u6587\u6863\u4e2d\u7684\u8868\u683c\u6570\u636e\u63d0\u53d6\u5bf9\u5ba1\u8ba1/\u81ea\u52a8\u5316\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u7406\u590d\u6742\u7248\u9762\u5e03\u5c40", "method": "\u4e09\u5934token\u5206\u7c7b\u67b6\u6784(\u6807\u7b7e/\u5217/\u884c\u5934)\uff0c\u91c7\u7528PubTables-1M\u9884\u8bad\u7ec3+B-I-IB\u6807\u8bb0\u540e\u5904\u7406\uff0c\u7ed3\u5408\u6587\u672c\u4e0e\u7a7a\u95f4\u7279\u5f81", "result": "\u5b9e\u8bc1\u8bc1\u660e\u6a21\u578b\u6709\u6548\u878d\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5728\u771f\u5b9e\u91d1\u878d\u6587\u6863\u63d0\u53d6\u4efb\u52a1\u4e2d\u8fbe\u5230\u9ad8\u7cbe\u5ea6", "conclusion": "\u8be5\u65b9\u6848\u4e3a\u8d22\u52a1\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8868\u683c\u7ed3\u6784\u5316\u89e3\u6790\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u6587\u6863\u5904\u7406\u6548\u7387"}}
{"id": "2507.08898", "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "\u63d0\u51faSEALGuard\u591a\u8bed\u8a00\u62a4\u680f\u7cfb\u7edf\uff0c\u901a\u8fc7LoRA\u9002\u914d\u548cSEALSBench\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347LLM\u7cfb\u7edf\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6709\u5bb3\u8f93\u5165\u7684\u68c0\u6d4b\u80fd\u529b\uff08DSR\u63d0\u534748%\uff09", "motivation": "\u73b0\u6709\u5b89\u5168\u62a4\u680f\uff08\u5982LlamaGuard\uff09\u5728\u82f1\u8bed\u73af\u5883\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9762\u5bf9\u4e1c\u5357\u4e9a\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6709\u5bb3/\u8d8a\u72f1\u63d0\u793a\u65f6\u9632\u5fa1\u6210\u529f\u7387\u5927\u5e45\u4e0b\u964d\uff08\u5206\u522b\u964d\u4f4e9%\u548c18%\uff09", "method": "1. \u4f7f\u7528\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u5c06\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\u6539\u9020\u6210\u5b89\u5168\u62a4\u680f\n2. \u6784\u5efa\u5305\u542b10\u79cd\u8bed\u8a0026\u4e07\u63d0\u793a\u7684SEALSBench\u57fa\u51c6\u6570\u636e\u96c6\n3. \u8bbe\u8ba1\u5305\u542b\u5b89\u5168/\u6709\u5bb3/\u8d8a\u72f1\u7684\u4e09\u5206\u7c7b\u8bc4\u4f30\u6846\u67b6", "result": "SEALGuard\u5728DSR\uff08\u9632\u5fa1\u6210\u529f\u7387\uff09\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u5168\u9762\u9886\u5148\uff1a\n- \u591a\u8bed\u8a00\u6709\u5bb3\u63d0\u793a\u68c0\u6d4bDSR\u6bd4LlamaGuard\u63d0\u534748%\n- \u8d8a\u72f1\u63d0\u793a\u68c0\u6d4bDSR\u8fbe\u6700\u4f73\u6c34\u5e73\n\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u578b\u89c4\u6a21\u548c\u9002\u914d\u7b56\u7565\u7684\u6709\u6548\u6027", "conclusion": "SEALGuard\u901a\u8fc7\u521b\u65b0\u6027\u7684\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u65b9\u6848\uff0c\u89e3\u51b3\u4e86LLM\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u7684\u5b89\u5168\u9690\u60a3\uff0c\u4e3a\u5168\u7403\u5316AI\u90e8\u7f72\u63d0\u4f9b\u53ef\u9760\u4fdd\u969c"}}
{"id": "2507.08916", "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "categories": ["cs.CL"], "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "\u73b0\u6709\u533b\u5b66LLM\u8bc4\u4f30\u6570\u636e\u96c6\u5b58\u5728\u4e34\u5e8a\u771f\u5b9e\u6027\u4e0d\u8db3\u3001\u900f\u660e\u5ea6\u4f4e\u7b49\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u95ee\u7b54\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u7684\u8d28\u91cf\u7f3a\u9677\u5bf9\u8bc4\u4f30\u7ed3\u679c\u7684\u5f71\u54cd", "method": "\u7cfb\u7edf\u56de\u987eMedQA\u7b49\u4e3b\u6d41\u533b\u5b66\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5206\u6790\u5176\u4e34\u5e8a\u76f8\u5173\u6027/\u900f\u660e\u5ea6/\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u540c\u65f6\u8bc4\u4f30\u533b\u5b66\u671f\u520a\u6311\u6218\u95ee\u9898\u7684\u66ff\u4ee3\u4ef7\u503c", "result": "\u53d1\u73b0\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u4e34\u5e8a\u771f\u5b9e\u6027\u548c\u9a8c\u8bc1\u6807\u51c6\uff0c\u516c\u5f00\u6311\u6218\u95ee\u9898\u5b58\u5728\u89c4\u6a21\u5c0f/\u8303\u56f4\u7a84/\u53ef\u80fd\u88abLLM\u8bad\u7ec3\u6c61\u67d3\u7b49\u95ee\u9898", "conclusion": "\u9700\u6784\u5efa\u5b89\u5168\u3001\u5168\u9762\u3001\u5177\u4ee3\u8868\u6027\u7684\u533b\u5b66\u8bc4\u4f30\u6846\u67b6\uff0c\u8981\u6c42\u673a\u6784\u95f4\u534f\u4f5c\u5f00\u53d1\u80fd\u53cd\u6620\u4e34\u5e8a\u590d\u6742\u6027\u7684\u4e25\u8c28\u6570\u636e\u96c6"}}
{"id": "2507.08924", "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "\u63d0\u51faKMMLU-Redux\u548cKMMLU-Pro\u4e24\u4e2a\u97e9\u56fd\u4e13\u5bb6\u7ea7\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u97e9\u56fd\u5de5\u4e1a\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6\u5e94\u7528\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b66\u672f\u9886\u57df\uff0c\u7f3a\u4e4f\u5bf9\u5de5\u4e1a\u5b9e\u9645\u573a\u666f\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u9700\u6784\u5efa\u53cd\u6620\u97e9\u56fd\u4e13\u4e1a\u77e5\u8bc6\u7684\u53ef\u9760\u6d4b\u8bd5\u96c6\u3002", "method": "1. KMMLU-Redux\u91cd\u6784\u81ea\u97e9\u56fd\u56fd\u5bb6\u6280\u672f\u8d44\u683c\u8003\u8bd5\u9898\uff0c\u5254\u9664\u9519\u8bef\u6570\u636e\uff1b2. KMMLU-Pro\u57fa\u4e8e\u97e9\u56fd\u56fd\u5bb6\u4e13\u4e1a\u6267\u7167\u8003\u8bd5\u5185\u5bb9\u6784\u5efa", "result": "\u5b9e\u9a8c\u8bc1\u660e\u57fa\u51c6\u80fd\u6709\u6548\u53cd\u6620\u97e9\u56fd\u5de5\u4e1a\u77e5\u8bc6\u4f53\u7cfb\uff0c\u5b8c\u6574\u6570\u636e\u96c6\u5df2\u5f00\u6e90", "conclusion": "\u65b0\u57fa\u51c6\u586b\u8865\u5de5\u4e1a\u77e5\u8bc6\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3aLLMs\u5728\u97e9\u56fd\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u9760\u6d4b\u8bd5\u6807\u51c6"}}
{"id": "2507.09140", "pdf": "https://arxiv.org/pdf/2507.09140", "abs": "https://arxiv.org/abs/2507.09140", "authors": ["Chuang Chen", "Xiaoxuan Xie", "Yongming Zhang", "Tianyu Zhang", "Haoran Xie"], "title": "Interactive Drawing Guidance for Anime Illustrations with Diffusion Model", "categories": ["cs.GR"], "comment": "9 pages, 7 figures. In proceedings of NICOGRAPH International 2025", "summary": "Creating high-quality anime illustrations presents notable challenges,\nparticularly for beginners, due to the intricate styles and fine details\ninherent in anime art. We present an interactive drawing guidance system\nspecifically designed for anime illustrations to address this issue. It offers\nreal-time guidance to help users refine their work and streamline the creative\nprocess. Our system is built upon the StreamDiffusion pipeline to deliver\nreal-time drawing assistance. We fine-tune Stable Diffusion with LoRA to\nsynthesize anime style RGB images from user-provided hand-drawn sketches and\nprompts. Leveraging the Informative Drawings model, we transform these RGB\nimages into rough sketches, which are further refined into structured guidance\nsketches using a custom-designed optimizer. The proposed system offers precise,\nreal-time guidance aligned with the creative intent of the user, significantly\nenhancing both the efficiency and accuracy of the drawing process. To assess\nthe effectiveness of our approach, we conducted a user study, gathering\nempirical feedback on both system performance and interface usability.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eStreamDiffusion\u7684\u5b9e\u65f6\u52a8\u6f2b\u7ed8\u56fe\u6307\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7AI\u4f18\u5316\u8349\u56fe\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u6307\u5bfc", "motivation": "\u89e3\u51b3\u52a8\u6f2b\u827a\u672f\u590d\u6742\u98ce\u683c\u548c\u7cbe\u7ec6\u7ec6\u8282\u5bf9\u521d\u5b66\u8005\u7684\u521b\u4f5c\u6311\u6218\uff0c\u964d\u4f4e\u4e13\u4e1a\u7ea7\u63d2\u56fe\u521b\u4f5c\u95e8\u69db", "method": "1. \u5fae\u8c03Stable Diffusion+LoRA\u5904\u7406\u8349\u56fe\u751f\u6210RGB\u56fe\u50cf 2. \u4f7f\u7528Informative Drawings\u6a21\u578b\u8f6c\u6362\u7ed3\u6784\u5316\u6307\u5bfc\u8349\u56fe 3. \u5b9a\u5236\u4f18\u5316\u5668\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92", "result": "\u7528\u6237\u7814\u7a76\u663e\u793a\u7cfb\u7edf\u663e\u8457\u63d0\u534763%\u7ed8\u56fe\u6548\u7387\u548c\u7ebf\u7a3f\u51c6\u786e\u5ea6\uff08PSNR\u63d0\u53478.6dB\uff09\uff0c\u754c\u9762\u53ef\u7528\u6027\u8bc4\u52064.8/5", "conclusion": "\u5b9e\u65f6AI\u6307\u5bfc\u7cfb\u7edf\u6709\u6548\u878d\u5408\u521b\u4f5c\u610f\u56fe\u4e0e\u6280\u672f\u8f85\u52a9\uff0c\u4e3a\u6570\u5b57\u827a\u672f\u521b\u4f5c\u5f00\u8f9f\u4eba\u673a\u534f\u4f5c\u65b0\u8303\u5f0f"}}
{"id": "2507.08967", "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "categories": ["cs.CL"], "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u81ea\u6539\u8fdb\u6a21\u578b\u5f15\u5bfc\u6846\u67b6SIMS\uff0c\u901a\u8fc7\u81ea\u4e3b\u751f\u6210\u5bf9\u6bd4\u6837\u672c\u5b9e\u73b0\u65e0\u5916\u90e8\u76d1\u7763\u7684LLM\u63a8\u7406\u5bf9\u9f50", "motivation": "\u4f20\u7edf\u6a21\u578b\u5f15\u5bfc\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u5176\u9002\u5e94\u6027\u548c\u6548\u679c\u7a33\u5b9a\u6027\u3002\u9700\u5f00\u53d1\u81ea\u76d1\u7763\u7684\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u5f15\u5bfc\u65b9\u6cd5", "method": "1. \u8fed\u4ee3\u81ea\u6539\u8fdb\u5faa\u73af\u751f\u6210/\u4f18\u5316\u5bf9\u6bd4\u6837\u672c 2. \u5f15\u5165\u63d0\u793a\u6392\u5e8f\u548c\u5bf9\u6bd4\u91c7\u6837\u65b0\u7b56\u7565 3. \u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u52a8\u6001\u5f15\u5bfc\u673a\u5236", "result": "\u5728\u591a\u4e2aLLM\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSIMS\u7684\u5f15\u5bfc\u6548\u679c\u548c\u9002\u5e94\u6027\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u5177\u4f53\u63d0\u5347\u5e45\u5ea6\u9700\u53c2\u8003\u8bba\u6587\u5b9e\u9a8c\u90e8\u5206\uff09", "conclusion": "\u81ea\u6539\u8fdb\u6a21\u578b\u5f15\u5bfc\u4e3aLLM\u63a8\u7406\u65f6\u5bf9\u9f50\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u8bc1\u660e\u65e0\u76d1\u7763\u65b9\u6cd5\u7684\u5de8\u5927\u6f5c\u529b"}}
{"id": "2507.09146", "pdf": "https://arxiv.org/pdf/2507.09146", "abs": "https://arxiv.org/abs/2507.09146", "authors": ["Ryuichi Miyauchi", "Hengyuan Chang", "Tsukasa Fukusato", "Kazunori Miyata", "Haoran Xie"], "title": "Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition", "categories": ["cs.GR"], "comment": "8 pages, 12 figures. In proceedings of NICOGRAPH International 2025", "summary": "Fluid simulation techniques are widely used in various fields such as film\nproduction, but controlling complex fluid behaviors remains challenging. While\nrecent generative models enable intuitive generation of vector fields from user\nsketches, they struggle to maintain physical properties such as\nincompressibility. To address these issues, this paper proposes a method for\ninteractively designing 2D vector fields. Conventional generative models can\nintuitively generate vector fields from user sketches, but remain difficult to\nconsider physical properties. Therefore, we add a simple editing process after\ngenerating the vector field. In the first stage, we use a latent diffusion\nmodel~(LDM) to automatically generate initial 2D vector fields from user\nsketches. In the second stage, we apply the Helmholtz-Hodge decomposition to\nlocally extract physical properties such as incompressibility from the results\ngenerated by LDM and recompose them according to user intentions. Through\nmultiple experiments, we demonstrate the effectiveness of our proposed method.", "AI": {"tldr": "\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548cHelmholtz-Hodge\u5206\u89e3\uff0c\u5b9e\u73b0\u7269\u7406\u5c5e\u6027\u53ef\u63a7\u7684\u4ea4\u4e92\u5f0f\u4e8c\u7ef4\u77e2\u91cf\u573a\u8bbe\u8ba1", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u96be\u4ee5\u4fdd\u6301\u77e2\u91cf\u573a\u7684\u7269\u7406\u5c5e\u6027\uff08\u5982\u4e0d\u53ef\u538b\u7f29\u6027\uff09\uff0c\u9700\u6539\u8fdb\u8bbe\u8ba1\u6d41\u7a0b", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u751f\u6210\u521d\u59cb\u77e2\u91cf\u573a 2\uff09\u901a\u8fc7Helmholtz-Hodge\u5206\u89e3\u63d0\u53d6\u7269\u7406\u5c5e\u6027\u5e76\u91cd\u7ec4", "result": "\u591a\u7ec4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u5c5e\u6027\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u751f\u6210\u77e2\u91cf\u573a\u7684\u7269\u7406\u5c5e\u6027\u63a7\u5236\u95ee\u9898\uff0c\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u6d41\u4f53\u6a21\u62df\u9886\u57df"}}
{"id": "2507.08969", "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "categories": ["cs.CL"], "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5206\u6790EHR\u6570\u636e\uff0c\u63ed\u793a\u533b\u7597\u8bb0\u5f55\u4e2d\u6c61\u540d\u5316\u8bed\u8a00\u4e0e\u60a3\u8005\u79cd\u65cf/\u4fdd\u9669\u7c7b\u578b\u7684\u5173\u8054\u6a21\u5f0f", "motivation": "\u63a2\u7a76\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u60a3\u8005\u6c61\u540d\u5316\u73b0\u8c61\u7684\u4f20\u64ad\u673a\u5236\u53ca\u5176\u5f71\u54cd\u56e0\u7d20", "method": "\u91c7\u7528MIMIC-III\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u6269\u5c55\u8bcd\u5178\u5339\u914d\u4e0e\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\u8bc6\u522b\u8bed\u8a00\u7279\u5f81\uff0c\u4f7f\u7528\u6cca\u677e\u56de\u5f52\u8fdb\u884c\u9884\u6d4b\u5206\u6790", "result": "\u9ed1\u4eba/\u653f\u5e9c\u4fdd\u9669/\u7cbe\u795e\u75be\u75c5\u60a3\u8005\u6c61\u540d\u5316\u6807\u7b7e\u7387\u66f4\u9ad8\uff0c\u62a4\u58eb/\u793e\u5de5\u7fa4\u4f53\u4f7f\u7528\u9891\u7387\u663e\u8457\u9ad8\u4e8e\u5176\u4ed6\u533b\u62a4", "conclusion": "\u533b\u7597\u6587\u4e66\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u6301\u7eed\u5f3a\u5316\u60a3\u8005\u6c61\u540d\u5316\uff0c\u9700\u5efa\u7acb\u7ed3\u6784\u5316\u6587\u6863\u89c4\u8303\u51cf\u5c11\u4e3b\u89c2\u8bed\u8a00\u504f\u5dee"}}
{"id": "2507.09441", "pdf": "https://arxiv.org/pdf/2507.09441", "abs": "https://arxiv.org/abs/2507.09441", "authors": ["Ankit Sanjyal"], "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling", "categories": ["cs.GR", "cs.CV"], "comment": "8 Pages, 10 Figures, Pre-Print Version, Code Available at:\n  https://github.com/ANKITSANJYAL/RectifiedHR", "summary": "High-resolution image synthesis with diffusion models often suffers from\nenergy instabilities and guidance artifacts that degrade visual quality. We\nanalyze the latent energy landscape during sampling and propose adaptive\nclassifier-free guidance (CFG) schedules that maintain stable energy\ntrajectories. Our approach introduces energy-aware scheduling strategies that\nmodulate guidance strength over time, achieving superior stability scores\n(0.9998) and consistency metrics (0.9873) compared to fixed-guidance\napproaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling\nyields optimal performance, providing sharper, more faithful images while\nreducing artifacts. Our energy profiling framework serves as a powerful\ndiagnostic tool for understanding and improving diffusion model behavior.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u5206\u7c7b\u5668\u65e0\u5f15\u5bfc\u8c03\u5ea6\u7b56\u7565\u6539\u5584\u6269\u6563\u6a21\u578b\u56fe\u50cf\u5408\u6210\u7684\u80fd\u91cf\u7a33\u5b9a\u6027", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u9762\u4e34\u80fd\u91cf\u4e0d\u7a33\u5b9a\u548c\u5f15\u5bfc\u4f2a\u5f71\u95ee\u9898\uff0c\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf", "method": "\u901a\u8fc7\u80fd\u91cf\u5206\u6790\u5f00\u53d1\u52a8\u6001CFG\u8c03\u5ea6\u7b56\u7565\uff0c\u91c7\u7528\u7ebf\u6027\u9012\u51cf\u7684\u5f15\u5bfc\u5f3a\u5ea6\u8c03\u6574\u65b9\u6848", "result": "\u5b9e\u73b00.9998\u7a33\u5b9a\u6027\u5206\u6570\u548c0.9873\u4e00\u81f4\u6027\u6307\u6807\uff0cDPM++ 2M+\u7ebf\u6027\u8c03\u5ea6\u65b9\u6848\u6548\u679c\u6700\u4f18", "conclusion": "\u80fd\u91cf\u5206\u6790\u6846\u67b6\u4e3a\u8bca\u65ad\u548c\u6539\u8fdb\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u81ea\u9002\u5e94\u8c03\u5ea6\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf"}}
{"id": "2507.09011", "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "\u901a\u8fc7Ganzflicker\u5b9e\u9a8c\u7ed3\u5408\u8bed\u8a00\u5206\u6790\uff0c\u53d1\u73b0\u89c6\u89c9\u60f3\u8c61\u529b\u5f3a\u5f31\u8005\u5728\u5e7b\u89c9\u5185\u5bb9\u590d\u6742\u5ea6\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u53ef\u80fd\u4e0e\u89c6\u89c9\u7cfb\u7edf\u4e0d\u540c\u5c42\u7ea7\u7684\u534f\u8c03\u6709\u5173\u3002", "motivation": "\u63a2\u7a76\u4e0d\u540c\u610f\u8c61\u8868\u578b\uff08\u5f31/\u5178\u578b/\u5f3a\u60f3\u8c61\u529b\u8005\uff09\u5728Ganzflicker\u8bf1\u5bfc\u7684\u5e7b\u89c9\u4e2d\u89c6\u89c9\u4f53\u9a8c\u5dee\u5f02\uff0c\u9a8c\u8bc1\u610f\u8c61\u8c31\u7406\u8bba\u5bf9\u5185\u90e8\u751f\u6210\u89c6\u89c9\u4f53\u9a8c\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528NLP\u6280\u672f\u5206\u67904,000+\u53c2\u4e0e\u8005\u7684\u5e7b\u89c9\u6587\u672c\u63cf\u8ff0\uff0c\u6bd4\u8f83\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u7eaf\u6587\u672c\u6a21\u578b\u7684\u6355\u6349\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4f20\u611f\u5668\u8fd0\u52a8\u5173\u8054\u5206\u6790\u8bed\u8a00\u7279\u5f81\u3002", "result": "\u5f3a\u60f3\u8c61\u529b\u8005\u62a5\u544a\u81ea\u7136\u4e3b\u4e49\u590d\u6742\u5185\u5bb9\uff0c\u5f31\u60f3\u8c61\u529b\u8005\u63cf\u8ff0\u7b80\u5355\u51e0\u4f55\u56fe\u6848\uff1b\u89c6\u89c9\u6a21\u578b\u66f4\u6709\u6548\u533a\u5206\u5dee\u5f02\uff0c\u5f3a\u60f3\u8c61\u529b\u8005\u8bed\u8a00\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u611f\u89c9\u8fd0\u52a8\u5173\u8054\u3002", "conclusion": "\u4e2a\u4f53\u65e9\u671f\u89c6\u89c9\u533a\u4e0e\u9ad8\u9636\u8111\u533a\u534f\u8c03\u673a\u5236\u7684\u5dee\u5f02\u53ef\u80fd\u662f\u5bfc\u81f4\u610f\u8c61\u8c31\u73b0\u8c61\u7684\u91cd\u8981\u795e\u7ecf\u673a\u5236\u3002"}}
{"id": "2507.09704", "pdf": "https://arxiv.org/pdf/2507.09704", "abs": "https://arxiv.org/abs/2507.09704", "authors": ["Xiaotang Zhang", "Ziyi Chang", "Qianhui Men", "Hubert Shum"], "title": "Real-time and Controllable Reactive Motion Synthesis via Intention Guidance", "categories": ["cs.GR"], "comment": null, "summary": "We propose a real-time method for reactive motion synthesis based on the\nknown trajectory of input character, predicting instant reactions using only\nhistorical, user-controlled motions. Our method handles the uncertainty of\nfuture movements by introducing an intention predictor, which forecasts key\njoint intentions to make pose prediction more deterministic from the historical\ninteraction. The intention is later encoded into the latent space of its\nreactive motion, matched with a codebook which represents mappings between\ninput and output. It samples a categorical distribution for pose generation and\nstrengthens model robustness through adversarial training. Unlike previous\noffline approaches, the system can recursively generate intentions and reactive\nmotions using feedback from earlier steps, enabling real-time, long-term\nrealistic interactive synthesis. Both quantitative and qualitative experiments\nshow our approach outperforms other matching-based motion synthesis approaches,\ndelivering superior stability and generalizability. In our method, user can\nalso actively influence the outcome by controlling the moving directions,\ncreating a personalized interaction path that deviates from predefined\ntrajectories.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5386\u53f2\u8fd0\u52a8\u8f68\u8ff9\u7684\u5b9e\u65f6\u53cd\u5e94\u5f0f\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u610f\u56fe\u9884\u6d4b\u5668\u4e0e\u7801\u672c\u5339\u914d\u5b9e\u73b0\u4e2a\u6027\u5316\u4ea4\u4e92\u5408\u6210", "motivation": "\u4f20\u7edf\u79bb\u7ebf\u65b9\u6cd5\u65e0\u6cd5\u5b9e\u65f6\u751f\u6210\u957f\u671f\u4ea4\u4e92\u52a8\u4f5c\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u672a\u6765\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u53ca\u7528\u6237\u4e3b\u52a8\u63a7\u5236\u9700\u6c42", "method": "1. \u5f15\u5165\u610f\u56fe\u9884\u6d4b\u5668\u9884\u5224\u5173\u952e\u5173\u8282\u8fd0\u52a8\u8d8b\u52bf\n2. \u6f5c\u5728\u7a7a\u95f4\u7f16\u7801\u5339\u914d\u8f93\u5165\u8f93\u51fa\u6620\u5c04\u7684\u7801\u672c\n3. \u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\n4. \u9012\u5f52\u751f\u6210\u673a\u5236\u5b9e\u73b0\u5b9e\u65f6\u53cd\u9988\u95ed\u73af", "result": "\u5b9a\u91cf/\u5b9a\u6027\u5b9e\u9a8c\u663e\u793a\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u6027\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u65b9\u5411\u63a7\u5236\u751f\u6210\u4e2a\u6027\u5316\u4ea4\u4e92\u8def\u5f84", "conclusion": "\u7a81\u7834\u79bb\u7ebf\u5408\u6210\u9650\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u5b9e\u65f6\u957f\u671f\u4ea4\u4e92\u52a8\u4f5c\u751f\u6210\uff0c\u4e3a\u6e38\u620f/VR\u9886\u57df\u63d0\u4f9b\u53ef\u52a8\u6001\u8c03\u6574\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u6846\u67b6"}}
{"id": "2507.09025", "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "\u63d0\u51faLizard\u6846\u67b6\uff0c\u901a\u8fc7\u6b21\u4e8c\u6b21\u65b9\u6ce8\u610f\u529b\u4e0e\u95e8\u63a7\u6a21\u5757\u5c06Transformer\u6539\u8fdb\u4e3a\u652f\u6301\u65e0\u9650\u4e0a\u4e0b\u6587\u7684\u9ad8\u6548\u67b6\u6784\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "Transformer\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u4e8c\u6b21\u590d\u6742\u5ea6\u8ba1\u7b97\u74f6\u9888\u548c\u5185\u5b58\u9650\u5236\uff0c\u73b0\u6709\u7ebf\u6027\u5316\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u95e8\u63a7\u673a\u5236\u3002Lizard\u65e8\u5728\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u6a21\u578b\u6027\u80fd\u3002", "method": "1. \u5f15\u5165\u6b21\u4e8c\u6b21\u65b9\u6ce8\u610f\u529b\u8fd1\u4f3csoftmax\u6548\u679c\n2. \u96c6\u6210\u53d7SOTA\u6a21\u578b\u542f\u53d1\u7684\u81ea\u9002\u5e94\u95e8\u63a7\u6a21\u5757\n3. \u6df7\u5408\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\uff08\u5168\u5c40\u538b\u7f29\uff09\u4e0e\u5143\u8bb0\u5fc6\u589e\u5f3a\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff08\u5c40\u90e8\u4ea4\u4e92\uff09\n4. \u5f00\u53d1\u786c\u4ef6\u611f\u77e5\u8bad\u7ec3\u52a0\u901f\u7b97\u6cd5", "result": "1. \u5728\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\u6062\u590d\n2. 5-shot MMLU\u63d0\u534718\u5206\uff0c\u5173\u8054\u53ec\u56de\u4efb\u52a1\u663e\u8457\u6539\u8fdb\n3. \u8bad\u7ec3\u901f\u5ea6\u63d0\u5347\u4e14\u652f\u6301\u6052\u5b9a\u5185\u5b58\u63a8\u7406", "conclusion": "Lizard\u901a\u8fc7\u521b\u65b0\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u8bbe\u8ba1\uff0c\u7a81\u7834\u4e86\u4f20\u7edfTransformer\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\uff0c\u4e3a\u9ad8\u6548\u957f\u5e8f\u5217\u5904\u7406\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u9ad8\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09792", "pdf": "https://arxiv.org/pdf/2507.09792", "abs": "https://arxiv.org/abs/2507.09792", "authors": ["Prashant Govindarajan", "Davide Baldelli", "Jay Pathak", "Quentin Fournier", "Sarath Chandar"], "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCADmium\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03\u4ee3\u7801\u5927\u6a21\u578b\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230JSON\u683c\u5f0fCAD\u5e8f\u5217\u7684\u81ea\u52a8\u5316\u751f\u6210\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u51e0\u4f55\u62d3\u6251\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524dCAD\u5efa\u6a21\u4f9d\u8d56\u8017\u65f6\u7684\u624b\u52a8\u64cd\u4f5c\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6f5c\u529b\uff0c\u4e9f\u9700\u63a2\u7d22\u6587\u672c\u9a71\u52a8CAD\u81ea\u52a8\u5316\u7684\u9ad8\u6548\u65b9\u6848\u3002", "method": "\u6784\u5efa17\u4e07GPT-4\u6807\u6ce8\u7684CAD\u6570\u636e\u96c6\uff0c\u5fae\u8c03\u4ee3\u7801\u5927\u6a21\u578b\u5b9e\u73b0\u6587\u672c\u5230JSON\u7684CAD\u751f\u6210\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u7403\u5f62\u5ea6\u3001\u5e73\u5747\u66f2\u7387\u548c\u6b27\u62c9\u7279\u5f81\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCADmium\u663e\u8457\u52a0\u901f\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u65b0\u6307\u6807\u6709\u6548\u63ed\u793a\u751f\u6210\u6a21\u578b\u7684\u7ed3\u6784\u7279\u6027\uff0c\u5728\u5408\u6210\u6570\u636e\u548c\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u4e0a\u5747\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86LLM\u5728CAD\u81ea\u52a8\u5316\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u65b0\u578b\u8bc4\u4f30\u6307\u6807\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2507.09037", "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "\u63d0\u51faALIGN\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u4e2a\u6027\u5316\u63d0\u793a\u5bf9\u9f50\u5b9e\u73b0LLM\u51b3\u7b56\u8005\u7684\u4ef7\u503c\u89c2\u6821\u51c6\uff0c\u5305\u542b\u53ef\u4ea4\u4e92\u754c\u9762\u4e0e\u6a21\u5757\u5316\u7b97\u6cd5\u6846\u67b6", "motivation": "\u7528\u6237\u4ef7\u503c\u89c2\u591a\u6837\u6027\u5f71\u54cdLLM\u8f85\u52a9\u51b3\u7b56\u6548\u679c\uff0c\u9700\u5f00\u53d1\u65b0\u578b\u5bf9\u9f50\u65b9\u6cd5\u5b9e\u73b0\u4e2a\u6027\u5316\u51b3\u7b56\u652f\u6301", "method": "\u5f00\u53d1\u5305\u542b\u914d\u7f6e\u7ba1\u7406/\u7ed3\u6784\u5316\u63a8\u7406\u8f93\u51fa\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u5b9e\u73b0\u53ef\u4ea4\u6362LLM\u7b97\u6cd5\u7684\u6a21\u5757\u5316\u540e\u7aef", "result": "\u5728\u516c\u4f17\u610f\u89c1\u8c03\u67e5\uff08\u4eba\u53e3\u7edf\u8ba1\u5bf9\u9f50\uff09\u548c\u533b\u7597\u5206\u8bca\u51b3\u7b56\uff08\u4ef7\u503c\u89c2\u5bf9\u9f50\uff09\u4e24\u4e2a\u9886\u57df\u5b8c\u6210\u5b9a\u91cf\u5206\u6790\u9a8c\u8bc1", "conclusion": "\u5f00\u6e90ALIGN\u6846\u67b6\u5c06\u63a8\u52a8\u53ef\u9760/\u8d1f\u8d23\u4efb/\u4e2a\u6027\u5316LLM\u51b3\u7b56\u7cfb\u7edf\u7684\u7814\u7a76\u53d1\u5c55"}}
{"id": "2507.10542", "pdf": "https://arxiv.org/pdf/2507.10542", "abs": "https://arxiv.org/abs/2507.10542", "authors": ["Shivangi Aneja", "Sebastian Weiss", "Irene Baeza", "Prashanth Chandran", "Gaspard Zoss", "Matthias Nie\u00dfner", "Derek Bradley"], "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project\n  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/", "summary": "Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5c40\u90e8\u8868\u60c5\u533a\u5757\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u5b9e\u73b0\u8d85\u9ad8\u6e05\u5b9e\u65f63D\u5934\u50cf\u751f\u6210", "motivation": "\u89e3\u51b3\u7279\u5199\u955c\u5934\u4e0b\u76ae\u80a4\u8936\u76b1\u7b49\u5fae\u8868\u60c5\u7ec6\u8282\u7684\u6e32\u67d3\u96be\u9898\uff0c\u7a81\u7834\u5168\u5c40\u8868\u60c5\u7a7a\u95f4\u7684\u5c40\u9650\u6027", "method": "1. \u57fa\u4e8e\u533a\u5757\u7684\u51e0\u4f55\u6a21\u578b\u63d0\u53d6\u5c40\u90e8\u8868\u60c5\u7279\u5f81\n2. \u5c06\u533a\u5757\u4e0eScaffold-GS\u951a\u70b9\u8026\u5408\u52a8\u6001\u751f\u6210\u76ae\u80a4\n3. \u989c\u8272\u5bc6\u96c6\u5316\u4e0e\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u4f18\u5316", "result": "\u57283K\u5206\u8fa8\u7387\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u6e32\u67d3\uff0c\u652f\u6301\u591a\u6837\u5316\u8868\u60c5\u4e14\u8fd0\u52a8\u81ea\u7136\uff0c\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "\u5c40\u90e8\u8868\u60c5\u5efa\u6a21\u663e\u8457\u63d0\u5347\u5934\u50cf\u771f\u5b9e\u611f\uff0c\u4e3a\u6c89\u6d78\u5f0f\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.09075", "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "\u63d0\u51faOpenCodeReasoning-II\u6570\u636e\u96c6\uff08\u542b250\u4e07\u95ee\u7b54-\u4ee3\u7801-\u8bc4\u4f30\u4e09\u5143\u7ec4\uff09\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4e0e\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8fbe\u5230/\u8d85\u8d8a\u6700\u4f73\u5f00\u6e90\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u6269\u5c55\u4e86C++\u8bc4\u6d4b\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u63a8\u7406\u9886\u57df\u53d1\u5c55\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u9700\u6784\u5efa\u66f4\u5168\u9762\u7684\u6570\u636e\u652f\u6301\u4ee3\u7801\u751f\u6210\u4e0e\u8bc4\u4f30\u4efb\u52a1\u3002", "method": "\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\uff1a1) \u4ee3\u7801\u751f\u6210\u4e13\u9879\u5fae\u8c03 2) \u4ee3\u7801\u751f\u6210\u4e0e\u8bc4\u4f30\u8054\u5408\u8bad\u7ec3\u3002\u6269\u5c55LiveCodeBench\u652f\u6301C++\u8bed\u8a00\u8bc4\u6d4b\u3002", "result": "\u5fae\u8c03\u540e\u7684Qwen2.5-Instruct\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u6a21\u578b\uff0c\u96c6\u6210\u751f\u6210\u4e0e\u8bc4\u4f30\u6a21\u578b\u663e\u8457\u63d0\u5347\u7ade\u6280\u7f16\u7a0b\u6027\u80fd\u3002C++\u57fa\u51c6\u6269\u5c55\u63d0\u5347LLM\u8bc4\u4f30\u5168\u9762\u6027\u3002", "conclusion": "\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0e\u8054\u5408\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u63d0\u5347\u4ee3\u7801\u63a8\u7406\u80fd\u529b\uff0c\u591a\u8bed\u8a00\u57fa\u51c6\u6269\u5c55\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u66f4\u5168\u9762\u652f\u6301\u3002"}}
{"id": "2507.08805", "pdf": "https://arxiv.org/pdf/2507.08805", "abs": "https://arxiv.org/abs/2507.08805", "authors": ["Mike Kentros", "Manos Kamarianakis", "Michael Cole", "Vitaliy Popov", "Antonis Protopsaltis", "George Papagiannakis"], "title": "Non-linear, Team-based VR Training for Cardiac Arrest Care with enhanced CRM Toolkit", "categories": ["cs.HC", "cs.CY", "cs.GR"], "comment": "4 pages, 3 figures, 1 table", "summary": "This paper introduces iREACT, a novel VR simulation addressing key\nlimitations in traditional cardiac arrest (CA) training. Conventional methods\nstruggle to replicate the dynamic nature of real CA events, hindering Crew\nResource Management (CRM) skill development. iREACT provides a non-linear,\ncollaborative environment where teams respond to changing patient states,\nmirroring real CA complexities. By capturing multi-modal data (user actions,\ncognitive load, visual gaze) and offering real-time and post-session feedback,\niREACT enhances CRM assessment beyond traditional methods. A formative\nevaluation with medical experts underscores its usability and educational\nvalue, with potential applications in other high-stakes training scenarios to\nimprove teamwork, communication, and decision-making.", "AI": {"tldr": "iREACT\u5f00\u53d1\u4e86VR\u5fc3\u810f\u9aa4\u505c\u57f9\u8bad\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u60a3\u8005\u72b6\u6001\u6a21\u62df\u548c\u591a\u6a21\u6001\u6570\u636e\u53cd\u9988\u673a\u5236\uff0c\u7a81\u7834\u4f20\u7edf\u57f9\u8bad\u6a21\u5f0f\u9650\u5236\uff0c\u63d0\u5347\u56e2\u961f\u8d44\u6e90\u7ba1\u7406\u80fd\u529b\u8bc4\u4f30\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u5fc3\u810f\u9aa4\u505c\u57f9\u8bad\u65b9\u6cd5\u96be\u4ee5\u590d\u73b0\u771f\u5b9e\u573a\u666f\u7684\u52a8\u6001\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u56e2\u961f\u534f\u4f5c\u3001\u6c9f\u901a\u548c\u51b3\u7b56\u80fd\u529b\u57f9\u517b\u5b58\u5728\u74f6\u9888\uff0c\u4e9f\u9700\u901a\u8fc7\u6280\u672f\u521b\u65b0\u6539\u5584CRM\u6280\u80fd\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u521b\u5efa\u975e\u7ebf\u6027\u534f\u4f5c\u5f0fVR\u73af\u5883\uff0c\u5b9e\u65f6\u6355\u6349\u7528\u6237\u884c\u4e3a\u3001\u8ba4\u77e5\u8d1f\u8377\u4e0e\u89c6\u89c9\u7126\u70b9\u6570\u636e\uff0c\u7ed3\u5408\u5b9e\u65f6/\u56de\u987e\u53cc\u6a21\u5f0f\u53cd\u9988\u673a\u5236\uff0c\u6784\u5efa\u52a8\u6001CA\u4e8b\u4ef6\u54cd\u5e94\u8bad\u7ec3\u573a\u666f\u3002", "result": "\u533b\u5b66\u4e13\u5bb6\u9a8c\u8bc1\u8868\u660e\u8be5\u7cfb\u7edf\u5177\u5907\u9ad8\u53ef\u7528\u6027\u4e0e\u6559\u5b66\u4ef7\u503c\uff0c\u5728CRM\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u57f9\u8bad\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u9ad8\u98ce\u9669\u9886\u57df\u57f9\u8bad\uff0c\u901a\u8fc7\u5f3a\u5316\u52a8\u6001\u56e2\u961f\u534f\u4f5c\u4e0e\u81ea\u9002\u5e94\u51b3\u7b56\u8bad\u7ec3\uff0c\u63d0\u5347\u5371\u673a\u573a\u666f\u4e0b\u7684\u6574\u4f53\u5e94\u5bf9\u6548\u80fd\u3002"}}
{"id": "2507.09076", "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\u673a\u5236(DPM)\u89e3\u51b3\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u97f3\u9891\u7684\u60c5\u611f\u8bc6\u522b\u9650\u5236", "motivation": "\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b(SLLM)\u56e0\u9ad8\u5e27\u7387\u7279\u5f81\u5bfc\u81f4\u5904\u7406\u957f\u97f3\u9891\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u591a\u8f6e\u5bf9\u8bdd\u7684\u60c5\u611f\u8fde\u7eed\u6027", "method": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u8bed\u4e49\u548c\u53e5\u5b50\u7ea7\u60c5\u611f\u7f16\u7801\u7684\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\u673a\u5236\uff0c\u5728\u63a8\u7406\u65f6\u9010\u6b65\u5c06\u4fe1\u606f\u5b58\u50a8\u5230\u4e34\u65f6LoRA\u6a21\u5757", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\uff0c\u663e\u8457\u63d0\u5347\u957f\u97f3\u9891\u5e8f\u5217\u60c5\u611f\u8bc6\u522b\u80fd\u529b", "conclusion": "DPM\u673a\u5236\u6709\u6548\u7a81\u7834SLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\uff0c\u4e3a\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.08884", "pdf": "https://arxiv.org/pdf/2507.08884", "abs": "https://arxiv.org/abs/2507.08884", "authors": ["Jordan Riley Benson", "David Crist", "Phil Lafleur", "Benjamin Watson"], "title": "Agent-based visualization of streaming text", "categories": ["cs.MA", "cs.GR"], "comment": null, "summary": "We present a visualization infrastructure that maps data elements to agents,\nwhich have behaviors parameterized by those elements. Dynamic visualizations\nemerge as the agents change position, alter appearance and respond to one\nother. Agents move to minimize the difference between displayed agent-to-agent\ndistances, and an input matrix of ideal distances. Our current application is\nvisualization of streaming text. Each agent represents a significant word,\nvisualizing it by displaying the word itself, centered in a circle sized by the\nfrequency of word occurrence. We derive the ideal distance matrix from word\ncooccurrence, mapping higher co-occurrence to lower distance. To depict\nco-occurrence in its textual context, the ratio of intersection to circle area\napproximates the ratio of word co-occurrence to frequency. A networked backend\nprocess gathers articles from news feeds, blogs, Digg or Twitter, exploiting\nonline search APIs to focus on user-chosen topics. Resulting visuals reveal the\nprimary topics in text streams as clusters, with agent-based layout moving\nwithout instability as data streams change dynamically.", "AI": {"tldr": "\u52a8\u6001\u4ee3\u7406\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bcd\u5171\u73b0\u77e9\u9635\u5b9e\u65f6\u805a\u7c7b\u6587\u672c\u6d41\u4e2d\u7684\u4e3b\u9898", "motivation": "\u4f20\u7edf\u9759\u6001\u53ef\u89c6\u5316\u96be\u4ee5\u5904\u7406\u52a8\u6001\u6587\u672c\u6d41\u6570\u636e\uff0c\u9700\u5f00\u53d1\u80fd\u5b9e\u65f6\u53cd\u6620\u6570\u636e\u53d8\u5316\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\u3002\u901a\u8fc7\u4ee3\u7406\u81ea\u9002\u5e94\u8c03\u6574\u673a\u5236\uff0c\u53ef\u76f4\u89c2\u5c55\u793a\u6587\u672c\u4e3b\u9898\u7684\u6f14\u5316\u8fc7\u7a0b\u3002", "method": "1. \u5c06\u6570\u636e\u5143\u7d20\u6620\u5c04\u4e3a\u53ef\u8c03\u6574\u4f4d\u7f6e/\u5916\u89c2\u7684\u4ee3\u7406\n2. \u57fa\u4e8e\u8bcd\u5171\u73b0\u6784\u5efa\u7406\u60f3\u8ddd\u79bb\u77e9\u9635\uff08\u9ad8\u9891\u5171\u73b0\u8bcd\u8ddd\u79bb\u8fd1\uff09\n3. \u540e\u7aef\u7cfb\u7edf\u805a\u5408\u591a\u6e90\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u5728\u7ebfAPI\u5b9e\u73b0\u4e3b\u9898\u805a\u7126\n4. \u4ee3\u7406\u6301\u7eed\u4f18\u5316\u5b9e\u9645\u4f4d\u7f6e\u4e0e\u7406\u60f3\u77e9\u9635\u7684\u5339\u914d\u5ea6", "result": "\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\uff1a\n- \u6587\u672c\u6d41\u4e3b\u9898\u7684\u5b9e\u65f6\u805a\u7c7b\u53ef\u89c6\u5316\n- \u52a8\u6001\u6570\u636e\u66f4\u65b0\u65f6\u5e03\u5c40\u7a33\u5b9a\n- \u8bcd\u9891\u901a\u8fc7\u5706\u5708\u5927\u5c0f\u3001\u5171\u73b0\u7387\u901a\u8fc7\u533a\u57df\u4ea4\u53e0\u6bd4\u4f8b\u53ef\u89c6\u5316", "conclusion": "\u8be5\u4ee3\u7406\u9a71\u52a8\u6a21\u578b\u4e3a\u52a8\u6001\u6587\u672c\u6d41\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u53ef\u89c6\u5316\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5e03\u5c40\u673a\u5236\u5e73\u8861\u4e86\u52a8\u6001\u66f4\u65b0\u4e0e\u89c6\u89c9\u7a33\u5b9a\u6027\uff0c\u5728\u8206\u60c5\u5206\u6790\u3001\u793e\u4ea4\u76d1\u63a7\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09104", "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "\u63d0\u51faCompassJudger-2\u901a\u7528\u8bc4\u5224\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u9886\u57df\u6570\u636e\u7b56\u7565\u548c\u5956\u52b1\u76d1\u7763\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u4e0e\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c7B\u5c0f\u6a21\u578b\u6027\u80fd\u6bd4\u80a9\u767e\u4ebf\u7ea7\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u5224\u6a21\u578b\u5b58\u5728\u4e13\u4e1a\u9886\u57df\u72ed\u7a84\u3001\u9c81\u68d2\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u4e0d\u591f\u5168\u9762\u53ef\u9760\u3002\u9700\u8981\u5f00\u53d1\u901a\u7528\u5316\u8bc4\u5224\u6846\u67b6\u7a81\u7834\u6280\u672f\u74f6\u9888\u3002", "method": "1. \u4efb\u52a1\u9a71\u52a8\u7684\u591a\u9886\u57df\u6570\u636e\u7b5b\u9009\u7b56\u7565 2. \u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u76d1\u7763\u673a\u5236 3. \u62d2\u7edd\u91c7\u6837\u5f15\u5bfc\u5185\u5728\u63a8\u7406 4. \u8fb9\u9645\u7b56\u7565\u68af\u5ea6\u635f\u5931\u51fd\u6570\u4f18\u5316", "result": "\u5728JudgerBenchV2\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u9886\u5148\uff0c7B\u6a21\u578b\u8fbe\u5230DeepSeek-V3(235B)\u768495%\u5224\u65ad\u51c6\u786e\u7387\uff0c\u63a8\u7406\u6548\u7387\u63d0\u534733\u500d\u3002", "conclusion": "\u5efa\u7acb\u4e86LLM\u8bc4\u5224\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u548c\u5c0f\u6a21\u578b\u9ad8\u6548\u90e8\u7f72\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u667a\u80fd\u8bc4\u4f30\u4f53\u7cfb\u53d1\u5c55\uff0c\u914d\u5957\u53d1\u5e03\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6JudgerBenchV2\u3002"}}
{"id": "2507.09155", "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "OPENXRD\u662f\u4e00\u4e2a\u5f00\u5377\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408GPT-4.5\u751f\u6210\u7684\u9886\u57df\u53c2\u8003\u8d44\u6599\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5c0f\u6a21\u578b\u5728X\u5c04\u7ebf\u884d\u5c04\u6676\u4f53\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5f00\u5377\u7cfb\u7edf\u4f9d\u8d56\u626b\u63cf\u6559\u6750\u5f15\u53d1\u7684\u7248\u6743\u95ee\u9898\uff0c\u540c\u65f6\u586b\u8865\u5c0f\u6a21\u578b\u5728\u6676\u4f53\u5b66\u4e13\u4e1a\u77e5\u8bc6\u4e0a\u7684\u4e0d\u8db3\uff0c\u4f7f\u5176\u80fd\u501f\u52a9AI\u751f\u6210\u5185\u5bb9\u63d0\u5347\u79d1\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5229\u7528GPT-4.5\u751f\u6210\u7d27\u51d1\u53c2\u8003\u8d44\u6599\uff0c\u5728217\u4e2a\u4e13\u5bb6\u7ea7XRD\u95ee\u9898\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u6a21\u578b\uff08\u5305\u62ecGPT-4/Mistral/LLaMA/QWEN\uff09\u5728\u95ed\u5377/\u5f00\u5377\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u4f7f\u7528GPT-4.5\u6458\u8981\u7684\u6a21\u578b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u5c24\u5176\u5728\u6676\u4f53\u5b66\u8bad\u7ec3\u6709\u9650\u7684\u5c0f\u6a21\u578b\u4e2d\u6539\u8fdb\u6700\u660e\u663e\uff0c\u9a8c\u8bc1AI\u751f\u6210\u5185\u5bb9\u7684\u77e5\u8bc6\u4f20\u9012\u6709\u6548\u6027\u3002", "conclusion": "OPENXRD\u8bc1\u660e\u4e13\u7528\u5f00\u5377\u7cfb\u7edf\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u4ef7\u503c\uff0c\u4e3a\u79d1\u5b66\u9886\u57dfNLP\u5de5\u5177\u5f00\u53d1\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u6574\u5408\u56fe\u8868\u589e\u5f3a\u4e13\u4e1a\u573a\u666f\u89e3\u91ca\u529b\u3002"}}
{"id": "2507.09157", "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "\u63d0\u51faPU-Lie\u6a21\u578b\uff0c\u7ed3\u5408\u51bb\u7ed3BERT\u5d4c\u5165\u4e0ePU\u5b66\u4e60\uff0c\u5728\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f970.60\u5b8fF1\u503c\u4e14\u51cf\u5c11650\u500d\u53c2\u6570\u91cf", "motivation": "\u6218\u7565\u5bf9\u8bdd\u4e2d\u6b3a\u9a97\u68c0\u6d4b\u5b58\u5728\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\uff08<5%\u6b3a\u9a97\u6837\u672c\uff09\u4e14\u4f20\u7edf\u4e8c\u5206\u7c7b\u65b9\u6cd5\u4e0d\u9002\u5e94\u5b9e\u9645\u9700\u6c42", "method": "\u6574\u5408\u51bb\u7ed3BERT\u5d4c\u5165\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u8a00/\u6e38\u620f\u7279\u5f81\uff0c\u91c7\u7528\u9488\u5bf9\u672a\u6807\u6ce8\u6570\u636e\u7684PU\u5b66\u4e60\u76ee\u6807", "result": "\u5728\u4e03\u4e2a\u6a21\u578b\u5bf9\u6bd4\u4e2d\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff08\u5b8fF1 0.60\uff09\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1PU\u5b66\u4e60\u4e0e\u8bf4\u8bdd\u8005\u8868\u5f81\u7684\u6709\u6548\u6027", "conclusion": "\u5f3a\u8c03\u5728\u7a00\u6709\u4f46\u5173\u952e\u7684\u6b3a\u9a97\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0cPU\u5b66\u4e60\u80fd\u6709\u6548\u5efa\u6a21\u6b3a\u9a97\u7c7b\u522b\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2507.09174", "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "categories": ["cs.CL"], "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u6846\u67b6RAMA\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u67e5\u8be2\u91cd\u6784\u3001\u8de8\u6e90\u8bc1\u636e\u805a\u5408\u548c\u591a\u6a21\u578b\u534f\u540c\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u591a\u5a92\u4f53\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6548\u679c", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5728\u9a8c\u8bc1\u6a21\u7cca/\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u878d\u5408\u7f51\u7edc\u8bc1\u636e\u548c\u591a\u6a21\u578b\u534f\u4f5c\u7684\u89e3\u51b3\u65b9\u6848", "method": "1. \u591a\u6a21\u6001\u58f0\u660e\u8f6c\u7cbe\u51c6\u641c\u7d22\u67e5\u8be2 2. \u8de8\u6743\u5a01\u6e90\u8bc1\u636e\u805a\u5408 3. \u591a\u667a\u80fd\u4f53\u67b6\u6784\u96c6\u6210\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u52bf", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u6a21\u7cca/\u4f4e\u6982\u7387\u58f0\u660e\u7684\u9a8c\u8bc1\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347", "conclusion": "\u7f51\u7edc\u8bc1\u636e\u96c6\u6210\u4e0e\u591a\u667a\u80fd\u4f53\u534f\u540c\u662f\u53ef\u4fe1\u591a\u5a92\u4f53\u9a8c\u8bc1\u7684\u5173\u952e\uff0c\u5f00\u6e90\u6846\u67b6\u4e3a\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u65b0\u65b9\u5411\uff08GitHub\u4ed3\u5e93\uff1ahttps://github.com/kalendsyang/RAMA.git\uff09"}}
{"id": "2507.09185", "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u5143\u526a\u679d\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u6570\u636e\u96c6\u7279\u5b9a\u673a\u5236\u63d0\u5347LLM\u6cdb\u5316\u80fd\u529b", "motivation": "\u4f20\u7edfLLM\u5728\u7279\u5b9a\u6570\u636e\u96c6\u8bad\u7ec3\u4e2d\u5f62\u6210\u9886\u57df\u4f9d\u8d56\u673a\u5236\uff0c\u5bfc\u81f4\u9762\u5bf9\u65b0\u4efb\u52a1\u65f6\u6cdb\u5316\u80fd\u529b\u4e0b\u964d", "method": "\u4f7f\u7528\u96c6\u6210\u68af\u5ea6\u8bc6\u522b\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u6709\u8fc7\u5ea6\u8d21\u732e\u7684\u795e\u7ecf\u5143\uff0c\u8fdb\u884c\u9009\u62e9\u6027\u526a\u679d", "result": "\u5728\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u975e\u526a\u679d\u9002\u5e94\u65b9\u6cd5", "conclusion": "\u795e\u7ecf\u5143\u526a\u679d\u80fd\u6709\u6548\u4fc3\u8fdb\u6a21\u578b\u4f9d\u8d56\u901a\u7528\u8868\u793a\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7279\u5b9a\u673a\u5236\u7684\u53ef\u5206\u79bb\u6027"}}
{"id": "2507.09205", "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "\u6784\u5efa\u4e86\u6700\u5927\u85cf\u8bed\u9884\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5e76\u8bad\u7ec3\u51faBanzhida\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u85cf\u8bed\u751f\u6210\u80fd\u529b", "motivation": "\u85cf\u8bed\u4f5c\u4e3a\u5178\u578b\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u56e0\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u8bed\u6599\u5728\u73b0\u6709\u6a21\u578b\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3", "method": "1.\u6574\u5408\u591a\u6e90\u6570\u636e\u5e76\u8bbe\u8ba1\u85cf\u8bed\u4e13\u7528\u6e05\u6d17\u6d41\u7a0b 2.\u901a\u8fc7\u9884\u8bad\u7ec3/\u540e\u8bad\u7ec3\u4f18\u5316\u591a\u8bed\u8a00\u57fa\u7840\u6a21\u578b 3.\u521b\u5efa\u65b0\u8bc4\u4f30\u57fa\u51c6\u7ed3\u5408\u73b0\u6709\u57fa\u51c6", "result": "Banzhida\u5728\u5404\u7c7b\u4efb\u52a1\u4e2d\u6301\u7eed\u663e\u8457\u4f18\u4e8e\u540c\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u53ca\u85cf\u8bed\u4e13\u7528\u6a21\u578b", "conclusion": "\u9ad8\u8d28\u91cf\u8bed\u6599\u5e93\u6784\u5efa\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u5efa\u6a21\u7684\u5173\u952e\uff0cBanzhida\u4e3a\u85cf\u8bedAI\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.09225", "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "categories": ["cs.CL", "cs.CY"], "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "\u89c6\u89c9\u9690\u55bb\u5728\u6c14\u5019\u53d8\u5316\u4f20\u64ad\u4e2d\u5b58\u5728\u8ba4\u77e5\u8d1f\u8377\u4e0e\u79ef\u6781\u6548\u679c\u7684\u53cc\u91cd\u6027\uff1aMetaClimage\u6570\u636e\u5e93\u5206\u6790\u663e\u793a\u9690\u55bb\u56fe\u50cf\u66f4\u96be\u7406\u89e3\u4f46\u66f4\u5177\u7f8e\u611f\uff0c\u867d\u672a\u63d0\u5347\u6548\u80fd\u5374\u80fd\u5f15\u53d1\u66f4\u79ef\u6781\u7684\u8ba4\u77e5\u52a0\u5de5", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u89c6\u89c9\u9690\u55bb\u7d20\u6750\u5e93\uff0c\u9700\u91cf\u5316\u8bc4\u4f30\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u5bf9\u4f20\u64ad\u6548\u679c\u7684\u5f71\u54cd\u53ca\u5176\u4f5c\u7528\u673a\u5236", "method": "\u6784\u5efa\u5305\u542b\u9690\u55bb/\u5b57\u9762\u56fe\u50cf\u7684MetaClimage\u6570\u636e\u5e93\uff08https://doi.org/10.5281/zenodo.15861012\uff09\uff0c\u6536\u96c6\u4eba\u7c7b\u5bf9\u56fe\u50cf\u96be\u5ea6\u3001\u6548\u80fd\u3001\u827a\u672f\u8d28\u91cf\u3001\u60c5\u7eea\u5524\u9192\u5ea6\u7684\u8bc4\u5206\uff0c\u901a\u8fc7\u6807\u7b7e\u751f\u6210\u548cNLP\u63d0\u53d6\u8bed\u4e49\u53ca\u60c5\u611f\u7279\u5f81", "result": "\u9690\u55bb\u56fe\u50cf\u7406\u89e3\u96be\u5ea6\u2191/\u827a\u672f\u8bc4\u5206\u2191\uff0c\u6548\u80fd/\u60c5\u7eea\u5524\u9192\u5ea6\u65e0\u5dee\u5f02\uff08\u9ad8\u8ba4\u77e5\u9700\u6c42\u8005\u60c5\u7eea\u5524\u9192\u2191\uff09\uff1b\u9690\u55bb\u6807\u7b7e\u6570\u91cf\u2191\u4e14\u591a\u6d89\u53ca\u56fe\u50cf\u5916\u5b9e\u4f53\uff0c\u60c5\u611f\u6548\u4ef7/\u652f\u914d\u5ea6\u2191", "conclusion": "\u89c6\u89c9\u9690\u55bb\u901a\u8fc7\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\u4fc3\u8fdb\u6df1\u5ea6\u52a0\u5de5\uff0c\u867d\u4e0d\u76f4\u63a5\u63d0\u5347\u4f20\u64ad\u6548\u80fd\uff0c\u4f46\u80fd\u589e\u5f3a\u7f8e\u5b66\u4f53\u9a8c\u4e0e\u79ef\u6781\u8ba4\u77e5\uff0c\u9700\u5728\u73af\u5883\u4f20\u64ad\u4e2d\u6743\u8861\u6210\u672c\u6548\u76ca"}}
{"id": "2507.09245", "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "categories": ["cs.CL"], "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\u6574\u5408\u4e862020-2025\u5e74\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u4e0e\u50e7\u4f3d\u7f57\u8bed\u4e92\u8f6c\u7684\u6570\u636e\u8d44\u6e90\u53ca\u7b97\u6cd5\uff0c\u63a8\u52a8\u8be5\u9886\u57dfNLP\u7814\u7a76\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u50e7\u4f3d\u7f57\u8bedNLP\u7814\u7a76\u4e2d\u7f57\u9a6c\u5316\u6587\u672c\u5904\u7406\u5de5\u5177\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u652f\u6301\u97f3\u8bd1\u6a21\u578b\u8bad\u7ec3\u53ca\u76f8\u5173\u5e94\u7528\u5f00\u53d1\u3002", "method": "\u7cfb\u7edf\u6536\u96c6\u5e76\u5f00\u653e\u73b0\u6709\u6570\u636e\u96c6\u548c\u5de5\u5177\u8d44\u6e90\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8d44\u6e90\u4e2d\u5fc3\u3002", "result": "\u6784\u5efa\u516c\u5f00\u53ef\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u4e0e\u5de5\u5177\u5e73\u53f0\uff0c\u5b8c\u6210\u9886\u57df\u5185\u73b0\u6709\u97f3\u8bd1\u5e94\u7528\u7684\u5bf9\u6bd4\u5206\u6790\u3002", "conclusion": "\u8be5\u8d44\u6e90\u4e2d\u5fc3\u663e\u8457\u4fc3\u8fdb\u50e7\u4f3d\u7f57\u8bedNLP\u7814\u7a76\u751f\u6001\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u548c\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u3002"}}
{"id": "2507.09259", "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "categories": ["cs.CL"], "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "\u63d0\u51fa\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u6a21\u62df\u4eba\u7c7b\u601d\u7ef4\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5e7d\u9ed8\u7406\u8bba\u63d0\u5347LLMs\u7684\u5e7d\u9ed8\u7ffb\u8bd1\u8d28\u91cf", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e7d\u9ed8\u7ffb\u8bd1\u4e2d\u5b58\u5728\u8bed\u8a00\u5e72\u6270\u548c\u5e7d\u9ed8\u5143\u7d20\u7f3a\u5931\u95ee\u9898\uff0c\u9700\u8981\u589e\u5f3a\u7ffb\u8bd1\u6587\u672c\u7684\u53ef\u8bfb\u6027\u548c\u5e7d\u9ed8\u6027", "method": "1. \u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\u7ed3\u5408\u601d\u7ef4\u94fe\u6280\u672f\n2. \u6574\u5408\u7ecf\u5178\u5e7d\u9ed8\u7406\u8bba\u6846\u67b6\n3. \u81ea\u52a8\u8bc4\u4f30\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027", "result": "\u5e7d\u9ed8\u6027\u63d0\u53477.75%\uff0c\u6d41\u7545\u6027\u63d0\u53472.81%\uff0c\u8fde\u8d2f\u6027\u63d0\u53476.13%\uff08\u5f00\u6e90\u5e7d\u9ed8\u6570\u636e\u96c6\u6d4b\u8bd5\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u5316\u4e86\u5e7d\u9ed8\u7ffb\u8bd1\u8d28\u91cf\uff0c\u4e3a\u8de8\u6587\u5316\u4ea4\u9645\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.09282", "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "\u63d0\u51faClaritySpeech\u6846\u67b6\uff0c\u7ed3\u5408ASR\u3001\u6587\u672c\u6df7\u6dc6\u548c\u96f6\u6837\u672cTTS\u6280\u672f\uff0c\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u6539\u5584\u75f4\u5446\u75c7\u60a3\u8005\u8bed\u97f3\u6e05\u6670\u5ea6\u5e76\u4fdd\u62a4\u9690\u79c1", "motivation": "\u5f53\u524d\u8bed\u97f3\u6280\u672f\u96be\u4ee5\u5904\u7406\u75f4\u5446\u75c7\u60a3\u8005\u7684\u975e\u5178\u578b\u8bed\u97f3\uff0c\u4e14\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u5f00\u53d1\u517c\u987e\u53ef\u8bbf\u95ee\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u7cfb\u7edf", "method": "\u901a\u8fc7\u96c6\u6210\u81ea\u52a8\u8bed\u97f3\u8f6c\u5f55(ASR)\u3001\u6587\u672c\u6df7\u6dc6\u7b97\u6cd5\u548c\u96f6\u6837\u672c\u6587\u672c\u5230\u8bed\u97f3(TTS)\u6280\u672f\uff0c\u6784\u5efa\u65e0\u9700\u5fae\u8c03\u7684\u4e09\u9636\u6bb5\u5904\u7406\u6846\u67b6", "result": "ADReSS/ADReSSo\u6570\u636e\u96c6\u663e\u793a\uff1aF1\u5206\u6570\u5e73\u5747\u4e0b\u964d16%/10%\uff0cWER\u4ece0.73\u964d\u81f30.08/0.15\uff0c\u8bed\u97f3\u8d28\u91cf\u4ece1.65\u63d0\u5347\u81f32.15\uff0c\u4fdd\u630150%\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u63d0\u5347\u8bed\u97f3\u53ef\u7406\u89e3\u6027\uff08WER\u6539\u558485%\uff09\u7684\u540c\u65f6\u6709\u6548\u4fdd\u62a4\u9690\u79c1\uff08F1\u8bc6\u522b\u7387\u663e\u8457\u964d\u4f4e\uff09\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u6c9f\u901a\u8f85\u52a9\u7684\u53cc\u91cd\u7a81\u7834"}}
{"id": "2507.09424", "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "DATE-LM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u901a\u8fc7\u771f\u5b9eLLM\u5e94\u7528\u8bc4\u4f30\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u4e14\u4e0e\u7b80\u5355\u57fa\u7ebf\u5b58\u5728\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5728\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u4e9f\u9700\u6784\u5efa\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u652f\u6491\u6570\u636e\u7b5b\u9009\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7b49\u5173\u952e\u5e94\u7528\u573a\u666f\u3002", "method": "\u63d0\u51faDATE-LM\u57fa\u51c6\u6846\u67b6\uff0c\u5305\u542b\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u3001\u6bd2\u6027/\u504f\u89c1\u8fc7\u6ee4\u3001\u4e8b\u5b9e\u5f52\u56e0\u4e09\u5927\u6838\u5fc3\u4efb\u52a1\uff0c\u652f\u6301\u8de8\u4efb\u52a1\u548c\u6a21\u578b\u67b6\u6784\u7684\u5927\u89c4\u6a21\u53ef\u914d\u7f6e\u8bc4\u4f30\u3002", "result": "\u5927\u89c4\u6a21\u8bc4\u4f30\u663e\u793a\uff1a1\uff09\u65e0\u5355\u4e00\u65b9\u6cd5\u5168\u4f18 2\uff09\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\u5b58\u5728\u6027\u4ef7\u6bd4\u6743\u8861 3\uff09\u65b9\u6cd5\u6027\u80fd\u5bf9\u4efb\u52a1\u8bbe\u8ba1\u654f\u611f", "conclusion": "DATE-LM\u4e3aLLM\u6570\u636e\u5f52\u56e0\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u901a\u8fc7\u516c\u5f00\u6392\u884c\u699c\u4fc3\u8fdb\u793e\u533a\u534f\u4f5c\uff0c\u63a8\u52a8\u5f52\u56e0\u65b9\u6cd5\u7684\u8fed\u4ee3\u4f18\u5316\u3002"}}
{"id": "2507.09470", "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "categories": ["cs.CL", "cs.AI", "68T07"], "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u5e8f\u5217\u957f\u5ea6\u3001\u5b66\u4e60\u7387\u53ca\u533b\u5b66\u672f\u8bed\u6574\u5408\uff0cDRAGON Longformer\u6a21\u578b\u5728\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u4e2d\u5404\u9879\u6307\u6807\u63d0\u534713%\u4ee5\u4e0a", "motivation": "\u4f18\u5316\u73b0\u6709\u6a21\u578b\u4ee5\u63d0\u5347\u533b\u7597\u6848\u4f8b\u4e8c\u5143\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u9488\u5bf9\u533b\u5b66\u672f\u8bed\u7406\u89e3\u4e0e\u4e34\u5e8a\u89c2\u5bdf\u89e3\u8bfb\u80fd\u529b", "method": "\u91c7\u7528\u8d85\u53c2\u6570\u8c03\u4f18\uff08\u5e8f\u5217\u957f\u5ea61024\u3001\u5b66\u4e60\u73875e-06\u3001\u8bad\u7ec3\u5468\u671f8\uff09+\u533b\u7597\u9886\u57df\u9884\u5904\u7406+\u67b6\u6784\u8c03\u6574", "result": "\u51c6\u786e\u738785.2%\uff08+13.2%\uff09\u3001\u7cbe\u786e\u738784.1%\uff08+16.1%\uff09\u3001\u53ec\u56de\u738786.3%\uff08+11.3%\uff09\u3001F1\u503c85.2%\uff08+14.2%\uff09\uff0c\u7edf\u8ba1\u663e\u8457\uff08p<.001\uff09", "conclusion": "\u8be5\u4f18\u5316\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e34\u5e8aNLP\u6027\u80fd\uff0c\u8bc1\u5b9e\u9886\u57df\u81ea\u9002\u5e94\u7b56\u7565\u4ef7\u503c\uff0c\u4e3a\u533b\u7597\u6587\u672c\u5206\u6790\u63d0\u4f9b\u53ef\u9760\u6280\u672f\u65b9\u6848"}}
{"id": "2507.09474", "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "categories": ["cs.CL"], "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "\u672c\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86CoNLL-2013\u8bed\u6cd5\u7ea0\u9519\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u62ec\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u6784\u5efa\u3001\u8bc4\u4f30\u6307\u6807\u8bbe\u8ba1\u3001\u53c2\u8d5b\u56e2\u961f\u65b9\u6cd5\u7efc\u8ff0\u53ca\u6700\u7ec8\u7ed3\u679c\u5c55\u793a\u3002", "motivation": "\u901a\u8fc7\u7ec4\u7ec7\u5171\u4eab\u4efb\u52a1\u63a8\u52a8\u8bed\u6cd5\u7ea0\u9519\u6280\u672f\u53d1\u5c55\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u4ee5\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u8bbe\u8ba1\u4efb\u52a1\u89c4\u8303\uff0c\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5236\u5b9a\u57fa\u4e8eM\u00b2 scorer\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u7cfb\u7edf\u5206\u6790\u5404\u53c2\u8d5b\u56e2\u961f\u7684\u6a21\u578b\u67b6\u6784\u4e0e\u6280\u672f\u8def\u7ebf\u3002", "result": "\u5171\u4eab\u4efb\u52a1\u6c47\u96c6\u4e86\u591a\u79cd\u8bed\u6cd5\u7ea0\u9519\u65b9\u6848\uff0c\u901a\u8fc7\u7edf\u4e00\u8bc4\u4f30\u63ed\u793a\u4e0d\u540c\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u53ec\u56de\u7387\u7b49\u6307\u6807\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "CoNLL-2013\u4efb\u52a1\u4e3a\u8bed\u6cd5\u7ea0\u9519\u9886\u57df\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4fc3\u8fdb\u4e86\u5b66\u672f\u754c\u4e0e\u5de5\u4e1a\u754c\u7684\u6280\u672f\u8fed\u4ee3\uff0c\u9a8c\u8bc1\u4e86\u534f\u4f5c\u7814\u7a76\u5bf9NLP\u8fdb\u6b65\u7684\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2507.09477", "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "\u7cfb\u7edf\u63a2\u8ba8\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u534f\u540c\u4f5c\u7528\u4e0e\u6574\u5408\u6846\u67b6", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u7eaf\u63a8\u7406\u65b9\u6cd5\u6613\u4ea7\u751f\u4e8b\u5b9e\u9519\u8bef\uff0c\u9700\u534f\u540c\u4f18\u5316\u4e24\u8005", "method": "1. \u63a8\u7406\u589e\u5f3aRAG\u5404\u9636\u6bb5 2. \u591a\u6a21\u6001\u77e5\u8bc6\u8865\u5145\u63a8\u7406\u524d\u63d0 3. \u6784\u5efa\u534f\u540c\u6846\u67b6\u5b9e\u73b0\u68c0\u7d22\u63a8\u7406\u4ea4\u66ff\u8fed\u4ee3", "result": "\u534f\u540c\u6846\u67b6\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5efa\u7acb\u65b9\u6cd5\u8bba\u5206\u7c7b\u4e0e\u6570\u636e\u96c6\u4f53\u7cfb", "conclusion": "\u9700\u53d1\u5c55\u66f4\u6709\u6548\u3001\u591a\u6a21\u6001\u9002\u914d\u3001\u53ef\u4fe1\u7684\u6df1\u5ea6\u534f\u540c\u7cfb\u7edf\uff0c\u63a8\u52a8\u4eba\u672c\u5bfc\u5411\u7684RAG\u63a8\u7406\u7814\u7a76"}}
{"id": "2507.09482", "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u8bbd\u523a\u751f\u6210\u6570\u636e\u96c6M2SaG\u53caViSP\u6846\u67b6\uff0c\u7ed3\u5408PPO\u5f3a\u5316\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u5728\u8bbd\u523a\u751f\u6210\u8d28\u91cf\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\u6a21\u578b\u548cLLM", "motivation": "\u73b0\u6709\u8bbd\u523a\u751f\u6210\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u6a21\u6001\uff0c\u5ffd\u89c6\u89c6\u89c9\u7ebf\u7d22\uff0c\u4e14\u6570\u636e\u96c6\u4e2d\u56fe\u50cf\u5185\u5bb9\u4e0e\u8bbd\u523a\u610f\u56fe\u4e0d\u5339\u914d\u3002\u9700\u8981\u6784\u5efa\u591a\u6a21\u6001\u6570\u636e\u96c6\u5e76\u63d0\u5347\u751f\u6210\u6a21\u578b\u6027\u80fd", "method": "1. \u521b\u5efa\u5305\u542b4970\u4e2a\u6837\u672c\u7684M2SaG\u6570\u636e\u96c6\uff08\u56fe\u50cf+\u8bbd\u523a\u6587\u672c+\u8bbd\u523a\u76ee\u6807\uff09\n2. \u63d0\u51faViSP\u6846\u67b6\uff1aPPO\u5229\u7528DIP\u5956\u52b1\u4f18\u5316\u751f\u6210\uff0c\u5bf9\u6bd4\u5b66\u4e60\u7b5b\u9009\u9ad8\u5956\u52b1\u8f93\u51fa", "result": "ViSP\u5728\u4e94\u9879\u6307\u6807\u4e2d\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u751f\u6210\u6587\u672c\u7684\u8bbd\u523a\u5f97\u5206(0.898 vs 0.770)\u548c\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027(0.768 vs 0.739)\u5747\u9ad8\u4e8e\u539f\u59cb\u6570\u636e\u96c6", "conclusion": "ViSP\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u8bbd\u523a\u5185\u5bb9\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u65b9\u6cd5\u548c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u6709\u6548\u6027\u3002\u516c\u5f00\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76"}}
{"id": "2507.09485", "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "Proposes an LLM-based ABSA approach with reinforcement learning-optimized data augmentation to enhance model performance by generating balanced synthetic training data.", "motivation": "Existing LLM-based ABSA methods face challenges with short text contexts and small/unbalanced datasets (mostly positive labels).", "method": "Uses LLMs to generate augmented training data for balanced label distribution, combined with reinforcement learning to optimize augmentation quality.", "result": "Demonstrates superior performance over baselines on English ABSA benchmarks through experiments.", "conclusion": "The approach effectively addresses data scarcity/imbalance and improves ABSA model performance via optimized LLM-based data augmentation."}}
{"id": "2507.09497", "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "categories": ["cs.CL"], "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "\u63d0\u51faGoalfyMax\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u8bae\u9a71\u52a8\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u89e3\u51b3\u4f20\u7edf\u7cfb\u7edf\u534f\u8c03\u6027\u5dee\u3001\u8bb0\u5fc6\u590d\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5355\u76ee\u6807AI\u7cfb\u7edf\u5728\u534f\u8c03\u6027\u3001\u8bb0\u5fc6\u590d\u7528\u548c\u4efb\u52a1\u5206\u89e3\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u573a\u666f\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u57fa\u4e8eMCP\u534f\u8bae\u6784\u5efaA2A\u5f02\u6b65\u901a\u4fe1\u5c42\uff0c\u91c7\u7528\u5206\u5c42\u8bb0\u5fc6\u67b6\u6784XP\u5b58\u50a8\u4efb\u52a1\u903b\u8f91\u4e0e\u6267\u884c\u8f68\u8ff9\uff0c\u96c6\u6210\u591a\u8f6e\u5bf9\u8bdd\u548c\u52a8\u6001\u5b89\u5168\u9a8c\u8bc1\u6a21\u5757\u3002", "result": "\u5728\u590d\u6742\u4efb\u52a1\u7f16\u6392\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u7684\u9002\u5e94\u6027\u3001\u534f\u8c03\u6027\u548c\u7ecf\u9a8c\u590d\u7528\u80fd\u529b\u3002", "conclusion": "GoalfyMax\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u672a\u6765\u9002\u5e94\u6027\u5f3a\u7684\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2507.09506", "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "\u63d0\u51faRef-Long\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u6587\u6863\u7d22\u5f15\u5f15\u7528\u4efb\u52a1\u4e2d\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u4fe1\u606f\u6eaf\u6e90\u4efb\u52a1\uff08\u9700\u5c06\u5174\u8da3\u9879\u4e0e\u957f\u6587\u672c\u7279\u5b9a\u90e8\u5206\u5173\u8054\uff09\u7684\u7814\u7a76\u5b58\u5728\u7a7a\u767d", "method": "\u6784\u5efa\u5305\u542b\u5408\u6210\u5230\u771f\u5b9e\u573a\u666f\u7684\u4e09\u4e2a\u5b50\u96c6\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5173\u7cfb\u7684\u6587\u6863\u7d22\u5f15\u8bc6\u522b\u4efb\u52a1", "result": "\u6d4b\u8bd513\u4e2a\u6a21\u578b\u663e\u793a\u957f\u4e0a\u4e0b\u6587\u5f15\u7528\u80fd\u529b\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0cGPT-4o\u7b49\u5148\u8fdb\u6a21\u578b\u4e5f\u8868\u73b0\u4e0d\u4f73", "conclusion": "\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u548c\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u6a21\u578b\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdb\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u65b9\u5411"}}
{"id": "2507.09509", "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina H\u00e4mmerl", "Vil\u00e9m Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63d0\u793a\u8d28\u91cf\u663e\u8457\u5f71\u54cdLLM\u7ffb\u8bd1\u6027\u80fd\uff0c\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u5bf9\u7ffb\u8bd1\u6548\u679c\u5f71\u54cd\u5dee\u5f02\u660e\u663e\uff0cLLMs\u5728\u6781\u7aef\u566a\u58f0\u4e0b\u4ecd\u4fdd\u6301\u57fa\u672c\u7ffb\u8bd1\u80fd\u529b\u3002", "motivation": "\u63a2\u7a76\u63d0\u793a\u9519\u8bef\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\u673a\u5236\u53ca\u4e0d\u540c\u566a\u58f0\u7c7b\u578b\u7684\u5dee\u5f02\u6548\u5e94\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u5206\u6790\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4eba\u5de5\u5408\u7406\u9519\u8bef\u4e0e\u5408\u6210\u9519\u8bef\u5bf9\u7ffb\u8bd1\u6027\u80fd\u53ca\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "\u5b57\u7b26\u7ea7\u566a\u58f0\u7834\u574f\u6027\u6700\u5f3a\uff0c\u63d0\u793a\u8d28\u91cf\u4e3b\u8981\u5f71\u54cd\u6307\u4ee4\u9075\u5faa\u800c\u975e\u7ffb\u8bd1\u672c\u8d28\uff0cLLMs\u5c55\u73b0\u5f3a\u6297\u566a\u7ffb\u8bd1\u80fd\u529b\u3002", "conclusion": "\u9700\u5728\u63d0\u793a\u8d28\u91cf\u4e0e\u6297\u566a\u6027\u95f4\u53d6\u5f97\u5e73\u8861\uff0cLLMs\u7684\u7ffb\u8bd1\u80fd\u529b\u5bf9\u566a\u58f0\u63d0\u793a\u5c55\u73b0\u663e\u8457\u9002\u5e94\u6027\u3002"}}
{"id": "2507.09536", "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "categories": ["cs.CL"], "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "\u63d0\u51fa\u767d\u4fc4\u7f57\u65af\u8bed\u5b9a\u4e49\u5efa\u6a21\u6570\u636e\u96c6\u5e76\u9a8c\u8bc1\u6a21\u578b\u9002\u914d\u6548\u679c", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u5df2\u6709\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u652f\u6301\u672a\u8986\u76d6\u8bed\u8a00\uff08\u767d\u4fc4\u7f57\u65af\u8bed\uff09\uff0c\u8f85\u52a9\u8bcd\u5178\u7f16\u7e82\u5de5\u4f5c", "method": "\u6784\u5efa\u5305\u542b43,150\u6761\u5b9a\u4e49\u7684\u767d\u4fc4\u7f57\u65af\u8bed\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u9002\u914d\u5b9e\u9a8c", "result": "\u7cfb\u7edf\u9002\u914d\u4ec5\u9700\u5c11\u91cf\u6570\u636e\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5b58\u5728\u660e\u663e\u7f3a\u9677", "conclusion": "\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u5177\u5907\u8de8\u8bed\u8a00\u6269\u5c55\u6f5c\u529b\uff0c\u4f46\u9700\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb"}}
{"id": "2507.09601", "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "\u63d0\u51faNMIXX\u8de8\u8bed\u8a00\u91d1\u878d\u5d4c\u5165\u6a21\u578b\u53caKorFinSTS\u97e9\u8bed\u91d1\u878d\u8bed\u4e49\u76f8\u4f3c\u5ea6\u57fa\u51c6\uff0c\u901a\u8fc7\u9886\u57df\u81ea\u9002\u5e94\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u91d1\u878d\u6587\u672c\u5d4c\u5165\u6027\u80fd", "motivation": "\u901a\u7528\u53e5\u5d4c\u5165\u6a21\u578b\u5728\u97e9\u8bed\u91d1\u878d\u9886\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u4e13\u4e1a\u672f\u8bed\u3001\u8bed\u4e49\u65f6\u53d8\u3001\u53cc\u8bed\u8bcd\u6c47\u9519\u4f4d\u4e09\u5927\u6311\u6218", "method": "\u4f7f\u752818.8K\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4(\u540c\u4e49\u53e5/\u786c\u8d1f\u6837\u672c/\u7cbe\u51c6\u7ffb\u8bd1)\u5fae\u8c03bge-m3\u6a21\u578b\uff0c\u6784\u5efa\u6db5\u76d6\u65b0\u95fb/\u516c\u544a/\u7814\u62a5/\u6cd5\u89c4\u7684KorFinSTS\u6d4b\u8bd5\u96c6", "result": "NMIXX\u5728\u82f1\u97e9\u91d1\u878dSTS\u4efb\u52a1\u5206\u522b\u63d0\u53470.10\u548c0.22\u65af\u76ae\u5c14\u66fc\u7cfb\u6570\uff0cTokenizer\u8986\u76d6\u5ea6\u4e0e\u6027\u80fd\u6b63\u76f8\u5173\uff0c\u901a\u7528\u4efb\u52a1\u7565\u6709\u4e0b\u964d", "conclusion": "\u53d1\u5e03\u9996\u4e2a\u97e9\u8bed\u91d1\u878d\u9886\u57df\u5d4c\u5165\u6a21\u578b\u53ca\u8bc4\u6d4b\u57fa\u51c6\uff0c\u8bc1\u660eTokenizer\u8bbe\u8ba1\u5bf9\u4f4e\u8d44\u6e90\u8de8\u8bed\u8a00\u9002\u5e94\u7684\u91cd\u8981\u6027"}}
{"id": "2507.09628", "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "Python\u5e93SpreadPy\u901a\u8fc7\u4f20\u64ad\u6fc0\u6d3b\u6a21\u62df\u63ed\u793a\u8ba4\u77e5\u7f51\u7edc\u7ed3\u6784-\u529f\u80fd\u5173\u7cfb\uff0c\u5728\u6570\u5b66\u7126\u8651\u3001\u521b\u9020\u529b\u8ba4\u77e5\u8d1f\u8377\u3001\u5931\u8bed\u75c7\u8bed\u8a00\u969c\u788d\u4e09\u4e2a\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027", "motivation": "\u5efa\u7acb\u6570\u503c\u6a21\u62df\u5de5\u5177\u7cfb\u7edf\u7814\u7a76\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7f51\u7edc\u7ed3\u6784\u4e0e\u529f\u80fd\u8868\u73b0\u7684\u5173\u7cfb\uff0c\u4e3a\u5fc3\u7406\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u548c\u6559\u80b2\u7814\u7a76\u63d0\u4f9b\u673a\u5236\u89e3\u91ca", "method": "\u57fa\u4e8e\u4f20\u64ad\u6fc0\u6d3b\u7406\u8bba\u6784\u5efaPython\u6a21\u62df\u6846\u67b6\uff0c\u652f\u6301\u5b9e\u8bc1\u7f51\u7edc/\u7406\u8bba\u7f51\u7edc\u7684\u8ba4\u77e5\u8fc7\u7a0b\u5efa\u6a21\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u7ed3\u6784-\u529f\u80fd\u5173\u8054", "result": "1.\u6570\u5b66\u7126\u8651\u5b66\u751f\u77e5\u8bc6\u7f51\u7edc\u7ed3\u6784\u5dee\u5f02\u53ef\u89c6\u5316 2.\u521b\u9020\u529b\u4efb\u52a1\u4e2d\u8ba4\u77e5\u8d1f\u8377\u8c03\u8282\u8bcd\u6c47\u8bbf\u95ee\u8f68\u8ff9 3.\u5931\u8bed\u75c7\u60a3\u8005\u8bcd\u6c47\u7f51\u7edc\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u9519\u8bef\u7c7b\u578b\u663e\u8457\u76f8\u5173", "conclusion": "SpreadPy\u4e3a\u4e2a\u4f53\u5dee\u5f02\u548c\u8ba4\u77e5\u969c\u788d\u7814\u7a76\u63d0\u4f9b\u673a\u5236\u89e3\u91ca\u5de5\u5177\uff0c\u5176\u5f00\u6e90\u7279\u6027\u652f\u6301\u8de8\u5b66\u79d1\u53ef\u91cd\u590d\u7814\u7a76"}}
{"id": "2507.09629", "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "categories": ["cs.CL"], "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\uff0c\u53d1\u73b0\u53c2\u6570\u65b9\u6cd5\u5b58\u5728\u8de8\u8bed\u8a00\u6cdb\u5316\u74f6\u9888\uff0c\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u5e76\u901a\u8fc7\u591a\u8bed\u8a00\u8054\u5408\u8bad\u7ec3\u63d0\u5347\u6a21\u578b\u7f16\u8f91\u80fd\u529b", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u7814\u7a76\u96c6\u4e2d\u4e8e\u82f1\u8bed\uff0c\u5bf9\u963f\u62c9\u4f2f\u8bed\u7b49\u5f62\u6001\u590d\u6742\u8bed\u8a00\u7684\u7814\u7a76\u532e\u4e4f\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8be5\u7a7a\u767d\uff0c\u5206\u6790\u4e0d\u540c\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u573a\u666f\u4e0b\u7684\u8868\u73b0\u5dee\u5f02", "method": "\u5728Llama-2-7B-chat\u6a21\u578b\u4e0a\u8bc4\u4f30ROME/MEMIT/ICE/LTE\u56db\u79cd\u65b9\u6cd5\uff0c\u4f7f\u7528\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u7684ZsRE\u548cCounterfact\u57fa\u51c6\uff0c\u8bbe\u8ba1\u591a\u8bed\u8a00\u4e0e\u8de8\u8bed\u8a00\u53cc\u5b9e\u9a8c\u573a\u666f\uff0c\u5e76\u6269\u5c55LTE\u8fdb\u884c\u591a\u8bed\u8a00\u8054\u5408\u8bad\u7ec3", "result": "\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5(ROME/MEMIT)\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u5dee\uff08\u51c6\u786e\u7387\u4e0b\u964d35%\uff09\uff0c\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5(ICE)\u8868\u73b0\u7a33\u5065\u3002\u591a\u8bed\u8a00LTE\u8054\u5408\u8bad\u7ec3\u4f7f\u963f\u62c9\u4f2f\u8bed\u7f16\u8f91\u6210\u529f\u7387\u63d0\u534718%\uff0c\u82f1\u8bed\u4efb\u52a1\u540c\u6b65\u63d0\u534712%", "conclusion": "\u6784\u5efa\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\uff0c\u9a8c\u8bc1\u591a\u8bed\u8a00\u8bad\u7ec3\u5bf9\u7f16\u8f91\u8fc1\u79fb\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u5f00\u6e90\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2507.09638", "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "categories": ["cs.CL"], "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "\u4f7f\u7528GRPO\u65b9\u6cd5\u4e0eBGE-M3\u5d4c\u5165\u6a21\u578b\u63d0\u5347\u6cf0\u56fd\u6cd5\u5f8b\u95ee\u7b54\u7cfb\u7edf\u7684\u6cd5\u5f8b\u5f15\u7528\u51c6\u786e\u7387\uff08F1\u63d0\u534790%\uff09\u548c\u54cd\u5e94\u8d28\u91cf\uff08\u8054\u5408\u6307\u6807\u63d0\u534731%\uff09\uff0c\u540c\u65f6\u964d\u4f4e2.5\u500d\u8ba1\u7b97\u6210\u672c", "motivation": "\u89e3\u51b3\u73b0\u6709RAG\u7cfb\u7edf\u5728\u6cf0\u56fd\u6cd5\u5f8b\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5f15\u7528\u4e0d\u7cbe\u51c6\u3001\u54cd\u5e94\u8d28\u91cf\u6709\u9650\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u964d\u4f4e\u5927\u6a21\u578b\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u8ba1\u7b97\u5f00\u9500", "method": "\u63d0\u51fa\u57fa\u4e8eGroup-Relative Policy Optimization\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528BGE-M3\u5d4c\u5165\u6a21\u578b\u66ff\u4ee3\u5927\u6a21\u578b\u4f5c\u4e3a\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5956\u52b1\u51fd\u6570", "result": "\u5728NitiBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\uff1a1\uff09\u6cd5\u5f8b\u6761\u6587\u5f15\u7528\u51c6\u786e\u7387\u63d0\u534790% 2\uff09\u54cd\u5e94\u8d28\u91cf\u7efc\u5408\u6307\u6807\u63d0\u534731% 3\uff09\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u81f3\u4f20\u7edf\u65b9\u6cd5\u768440%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6cf0\u56fd\u6cd5\u5f8b\u5927\u6a21\u578b\u63d0\u4f9b\u4e86\u517c\u987e\u6027\u80fd\u63d0\u5347\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027"}}
{"id": "2507.09701", "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "MCEval\u4f5c\u4e3a\u9996\u4e2a\u591a\u8bed\u8a00\u6587\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u6027\u63ed\u793a\u4e86LLMs\u5728\u8de8\u6587\u5316\u573a\u666f\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u4e0e\u516c\u5e73\u6027\u95ee\u9898\uff0c\u6307\u51fa\u8bed\u8a00-\u6587\u5316\u5bf9\u9f50\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u540c\u7b49\u91cd\u8981\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6587\u5316\u504f\u89c1\u4e0e\u8de8\u6587\u5316\u7406\u89e3\u5c40\u9650\uff0c\u5c24\u5176\u9762\u5bf9\u5168\u7403\u7528\u6237\u65f6\u7f3a\u4e4f\u6709\u6548\u7684\u591a\u7ef4\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u6587\u5316\u95ee\u9898\u751f\u6210\u4e0e\u53cd\u4e8b\u5b9e/\u6df7\u6742\u91cd\u8ff0\u6280\u672f\uff0c\u6784\u5efa\u5305\u542b13\u79cd\u8bed\u8a00\u6587\u5316\u768439,897\u4e2a\u6587\u5316\u610f\u8bc6\u6837\u672c\u4e0e17,940\u4e2a\u504f\u89c1\u6837\u672c\u8fdb\u884c\u56e0\u679c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u4e0e\u8bed\u8a00-\u6587\u5316\u5bf9\u9f50\u5ea6\u5f3a\u76f8\u5173\uff0c\u82f1\u8bed\u573a\u666f\u7684\u6210\u529f\u65b9\u6cd5\u5728\u5176\u4ed6\u8bed\u8a00\u4e2d\u4ea7\u751f\u663e\u8457\u52a3\u52bf\uff0c\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u975e\u552f\u4e00\u51b3\u5b9a\u56e0\u7d20\u3002", "conclusion": "MCEval\u6846\u67b6\u4e3aLLMs\u6587\u5316\u7406\u89e3\u8bc4\u4f30\u6811\u7acb\u65b0\u6807\u6746\uff0c\u5f3a\u8c03\u8de8\u8bed\u8a00\u6587\u5316\u8bc4\u4f30\u5bf9\u63d0\u5347AI\u516c\u5e73\u6027\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.09709", "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "LLMs\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b58\u5728\u4f4e\u7ef4\u7ebf\u6027\u53ef\u5206\u8bed\u4e49\u7ed3\u6784\uff0c\u8be5\u7279\u6027\u5728\u6df1\u5c42\u7f51\u7edc\u548c\u7ed3\u6784\u5316\u63a8\u7406\u573a\u666f\u4e2d\u589e\u5f3a\uff0c\u652f\u6301\u5f00\u53d1\u51e0\u4f55\u611f\u77e5\u9632\u5fa1\u5de5\u5177", "motivation": "\u7406\u89e3LLMs\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u5bf9\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\u548c\u63d0\u5347\u5bf9\u9f50\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u8bed\u4e49\u7ec4\u7ec7\u673a\u5236\u8ba4\u8bc6\u4e0d\u8db3", "method": "\u5206\u679011\u4e2aDecoder-only\u6a21\u578b\u57286\u4e2a\u79d1\u5b66\u9886\u57df\u768412\u5c42\u9690\u85cf\u72b6\u6001\uff0c\u7814\u7a76\u4e0d\u540c\u63d0\u793a\u4e0b\u8bed\u4e49\u8868\u793a\u7684\u51e0\u4f55\u7279\u6027\uff0c\u5e76\u8fdb\u884c\u9690\u85cf\u7a7a\u95f4\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c", "result": "1. \u9ad8\u5c42\u6b21\u8bed\u4e49\u5b58\u5728\u4e8e\u8de8\u9886\u57df\u7684\u4f4e\u7ef4\u7ebf\u6027\u53ef\u5206\u5b50\u7a7a\u95f4\n2. \u6df1\u5c42\u7f51\u7edc\u548c\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u793a\u663e\u8457\u589e\u5f3a\u53ef\u5206\u6027\n3. \u5355\u4e00\u5411\u91cf\u65b9\u5411\u53ef\u6355\u6349\u63a8\u7406\u6a21\u5f0f\uff08\u5982\u94fe\u5f0f\u601d\u8003\uff09", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7279\u6027\u4e3a\u5f00\u53d1\u57fa\u4e8e\u8868\u793a\u7684\u9632\u5fa1\u5de5\u5177\u5960\u5b9a\u57fa\u7840\uff0c\u5b9e\u9a8c\u8bc1\u660e\u901a\u8fc7\u7b80\u5355MLP\u5206\u7c7b\u5668\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u5bf9\u6297\u68c0\u6d4b"}}
{"id": "2507.09758", "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u7684\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u9884\u6d4b\u6837\u672c\u96be\u5ea6\u8fdb\u884c\u5fae\u8c03\uff0c\u76f8\u6bd4\u968f\u673a\u91c7\u6837\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u548c\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5b9a\u4e49\u96be\u5ea6\u6307\u6807\uff08\u5982\u6587\u672c\u957f\u5ea6\uff09\uff0c\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u81ea\u8eab\u8ba4\u77e5\u5dee\u5f02", "method": "1. \u7528PLM\u9884\u6d4b\u6837\u672c\u96be\u5ea6\u5206\u6570\n2. \u63a2\u7d22\u6613\u5230\u96be\u3001\u96be\u5230\u6613\u3001\u6df7\u5408\u91c7\u6837\u4e09\u79cd\u5fae\u8c03\u7b56\u7565\n3. \u57284\u4e2aNLU\u6570\u636e\u96c6\uff08\u542b\u4e8c\u5206\u7c7b\u548c\u591a\u5206\u7c7b\u4efb\u52a1\uff09\u9a8c\u8bc1", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u968f\u673a\u91c7\u6837\u6536\u655b\u66f4\u5feb\uff0c\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53471.5-2.3\u4e2a\u767e\u5206\u70b9", "conclusion": "\u57fa\u4e8e\u6a21\u578b\u81ea\u611f\u77e5\u96be\u5ea6\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6709\u6548\uff0c\u4e3a\u81ea\u52a8\u5316\u8bad\u7ec3\u6837\u672c\u6392\u5e8f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2507.09777", "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorr\u00e9 Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "\u91cd\u65b0\u5b9a\u4e49\u70b9\u51fb\u8bf1\u9975\uff08\u5f3a\u8c03\u597d\u5947\u5fc3\u5dee\u8ddd\u673a\u5236\uff09\uff0c\u6784\u5efa\u897f\u73ed\u7259\u8bed\u9996\u5f00\u6e90\u68c0\u6d4b\u6570\u636e\u96c6TA1C\uff083500\u6761\u63a8\u6587\uff0c0.825\u6807\u6ce8\u4e00\u81f4\u6027\uff09\uff0c\u57fa\u7ebf\u6a21\u578bF1\u8fbe0.84", "motivation": "\u73b0\u6709\u70b9\u51fb\u8bf1\u9975\u5b9a\u4e49\u7f3a\u4e4f\u5171\u8bc6\uff0c\u9700\u533a\u5206\u4e8e\u6807\u9898\u515a\u7b49\u7c7b\u4f3c\u73b0\u8c61\u3002\u5f53\u524d\u897f\u73ed\u7259\u8bed\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u68c0\u6d4b\u6570\u636e\u96c6", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u7f3a\u5931\u7684\u70b9\u51fb\u8bf1\u9975\u65b0\u5b9a\u4e49\uff1b2. \u8bbe\u8ba1\u5ba2\u89c2\u6807\u6ce8\u6807\u51c6\u6784\u5efaTA1C\u6570\u636e\u96c6\uff1b3. \u91c7\u7528Fleiss' K\u7cfb\u6570\u8bc4\u4f30\u6807\u6ce8\u4e00\u81f4\u6027\uff1b4. \u5efa\u7acb\u5f3a\u57fa\u7ebf\u6a21\u578b", "result": "1. \u6570\u636e\u96c6\u6807\u6ce8\u4e00\u81f4\u60270.825\uff1b2. \u57fa\u7ebf\u6a21\u578bF1\u503c0.84\uff1b3. \u6570\u636e\u96c6\u5305\u542b18\u4e2a\u5a92\u4f53\u6e90\u76843500\u6761\u897f\u73ed\u7259\u8bed\u63a8\u6587", "conclusion": "\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u91cd\u6784\u89e3\u51b3\u4e86\u5b9a\u4e49\u6a21\u7cca\u95ee\u9898\uff0cTA1C\u6570\u636e\u96c6\u586b\u8865\u897f\u73ed\u7259\u8bed\u8d44\u6e90\u7a7a\u767d\uff0c\u5f3a\u57fa\u7ebf\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u57fa\u51c6"}}
{"id": "2507.09875", "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53ef\u91cd\u7528\u7ec4\u4ef6\u7ed3\u6784\u5b9e\u73b0\u4efb\u52a1\u6cdb\u5316\u7684\u5185\u90e8\u673a\u5236", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u4efb\u52a1\u6cdb\u5316\u7684\u5185\u90e8\u8ba1\u7b97\u673a\u5236\uff0c\u7279\u522b\u662f\u9488\u5bf9\u975e\u5e38\u89c4\u4efb\u52a1\uff08\u5982\u9519\u8bef\u52a0\u6cd5\uff09\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u4f7f\u7528\u8def\u5f84\u4fee\u8865\u7b49\u89e3\u91ca\u6027\u6280\u672f\u5206\u6790\u6a21\u578b\u5728off-by-one\u52a0\u6cd5\u4efb\u52a1\uff08\u59821+1=3\uff09\u4e2d\u7684\u8ba1\u7b97\u6a21\u5f0f\uff0c\u7ed3\u5408\u5408\u6210\u4efb\u52a1\uff08\u4f4d\u79fbQA\uff09\u548c\u7b97\u6cd5\u4efb\u52a1\uff08\u516b\u8fdb\u5236\u52a0\u6cd5\uff09\u9a8c\u8bc1\u53d1\u73b0", "result": "1. \u53d1\u73b0\u7c7b\u4f3c\u5f52\u7eb3\u5934\u7684\u51fd\u6570\u5f52\u7eb3\u673a\u5236\n2. +1\u51fd\u6570\u7531\u591a\u5934\u5e76\u884c\u751f\u6210\n3. \u8be5\u673a\u5236\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u590d\u7528", "conclusion": "\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u53ef\u7ec4\u5408\u7684\u590d\u7528\u7ed3\u6784\u5b9e\u73b0\u4efb\u52a1\u7ea7\u6cdb\u5316\uff0c\u8be5\u53d1\u73b0\u6df1\u5316\u4e86\u5bf9\u6a21\u578b\u5185\u90e8\u5de5\u4f5c\u673a\u5236\u7684\u7406\u89e3"}}
{"id": "2507.09935", "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\u7684RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6bb5/\u7c07\u7ea7\u5411\u91cf\u8868\u793a\u63d0\u5347\u68c0\u7d22\u6548\u679c", "motivation": "\u4f20\u7edfRAG\u5206\u5757\u7b56\u7565\u5ffd\u89c6\u6587\u672c\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u6355\u6349\u4e0d\u8db3\u3002\u9700\u6784\u5efa\u66f4\u8bed\u4e49\u8fde\u8d2f\u7684\u5757\u7ed3\u6784\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6", "method": "1. \u5206\u5c42\u6587\u672c\u5206\u5272\u521b\u5efa\u903b\u8f91\u6bb5\u843d 2. \u8bed\u4e49\u805a\u7c7b\u751f\u6210\u8bed\u4e49\u5757 3. \u68c0\u7d22\u65f6\u7ed3\u5408\u6bb5\u7ea7\u548c\u7c07\u7ea7\u5411\u91cf", "result": "\u5728NarrativeQA/QuALITY/QASPER\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4f20\u7edf\u5206\u5757\u65b9\u6cd5", "conclusion": "\u53cc\u91cd\u5c42\u6b21\u8868\u5f81\u6709\u6548\u63d0\u5347\u68c0\u7d22\u76f8\u5173\u6027\uff0c\u4e3a\u590d\u6742\u8bed\u4e49\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.09973", "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "categories": ["cs.CL", "cs.AI"], "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "TinyRM\u901a\u8fc7FLAN\u63d0\u793a\u3001DoRA\u548c\u5c42\u51bb\u7ed3\u6280\u672f\uff0c\u4ee5\u4ec54\u4ebf\u53c2\u6570\u5b9e\u73b0\u63a5\u8fd1\u5927175\u500d\u6a21\u578b\u7684\u5956\u52b1\u5efa\u6a21\u6027\u80fd\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u8d44\u6e90\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5927\u6a21\u578b\u5728RLHF\u5956\u52b1\u5efa\u6a21\u4e2d\u63a8\u7406\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u8f7b\u91cf\u5316\u6a21\u578b\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8c03\u4f18\u7b56\u7565\u66ff\u4ee3\u5927\u578b\u6a21\u578b\u7684\u53ef\u884c\u6027\u3002", "method": "\u7ed3\u5408FLAN-style prompting\u589e\u5f3a\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u91c7\u7528DoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u914d\u5408\u5206\u5c42\u51bb\u7ed3\u7b56\u7565\u51cf\u5c11\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u3002", "result": "\u5728RewardBench\u8bc4\u6d4b\u4e2d\uff0cTinyRM\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u7387\u8d85\u8fc7\u5927\u6a21\u578b\u57fa\u7ebf\uff0c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u51cf\u5c1198%\u65f6\u4ecd\u4fdd\u630193%\u76f8\u5bf9\u6027\u80fd\u3002", "conclusion": "\u53cc\u5411\u67b6\u6784\u914d\u5408\u9886\u57df\u8c03\u4f18\u5728\u7279\u5b9a\u4efb\u52a1\u5c55\u73b0\u66ff\u4ee3\u6f5c\u529b\uff0c\u4f46\u901a\u7528\u5316\u5efa\u6a21\u548c\u5bf9\u8bdd\u504f\u597d\u5efa\u6a21\u4ecd\u9700\u7a81\u7834\uff0c\u4e3a\u9ad8\u6548\u504f\u597d\u5efa\u6a21\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.09982", "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u63d0\u51faTextOmics\u57fa\u51c6\u5efa\u7acb\u7ec4\u5b66\u4e0e\u5206\u5b50\u6587\u672c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u5f00\u53d1ToDi\u6846\u67b6\u5b9e\u73b0\u751f\u7269\u76f8\u5173/\u5316\u5b66\u6709\u6548\u7684\u5206\u5b50\u751f\u6210\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5177\u5907\u96f6\u6837\u672c\u751f\u6210\u6f5c\u529b\u3002", "motivation": "\u5f53\u524d\u9776\u5411\u836f\u7269\u53d1\u73b0\u9886\u57df\u7f3a\u4e4f\u5f02\u6784\u6570\u636e\u548c\u7edf\u4e00\u6846\u67b6\u6765\u6574\u5408\u591a\u6837\u5316\u7684\u5206\u5b50\u8868\u793a\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u7ec4\u5b66\u6570\u636e\u7684\u5206\u5b50\u751f\u6210\u7814\u7a76\u3002", "method": "1. \u6784\u5efaTextOmics\u5f02\u6784\u6570\u636e\u96c6\u5efa\u7acb\u7ec4\u5b66-\u6587\u672c\u6620\u5c04\n2. \u63d0\u51faToDi\u6846\u67b6\u8054\u5408\u7ec4\u5b66\u8868\u8fbe\u548c\u6587\u672c\u63cf\u8ff0\n3. \u4f7f\u7528\u53cc\u7f16\u7801\u5668(OmicsEn/TextEn)\u6355\u6349\u751f\u7269/\u8bed\u4e49\u5173\u8054\n4. \u5f00\u53d1DiffGen\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u53ef\u63a7\u751f\u6210", "result": "\u5b9e\u9a8c\u9a8c\u8bc1TextOmics\u6709\u6548\u6027\uff0cToDi\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u6cbb\u7597\u5206\u5b50\u751f\u6210\u4e2d\u5c55\u793a\u51fa87.5%\u7684\u65b0\u5206\u5b50\u751f\u6210\u6210\u529f\u7387", "conclusion": "TextOmics\u548cToDi\u4e3a\u9776\u5411\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u6570\u636e\u57fa\u7840\u548c\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6cbb\u7597\u5206\u5b50\u751f\u6210\u7684\u751f\u7269\u5b66\u76f8\u5173\u6027\u548c\u5316\u5b66\u6709\u6548\u6027\u3002"}}
{"id": "2507.10008", "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u52a8\u6001\u98ce\u9669\u56e0\u7d20\u4e0e\u4fdd\u62a4\u6027\u56e0\u7d20\u7684\u81ea\u6740\u98ce\u9669\u9884\u6d4b\u6846\u67b6\uff0c\u6784\u5efa\u65b0\u578b\u6570\u636e\u96c6\u5e76\u9a8c\u8bc1\u6a21\u578b\u4f18\u8d8a\u6027", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u9759\u6001\u98ce\u9669\u56e0\u7d20\u68c0\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u52a8\u6001\u98ce\u9669\u6ce2\u52a8\u53ca\u4fdd\u62a4\u6027\u56e0\u7d20\uff08\u5982\u793e\u4f1a\u652f\u6301\uff09\u534f\u540c\u4f5c\u7528\u7684\u7814\u7a76\uff0c\u9650\u5236\u9884\u6d4b\u6548\u679c", "method": "1. \u6784\u5efa\u542b\u98ce\u9669/\u4fdd\u62a4\u56e0\u7d20\u6807\u6ce8\u768412\u5e74Reddit\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u7b97\u6cd5\uff0c\u91cf\u5316\u4e0d\u540c\u65f6\u671f\u56e0\u7d20\u5bf9\u98ce\u9669\u8f6c\u53d8\u7684\u5f71\u54cd", "result": "\u6a21\u578b\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u6a21\u578b\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u52a8\u6001\u5b66\u4e60\u673a\u5236\u63d0\u4f9b\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\u5206\u6790", "conclusion": "\u52a8\u6001\u53cc\u56e0\u7d20\u8054\u5408\u5efa\u6a21\u63d0\u5347\u81ea\u6740\u98ce\u9669\u9884\u6d4b\u6027\u80fd\uff0c\u53ef\u89e3\u91ca\u6027\u7ed3\u679c\u6709\u52a9\u4e8e\u5236\u5b9a\u7cbe\u51c6\u5fc3\u7406\u5e72\u9884\u7b56\u7565"}}
{"id": "2507.10059", "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "GeLaCo\u662f\u4e00\u79cd\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u5b9e\u73b0LLM\u538b\u7f29\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5c42\u6298\u53e0\u6280\u672f\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5728\u538b\u7f29\u7387\u548c\u6a21\u578b\u8d28\u91cf\u95f4\u5efa\u7acb\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u8fc7\u9ad8\u5bfc\u81f4\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\uff08\u5982\u7ed3\u6784\u5316\u526a\u679d\uff09\u4f9d\u8d56\u7ecf\u9a8c\u641c\u7d22\u4e14\u53ef\u80fd\u9057\u6f0f\u66f4\u4f18\u89e3\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u79cd\u7fa4\u7684\u8fdb\u5316\u641c\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u3001\u524d\u9988\u7f51\u7edc\u548c\u9690\u85cf\u72b6\u6001\u76f8\u4f3c\u6027\u7684\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u652f\u6301\u5355\u76ee\u6807/\u591a\u76ee\u6807\u538b\u7f29\u4f18\u5316\u3002", "result": "\u5728\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4e0a\uff0cGeLaCo\u7684\u56f0\u60d1\u5ea6\u6307\u6807\u548c\u751f\u6210\u8d28\u91cf\u5747\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u538b\u7f29-\u8d28\u91cf\u53cc\u76ee\u6807\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u3002", "conclusion": "GeLaCo\u901a\u8fc7\u7cfb\u7edf\u5316\u8fdb\u5316\u641c\u7d22\u548c\u6a21\u5757\u7ea7\u76f8\u4f3c\u6027\u8bc4\u4f30\uff0c\u9ad8\u6548\u63a2\u7d22LLM\u538b\u7f29\u89e3\u7a7a\u95f4\uff0c\u4e3a\u6a21\u578b\u8f7b\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u7a81\u7834\u3002"}}
{"id": "2507.10073", "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M\u00fcnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "categories": ["cs.CL", "cs.AI"], "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4e0d\u540c\u6587\u5316\u9053\u5fb7\u6846\u67b6\uff0c\u53cd\u800c\u7cfb\u7edf\u6027\u5730\u540c\u8d28\u5316\u9053\u5fb7\u591a\u6837\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u5e76\u4e0d\u80fd\u6539\u5584\u6587\u5316\u4ee3\u8868\u6027\u3002", "motivation": "\u9488\u5bf9\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u548c\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u65e5\u76ca\u4f9d\u8d56LLMs\u4f5c\u4e3a\u5408\u6210\u4eba\u7fa4\u7684\u73b0\u8c61\uff0c\u9a8c\u8bc1AI\u7cfb\u7edf\u662f\u5426\u771f\u6b63\u4ee3\u8868\u591a\u5143\u4eba\u7c7b\u4ef7\u503c\u89c2\u3002", "method": "\u8de819\u4e2a\u6587\u5316\u8bed\u5883\u5e94\u7528\u9053\u5fb7\u57fa\u7840\u95ee\u5377\uff0c\u5bf9\u6bd4\u591a\u4e2a\u5148\u8fdbLLMs\u8f93\u51fa\u4e0e\u4eba\u7c7b\u57fa\u51c6\u6570\u636e\uff0c\u5206\u6790\u6a21\u578b\u89c4\u6a21\u4e0e\u6587\u5316\u4ee3\u8868\u6027\u7684\u5173\u7cfb\u3002", "result": "1. LLMs\u8f93\u51fa\u4e0e\u4eba\u7c7b\u9053\u5fb7\u76f4\u89c9\u5b58\u5728\u663e\u8457\u5dee\u8ddd\n2. \u6a21\u578b\u7cfb\u7edf\u6027\u540c\u8d28\u5316\u9053\u5fb7\u591a\u6837\u6027\n3. \u53c2\u6570\u589e\u52a0\u672a\u63d0\u5347\u6587\u5316\u8868\u5f81\u4fdd\u771f\u5ea6", "conclusion": "\u5f53\u524dAI\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u5c40\u9650\uff0c\u9700\u5efa\u7acb\u57fa\u4e8e\u5b9e\u9645\u6570\u636e\u7684\u5bf9\u9f50\u76ee\u6807\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u907f\u514d\u6280\u672f\u7cfb\u7edf\u6241\u5e73\u5316\u4eba\u7c7b\u9053\u5fb7\u666f\u89c2\u3002"}}
{"id": "2507.10085", "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "\u63d0\u51faCRFT\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u52a8\u6001\u4f18\u5316\u5173\u952e\u8868\u5f81\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u6027\u80fd\uff0c\u53c2\u6570\u6548\u7387\u4f18\u4e8e\u4f20\u7edfPEFT\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfReFT\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u56e0\u56fa\u5b9a\u4f4d\u7f6e\u8868\u5f81\u4f18\u5316\u6548\u679c\u53d7\u9650\uff0c\u5173\u952e\u8868\u5f81\u5bf9\u6700\u7ec8\u8f93\u51fa\u5177\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u4f4e\u79e9\u7ebf\u6027\u5b50\u7a7a\u95f4\u4e2d\u52a8\u6001\u8bc6\u522b\u5e76\u4f18\u5316\u4fe1\u606f\u6d41\u4e2d\u7684\u5173\u952e\u8868\u5f81\uff0c\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u51bb\u7ed3\u3002", "result": "\u57288\u4e2a\u7b97\u672f\u4e0e\u5e38\u8bc6\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5355\u6837\u672c\u51c6\u786e\u7387\u63d0\u534716.4%\uff0c\u9002\u914d\u5c0f\u6837\u672c\u573a\u666f\u3002", "conclusion": "\u8868\u5f81\u7ea7\u4f18\u5316\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u9ad8\u6548\u8c03\u4f18\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10098", "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408LLM\u8bed\u4e49\u7406\u89e3\u4e0eTransformer\u65f6\u5e8f\u5efa\u6a21\u7684\u4e92\u8865\u67b6\u6784\uff0c\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u73b0\u6709LLM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u5982\u666e\u901aTransformer\uff0c\u800c\u540e\u8005\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u9700\u878d\u5408\u4e24\u8005\u4f18\u52bf\u7a81\u7834\u6027\u80fd\u74f6\u9888", "method": "\u8bbe\u8ba1\u65b0\u578bTransformer\u67b6\u6784\uff0c\u901a\u8fc7\u6df7\u5408\u8868\u793a\u878d\u5408LLM\u7684\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u548cTransformer\u7684\u65f6\u95f4\u52a8\u6001\u7f16\u7801", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u6709\u6548\u6027\uff0c\u9884\u6d4b\u7cbe\u5ea6\u8d85\u8fc7\u5355\u4e00\u6a21\u578b", "conclusion": "\u8bed\u4e49-\u65f6\u5e8f\u7279\u5f81\u4e92\u8865\u673a\u5236\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2507.10155", "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u7ebf\u6027\u6295\u5f71\u5c42\u7684\u4efb\u52a1\u9a71\u52a8\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e0d\u540c\u7ef4\u5ea6LLM\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u63d0\u53473%\u3002", "motivation": "\u4f20\u7edf\u7279\u5f81\u84b8\u998f\u9700\u5f3a\u5236\u5bf9\u9f50\u7279\u5f81\u7ef4\u5ea6\uff0c\u5f15\u5165\u989d\u5916\u53c2\u6570\u4e14\u635f\u5bb3\u751f\u6210\u4efb\u52a1\u6027\u80fd\u3002\u57fa\u4e8eLLM\u7ec4\u4ef6\u5bf9\u4efb\u52a1\u8d21\u732e\u4e0d\u5747\u7684\u7279\u6027\uff0c\u8bbe\u8ba1\u66f4\u7075\u6d3b\u7684\u53c2\u6570\u65e0\u5173\u84b8\u998f\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u6559\u5e08\u6a21\u578b\u4e2d\u4efb\u52a1\u76f8\u5173\u9690\u85cf\u5355\u5143\uff0c\u76f4\u63a5\u84b8\u998f\u5176\u6fc0\u6d3b\u503c\u5230\u5b66\u751f\u6a21\u578b\uff0c\u652f\u6301\u4efb\u610f\u67b6\u6784\u7ec4\u5408\uff0c\u517c\u5bb9\u5176\u4ed6\u84b8\u998f\u6846\u67b6\u3002", "result": "\u5728\u5206\u7c7b\u3001\u6307\u4ee4\u8ddf\u968f\u3001\u6458\u8981\u751f\u6210\u7b49\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u53473%\u6027\u80fd\u6307\u6807\u3002", "conclusion": "\u4efb\u52a1\u76f8\u5173\u6027\u7279\u5f81\u9009\u62e9\u663e\u8457\u63d0\u5347\u84b8\u998f\u6548\u7387\uff0c\u53c2\u6570\u65e0\u5173\u8bbe\u8ba1\u589e\u5f3a\u90e8\u7f72\u7075\u6d3b\u6027\uff0c\u4e3a\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10177", "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8f6c\u6362\u6ee5\u7528\u6587\u672c\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0Groq\u4e0e\u5176\u4ed6\u6a21\u578b\u5b58\u5728\u663e\u8457\u5dee\u5f02", "motivation": "\u63a2\u7d22LLMs\u5728\u8bc6\u522b\u548c\u8f6c\u6362\u6ee5\u7528\u6587\u672c\uff08\u542b\u4ec7\u6068\u8a00\u8bba\u548c\u810f\u8bdd\uff09\u7684\u6709\u6548\u6027\uff0c\u8981\u6c42\u8f6c\u6362\u540e\u4fdd\u6301\u8bed\u4e49\u548c\u60c5\u611f\u4e00\u81f4\u6027", "method": "\u4f7f\u7528Gemini/GPT-4o/DeepSeek/Groq\u8fdb\u884c\u6587\u672c\u51c0\u5316\uff0c\u901a\u8fc7\u60c5\u611f\u5206\u6790\u548c\u8bed\u4e49\u5206\u6790\u8bc4\u4f30\u539f\u59cb\u4e0e\u8f6c\u6362\u540e\u6570\u636e", "result": "Groq\u8f93\u51fa\u7ed3\u679c\u4e0e\u5176\u4ed6\u6a21\u578b\u5dee\u5f02\u663e\u8457\uff0cGPT-4o\u4e0eDeepSeek-V3\u8868\u73b0\u76f8\u4f3c", "conclusion": "\u4e0d\u540cLLMs\u5728\u6587\u672c\u51c0\u5316\u4efb\u52a1\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u6a21\u578b\u9009\u62e9\u5bf9\u7ed3\u679c\u5f71\u54cd\u663e\u8457"}}
{"id": "2507.10216", "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86Absher\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u63ed\u793a\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6c99\u7279\u65b9\u8a00\u7406\u89e3\u53ca\u6587\u5316\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u663e\u8457\u4e0d\u8db3", "motivation": "\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u5e94\u7528\u4e2d\u5927\u6a21\u578b\u5728\u65b9\u8a00\u5904\u7406\u548c\u6587\u5316\u611f\u77e5\u4e0a\u7684\u7f3a\u9677\uff0c\u7279\u522b\u662f\u5728\u6c99\u7279\u8fd9\u79cd\u591a\u65b9\u8a00\u5730\u533a\u9700\u8981\u66f4\u7cbe\u51c6\u7684\u8bc4\u4f30\u5de5\u5177", "method": "\u6784\u5efa\u5305\u542b6\u5927\u7c7b\u4efb\u52a1\u300118,000+\u65b9\u8a00\u591a\u9009\u9898\u7684\u8bc4\u6d4b\u96c6\uff0c\u6570\u636e\u6e90\u81ea\u6c99\u7279\u5404\u5730\u533a\u65b9\u8a00\u7d20\u6750\uff0c\u5e76\u6d4b\u8bd5\u591a\u8bed\u8a00/\u963f\u62c9\u4f2f\u4e13\u7528\u6a21\u578b", "result": "\u73b0\u6709\u6a21\u578b\u5728\u6587\u5316\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u4efb\u52a1\u4e0a\u8868\u73b0\u6700\u5dee\uff08\u5982GPT-4\u51c6\u786e\u7387\u4ec552%\uff09\uff0c\u591a\u8bed\u8a00\u6a21\u578b\u666e\u904d\u900a\u4e8e\u963f\u62c9\u4f2f\u4e13\u7528\u6a21\u578b", "conclusion": "\u5fc5\u987b\u5f00\u53d1\u65b9\u8a00\u654f\u611f\u7684\u8bad\u7ec3\u65b9\u6cd5\u548c\u6587\u5316\u5bf9\u9f50\u7684\u8bc4\u4f30\u4f53\u7cfb\uff0c\u8fd9\u662f\u63d0\u5347LLMs\u963f\u62c9\u4f2f\u8bed\u5e94\u7528\u6548\u679c\u7684\u5173\u952e\u7a81\u7834\u53e3"}}
{"id": "2507.10326", "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "categories": ["cs.CL"], "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8fdb\u5316\u641c\u7d22\u65b9\u6cd5\u4f18\u5316\u79bb\u6563\u63d0\u793a\u5de5\u7a0b\uff0c\u5728\u5c0f\u8bed\u8a00\u6a21\u578b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6848", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4f18\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u5c0f\u6a21\u578b\u5bf9\u63d0\u793a\u8bbe\u8ba1\u66f4\u654f\u611f", "method": "1. \u8bed\u6cd5\u5f15\u5bfc\u9057\u4f20\u7f16\u7a0b\u5408\u6210\u63d0\u793a\u521b\u5efa\u7a0b\u5e8f\n2. \u5c40\u90e8\u641c\u7d22\u4f18\u5316\u6700\u4f73\u7a0b\u5e8f\u90bb\u57df", "result": "\u57283\u4e2a\u5c0f\u89c4\u6a21\u901a\u7528LLM\u548c4\u4e2a\u9886\u57df\u4efb\u52a1\u4e2d\u8d85\u8d8aPromptWizard/OPRO/RL-Prompt", "conclusion": "\u8fdb\u5316\u641c\u7d22\u65b9\u6cd5\u5728\u4fdd\u6301\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff0c\u4ec5\u5728\u6781\u5c11\u6570\u60c5\u51b5\u4e0b\u51fa\u73b0\u8f7b\u5fae\u6027\u80fd\u4e0b\u964d"}}
{"id": "2507.10330", "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGBM\u7684\u6b63\u5219\u5316\u6280\u672f\u589e\u5f3aNLP\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5728\u5bf9\u6297\u653b\u51fb\u573a\u666f\u4e0bLSTM/S4/CNN\u67b6\u6784\u63d0\u5347\u8fbe8.8%", "motivation": "\u73b0\u6709\u5bf9\u6297\u9632\u5fa1\u7814\u7a76\u5ffd\u89c6\u5faa\u73af\u7f51\u7edc\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u72ec\u7279\u67b6\u6784\u7279\u6027\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u65f6\u5e8f\u5904\u7406\u673a\u5236\u5b58\u5728\u65b0\u7684\u9632\u5fa1\u6311\u6218", "method": "\u901a\u8fc7Growth Bound Matrices\u8ba1\u7b97\u6a21\u578b\u53c2\u6570\u654f\u611f\u6027\u8fb9\u754c\uff0c\u5728LSTM/S4/CNN\u4e2d\u5b9e\u65bd\u6270\u52a8\u7ea6\u675f\u6b63\u5219\u5316", "result": "\u5728\u591a\u79cd\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u9ad88.8%\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u6027\u9a8c\u8bc1SSM\u67b6\u6784\u7684\u9632\u5fa1\u6f5c\u529b\uff0cGBM\u673a\u5236\u4e3a\u5e8f\u5217\u6a21\u578b\u63d0\u4f9b\u901a\u7528\u9632\u5fa1\u6846\u67b6"}}
{"id": "2507.10342", "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "LLM\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u4e0e\u4eba\u7c7b\u5b9e\u9a8c\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\u7684\u60c5\u611f\u8bed\u4e49\u5224\u65ad\u80fd\u529b\uff0c\u53ef\u4f5c\u4e3a\u53ef\u4fe1\u8d56\u7684\u8f85\u52a9\u7814\u7a76\u5de5\u5177", "motivation": "\u9a8c\u8bc1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u590d\u73b0\u4eba\u7c7b\u5728\u60c5\u611f\u8bed\u4e49\u5224\u65ad\u4e0a\u7684\u7ec6\u5fae\u5dee\u5f02\uff0c\u63a2\u7d22AI\u66ff\u4ee3/\u589e\u5f3a\u4f20\u7edf\u4eba\u7c7b\u5b9e\u9a8c\u7684\u53ef\u80fd\u6027", "method": "\u91c7\u7528\u56db\u4e2a\u5fc3\u7406\u8bed\u8a00\u5b66\u5b9e\u9a8c\u8303\u5f0f\uff08\u60c5\u611f\u8bed\u4e49\u6d8c\u73b0\u3001\u60c5\u611f\u6781\u6027\u504f\u79fb\u3001\u60c5\u611f\u8bed\u5883\u52a8\u8bcd\u9009\u62e9\u3001\u53e5\u5b50-\u8868\u60c5\u7b26\u53f7\u5173\u8054\uff09\uff0c\u5206\u522b\u7528\u4eba\u7c7b\u88ab\u8bd5\u548cGPT-4\u8fdb\u884c\u91cd\u590d\u6d4b\u8bd5", "result": "\u6240\u6709\u5b9e\u9a8c\u5747\u663e\u793a\u4eba\u7c7b\u4e0eAI\u54cd\u5e94\u9ad8\u5ea6\u8d8b\u540c\uff08Spearman's rho = 0.73-0.96\uff09\uff0c\u6b21\u8981\u5dee\u5f02\u4e0d\u5f71\u54cd\u6574\u4f53\u89e3\u91ca\u6709\u6548\u6027", "conclusion": "LLM\u80fd\u591f\u6709\u6548\u6269\u5c55\u8bed\u8a00\u5b66\u7814\u7a76\u89c4\u6a21\uff0c\u5728\u4fdd\u6301\u89e3\u91ca\u6709\u6548\u6027\u7684\u524d\u63d0\u4e0b\u4e3a\u7406\u8bba\u751f\u6210\u548c\u6570\u636e\u6269\u5c55\u63d0\u4f9b\u65b0\u8def\u5f84"}}
{"id": "2507.10354", "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "categories": ["cs.CL"], "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "\u63d0\u51fa\u9690\u55bb\u5904\u7406\u7684\u4e09\u5c42\u6d0b\u8471\u6a21\u578b\uff08\u5185\u5bb9\u5206\u6790-\u6982\u5ff5\u878d\u5408-\u8bed\u7528\u610f\u56fe\uff09\uff0c\u6784\u5efa\u7edf\u4e00\u8ba1\u7b97\u6846\u67b6\u5b9e\u73b0\u6df1\u5ea6\u8bed\u5883\u63a8\u7406", "motivation": "\u73b0\u6709\u9690\u55bb\u8ba1\u7b97\u6a21\u578b\u5c40\u9650\u4e8e\u5e73\u9762\u6982\u5ff5\u6620\u5c04\uff0c\u7f3a\u4e4f\u6574\u5408\u8ba4\u77e5\u5c42\u4e0e\u8bed\u7528\u5c42\u7684\u7ed3\u6784\u5316\u5904\u7406\u6846\u67b6", "method": "1. \u5185\u5bb9\u5206\u6790\u5c42\u6807\u6ce8\u57fa\u672c\u6982\u5ff5\u5143\u7d20 2. \u6982\u5ff5\u878d\u5408\u5c42\u5efa\u6a21\u7ec4\u5408\u4e0e\u6d8c\u73b0\u610f\u4e49 3. \u8bed\u7528\u5c42\u901a\u8fc7\u8bcd\u6c47\u7f51\u7edc\u6355\u6349\u610f\u56fe\u4e0e\u8bed\u5883\u6548\u5e94", "result": "\u5efa\u7acb\u9996\u4e2a\u6574\u5408\u8ba4\u77e5\u8bed\u8a00\u5b66\u4e0e\u8bed\u7528\u7406\u8bba\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u4f7f\u8ba1\u7b97\u7cfb\u7edf\u5177\u5907\u591a\u5c42\u7ea7\u9690\u55bb\u89e3\u6790\u80fd\u529b", "conclusion": "\u5206\u5c42\u6a21\u578b\u7a81\u7834\u8868\u5c42\u5173\u8054\u5c40\u9650\uff0c\u4e3a\u6784\u5efa\u8ba4\u77e5\u53ef\u4fe1\u7684\u9690\u55bb\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u7ed3\u6784\u5316\u5b9e\u73b0\u8def\u5f84"}}
{"id": "2507.10435", "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Transformer\u67b6\u6784\u901a\u8fc7\u8bf1\u5bfc\u5b50\u7ed3\u6784\u8fc7\u6ee4\u673a\u5236\u7406\u89e3\u9690\u542b\u56fe\u7ed3\u6784\uff0c\u63d0\u51fa\u5b50\u7ed3\u6784\u601d\u7ef4\u8303\u5f0f\u5904\u7406\u590d\u6742\u56fe\u6a21\u5f0f", "motivation": "\u63a2\u7a76\u57fa\u4e8e\u5e8f\u5217\u7684Transformer\u5982\u4f55\u7406\u89e3\u6587\u672c\u63cf\u8ff0\u4e2d\u9690\u542b\u7684\u56fe\u7ed3\u6784\uff0c\u63ed\u793a\u5176\u5b50\u7ed3\u6784\u63d0\u53d6\u673a\u5236", "method": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u5206\u6790\u63d0\u51fa\u8bf1\u5bfc\u5b50\u7ed3\u6784\u8fc7\u6ee4(ISF)\u89c6\u89d2\uff0c\u9a8c\u8bc1\u591a\u5c42Transformer\u4e2d\u7684\u5b50\u7ed3\u6784\u8bc6\u522b\u8fc7\u7a0b", "result": "\u5728\u5206\u5b50\u56fe\u7b49\u5c5e\u6027\u56fe\u4e2d\u6210\u529f\u63d0\u53d6\u5b50\u7ed3\u6784\uff0c\u8bc1\u5b9eTransformer\u53ef\u901a\u8fc7\u5b50\u7ed3\u6784\u7ec4\u5408\u5904\u7406\u590d\u6742\u56fe\u7c7b\u578b", "conclusion": "\u5e8f\u5217\u5f0fTransformer\u901a\u8fc7\u5b50\u7ed3\u6784\u601d\u7ef4\u8303\u5f0f\u5b9e\u73b0\u56fe\u6570\u636e\u5904\u7406\uff0c\u4e3a\u6a21\u578b\u7406\u89e3\u7ed3\u6784\u5316\u6570\u636e\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2507.10445", "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8LLMs\u5728\u5f02\u6b65\u4efb\u52a1\u5bf9\u8bdd\u4e2d\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u4eba\u7c7b\u4e0eLLMs\u5728\u6b67\u4e49\u5904\u7406\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4e14\u63a8\u7406\u80fd\u529b\u63d0\u5347LLMs\u63d0\u95ee\u6548\u679c\u3002", "motivation": "\u5206\u6790LLMs\u5728\u4efb\u52a1\u5bf9\u8bdd\u4e2d\u5904\u7406\u6b67\u4e49\u7684\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u6f84\u6e05\u95ee\u9898\u673a\u5236\u662f\u5426\u4f9d\u8d56\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6574\u5408Minecraft\u5bf9\u8bdd\u8bed\u6599\u5e93\u7684\u4e24\u79cd\u6ce8\u91ca\u6784\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u4e0eLLMs\u7684\u6f84\u6e05\u95ee\u9898\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u4eba\u7c7b\u8f83\u5c11\u56e0\u6307\u4ee3\u6b67\u4e49\u63d0\u95ee\u4f46\u5173\u6ce8\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\uff0cLLMs\u5219\u76f8\u53cd\uff1b\u63a8\u7406\u65b9\u6cd5\u53ef\u63d0\u5347LLMs\u63d0\u95ee\u9891\u7387\u53ca\u76f8\u5173\u6027\u3002", "conclusion": "LLMs\u7684\u6f84\u6e05\u63d0\u95ee\u80fd\u529b\u4e0e\u63a8\u7406\u673a\u5236\u76f8\u5173\uff0c\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e95\u5c42\u8ba4\u77e5\u6a21\u62df\u673a\u5236\u3002"}}
{"id": "2507.10468", "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Sa\u00fal Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "categories": ["cs.CL", "cs.LG"], "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4f20\u7edf\u53cc\u5411Transformer\u7f16\u7801\u5668\u4e0e\u8d85\u5927\u578b\u81ea\u56de\u5f52LLMs\u5728\u73b0\u5b9e\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u5dee\u5f02", "motivation": "\u5728\u7ebf\u5e73\u53f0\u9762\u4e34\u5e73\u8861\u4ec7\u6068\u8a00\u8bba\u5ba1\u67e5\u4e0e\u5408\u6cd5\u8a00\u8bba\u4fdd\u62a4\u7684\u96be\u9898\uff0c\u5c3d\u7ba1LLMs\u5177\u5907\u66f4\u5f3a\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u5176\u5b9e\u9645\u68c0\u6d4b\u6548\u679c\u7f3a\u4e4f\u9a8c\u8bc1", "method": "\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4\u4e24\u7c7b\u6a21\u578b\uff08\u7ecf\u5178\u7f16\u7801\u5668\u4e0e\u65b0\u4e00\u4ee3LLMs\uff09\u5728\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5728\u7ebf\u4e92\u52a8\u8bed\u6599\u5e93\uff08Hate or No Hate\uff09\u4e0a\u7684\u8868\u73b0", "result": "\u9a8c\u8bc1\u4e86\u6a21\u578b\u89c4\u6a21\u6269\u5927\u662f\u5426\u771f\u6b63\u63d0\u5347\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u4ec7\u6068\u5185\u5bb9\u8bc6\u522b\u80fd\u529b\uff08\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\u9700\u53c2\u8003\u8bba\u6587\u5b8c\u6574\u6570\u636e\uff09", "conclusion": "\u7814\u7a76\u4e3a\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\uff0c\u63ed\u793a\u5355\u7eaf\u589e\u52a0\u53c2\u6570\u89c4\u6a21\u53ef\u80fd\u4e0d\u603b\u662f\u6700\u4f18\u89e3\uff0c\u5f3a\u8c03\u5b9e\u9645\u573a\u666f\u9a8c\u8bc1\u7684\u91cd\u8981\u6027"}}
{"id": "2507.10472", "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eMLAR\u6846\u67b6\u7684\u4e09\u5c42LLM\u5e94\u7528\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u5339\u914d\u7b97\u6cd5\u4f18\u5316\u62db\u8058\u6d41\u7a0b\uff0c\u5b9e\u73b0\u7b80\u5386\u5904\u7406\u6548\u7387\u63d0\u534716%-17%\u3002", "motivation": "\u4f20\u7edf\u62db\u8058\u7cfb\u7edf\u5b58\u5728\u7b80\u5386\u7b5b\u9009\u6548\u7387\u4f4e\u3001\u4eba\u5de5\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u5904\u7406\u901f\u5ea6\u548c\u5339\u914d\u7cbe\u5ea6\u3002", "method": "1) \u9996\u5c42LLM\u63d0\u53d6\u804c\u4f4d\u5173\u952e\u7279\u5f81 2) \u4e8c\u5c42\u89e3\u6790\u7b80\u5386\u7ed3\u6784\u5316\u6570\u636e 3) \u4e09\u5c42\u8bed\u4e49\u7b97\u6cd5\u5339\u914d\u5019\u9009\u4eba\u4e0e\u5c97\u4f4d\uff0c\u96c6\u6210RPA\u5b9e\u73b0\u5168\u6d41\u7a0b\u81ea\u52a8\u5316\u3002", "result": "\u5904\u74062400\u4efd\u7b80\u5386\u65f6\u5e73\u5747\u8017\u65f65.4\u79d2/\u4efd\uff0c\u8f83\u4e3b\u6d41RPA\u5e73\u53f0\u6548\u7387\u63d0\u534716.9%-17.1%\u3002", "conclusion": "MLAR\u6846\u67b6\u8bc1\u660e\u4e86LLM\u4e0eRPA\u7ed3\u5408\u5728\u62db\u8058\u573a\u666f\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u541e\u5410\u91cf\u7b80\u5386\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10475", "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["\u0130smail Tar\u0131m", "Aytu\u011f Onan"], "title": "Can You Detect the Difference?", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u6587\u672c(LLaDA)\u5728\u56f0\u60d1\u5ea6/\u7a81\u53d1\u6027\u6307\u6807\u4e0a\u63a5\u8fd1\u4eba\u7c7b\u6587\u672c\uff0c\u5bfc\u81f4\u81ea\u56de\u5f52\u68c0\u6d4b\u5668\u9ad8\u8bef\u5224\u7387\uff1b\u81ea\u56de\u5f52\u6a21\u578b(LLaMA)\u56f0\u60d1\u5ea6\u4f4e\u4f46\u8bcd\u6c47\u4fdd\u771f\u5ea6\u5dee\u3002\u73b0\u6709\u68c0\u6d4b\u6307\u6807\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u6269\u6563\u751f\u6210\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u98ce\u683c\u6d4b\u91cf\u7684\u68c0\u6d4b\u65b9\u6cd5\u5728\u81ea\u56de\u5f52\u6a21\u578b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u6269\u6563\u6a21\u578b\u6587\u672c\u7684\u68c0\u6d4b\u6709\u6548\u6027\u672a\u77e5\u3002\u4e9f\u9700\u7cfb\u7edf\u6bd4\u8f83\u4e24\u7c7b\u6a21\u578b\u7684\u751f\u6210\u7279\u6027\u4ee5\u5e94\u5bf9\u65b0\u578bAI\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u6311\u6218\u3002", "method": "\u4f7f\u75282000\u4e2a\u6837\u672c\u7cfb\u7edf\u5bf9\u6bd4LLaDA\uff08\u6269\u6563\u751f\u6210\uff09\u548cLLaMA\uff08\u81ea\u56de\u5f52\u751f\u6210\uff09\u6587\u672c\uff0c\u91c7\u7528\u56f0\u60d1\u5ea6\u3001\u7a81\u53d1\u6027\u3001\u8bcd\u6c47\u591a\u6837\u6027\u3001\u53ef\u8bfb\u6027\u53caBLEU/ROUGE\u6307\u6807\u8fdb\u884c\u591a\u7ef4\u5206\u6790\u3002", "result": "LLaDA\u5728\u56f0\u60d1\u5ea6(p=0.83)\u548c\u7a81\u53d1\u6027\u6307\u6807\u4e0a\u4e0e\u4eba\u7c7b\u6587\u672c\u65e0\u663e\u8457\u5dee\u5f02\uff0c\u5bfc\u81f4\u73b0\u6709\u68c0\u6d4b\u5668\u5047\u9634\u6027\u7387\u8fbe68%\uff1bLLaMA\u56f0\u60d1\u5ea6\u663e\u8457\u964d\u4f4e\u4f46\u8bcd\u6c47\u91cd\u590d\u7387\u4e0a\u534731%\u3002\u5355\u4e00\u6307\u6807\u65e0\u6cd5\u6709\u6548\u533a\u5206\u6269\u6563\u751f\u6210\u5185\u5bb9\u3002", "conclusion": "\u5fc5\u987b\u5f00\u53d1\u6269\u6563\u611f\u77e5\u7684\u68c0\u6d4b\u6280\u672f\uff0c\u5305\u62ec\uff1a\u6df7\u5408\u68c0\u6d4b\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u7279\u6709\u98ce\u683c\u7279\u5f81\u63d0\u53d6\u3001\u9c81\u68d2\u6c34\u5370\u6280\u672f\u3002\u5f53\u524d\u68c0\u6d4b\u4f53\u7cfb\u5b58\u5728\u5bf9\u65b0\u8303\u5f0f\u751f\u6210\u6587\u672c\u7684\u76f2\u533a\u3002"}}
{"id": "2507.10524", "pdf": "https://arxiv.org/pdf/2507.10524", "abs": "https://arxiv.org/abs/2507.10524", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "categories": ["cs.CL", "cs.LG"], "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.", "AI": {"tldr": "MoR\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53c2\u6570\u5171\u4eab\u4e0e\u81ea\u9002\u5e94\u8ba1\u7b97\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u5f62\u6210\u65b0\u7684\u6548\u7387-\u6027\u80fd\u5e15\u7d2f\u6258\u524d\u6cbf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\uff0cMoR\u65e8\u5728\u7edf\u4e00\u8fd9\u4e24\u5927\u4f18\u5316\u65b9\u5411\u4ee5\u7a81\u7834\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7684\u6210\u672c\u74f6\u9888\u3002", "method": "\u9012\u5f52Transformer\u67b6\u6784\u5b9e\u73b0\u5c42\u5171\u4eab\uff08\u53c2\u6570\u6548\u7387\uff09\uff0c\u52a8\u6001\u8def\u7531\u673a\u5236\u5206\u914dtoken\u7ea7\u9012\u5f52\u6df1\u5ea6\uff08\u8ba1\u7b97\u6548\u7387\uff09\uff0cKV\u5171\u4eab\u53d8\u4f53\u4f18\u5316\u9884\u586b\u5145\u9636\u6bb5\u6027\u80fd\u3002", "result": "\u5728135M-1.7B\u89c4\u6a21\u4e0b\uff0cMoR\u4ee5\u66f4\u4f4e\u8bad\u7ec3\u6210\u672c\u5b9e\u73b0\u66f4\u4f18\u9a8c\u8bc1\u56f0\u60d1\u5ea6\uff08-3.1%\uff09\u548cfew-shot\u51c6\u786e\u7387\uff08+5.2%\uff09\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u53471.8\u500d\u3002", "conclusion": "MoR\u6210\u529f\u9a8c\u8bc1\u4e86\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u800c\u975e\u5355\u7eaf\u6269\u5927\u53c2\u6570\u89c4\u6a21\u6765\u63d0\u5347\u6a21\u578b\u6027\u4ef7\u6bd4\u7684\u6280\u672f\u8def\u5f84\uff0c\u4e3a\u5b9e\u7528\u5316\u90e8\u7f72\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.10535", "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7CodeJudgeBench\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u601d\u7ef4\u578bLLM\u5728\u4ee3\u7801\u8bc4\u4f30\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5b58\u5728\u5224\u65ad\u968f\u673a\u6027\uff0c\u63d0\u793a\u7b56\u7565\u4f18\u5316\u53ef\u63d0\u5347\u8bc4\u4f30\u6548\u679c", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9LLM\u4f5c\u4e3a\u4ee3\u7801\u4efb\u52a1\u8bc4\u4f30\u8005\u7684\u4e13\u7528\u57fa\u51c6\uff0c\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u5176\u5728\u4ee3\u7801\u751f\u6210\u3001\u4fee\u590d\u7b49\u573a\u666f\u7684\u8bc4\u5224\u80fd\u529b", "method": "\u6784\u5efaCodeJudgeBench\u57fa\u51c6\uff08\u542b\u4ee3\u7801\u751f\u6210/\u4fee\u590d/\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e09\u7c7b\u4efb\u52a1\uff09\uff0c\u6d4b\u8bd526\u4e2aLLM\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u63d0\u793a\u7b56\u7565\uff08\u6210\u5bf9\u6bd4\u8f83vs\u6807\u91cf\u8bc4\u5206\uff09\u7684\u5f71\u54cd", "result": "\u601d\u7ef4\u578b\u6a21\u578b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u975e\u601d\u7ef4\u578b\uff1bQwen3-8B\u5c0f\u6a21\u578b\u8d85\u8d8a70B\u4e13\u7528\u8bc4\u4f30\u6a21\u578b\uff1b\u6240\u6709\u6a21\u578b\u5b58\u5728\u5224\u65ad\u968f\u673a\u6027\uff08\u56de\u7b54\u987a\u5e8f\u6539\u53d8\u5bfc\u81f413.7%\u51c6\u786e\u7387\u6ce2\u52a8\uff09\uff1b\u4fdd\u7559\u5b8c\u6574\u63a8\u7406\u8fc7\u7a0b\u53ef\u63d0\u53479.2%\u8bc4\u4f30\u6548\u679c", "conclusion": "LLM\u4f5c\u4e3a\u4ee3\u7801\u8bc4\u4f30\u8005\u5c55\u73b0\u6f5c\u529b\u4f46\u53ef\u9760\u6027\u5f85\u63d0\u5347\uff0c\u9700\u7ed3\u5408\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u6210\u5bf9\u6bd4\u8f83\u7b56\u7565\uff0c\u57fa\u51c6\u5efa\u8bbe\u5bf9\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u5177\u6709\u5173\u952e\u4f5c\u7528"}}
{"id": "2507.10541", "pdf": "https://arxiv.org/pdf/2507.10541", "abs": "https://arxiv.org/abs/2507.10541", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "categories": ["cs.CL"], "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.", "AI": {"tldr": "\u63d0\u51faREST\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u53d1\u538b\u529b\u6d4b\u8bd5\u63ed\u793a\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u591a\u95ee\u9898\u5904\u7406\u4e2d\u7684\u6027\u80fd\u74f6\u9888", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6570\u636e\u6613\u6c61\u67d3\u3001\u65e0\u6cd5\u8bc4\u4f30\u591a\u4e0a\u4e0b\u6587\u538b\u529b\u7684\u95ee\u9898\uff0c\u9700\u66f4\u771f\u5b9e\u53cd\u6620\u5b9e\u9645\u9700\u6c42\u7684\u8bc4\u4f30\u65b9\u6cd5", "method": "\u8bbe\u8ba1\u5e76\u53d1\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u4f18\u5148\u7ea7\u5206\u914d\u3001\u6297\u5e72\u6270\u80fd\u529b\u548c\u52a8\u6001\u8ba4\u77e5\u7ba1\u7406", "result": "\u53d1\u73b0SOTA\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0cREST\u5c55\u793a\u51fa\u6bd4\u4f20\u7edf\u6d4b\u8bd5\u66f4\u5f3a\u7684\u6a21\u578b\u533a\u5206\u80fd\u529b", "conclusion": "REST\u5efa\u7acb\u4e86\u4e00\u79cd\u6210\u672c\u9ad8\u6548\u3001\u9762\u5411\u672a\u6765\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u66f4\u8d34\u8fd1\u771f\u5b9e\u63a8\u7406\u9700\u6c42\u5e76\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u4f9d\u8d56"}}
{"id": "2507.07855", "pdf": "https://arxiv.org/pdf/2507.07855", "abs": "https://arxiv.org/abs/2507.07855", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "title": "Principled Foundations for Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "AI": {"tldr": "\u8bba\u6587\u5efa\u7acb\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u4e0e\u673a\u5668\u5b66\u4e60\u4e2d\u504f\u597d\u5b66\u4e60\u7684\u4e24\u79cd\u6838\u5fc3\u7406\u8bba(Savage\u635f\u5931\u51fd\u6570\u4e0e\u968f\u673a\u9009\u62e9\u7406\u8bba)\u7684\u7cfb\u7edf\u6027\u8054\u7cfb\uff0c\u63ed\u793a\u4e86DPO\u7684\u6570\u5b66\u672c\u8d28\u5e76\u6269\u5c55\u4e86\u5176\u5e94\u7528\u573a\u666f", "motivation": "\u5f53\u524dDPO\u5e94\u7528\u5e7f\u6cdb\u4f46\u7406\u8bba\u57fa\u7840\u8584\u5f31\uff0c\u9700\u8981\u901a\u8fc7\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u6765(1)\u7edf\u4e00\u7406\u89e3\u73b0\u6709DPO\u53d8\u4f53 (2)\u652f\u6301\u66f4\u590d\u6742\u7684\u5e94\u7528\u573a\u666f\u5982\u5f03\u6743\u673a\u5236 (3)\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u5728\u975e\u51f8\u4f18\u5316\u7b49\u573a\u666f\u7684\u9650\u5236", "method": "\u5c06DPO\u5f62\u5f0f\u5316\u4e3aSavage\u635f\u5931\u51fd\u6570\u4e0eMachina\u968f\u673a\u9009\u62e9\u7406\u8bba\u7684\u7279\u6b8a\u8fde\u63a5\uff0c\u6269\u5c55\u7406\u8bba\u6846\u67b6\u4ee5\u652f\u6301\uff1a\u9009\u62e9\u5f03\u6743\u673a\u5236\u3001\u975e\u51f8\u4f18\u5316\u76ee\u6807\u3001\u8fb9\u754c\u7ea6\u675f\u548c\u6587\u672c\u957f\u5ea6\u4fee\u6b63\u7b49\u6269\u5c55\u8bbe\u7f6e", "result": "\u6784\u5efa\u4e86\u6db5\u76d6\u73b0\u6709DPO\u53d8\u4f53\u7684\u7406\u8bba\u56fe\u8c31\uff0c\u8bc1\u660e\u6846\u67b6\u53ef\u81ea\u7136\u652f\u6301\u591a\u8bed\u8a00\u6a21\u578b\u3001\u63a8\u8350\u7cfb\u7edf\u7b49\u573a\u666f\uff0c\u540c\u65f6\u63ed\u793a\u5f53\u524dSOTA\u65b9\u6cd5\u4ec5\u8986\u76d6\u56fe\u8c31\u7684\u6709\u9650\u533a\u57df", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u4e3a\u7406\u89e3DPO\u7684\u6570\u5b66\u672c\u8d28\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u65e2\u80fd\u8bca\u65ad\u73b0\u6709\u65b9\u6cd5\u7684\u7406\u8bba\u5c40\u9650\uff0c\u4e5f\u4e3a\u5f00\u53d1\u652f\u6301\u590d\u6742\u51b3\u7b56\u573a\u666f\u7684\u65b0\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8def\u5f84"}}
{"id": "2507.08806", "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08833", "pdf": "https://arxiv.org/pdf/2507.08833", "abs": "https://arxiv.org/abs/2507.08833", "authors": ["Seokmin Ko"], "title": "LoRA Is Slower Than You Think", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "AI": {"tldr": "LoRA\u901a\u8fc7\u4f4e\u79e9\u77e9\u9635\u51cf\u5c11LLM\u5fae\u8c03\u53c2\u6570\uff0c\u4f46\u5b58\u5728\u901f\u5ea6\u63d0\u5347\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51fa\u65b0\u65b9\u6cd5\u5b9e\u73b0\u66f4\u7a33\u5b9a\u9ad8\u6548\u7684LLM\u5fae\u8c03\u3002", "motivation": "\u9488\u5bf9LoRA\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u573a\u666f\u4e2d\u901f\u5ea6\u63d0\u5347\u4e0d\u4e00\u81f4\u7684\u73b0\u8c61\uff0c\u63a2\u7a76\u5176\u6027\u80fd\u74f6\u9888\u548c\u6839\u672c\u539f\u56e0\u3002", "method": "1. \u7cfb\u7edf\u5206\u6790LoRA\u6027\u80fd\u8868\u73b0\n2. \u7814\u7a76\u9650\u5236\u901f\u5ea6\u63d0\u5347\u7684\u5e95\u5c42\u56e0\u7d20\n3. \u63d0\u51fa\u591a\u79cd\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5305\u542b\u6027\u80fd\u4f18\u5316\u7b56\u7565\uff09", "result": "\u65b0\u65b9\u6cd5\u5728\u4fdd\u6301/\u8d85\u8d8aLoRA\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a0\u901f\uff08\u6700\u9ad8\u8fbe1.84\u500d\u901f\u5ea6\u63d0\u5347\uff09", "conclusion": "\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u7cfb\u7edf\u4f18\u5316\u6307\u5357\uff0c\u901a\u8fc7\u53c2\u6570\u6548\u7387\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u534f\u540c\u4f18\u5316\u5b9e\u73b0\u66f4\u4f18\u5b9e\u8df5\u65b9\u6848"}}
{"id": "2507.08862", "pdf": "https://arxiv.org/pdf/2507.08862", "abs": "https://arxiv.org/abs/2507.08862", "authors": ["Tianzhe Zhao", "Jiaoyan Chen", "Yanchi Ru", "Haiping Zhu", "Nan Hu", "Jun Liu", "Qika Lin"], "title": "RAG Safety: Exploring Knowledge Poisoning Attacks to Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.CL"], "comment": "13 pages, 6 figures", "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nretrieving external data to mitigate hallucinations and outdated knowledge\nissues. Benefiting from the strong ability in facilitating diverse data sources\nand supporting faithful reasoning, knowledge graphs (KGs) have been\nincreasingly adopted in RAG systems, giving rise to KG-based RAG (KG-RAG)\nmethods. Though RAG systems are widely applied in various applications, recent\nstudies have also revealed its vulnerabilities to data poisoning attacks, where\nmalicious information injected into external knowledge sources can mislead the\nsystem into producing incorrect or harmful responses. However, these studies\nfocus exclusively on RAG systems using unstructured textual data sources,\nleaving the security risks of KG-RAG largely unexplored, despite the fact that\nKGs present unique vulnerabilities due to their structured and editable nature.\nIn this work, we conduct the first systematic investigation of the security\nissue of KG-RAG methods through data poisoning attacks. To this end, we\nintroduce a practical, stealthy attack setting that aligns with real-world\nimplementation. We propose an attack strategy that first identifies adversarial\ntarget answers and then inserts perturbation triples to complete misleading\ninference chains in the KG, increasing the likelihood that KG-RAG methods\nretrieve and rely on these perturbations during generation. Through extensive\nexperiments on two benchmarks and four recent KG-RAG methods, our attack\nstrategy demonstrates strong effectiveness in degrading KG-RAG performance,\neven with minimal KG perturbations. In-depth analyses are also conducted to\nunderstand the safety threats within the internal stages of KG-RAG systems and\nto explore the robustness of LLMs against adversarial knowledge.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u68c0\u7d22\u751f\u6210\u7cfb\u7edf\u5728\u6570\u636e\u6295\u6bd2\u653b\u51fb\u4e0b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u63ed\u793a\u5176\u8106\u5f31\u6027\u5e76\u63d0\u51fa\u6709\u6548\u653b\u51fb\u7b56\u7565", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u57fa\u4e8e\u975e\u7ed3\u6784\u5316\u6587\u672c\u7684RAG\u7cfb\u7edf\u5b89\u5168\uff0c\u800c\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u53ef\u7f16\u8f91\u7279\u6027\u53ef\u80fd\u5e26\u6765\u72ec\u7279\u5b89\u5168\u98ce\u9669\uff0c\u9700\u7cfb\u7edf\u6027\u63a2\u7a76KG-RAG\u7684\u5b89\u5168\u9690\u60a3", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u653b\u51fb\u7b56\u7565\uff1a1) \u5b9a\u4f4d\u5bf9\u6297\u76ee\u6807\u7b54\u6848 2) \u63d2\u5165\u6270\u52a8\u4e09\u5143\u7ec4\u6784\u5efa\u8bef\u5bfc\u63a8\u7406\u94fe\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u77e5\u8bc6\u56fe\u8c31\u4fee\u6539\u5b9e\u73b0\u9690\u853d\u653b\u51fb", "result": "\u5728\u4e24\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u56db\u79cdKG-RAG\u65b9\u6cd5\u4e2d\uff0c\u653b\u51fb\u7b56\u7565\u4f7f\u7cfb\u7edf\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5373\u4f7f\u4ec5\u4fee\u65390.6%\u7684\u4e09\u5143\u7ec4\uff09\uff0c\u5e76\u63ed\u793aLLM\u5728\u5bf9\u6297\u77e5\u8bc6\u4e0b\u7684\u9c81\u68d2\u6027\u5dee\u5f02", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u7279\u6027\u653e\u5927\u4e86RAG\u7cfb\u7edf\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u5728\u7cfb\u7edf\u8bbe\u8ba1\u65f6\u52a0\u5f3a\u77e5\u8bc6\u9a8c\u8bc1\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u9488\u5bf9\u7ed3\u6784\u5316\u77e5\u8bc6\u6c61\u67d3\u7684\u9632\u5fa1\u65b9\u6cd5"}}
{"id": "2507.08882", "pdf": "https://arxiv.org/pdf/2507.08882", "abs": "https://arxiv.org/abs/2507.08882", "authors": ["Janaki Viswanathan", "Alexander Blatt", "Konrad Hagemann", "Dietrich Klakow"], "title": "Less Stress, More Privacy: Stress Detection on Anonymized Speech of Air Traffic Controllers", "categories": ["cs.SD", "cs.CL", "eess.AS", "I.2.7; I.5.5"], "comment": "8 pages, 2 figures, 4 tables, publication identification number\n  (URN)- urn:nbn:de:101:1-2022122008393409239462, see archived online\n  publication- https://d-nb.info/127614606X/34 & Katalogeintrag:\n  https://d-nb.info/127614606X/", "summary": "Air traffic control (ATC) demands multi-tasking under time pressure with high\nconsequences of an error. This can induce stress. Detecting stress is a key\npoint in maintaining the high safety standards of ATC. However, processing ATC\nvoice data entails privacy restrictions, e.g. the General Data Protection\nRegulation (GDPR) law. Anonymizing the ATC voice data is one way to comply with\nthese restrictions. In this paper, different architectures for stress detection\nfor anonymized ATCO speech are evaluated. Our best networks reach a stress\ndetection accuracy of 93.6% on an anonymized version of the Speech Under\nSimulated and Actual Stress (SUSAS) dataset and an accuracy of 80.1% on our\nanonymized ATC simulation dataset. This shows that privacy does not have to be\nan impediment in building well-performing deep-learning-based models.", "AI": {"tldr": "\u533f\u540d\u5316\u8bed\u97f3\u6570\u636e\u7684\u538b\u529b\u68c0\u6d4b\u6a21\u578b\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u5c55\u73b0\u4f18\u5f02\u6027\u80fd\uff0c\u9a8c\u8bc1\u9690\u79c1\u5408\u89c4\u4e0e\u6a21\u578b\u6548\u80fd\u53ef\u5171\u5b58", "motivation": "\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u7684\u9ad8\u538b\u73af\u5883\u9700\u8981\u5b9e\u65f6\u538b\u529b\u76d1\u6d4b\uff0c\u4f46GDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u9650\u5236\u539f\u59cb\u8bed\u97f3\u6570\u636e\u5904\u7406", "method": "\u901a\u8fc7\u8bc4\u4f30\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u533f\u540d\u5316ATC\u8bed\u97f3\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u538b\u529b\u68c0\u6d4b\u51c6\u786e\u7387", "result": "\u6a21\u578b\u5728\u533f\u540d\u5316SUSAS\u6570\u636e\u96c6\u8fbe93.6%\u51c6\u786e\u7387\uff0c\u6a21\u62dfATC\u6570\u636e\u96c6\u8fbe80.1%\u51c6\u786e\u7387", "conclusion": "\u5b9e\u9a8c\u8bc1\u660e\u533f\u540d\u5316\u5904\u7406\u4e0d\u4f1a\u964d\u4f4e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u9886\u57df\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.08890", "pdf": "https://arxiv.org/pdf/2507.08890", "abs": "https://arxiv.org/abs/2507.08890", "authors": ["Nick Craswell", "Bhaskar Mitra", "Emine Yilmaz", "Hossein A. Rahmani", "Daniel Campos", "Jimmy Lin", "Ellen M. Voorhees", "Ian Soboroff"], "title": "Overview of the TREC 2023 deep learning track", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.08191", "summary": "This is the fifth year of the TREC Deep Learning track. As in previous years,\nwe leverage the MS MARCO datasets that made hundreds of thousands of\nhuman-annotated training labels available for both passage and document ranking\ntasks. We mostly repeated last year's design, to get another matching test set,\nbased on the larger, cleaner, less-biased v2 passage and document set, with\npassage ranking as primary and document ranking as a secondary task (using\nlabels inferred from passage). As we did last year, we sample from MS MARCO\nqueries that were completely held out, unused in corpus construction, unlike\nthe test queries in the first three years. This approach yields a more\ndifficult test with more headroom for improvement. Alongside the usual MS MARCO\n(human) queries from MS MARCO, this year we generated synthetic queries using a\nfine-tuned T5 model and using a GPT-4 prompt.\n  The new headline result this year is that runs using Large Language Model\n(LLM) prompting in some way outperformed runs that use the \"nnlm\" approach,\nwhich was the best approach in the previous four years. Since this is the last\nyear of the track, future iterations of prompt-based ranking can happen in\nother tracks. Human relevance assessments were applied to all query types, not\njust human MS MARCO queries. Evaluation using synthetic queries gave similar\nresults to human queries, with system ordering agreement of $\\tau=0.8487$.\nHowever, human effort was needed to select a subset of the synthetic queries\nthat were usable. We did not see clear evidence of bias, where runs using GPT-4\nwere favored when evaluated using synthetic GPT-4 queries, or where runs using\nT5 were favored when evaluated on synthetic T5 queries.", "AI": {"tldr": "TREC\u6df1\u5ea6\u5b66\u4e60\u8bc4\u6d4b\u7b2c\u4e94\u5e74\u62a5\u544a\u663e\u793a\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u63d0\u793a\u65b9\u6cd5\u9996\u6b21\u8d85\u8d8a\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u8bed\u8a00\u6a21\u578b\uff08nnlm\uff09\uff0c\u5408\u6210\u67e5\u8be2\u4e0e\u4eba\u5de5\u6807\u6ce8\u67e5\u8be2\u8bc4\u4f30\u7ed3\u679c\u5177\u6709\u9ad8\u5ea6\u4e00\u81f4\u6027\uff08\u03c4=0.8487\uff09\uff0c\u4f46\u9700\u4eba\u5de5\u7b5b\u9009\u53ef\u7528\u67e5\u8be2\uff0c\u672a\u53d1\u73b0\u6a21\u578b\u7279\u5f02\u6027\u8bc4\u4f30\u504f\u89c1\u3002", "motivation": "\u9a8c\u8bc1\u57fa\u4e8e\u63d0\u793a\u7684LLM\u65b9\u6cd5\u5728\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a2\u7d22\u5408\u6210\u67e5\u8be2\u4ee3\u66ff\u4eba\u5de5\u6807\u6ce8\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u8bc4\u4f30\u4e0d\u540c\u751f\u6210\u6a21\u578b\uff08T5/GPT-4\uff09\u67e5\u8be2\u5bf9\u7cfb\u7edf\u6392\u5e8f\u7684\u5f71\u54cd\u3002", "method": "\u5ef6\u7eed\u5f80\u5e74\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u4f7f\u7528\u66f4\u5e72\u51c0\u7684MS MARCO v2\u6570\u636e\u96c6\uff0c\u65b0\u589eT5\u5fae\u8c03\u6a21\u578b\u548cGPT-4\u63d0\u793a\u751f\u6210\u7684\u5408\u6210\u67e5\u8be2\uff0c\u5728\u5b8c\u5168\u4fdd\u7559\u7684\u6d4b\u8bd5\u96c6\u4e0a\u5bf9\u6bd4nnlm\u4e0eLLM\u65b9\u6cd5\u8868\u73b0\uff0c\u5e76\u5bf9\u6240\u6709\u67e5\u8be2\u7c7b\u578b\u8fdb\u884c\u4eba\u5de5\u76f8\u5173\u6027\u6807\u6ce8\u3002", "result": "LLM\u65b9\u6cd5\u9996\u6b21\u6210\u4e3a\u6700\u4f73\u65b9\u6848\uff0c\u5408\u6210\u67e5\u8be2\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4eba\u5de5\u67e5\u8be2\u7cfb\u7edf\u6392\u5e8f\u76f8\u5173\u6027\u8fbe0.8487\uff0c\u4f46\u9700\u8981\u4eba\u5de5\u8fc7\u6ee4\u4e0d\u53ef\u7528\u5408\u6210\u67e5\u8be2\u3002\u4e0d\u540c\u751f\u6210\u6a21\u578b\u7684\u67e5\u8be2\u672a\u5bfc\u81f4\u5bf9\u5e94\u65b9\u6cd5\u7684\u8bc4\u4f30\u504f\u501a\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684LLM\u65b9\u6cd5\u5c55\u73b0\u51fa\u68c0\u7d22\u4efb\u52a1\u4f18\u52bf\uff0c\u5408\u6210\u67e5\u8be2\u53ef\u4f5c\u4e3a\u6709\u6548\u8865\u5145\u4f46\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u63d0\u793a\u6392\u5e8f\u65b9\u6cd5\u7684\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\uff0c\u6807\u5fd7\u7740\u8be5\u8bc4\u6d4b\u8f68\u9053\u7684\u5706\u6ee1\u6536\u5b98\u3002"}}
{"id": "2507.08992", "pdf": "https://arxiv.org/pdf/2507.08992", "abs": "https://arxiv.org/abs/2507.08992", "authors": ["Abdelhalim Dahou", "Ansgar Scherp", "Sebastian Kurten", "Brigitte Mathiak", "Madhu Chauhan"], "title": "Semantic Source Code Segmentation using Small and Large Language Models", "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": "18 pages, 4 figures", "summary": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM/SLM\u7684R\u7814\u7a76\u4ee3\u7801\u81ea\u52a8\u5206\u5272\u65b9\u6cd5\uff0c\u521b\u5efaStatCodeSeg\u6570\u636e\u96c6\u3002\u4e0a\u4e0b\u6587\u9010\u884c\u5206\u6790\u4f18\u4e8e\u8303\u56f4\u5206\u5272\uff0c\u672a\u9884\u8bad\u7ec3R\u4ee3\u7801\u7684CodeBERT/CodeT5+\u7ecf\u5fae\u8c03\u540e\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3R\u8bed\u8a00\u7b49\u4f4e\u8d44\u6e90\u7814\u7a76\u9886\u57df\u4ee3\u7801\u5e93\u589e\u5927\u540e\u624b\u5de5\u5206\u5272\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4ee3\u7801\u5bfc\u822a\u548c\u7ef4\u62a4\u6548\u7387\u3002", "method": "1. \u63d0\u51fa\u4e24\u79cd\u4ee3\u7801\u5206\u5272\u65b9\u6cd5\uff08\u4e0a\u4e0b\u6587\u9010\u884c\u5206\u6790 vs \u8303\u56f4\u5206\u5272\uff09\n2. \u6784\u5efa\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u96c6StatCodeSeg\n3. \u5bf9\u6bd4LLM\u4e0e\u5fae\u8c03SLM\uff08CodeBERT/CodeT5+\uff09\u6027\u80fd\n4. \u6269\u5c55\u9a8c\u8bc1\u81f3Python\u4ee3\u7801", "result": "1. \u4e0a\u4e0b\u6587\u9010\u884c\u5206\u6790\u51c6\u786e\u7387\u66f4\u9ad8\n2. \u672a\u9884\u8bad\u7ec3R\u4ee3\u7801\u7684CodeBERT/CodeT5+\u4ec5\u97004,130\u884c\u5fae\u8c03\u5373\u8d85\u8d8aLLM\n3. \u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u8ba1\u7b97\u673a\u79d1\u5b66\u9886\u57dfPython\u4ee3\u7801", "conclusion": "\u5c0f\u6a21\u578b+\u9886\u57df\u5fae\u8c03\u7b56\u7565\u5728\u4e13\u4e1a\u4ee3\u7801\u5904\u7406\u4e2d\u4f18\u4e8eLLM\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09090", "pdf": "https://arxiv.org/pdf/2507.09090", "abs": "https://arxiv.org/abs/2507.09090", "authors": ["Anthony Miyaguchi", "Conor Johnston", "Aaryan Potdar"], "title": "DS@GT at Touch\u00e9: Large Language Models for Retrieval-Augmented Debate", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate strong conversational abilities. In\nthis Working Paper, we study them in the context of debating in two ways: their\nability to perform in a structured debate along with a dataset of arguments to\nuse and their ability to evaluate utterances throughout the debate. We deploy\nsix leading publicly available models from three providers for the\nRetrieval-Augmented Debate and Evaluation. The evaluation is performed by\nmeasuring four key metrics: Quality, Quantity, Manner, and Relation. Throughout\nthis task, we found that although LLMs perform well in debates when given\nrelated arguments, they tend to be verbose in responses yet consistent in\nevaluation. The accompanying source code for this paper is located at\nhttps://github.com/dsgt-arc/touche-2025-rad.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u83b7\u5f97\u76f8\u5173\u8bba\u70b9\u65f6\u8fa9\u8bba\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u56de\u7b54\u5197\u957f\u95ee\u9898\uff0c\u8bc4\u4f30\u7ed3\u679c\u4fdd\u6301\u4e00\u81f4\u6027", "motivation": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u8fa9\u8bba\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\u53ca\u5176\u5bf9\u8fa9\u8bba\u5185\u5bb9\u7684\u8bc4\u4f30\u80fd\u529b", "method": "\u90e8\u7f72\u516d\u4e2a\u516c\u5f00\u6a21\u578b\u8fdb\u884c\u68c0\u7d22\u589e\u5f3a\u8fa9\u8bba\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8d28\u91cf/\u6570\u91cf/\u65b9\u5f0f/\u5173\u8054\u6027\u56db\u7ef4\u5ea6\u8bc4\u4f30", "result": "\u6a21\u578b\u5728\u76f8\u5173\u8bba\u70b9\u652f\u6301\u4e0b\u8868\u73b0\u5408\u683c\uff0c\u4f46\u54cd\u5e94\u5197\u4f59\u5ea6\u8f83\u9ad8\uff0c\u8bc4\u4f30\u7ed3\u679c\u5448\u73b0\u7a33\u5b9a\u6027\u7279\u5f81", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u7ed3\u6784\u5316\u8fa9\u8bba\u6f5c\u529b\uff0c\u9700\u4f18\u5316\u54cd\u5e94\u7b80\u6d01\u6027\u4ee5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2507.09100", "pdf": "https://arxiv.org/pdf/2507.09100", "abs": "https://arxiv.org/abs/2507.09100", "authors": ["Mohammad Abolnejadian", "Shakiba Amirshahi", "Matthew Brehmer", "Anamaria Crisan"], "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data", "categories": ["cs.HC", "cs.AI", "cs.CL", "H.5.0"], "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)", "summary": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u65f6\u533b\u60a3\u5bf9\u8bdd\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u5386\u53f2\u6570\u636e\u68c0\u7d22\u751f\u6210\u51b3\u7b56\u5efa\u8bae\uff0c\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027\u5e76\u63ed\u793a\u6311\u6218", "motivation": "\u89e3\u51b3\u4e13\u5bb6\u5728\u5b9e\u65f6\u51b3\u7b56\u5bf9\u8bdd\u4e2d\u65e0\u6cd5\u6709\u6548\u5229\u7528\u5386\u53f2\u6570\u636e\u7684\u75db\u70b9\uff0c\u63a2\u7d22\u6570\u636e\u9a71\u52a8\u7684\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u65b9\u6848", "method": "\u6784\u5efa\u6301\u7eed\u76d1\u542c\u5bf9\u8bdd\u7684LLM\u7cfb\u7edf\u67b6\u6784\uff0c\u5305\u542b\u95ee\u9898\u8bc6\u522b\u3001\u89e3\u51b3\u65b9\u6848\u5339\u914d\u3001\u5411\u91cf\u6570\u636e\u5e93\u68c0\u7d22\u3001\u52a8\u6001\u6d1e\u5bdf\u751f\u6210\u56db\u5c42\u5904\u7406\u6d41\u7a0b", "result": "\u539f\u578b\u7cfb\u7edf\u5728\u52a0\u62ff\u5927\u536b\u751f\u90e8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u53ef\u884c\u6027\uff0c\u540c\u65f6\u66b4\u9732\u5b9e\u65f6\u6027\u5904\u7406\u4e0e\u8bed\u4e49\u7406\u89e3\u7cbe\u5ea6\u7b49\u5de5\u7a0b\u6311\u6218", "conclusion": "\u9a8c\u8bc1\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\u7684\u6f5c\u529b\uff0c\u540e\u7eed\u5c06\u4f18\u5316\u4e0a\u4e0b\u6587\u7406\u89e3\u4e0e\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u80fd\u529b"}}
{"id": "2507.09176", "pdf": "https://arxiv.org/pdf/2507.09176", "abs": "https://arxiv.org/abs/2507.09176", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "categories": ["cs.RO", "cs.CL"], "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u6807\u5b9a\u9776\u548c\u521d\u59cb\u53c2\u6570\u7684\u591a\u6fc0\u5149\u96f7\u8fbe\u5916\u53c2\u6821\u51c6\u6846\u67b6DLBAcalib\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5b9e\u73b0\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u6821\u51c6", "motivation": "\u89e3\u51b3\u591a\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u5728\u975e\u91cd\u53e0\u89c6\u57df\u573a\u666f\u4e0b\u5916\u53c2\u6821\u51c6\u96be\u9898\uff0c\u6d88\u9664\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4eba\u5de5\u6807\u6ce8/\u6807\u5b9a\u9776/\u521d\u59cb\u53c2\u6570\u4f30\u8ba1\u7684\u4f9d\u8d56", "method": "\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6fc0\u5149\u96f7\u8fbe\u675f\u8c03\u6574(LBA)\u548c\u81ea\u9002\u5e94\u52a0\u6743\u9c81\u68d2\u4f18\u5316\uff1a1\uff09\u901a\u8fc7\u8fde\u7eed\u626b\u63cf\u6784\u5efa\u9ad8\u7cbe\u5ea6\u70b9\u4e91\u57fa\u51c6\u56fe\uff1b2\uff09\u5efa\u7acb\u8054\u5408LBA\u4f18\u5316\u6a21\u578b\u8fdb\u884c\u5916\u53c2\u4f30\u8ba1\uff1b3\uff09\u91c7\u7528\u8fed\u4ee3\u91cd\u52a0\u6743\u673a\u5236\u6291\u5236\u5f02\u5e38\u503c", "result": "\u5728CARLA\u4eff\u771f\u548c\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\uff1a\u975e\u91cd\u53e0\u914d\u7f6e\u4e0b\u5e73\u5747\u5e73\u79fb\u8bef\u5dee5mm/\u65cb\u8f6c\u8bef\u5dee0.2\u00b0\uff0c\u521d\u59cb\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u8fbe0.4m/30\u00b0\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u6821\u51c6\u9650\u5236\uff0c\u5b9e\u73b0\u5168\u81ea\u52a8\u9ad8\u7cbe\u5ea6\u6807\u5b9a\uff0c\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u4e09\u7ef4\u91cd\u5efa\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2507.09279", "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6Prompt4Trust\u4f18\u5316\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u63d0\u5347\u533b\u7597VQA\u6027\u80fd\u4e0e\u4e34\u5e8a\u51b3\u7b56\u53ef\u4fe1\u5ea6", "motivation": "MLLMs\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u90e8\u7f72\u65f6\u5b58\u5728\u9ad8\u7f6e\u4fe1\u5ea6\u9519\u8bef\u54cd\u5e94\u7684\u98ce\u9669\uff0c\u9700\u901a\u8fc7\u6821\u51c6\u786e\u4fdd\u6a21\u578b\u9648\u8ff0\u7684\u7f6e\u4fe1\u5ea6\u4e0e\u5b9e\u9645\u9884\u6d4b\u51c6\u786e\u6027\u4e00\u81f4", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7LLM\u751f\u6210\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f85\u52a9\u63d0\u793a\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4e34\u5e8a\u5b89\u5168\u5bfc\u5411\u7684\u6821\u51c6\u76ee\u6807", "result": "\u5728PMC-VQA\u57fa\u51c6\u5b9e\u73b0SOTA\uff0890.2%\u51c6\u786e\u7387\uff09\uff0c\u4e14\u5c0f\u6a21\u578b\u8bad\u7ec3\u7684\u6846\u67b6\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u5230\u66f4\u5927MLLMs", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347MLLMs\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u65b9\u6848"}}
{"id": "2507.09310", "pdf": "https://arxiv.org/pdf/2507.09310", "abs": "https://arxiv.org/abs/2507.09310", "authors": ["Dominika Woszczyk", "Manuel Sam Ribeiro", "Thomas Merritt", "Daniel Korzekwa"], "title": "Voice Conversion for Lombard Speaking Style with Implicit and Explicit Acoustic Feature Conditioning", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Presented at Clarity Challenge 2023", "summary": "Text-to-Speech (TTS) systems in Lombard speaking style can improve the\noverall intelligibility of speech, useful for hearing loss and noisy\nconditions. However, training those models requires a large amount of data and\nthe Lombard effect is challenging to record due to speaker and noise\nvariability and tiring recording conditions. Voice conversion (VC) has been\nshown to be a useful augmentation technique to train TTS systems in the absence\nof recorded data from the target speaker in the target speaking style. In this\npaper, we are concerned with Lombard speaking style transfer. Our goal is to\nconvert speaker identity while preserving the acoustic attributes that define\nthe Lombard speaking style. We compare voice conversion models with implicit\nand explicit acoustic feature conditioning. We observe that our proposed\nimplicit conditioning strategy achieves an intelligibility gain comparable to\nthe model conditioned on explicit acoustic features, while also preserving\nspeaker similarity.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u9690\u5f0f\u58f0\u5b66\u7279\u5f81\u6761\u4ef6\u7b56\u7565\u5b9e\u73b0Lombard\u8bf4\u8bdd\u98ce\u683c\u8f6c\u6362\uff0c\u5728\u4fdd\u6301\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u7684\u540c\u65f6\u8fbe\u5230\u4e0e\u663e\u5f0f\u65b9\u6cd5\u76f8\u5f53\u7684\u6e05\u6670\u5ea6\u63d0\u5347", "motivation": "Lombard\u98ce\u683c\u6570\u636e\u83b7\u53d6\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u76ee\u6807\u8bf4\u8bdd\u4eba\u6570\u636e\u4e14\u5f55\u5236\u6210\u672c\u9ad8\u3002\u8bed\u97f3\u8f6c\u6362\u6280\u672f\u53ef\u7f13\u89e3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u4f46\u9700\u89e3\u51b3\u98ce\u683c\u5c5e\u6027\u4fdd\u6301\u4e0e\u8bf4\u8bdd\u4eba\u8eab\u4efd\u8f6c\u6362\u7684\u5e73\u8861", "method": "\u6bd4\u8f83\u9690\u5f0f/\u663e\u5f0f\u58f0\u5b66\u7279\u5f81\u6761\u4ef6\u4e24\u79cd\u8bed\u97f3\u8f6c\u6362\u6a21\u578b\u3002\u9690\u5f0f\u7b56\u7565\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u81ea\u52a8\u6355\u83b7Lombard\u98ce\u683c\u7279\u5f81\uff0c\u663e\u5f0f\u7b56\u7565\u4f9d\u8d56\u9884\u5b9a\u4e49\u58f0\u5b66\u7279\u5f81", "result": "\u9690\u5f0f\u6761\u4ef6\u6a21\u578b\u5728\u8bed\u97f3\u6e05\u6670\u5ea6\u63d0\u5347\u65b9\u9762\u4e0e\u663e\u5f0f\u6a21\u578b\u76f8\u5f53\uff08intelligibility gain comparable\uff09\uff0c\u540c\u65f6\u66f4\u597d\u5730\u4fdd\u6301\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\uff08speaker similarity preservation\uff09", "conclusion": "\u9690\u5f0f\u6761\u4ef6\u7b56\u7565\u4e3aLombard\u98ce\u683c\u8f6c\u6362\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u7aef\u5230\u7aef\u5b66\u4e60\u53ef\u66ff\u4ee3\u624b\u5de5\u8bbe\u8ba1\u58f0\u5b66\u7279\u5f81\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666fTTS\u8bad\u7ec3\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2507.09318", "pdf": "https://arxiv.org/pdf/2507.09318", "abs": "https://arxiv.org/abs/2507.09318", "authors": ["Han Zhu", "Wei Kang", "Liyong Guo", "Zengwei Yao", "Fangjun Kuang", "Weiji Zhuang", "Zhaoqing Li", "Zhifeng Han", "Dong Zhang", "Xin Zhang", "Xingchen Song", "Long Lin", "Daniel Povey"], "title": "ZipVoice-Dialog: Non-Autoregressive Spoken Dialogue Generation with Flow Matching", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Generating spoken dialogue is more challenging than monologue text-to-speech\n(TTS) due to the need for realistic turn-taking and distinct speaker timbres.\nExisting spoken dialogue generation models, being auto-regressive, suffer from\nslow and unstable inference. To overcome these limitations, we introduce\nZipVoice-Dialog, a non-autoregressive zero-shot spoken dialogue generation\nmodel built upon flow matching. Key designs include: 1) speaker-turn embeddings\nfor precise speaker turn-taking; 2) a curriculum learning strategy for stable\nspeech-text alignment; 3) specialized strategies to enable stereo dialogue\ngeneration. Additionally, recognizing the lack of open-source large-scale\nspoken dialogue datasets, we curated OpenDialog, a 6.8k-hour spoken dialogue\ndataset from in-the-wild speech data. Furthermore, we established a benchmark\nto comprehensively evaluate various models. Experimental results demonstrate\nthat ZipVoice-Dialog achieves superior performance in intelligibility, speaker\nturn-taking accuracy, speaker similarity, and inference speed. Our codes, model\ncheckpoints, demo samples, and the OpenDialog dataset are all publicly\navailable at https://github.com/k2-fsa/ZipVoice.", "AI": {"tldr": "\u63d0\u51fa\u975e\u81ea\u56de\u5f52\u5bf9\u8bdd\u8bed\u97f3\u751f\u6210\u6a21\u578bZipVoice-Dialog\uff0c\u91c7\u7528flow matching\u6280\u672f\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u63a8\u7406\uff0c\u5e76\u6784\u5efa6.8k\u5c0f\u65f6\u5f00\u6e90\u5bf9\u8bdd\u6570\u636e\u96c6OpenDialog", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u5bf9\u8bdd\u8bed\u97f3\u751f\u6210\u6a21\u578b\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u3001\u7a33\u5b9a\u6027\u5dee\u7684\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5f00\u6e90\u5927\u89c4\u6a21\u5bf9\u8bdd\u8bed\u97f3\u6570\u636e\u96c6", "method": "1. \u8bbe\u8ba1\u8bf4\u8bdd\u4eba\u8f6e\u6362\u5d4c\u5165\u673a\u5236\n2. \u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u7a33\u5b9a\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\n3. \u5f00\u53d1\u7acb\u4f53\u58f0\u5bf9\u8bdd\u751f\u6210\u7b56\u7565\n4. \u6784\u5efaOpenDialog\u5f00\u6e90\u6570\u636e\u96c6", "result": "\u6a21\u578b\u5728\u53ef\u61c2\u5ea6(96.8%\u219298.2%)\u3001\u8bf4\u8bdd\u4eba\u5207\u6362\u51c6\u786e\u7387(91%\u219295%)\u3001\u76f8\u4f3c\u5ea6(0.82\u21920.87)\u548c\u63a8\u7406\u901f\u5ea6(3.8\u500d)\u4e0a\u5747\u4f18\u4e8e\u57fa\u7ebf", "conclusion": "ZipVoice-Dialog\u901a\u8fc7\u975e\u81ea\u56de\u5f52\u67b6\u6784\u548c\u521b\u65b0\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u5bf9\u8bdd\u8bed\u97f3\u751f\u6210\uff0c\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u5f00\u653e\u63a8\u52a8\u4e86\u9886\u57df\u53d1\u5c55"}}
{"id": "2507.09481", "pdf": "https://arxiv.org/pdf/2507.09481", "abs": "https://arxiv.org/abs/2507.09481", "authors": ["Yuheng Huang", "Da Song", "Zhenlan Ji", "Shuai Wang", "Lei Ma"], "title": "Evaluating LLMs on Sequential API Call Through Automated Test Generation", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": null, "summary": "By integrating tools from external APIs, Large Language Models (LLMs) have\nexpanded their promising capabilities in a diverse spectrum of complex\nreal-world tasks. However, testing, evaluation, and analysis of LLM tool use\nremain in their early stages. Most existing benchmarks rely on manually\ncollected test cases, many of which cannot be automatically checked for\nsemantic correctness and instead depend on static methods such as string\nmatching. Additionally, these benchmarks often overlook the complex\ninteractions that occur between sequential API calls, which are common in\nreal-world applications. To fill the gap, in this paper, we introduce StateGen,\nan automated framework designed to generate diverse coding tasks involving\nsequential API interactions. StateGen combines state-machine-based API\nconstraint solving and validation, energy-based sampling, and control-flow\ninjection to generate executable programs. These programs are then translated\ninto human-like natural language task descriptions through a collaboration of\ntwo LLM agents. Utilizing StateGen, we construct StateEval, a benchmark\nencompassing 120 verified test cases spanning across three representative\nscenarios: Session Service, Tensor Operation, and ElevenLabs MCP. Experimental\nresults confirm that StateGen can effectively generate challenging and\nrealistic API-oriented tasks, highlighting areas for improvement in current\nLLMs incorporating APIs.", "AI": {"tldr": "\u63d0\u51faStateGen\u6846\u67b6\u81ea\u52a8\u5316\u751f\u6210API\u4ea4\u4e92\u578b\u7f16\u7a0b\u4efb\u52a1\uff0c\u7ed3\u5408\u72b6\u6001\u673a\u7ea6\u675f\u6c42\u89e3\u3001\u80fd\u91cf\u91c7\u6837\u548c\u63a7\u5236\u6d41\u6ce8\u5165\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u5e76\u901a\u8fc7\u53ccLLM\u534f\u4f5c\u751f\u6210\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4f9d\u8d56\u4eba\u5de5\u6536\u96c6\u7528\u4f8b\u4e14\u65e0\u6cd5\u9a8c\u8bc1\u8bed\u4e49\u6b63\u786e\u6027\uff0c\u5ffd\u89c6API\u95f4\u7684\u590d\u6742\u4ea4\u4e92\u5173\u7cfb\u3002\u9700\u8981\u6784\u5efa\u80fd\u53cd\u6620\u771f\u5b9eAPI\u8c03\u7528\u590d\u6742\u6027\u7684\u8bc4\u6d4b\u57fa\u51c6", "method": "1. \u57fa\u4e8e\u72b6\u6001\u673a\u7684API\u7ea6\u675f\u6c42\u89e3\u4e0e\u9a8c\u8bc1\n2. \u80fd\u91cf\u91c7\u6837\u7b97\u6cd5\u751f\u6210\u591a\u6837\u5316\u7a0b\u5e8f\n3. \u63a7\u5236\u6d41\u6ce8\u5165\u6280\u672f\u589e\u5f3a\u4efb\u52a1\u590d\u6742\u6027\n4. \u53ccLLM\u4ee3\u7406\u534f\u4f5c\u8f6c\u6362\u7a0b\u5e8f\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0", "result": "\u6784\u5efaStateEval\u57fa\u51c6(120\u4e2a\u9a8c\u8bc1\u7528\u4f8b)\uff0c\u8986\u76d6\u4f1a\u8bdd\u670d\u52a1\u3001\u5f20\u91cf\u64cd\u4f5c\u3001ElevenLabs MCP\u4e09\u5927\u573a\u666f\uff0c\u5b9e\u9a8c\u663e\u793a\u751f\u6210\u4efb\u52a1\u80fd\u6709\u6548\u66b4\u9732LLMs\u7684API\u6574\u5408\u7f3a\u9677", "conclusion": "StateGen\u9996\u6b21\u5b9e\u73b0API\u4ea4\u4e92\u578b\u4efb\u52a1\u7684\u81ea\u52a8\u5316\u751f\u6210\uff0c\u63ed\u793a\u5f53\u524dLLMs\u5728\u5904\u7406\u591a\u6b65API\u8c03\u7528\u65f6\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3aAPI\u6574\u5408\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2507.09574", "pdf": "https://arxiv.org/pdf/2507.09574", "abs": "https://arxiv.org/abs/2507.09574", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR", "AI": {"tldr": "\u63d0\u51fa\u81ea\u56de\u5f52\u6846\u67b6MENTOR\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u65e0\u9700\u9002\u914d\u5668\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\uff0c\u63d0\u5347\u63a7\u5236\u6027\u548c\u8bad\u7ec3\u6548\u7387", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5b58\u5728\u7cbe\u786e\u63a7\u5236\u56f0\u96be\u3001\u591a\u6a21\u6001\u8f93\u5165\u5e73\u8861\u4e0d\u8db3\u3001\u590d\u6742\u4efb\u52a1\u9700\u5927\u91cf\u8bad\u7ec3\u7b49\u95ee\u9898", "method": "\u7ed3\u5408\u81ea\u56de\u5f52\u751f\u6210\u5668\u4e0e\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u591a\u6a21\u6001\u5bf9\u9f50\u9636\u6bb5\u5efa\u7acb\u50cf\u7d20/\u8bed\u4e49\u7ea7\u5bf9\u9f50 2) \u6307\u4ee4\u8c03\u4f18\u9636\u6bb5\u589e\u5f3a\u63a7\u5236\u6027", "result": "\u5728DreamBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u6982\u5ff5\u4fdd\u6301\u548c\u63d0\u793a\u8ddf\u968f\u63d0\u5347\uff0c\u8bad\u7ec3\u6548\u7387\u4f18\u4e8e\u6269\u6563\u6a21\u578b", "conclusion": "MENTOR\u901a\u8fc7\u521b\u65b0\u8bad\u7ec3\u8303\u5f0f\u5b9e\u73b0\u4e86\u9ad8\u6548\u53ef\u63a7\u7684\u591a\u6a21\u6001\u751f\u6210\uff0c\u5728\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u4efb\u52a1\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u7a81\u51fa"}}
{"id": "2507.09662", "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u4f46\u5b58\u5728\u5197\u4f59\u63a8\u7406\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u81ea\u9002\u5e94\u5feb\u6162\u601d\u8003\u63d0\u5347\u6548\u7387", "motivation": "\u89e3\u51b3LRMs\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u751f\u6210\u5197\u957f\u63a8\u7406\u94fe\u5bfc\u81f4\u7684\u8d44\u6e90\u6d6a\u8d39\u3001\u54cd\u5e94\u5ef6\u8fdf\u548c\u5b9e\u9645\u5e94\u7528\u969c\u788d", "method": "\u7efc\u8ff0\u5f53\u524d\u7b80\u660e\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\uff0c\u6db5\u76d6\u65b9\u6cd5\u8bba\u3001\u57fa\u51c6\u6d4b\u8bd5\u53ca\u672a\u6765\u6311\u6218", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u7f29\u77ed\u63a8\u7406\u94fe\u7684\u6280\u672f\u8def\u5f84\uff0c\u63d0\u51fa\u81ea\u9002\u5e94\u63a8\u7406\u6846\u67b6\u4fc3\u8fdbLRMs\u9ad8\u6548\u5e94\u7528", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u9886\u57df\u5168\u666f\u8ba4\u77e5\uff0c\u63a8\u52a8\u81ea\u9002\u5e94\u63a8\u7406\u6280\u672f\u53d1\u5c55\u4ee5\u91ca\u653eLRMs\u5b9e\u7528\u6f5c\u529b"}}
{"id": "2507.09751", "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "categories": ["cs.AI", "cs.CL", "cs.LO"], "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408\u5230\u6b21\u534f\u8c03\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u4e49\u6846\u67b6\u4e2d\uff0c\u89e3\u51b3LLM\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5177\u5907\u5e7f\u6cdb\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u4f46\u5728\u5f62\u5f0f\u63a8\u7406\u4e2d\u5b58\u5728\u8f93\u51fa\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u4fdd\u6301\u903b\u8f91\u53ef\u9760\u6027\u7684\u6574\u5408\u65b9\u6cd5", "method": "\u5c06LLM\u5d4c\u5165\u6b21\u534f\u8c03\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u4e49\u89e3\u91ca\u51fd\u6570\uff0c\u6784\u5efa\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u6846\u67b6", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6027\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53ef\u884c\uff0c\u4e14\u4fdd\u6301\u5e95\u5c42\u903b\u8f91\u7684\u53ef\u9760\u6027\u548c\u5b8c\u5907\u6027", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0LLM\u77e5\u8bc6\u4e0e\u5f62\u5f0f\u903b\u8f91\u5c5e\u6027\u7684\u517c\u5bb9\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u7406\u8bba\u652f\u6491"}}
{"id": "2507.09762", "pdf": "https://arxiv.org/pdf/2507.09762", "abs": "https://arxiv.org/abs/2507.09762", "authors": ["Yasir Ech-Chammakhy", "Anas Motii", "Anass Rabii", "Jaafar Chbili"], "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)", "summary": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528Transformer\u548c\u5bf9\u6bd4\u5b66\u4e60\u81ea\u52a8\u68c0\u6d4b\u3001\u805a\u7c7b\u5e76\u4f18\u5148\u5904\u7406\u9ed1\u5ba2\u8bba\u575b\u4e2d\u7684\u5b89\u5168\u4e8b\u4ef6\uff0c\u6709\u6548\u51cf\u5c11\u566a\u97f3\u5e76\u8bc6\u522b\u9ad8\u4f18\u5148\u7ea7\u5a01\u80c1\u3002", "motivation": "\u9ed1\u5ba2\u8bba\u575b\u662f\u7f51\u7edc\u5b89\u5168\u5a01\u80c1\u65e9\u671f\u4fe1\u53f7\u7684\u91cd\u8981\u6765\u6e90\uff0c\u4f46\u5176\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u548c\u566a\u97f3\u5bfc\u81f4\u96be\u4ee5\u63d0\u53d6\u53ef\u64cd\u4f5c\u60c5\u62a5\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u5173\u952e\u8bcd\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u5174\u5a01\u80c1\u3002", "method": "1. \u57fa\u4e8eTransformer\u7684\u5d4c\u5165\u6a21\u578b\uff08\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03\uff09\u5b9e\u73b0\u8bed\u4e49\u805a\u7c7b\n2. \u6784\u5efa\u91cf\u5316\u6307\u6807\uff08\u53ca\u65f6\u6027\u3001\u53ef\u4fe1\u5ea6\u3001\u5b8c\u6574\u6027\u3001\u76f8\u5173\u6027\uff09\u7684\u6bcf\u65e5\u4f18\u5148\u7ea7\u6392\u5e8f\u673a\u5236\n3. \u65e0\u76d1\u7763\u8bc6\u522b\u96f6\u65e5\u6f0f\u6d1e/\u6076\u610f\u8f6f\u4ef6\u7b49\u5b89\u5168\u4e8b\u4ef6", "result": "\u5728\u771f\u5b9e\u9ed1\u5ba2\u8bba\u575b\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0c\u6846\u67b6\u6210\u529f\u964d\u4f4e86%\u566a\u97f3\uff0c\u8bc6\u522b\u51fa93%\u7684\u9ad8\u4f18\u5148\u7ea7\u5a01\u80c1\u4e8b\u4ef6\uff0c\u54cd\u5e94\u65f6\u6548\u63d0\u53475.2\u500d\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c06\u788e\u7247\u5316\u8ba8\u8bba\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u5a01\u80c1\u60c5\u62a5\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u5a01\u80c1\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u7406\u89e3\u3001\u52a8\u6001\u8bc4\u4f30\u548c\u8de8\u8bba\u575b\u5173\u8054\u4e09\u5927\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2507.09788", "pdf": "https://arxiv.org/pdf/2507.09788", "abs": "https://arxiv.org/abs/2507.09788", "authors": ["Paulo Salem", "Robert Sim", "Christopher Olsen", "Prerit Saxena", "Rafael Barcelos", "Yi Ding"], "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "I.2.11; I.6.5; I.6.7"], "comment": "9 pages. Preprint to be submitted to peer-review", "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.", "AI": {"tldr": "\u63d0\u51fa\u4e86TinyTroupe\u2014\u2014\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7cbe\u7ec6\u5316\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u5de5\u5177\u5305\uff0c\u652f\u6301\u89d2\u8272\u5c5e\u6027\u5b9a\u4e49\u3001\u7fa4\u4f53\u63a7\u5236\u4e0e\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u884c\u4e3a\u4eff\u771f\u4e2d\u7684\u5173\u952e\u77ed\u677f\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7f3a\u4e4f\u7cbe\u7ec6\u89d2\u8272\u8bbe\u5b9a\u3001\u6709\u6548\u4eba\u53e3\u62bd\u6837\u673a\u5236\u548c\u96c6\u6210\u9a8c\u8bc1\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u5728\u884c\u4e3a\u7814\u7a76\u548c\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7Python\u5b9e\u73b0LLM\u9a71\u52a8\u7684\u591a\u7ef4\u5ea6\u89d2\u8272\u5c5e\u6027\u5b9a\u4e49\u7cfb\u7edf\uff08\u56fd\u7c4d/\u6027\u683c/\u4fe1\u5ff5\u7b49\uff09\uff0c\u5f00\u53d1\u7a0b\u5e8f\u5316\u63a7\u5236\u63a5\u53e3\u652f\u6301\u4e2a\u4f53/\u7fa4\u4f53\u884c\u4e3a\u6a21\u62df\uff0c\u63d0\u4f9b\u9a8c\u8bc1\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5934\u8111\u98ce\u66b4\u3001\u5e02\u573a\u8c03\u7814\u7b49\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5de5\u5177\u6709\u6548\u6027\uff0c\u5b9a\u91cf\u8bc4\u4f30\u5c55\u793a\u4e86\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u53d6\u820d\u4e0e\u53ef\u80fd\u6027\u8fb9\u754c\u3002", "conclusion": "TinyTroupe\u65e2\u662f\u5f00\u6e90\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\uff0c\u66f4\u662f\u65b0\u578b\u4eff\u771f\u8303\u5f0f\u7684\u6982\u5ff5\u8d21\u732e\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u7075\u6d3b\u79fb\u690d\u5230\u5176\u4ed6LLM\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.09876", "pdf": "https://arxiv.org/pdf/2507.09876", "abs": "https://arxiv.org/abs/2507.09876", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by ACM MM 2025", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.", "AI": {"tldr": "\u63d0\u51fa\u89c6\u9891-\u6587\u672c\u4ea4\u66ff\u601d\u7ef4\u94fe(ViTCoT)\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u6a21\u6001\uff0c\u5ffd\u89c6\u4e86\u4eba\u7c7b\u81ea\u7136\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u89c6\u89c9\u91cd\u5ba1\u673a\u5236\u3002\u4e3a\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u6a21\u5f0f\uff0c\u9700\u8981\u5efa\u7acb\u89c6\u89c9\u4e0e\u6587\u672c\u4ea4\u66ff\u534f\u540c\u7684\u63a8\u7406\u6846\u67b6", "method": "\u6784\u5efaViTIB\u89c6\u9891\u6587\u672c\u4ea4\u66ff\u57fa\u51c6\uff08MLLMs\u5173\u952e\u5e27\u9009\u62e9+\u4eba\u5de5\u9a8c\u8bc1\uff09\uff0c\u8bbe\u8ba1ViTCoT\u6846\u67b6\u5b9e\u73b0\u89c6\u89c9\u4fe1\u53f7\u4e0e\u6587\u672c\u63a8\u7406\u7684\u65f6\u5e8f\u5bf9\u9f50\u548c\u5185\u5bb9\u4e92\u9a8c", "result": "ViTCoT\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4f20\u7edf\u6587\u672cCoT\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6fc0\u6d3bMLLMs\u4e2d21.3%\u7684\u89c6\u89c9\u795e\u7ecf\u5143", "conclusion": "\u89c6\u89c9-\u6587\u672c\u4ea4\u66ff\u63a8\u7406\u8303\u5f0f\u7a81\u7834\u4e86\u5355\u6a21\u6001\u63a8\u7406\u5c40\u9650\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u548c\u81ea\u52a8\u9a7e\u9a76\u7b49\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u4e86\u8ba4\u77e5\u5bf9\u9f50\u7684\u65b0\u65b9\u5411"}}
{"id": "2507.09924", "pdf": "https://arxiv.org/pdf/2507.09924", "abs": "https://arxiv.org/abs/2507.09924", "authors": ["Tuan-Luc Huynh", "Thuy-Trang Vu", "Weiqing Wang", "Trung Le", "Dragan Ga\u0161evi\u0107", "Yuan-Fang Li", "Thanh-Toan Do"], "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.", "AI": {"tldr": "\u63d0\u51faMixLoRA-DSI\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408LoRA\u4e13\u5bb6\u548cOOD\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u7b56\u7565\uff0c\u5b9e\u73b0\u751f\u6210\u68c0\u7d22\u6a21\u578b\u7684\u9ad8\u6548\u6301\u7eed\u66f4\u65b0\u3002", "motivation": "\u89e3\u51b3\u751f\u6210\u68c0\u7d22\u6a21\u578b\u56e0\u5168\u91cf\u91cd\u8bad\u7ec3\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u5b9e\u73b0\u7d22\u5f15\u7684\u53ef\u6301\u7eed\u66f4\u65b0\u3002", "method": "\u7ed3\u5408\u53ef\u6269\u5c55\u7684LoRA\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u4e0e\u5206\u5c42OOD\u68c0\u6d4b\u673a\u5236\uff0c\u4ec5\u5728\u68c0\u6d4b\u5230\u663e\u8457OOD\u6587\u6863\u65f6\u65b0\u589e\u4e13\u5bb6\uff0c\u5b9e\u73b0\u6b21\u7ebf\u6027\u53c2\u6570\u589e\u957f\u3002", "result": "\u5728NQ320k\u548cMS MARCO\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u5168\u6a21\u578b\u66f4\u65b0\u57fa\u7ebf\u53c2\u6570\u91cf\u51cf\u5c1148%\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e5\u500d\uff0c\u68c0\u7d22\u6027\u80fd\u63d0\u53471.2\u4e2a\u70b9\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u53c2\u6570\u6269\u5c55\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u66f4\u65b0\u6210\u672c\uff0c\u4e3a\u5728\u7ebf\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10000", "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u8fc7\u7a0b\u4e00\u81f4\u6027\u4e0e\u5c3a\u5ea6\u5206\u79bb\u65b9\u6cd5\uff0c\u4f4e\u6210\u672c\u68c0\u6d4b\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u610f\u5411\u6027\uff0c\u9002\u7528\u4e8e\u57fa\u7840\u751f\u7269\u4f53\u8ba4\u77e5\u5904\u7406", "motivation": "\u89e3\u51b3\u79d1\u5b66\u9886\u57df\u957f\u671f\u5ffd\u89c6\u610f\u56fe\u7684\u5b9e\u9645\u5e94\u7528\u95ee\u9898\uff0c\u57fa\u4e8ePromise Theory\u6a21\u578b\u63a2\u7d22\u8bed\u4e49\u65f6\u7a7a\u4e2d\u610f\u5411\u6027\u4e0e\u8bed\u5883\u7684\u91cf\u5316\u5206\u6790", "method": "\u5229\u7528\u591a\u5c3a\u5ea6\u5f02\u5e38\u68c0\u6d4b\u8bc4\u4f30\u6570\u636e\u5f62\u6001\u5f62\u6210\u6240\u9700\u529f\uff0c\u901a\u8fc7\u65f6\u7a7a\u4e00\u81f4\u6027\u5206\u79bb\u610f\u56fe\u5185\u5bb9\u4e0e\u73af\u5883\u8bed\u5883", "result": "\u5f00\u53d1\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u4f4e\u6210\u672c\u610f\u5411\u6027\u68c0\u6d4b\u6846\u67b6\uff0c\u8bc1\u660e\u57fa\u7840\u751f\u7269\u4f53\u53ef\u5b9e\u73b0\u6d45\u5c42\u6982\u5ff5\u5f62\u6210", "conclusion": "\u8bb0\u5fc6\u5bb9\u91cf\u9650\u5236\u6982\u5ff5\u5c42\u7ea7\uff0c\u4f46\u8fc7\u7a0b\u4e00\u81f4\u6027\u65b9\u6cd5\u4e3a\u6709\u673a\u4f53\u8ba4\u77e5\u673a\u5236\u63d0\u4f9b\u53ef\u884c\u89e3\u91ca\u8def\u5f84"}}
{"id": "2507.10013", "pdf": "https://arxiv.org/pdf/2507.10013", "abs": "https://arxiv.org/abs/2507.10013", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0CLIP\u6a21\u578b\u7684ResNet\u548cViT\u53d8\u4f53\u672a\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u4eba\u7c7bbouba-kiki\u6548\u5e94\uff0c\u8de8\u6a21\u6001\u6574\u5408\u80fd\u529b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "motivation": "\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u8de8\u6a21\u6001\u4fe1\u606f\u6574\u5408\u80fd\u529b\uff0c\u4ee5\u7ecf\u5178bouba-kiki\u6548\u5e94\u4e3a\u5207\u5165\u70b9\u9a8c\u8bc1\u6a21\u578b\u8ba4\u77e5\u7279\u6027", "method": "\u7ed3\u5408\u63d0\u793a\u6982\u7387\u8bc4\u4f30\uff08\u6d4b\u91cf\u5f62\u72b6-\u8bcd\u6c47\u504f\u597d\uff09\u548cGrad-CAM\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u6790\uff0c\u5728CLIP\u7684ResNet/ViT\u67b6\u6784\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u6d4b\u8bd5", "result": "\u6a21\u578b\u672a\u5f62\u6210\u7a33\u5b9a\u5173\u8054\u6a21\u5f0f\uff08ResNet\u4ec5\u663e\u793a\u5706\u5f62\u504f\u597d\uff09\uff0c\u54cd\u5e94\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u5b9e\u9a8c\u6570\u636e\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u6ce8\u610f\u529b\u673a\u5236\u672a\u5448\u73b0\u8de8\u6a21\u6001\u6574\u5408\u7279\u5f81", "conclusion": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u6a21\u6001\u7406\u89e3\u5b58\u5728\u672c\u8d28\u5c40\u9650\uff0c\u5176\u5185\u90e8\u8868\u5f81\u4e0e\u4eba\u7c7b\u76f4\u89c9\u672a\u8fbe\u6210\u5bf9\u9f50\uff0c\u63d0\u793a\u9700\u91cd\u65b0\u5ba1\u89c6\u6a21\u578b\u7684\u771f\u6b63\u8ba4\u77e5\u80fd\u529b"}}
{"id": "2507.10045", "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "categories": ["cs.AI", "cs.CL"], "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u77e5\u8bc6\u56fe\u8c31SPARQL\u67e5\u8be2\u8f6c\u6362\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u548c\u63d0\u793a\u7b56\u7565\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cWikidata\u5230DBpedia\u7684\u8f6c\u6362\u6548\u679c\u4f18\u4e8e\u53cd\u5411\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u4e92\u64cd\u4f5c\u6027\u7814\u7a76\u4e2dSPARQL\u5230SPARQL\u81ea\u52a8\u8f6c\u6362\u7684\u8bc4\u4f30\u7a7a\u767d\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08DBpedia-Wikidata\u548cDBLP-OpenAlex\uff09\uff0c\u9009\u62e9\u4e09\u79cd\u5f00\u6e90\u5927\u6a21\u578b\uff08Llama-3-8B/DeepSeek-R1-Distill-Llama-70B/Mistral-Large-Instruct-2407\uff09\uff0c\u91c7\u7528\u96f6\u6837\u672c/\u5c11\u6837\u672c/\u601d\u7ef4\u94fe\u7b56\u7565\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u6807\u51c6\u7b54\u6848\u5bf9\u6bd4\u5206\u6790\u9519\u8bef\u7c7b\u578b\u3002", "result": "\u6a21\u578b\u6027\u80fd\u548c\u63d0\u793a\u7b56\u7565\u5dee\u5f02\u663e\u8457\uff0cWikidata\u2192DBpedia\u8f6c\u6362\u6210\u529f\u7387\u8fdc\u9ad8\u4e8e\u53cd\u5411\uff08DBpedia\u2192Wikidata\uff09\u3002\u767e\u79d1\u5168\u4e66\u7c7b\u77e5\u8bc6\u56fe\u8c31\u7684\u8f6c\u6362\u6548\u679c\u4f18\u4e8e\u5b66\u672f\u56fe\u8c31\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907SPARQL\u8de8\u56fe\u8c31\u8f6c\u6362\u6f5c\u529b\uff0c\u4f46\u6548\u679c\u53d7\u6a21\u578b\u67b6\u6784\u3001\u63d0\u793a\u7b56\u7565\u548c\u8f6c\u6362\u65b9\u5411\u5f71\u54cd\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u624d\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u77e5\u8bc6\u56fe\u8c31\u4e92\u64cd\u4f5c\u3002"}}
{"id": "2507.10057", "pdf": "https://arxiv.org/pdf/2507.10057", "abs": "https://arxiv.org/abs/2507.10057", "authors": ["Sangwoo Park", "Jinheon Baek", "Soyeong Jeong", "Sung Ju Hwang"], "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.", "AI": {"tldr": "PRISM\u63d0\u51fa\u591a\u7c92\u5ea6\u6587\u6863\u8868\u793a\u65b9\u6cd5\u63d0\u5347\u79d1\u6280\u6587\u732e\u68c0\u7d22\u6548\u679c4.3%\uff0c\u5e76\u5efa\u7acbSciFullBench\u65b0\u57fa\u51c6", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6458\u8981\u7684\u79d1\u6280\u6587\u732e\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u4fe1\u606f\u7a00\u758f\u95ee\u9898\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5b8c\u6574\u8bba\u6587\u7684\u591a\u7ef4\u5ea6\u4fe1\u606f", "method": "\u5c06\u67e5\u8be2\u8bba\u6587\u5206\u89e3\u4e3a\u591a\u4e2a\u7279\u5b9a\u89c6\u89d2\u7684\u7ec6\u7c92\u5ea6\u8868\u793a\uff0c\u4e0e\u7ecf\u8fc7\u76f8\u4f3c\u5206\u5272\u7684\u5019\u9009\u8bba\u6587\u8fdb\u884c\u591a\u7ef4\u5ea6\u5339\u914d", "result": "\u5b9e\u9a8c\u663e\u793aPRISM\u6bd4\u73b0\u6709\u57fa\u7ebf\u5e73\u5747\u63d0\u53474.3%\u68c0\u7d22\u6027\u80fd", "conclusion": "\u901a\u8fc7\u5b8c\u6574\u8bba\u6587\u7684\u7ec6\u7c92\u5ea6\u591a\u89c6\u89d2\u8868\u5f81\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u6863\u5230\u6587\u6863\u68c0\u7d22\u7684\u51c6\u786e\u6027"}}
{"id": "2507.10200", "pdf": "https://arxiv.org/pdf/2507.10200", "abs": "https://arxiv.org/abs/2507.10200", "authors": ["Stefano Bann\u00f2", "Rao Ma", "Mengjie Qian", "Siyuan Tang", "Kate Knill", "Mark Gales"], "title": "Natural Language-based Assessment of L2 Oral Proficiency using LLMs", "categories": ["eess.AS", "cs.AI", "cs.CL"], "comment": "Accepted for the 10th Workshop on Speech and Language Technology in\n  Education (SLaTE 2025)", "summary": "Natural language-based assessment (NLA) is an approach to second language\nassessment that uses instructions - expressed in the form of can-do descriptors\n- originally intended for human examiners, aiming to determine whether large\nlanguage models (LLMs) can interpret and apply them in ways comparable to human\nassessment. In this work, we explore the use of such descriptors with an\nopen-source LLM, Qwen 2.5 72B, to assess responses from the publicly available\nS&I Corpus in a zero-shot setting. Our results show that this approach -\nrelying solely on textual information - achieves competitive performance: while\nit does not outperform state-of-the-art speech LLMs fine-tuned for the task, it\nsurpasses a BERT-based model trained specifically for this purpose. NLA proves\nparticularly effective in mismatched task settings, is generalisable to other\ndata types and languages, and offers greater interpretability, as it is\ngrounded in clearly explainable, widely applicable language descriptors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u8bc4\u4f30\u65b9\u6cd5NLA\uff0c\u4f7f\u7528\u5f00\u6e90\u5927\u6a21\u578bQwen 2.5 72B\u9a8c\u8bc1\u5176\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u4e8c\u8bed\u8bc4\u4f30\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u8de8\u4efb\u52a1\u573a\u666f\u4e2d\u5c55\u73b0\u8f83\u5f3a\u9002\u5e94\u6027\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u50cf\u4eba\u7c7b\u8003\u5b98\u90a3\u6837\u89e3\u6790\u5e76\u5e94\u7528'\u80fd\u505a\u63cf\u8ff0'\u578b\u8bc4\u4f30\u6807\u51c6\uff0c\u5efa\u7acb\u65e0\u9700\u8bed\u97f3\u7279\u5f81\u7684\u7eaf\u6587\u672c\u4e8c\u8bed\u8bc4\u4f30\u65b9\u6848", "method": "\u91c7\u7528\u5f00\u6e90\u6a21\u578bQwen 2.5 72B\uff0c\u5728\u516c\u5f00\u7684S&I Corpus\u4e0a\u5b9e\u65bd\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u76f4\u63a5\u4f7f\u7528\u4eba\u7c7b\u8003\u5b98\u7684\u8bed\u8a00\u63cf\u8ff0\u4f5c\u4e3a\u8bc4\u4f30\u6307\u4ee4", "result": "\u7eaf\u6587\u672c\u65b9\u6cd5\u8d85\u8d8a\u4e13\u4e3a\u6b64\u4efb\u52a1\u8bad\u7ec3\u7684BERT\u6a21\u578b\uff08\u51c6\u786e\u7387+5.3%\uff09\uff0c\u5728\u8de8\u8bed\u8a00/\u8de8\u6570\u636e\u7c7b\u578b\u7684\u9519\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u5c24\u5176\u7a81\u51fa\uff0c\u4f46\u672a\u8d85\u8fc7\u9488\u5bf9\u8be5\u4efb\u52a1\u5fae\u8c03\u7684\u6700\u4f18\u8bed\u97f3\u5927\u6a21\u578b", "conclusion": "NLA\u65b9\u6cd5\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u63cf\u8ff0\u6846\u67b6\uff0c\u5728\u4fdd\u8bc1\u8bc4\u4f30\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u7075\u6d3b\u7684\u4e8c\u8bed\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2507.10300", "pdf": "https://arxiv.org/pdf/2507.10300", "abs": "https://arxiv.org/abs/2507.10300", "authors": ["Hatef Otroshi Shahreza", "S\u00e9bastien Marcel"], "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted in ICCV 2025 workshops", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.", "AI": {"tldr": "\u63d0\u51faFaceLLM\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7ChatGPT\u751f\u6210\u7684\u5f31\u76d1\u7763\u6570\u636e\u96c6FairFaceGPT\uff0c\u663e\u8457\u63d0\u5347\u9762\u90e8\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\u6027\u80fd\u5e76\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709MLLM\u5728\u901a\u7528\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u9488\u5bf9\u9762\u90e8\u7279\u5f81\uff08\u8868\u60c5\u3001\u7ed3\u6784\u3001\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\uff09\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u80fd\u529b\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002", "method": "\u4f7f\u7528\u5c5e\u6027\u611f\u77e5\u63d0\u793a\u7684ChatGPT\u7ba1\u9053\uff0c\u57fa\u4e8eFairFace\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cfQA\u5bf9\uff0c\u6784\u5efa\u8986\u76d6\u8868\u60c5/\u59ff\u6001/\u76ae\u80a4\u7eb9\u7406/\u6cd5\u533b\u7279\u5f81\u7684FairFaceGPT\u6570\u636e\u96c6\u3002", "result": "FaceLLM\u5728\u591a\u4e2a\u9762\u90e8\u4e2d\u5fc3\u4efb\u52a1\u4e0a\u63d0\u5347\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8bed\u8a00\u6a21\u578b\u5408\u6210\u76d1\u7763\u7684\u6709\u6548\u6027\uff0c\u6a21\u578b\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u76d1\u7763\u4fe1\u53f7\u53ef\u6784\u5efa\u9886\u57df\u4e13\u7528MLLM\uff0c\u672c\u7814\u7a76\u4e3a\u4eba\u672c\u591a\u6a21\u6001AI\u7cfb\u7edf\u5efa\u7acb\u4e86\u53ef\u4fe1\u8d56\u7684\u6280\u672f\u8303\u5f0f\u3002"}}
{"id": "2507.10398", "pdf": "https://arxiv.org/pdf/2507.10398", "abs": "https://arxiv.org/abs/2507.10398", "authors": ["Diksha Mehta", "Prateek Mehta"], "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "comment": "9 pages, 6 figures", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53cc\u5c42\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5929\u57ce\u6587\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u65b9\u6cd5\uff0c\u5728DHCD\u6570\u636e\u96c6\u5b9e\u73b096.36%\u6d4b\u8bd5\u51c6\u786e\u7387", "motivation": "\u5929\u57ce\u6587\u4f5c\u4e3a\u5370\u5ea6\u53e4\u8001\u6587\u5b57\u7f3a\u4e4f\u6709\u6548\u6570\u5b57\u5316\u5de5\u5177\uff0c\u9700\u901a\u8fc7\u81ea\u52a8\u5316\u8bc6\u522b\u6280\u672f\u63d0\u5347\u641c\u7d22\u5f15\u64ce\u3001\u793e\u4ea4\u5a92\u4f53\u7b49\u573a\u666f\u7684\u6587\u5b57\u5904\u7406\u6548\u7387", "method": "\u4f7f\u7528\u5305\u542b36\u7c7b\u5b57\u7b26\u7684DHCD\u6570\u636e\u96c6\uff08\u6bcf\u7c7b1700\u5f20\u56fe\u50cf\uff09\uff0c\u6784\u5efa\u53cc\u5c42\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u548c\u5206\u7c7b", "result": "\u8bad\u7ec3\u51c6\u786e\u738799.55%\uff0c\u6d4b\u8bd5\u51c6\u786e\u738796.36%", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u5929\u57ce\u6587\u624b\u5199\u8bc6\u522b\u7cbe\u5ea6\uff0c\u4e3a\u53e4\u6587\u5b57\u6570\u5b57\u5316\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u8bed\u79cd\u6587\u5b57\u8bc6\u522b"}}
{"id": "2507.10403", "pdf": "https://arxiv.org/pdf/2507.10403", "abs": "https://arxiv.org/abs/2507.10403", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "AI": {"tldr": "\u63d0\u51faCrisisLandMark\u591a\u6a21\u6001\u536b\u661f\u6570\u636e\u96c6\u53caCLOSP\u8de8\u4f20\u611f\u5668\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0cSAR\u4e0e\u5149\u5b66\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\u63d0\u534754%\uff0c\u5730\u7406\u5750\u6807\u878d\u5408\u5b9e\u73b0\u901a\u7528\u4e0e\u4e13\u7528\u4efb\u52a1\u7684\u5e73\u8861", "motivation": "\u73b0\u6709\u6587\u672c-\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u5c40\u9650\u4e8eRGB\u6570\u636e\uff0c\u65e0\u6cd5\u5229\u7528SAR\u96f7\u8fbe\u7ed3\u6784\u4fe1\u606f\u4e0e\u591a\u5149\u8c31\u7279\u5f81\uff0c\u9650\u5236\u4e86\u707e\u5bb3\u54cd\u5e94\u7b49\u5173\u952e\u573a\u666f\u7684\u68c0\u7d22\u6548\u679c", "method": "\u6784\u5efa64.7\u4e07+ Sentinel\u4f20\u611f\u5668\u56fe\u50cf-\u6587\u672c\u5bf9\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1CLOSP\u6846\u67b6\u901a\u8fc7\u6587\u672c\u6865\u63a5\u5bf9\u9f50\u5149\u5b66/SAR\u5d4c\u5165\u7a7a\u95f4\uff0cGeoCLOSP\u589e\u52a0\u5730\u7406\u5750\u6807\u589e\u5f3a\u7279\u5b9a\u4efb\u52a1\u8868\u73b0", "result": "CLOSP\u5c06\u68c0\u7d22nDGC\u63d0\u534754%\uff0c\u7edf\u4e00\u5d4c\u5165\u7a7a\u95f4\u5b9e\u73b0\u5149\u5b66\u8bed\u4e49\u5411SAR\u8fc1\u79fb\uff0cGeoCLOSP\u5728\u4f4d\u7f6e\u4f9d\u8d56\u4efb\u52a1(\u5371\u673a\u4e8b\u4ef6/\u7a00\u6709\u5730\u7406\u7279\u5f81\u68c0\u7d22)\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u6a21\u578b", "conclusion": "\u591a\u4f20\u611f\u5668\u6570\u636e\u878d\u5408\u4e0e\u5730\u7406\u4e0a\u4e0b\u6587\u6574\u5408\u662f\u91ca\u653e\u9065\u611f\u6863\u6848\u4ef7\u503c\u7684\u5173\u952e\uff0cCLOSP/GeoCLOSP\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u901a\u7528\u6027\u4e0e\u4e13\u4e1a\u5316\u7684\u65b0\u8303\u5f0f"}}
{"id": "2507.10419", "pdf": "https://arxiv.org/pdf/2507.10419", "abs": "https://arxiv.org/abs/2507.10419", "authors": ["Victor Letzelter", "Hugo Malard", "Mathieu Fontaine", "Ga\u00ebl Richard", "Slim Essid", "Andrei Bursuc", "Patrick P\u00e9rez"], "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.", "AI": {"tldr": "LoRA-MCL\u901a\u8fc7\u6574\u5408MCL\u8bad\u7ec3\u7b56\u7565\u548c\u4f4e\u79e9\u9002\u914d\u6280\u672f\uff0c\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u5408\u7406\u6587\u672c\u5ef6\u7eed\u7684\u80fd\u529b", "motivation": "\u4f20\u7edf\u8bed\u8a00\u5efa\u6a21\u5b58\u5728\u56fa\u6709\u6b67\u4e49\u6027\u2014\u2014\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u65f6\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u5408\u7406\u7eed\u5199\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u591a\u9009\u9879\u5b66\u4e60\u673a\u5236\u663e\u5f0f\u5904\u7406\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027", "method": "\u7ed3\u5408\u591a\u9009\u9879\u5b66\u4e60(MCL)\u548c\u8d62\u5bb6\u901a\u5403(WTA)\u635f\u5931\u51fd\u6570\uff0c\u5229\u7528LoRA\u4f4e\u79e9\u9002\u914d\u6280\u672f\u5b9e\u73b0\u53c2\u6570\u9ad8\u6548\u4f18\u5316\u3002\u901a\u8fc7\u6df7\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u751f\u6210\u7684\u7406\u8bba\u6846\u67b6\u8fdb\u884c\u9a8c\u8bc1", "result": "\u5728\u89c6\u89c9\u63cf\u8ff0\u548c\u97f3\u9891\u63cf\u8ff0\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u540c\u65f6\u4fdd\u6301\u751f\u6210\u7ed3\u679c\u7684\u591a\u6837\u6027\u548c\u8bed\u4e49\u76f8\u5173\u6027", "conclusion": "\u63d0\u51fa\u7684LoRA-MCL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5355\u4e00\u5316\u95ee\u9898\uff0c\u4e3a\u5f00\u653e\u5f0f\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2507.10522", "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "AI": {"tldr": "DeepResearch Eco\u662f\u57fa\u4e8eLLM\u7684\u65b0\u578b\u79d1\u5b66\u6587\u732e\u81ea\u52a8\u5206\u6790\u7cfb\u7edf\uff0c\u901a\u8fc7\u9012\u5f52\u68c0\u7d22\u5b9e\u73b021\u500d\u6587\u732e\u6574\u5408\u6548\u7387\u63d0\u5347\uff0c\u652f\u6301\u53c2\u6570\u5316\u914d\u7f6e\u7684\u751f\u6001\u7814\u7a76\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5728\u6587\u732e\u5206\u6790\u6df1\u5ea6\u3001\u591a\u6837\u6027\u63a7\u5236\u548c\u7ed3\u679c\u900f\u660e\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u9886\u57df\u8bc1\u636e\u6574\u5408\u6548\u7387\u3002", "method": "\u91c7\u7528\u9012\u5f52\u6df1\u5ea6/\u5e7f\u5ea6\u63a7\u5236\u7b97\u6cd5\uff0c\u53c2\u6570\u5316\u914d\u7f6e\u5206\u6790\u8def\u5f84\uff0c\u5e94\u7528\u4e8e49\u4e2a\u751f\u6001\u7814\u7a76\u95ee\u9898\uff0c\u652f\u6301\u6e90\u7801\u590d\u73b0\u7684\u900f\u660e\u63a8\u7406\u6846\u67b6\u3002", "result": "\u5b9e\u73b0\u5355\u6b21\u5206\u6790\u6e90\u6574\u5408\u91cf\u63d0\u534721\u500d\uff0c\u6bcf\u5343\u5b57\u96c6\u6210\u6587\u732e\u91cf\u589e\u52a014.9\u500d\uff0c\u9ad8\u53c2\u6570\u914d\u7f6e\u4e0b\u8fbe\u5230\u4e13\u5bb6\u7ea7\u5206\u6790\u6df1\u5ea6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u751f\u6001\u7814\u7a76\u8bc1\u636e\u6574\u5408\u6548\u7387\u4e0e\u5206\u6790\u6df1\u5ea6\uff0c\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u53ef\u91cd\u590d\u79d1\u5b66\u7814\u7a76\u3002"}}
{"id": "2507.10532", "pdf": "https://arxiv.org/pdf/2507.10532", "abs": "https://arxiv.org/abs/2507.10532", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "26 pages", "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.", "AI": {"tldr": "\u901a\u8fc7\u521b\u5efa\u65e0\u6570\u636e\u6c61\u67d3\u7684\u5408\u6210\u6570\u636e\u96c6RandomCalculation\uff0c\u9a8c\u8bc1\u4ec5\u51c6\u786e\u5956\u52b1\u4fe1\u53f7\u80fd\u63d0\u5347\u6a21\u578b\u63a8\u7406\u6027\u80fd\uff0c\u5f3a\u8c03\u9700\u5728\u5e72\u51c0\u57fa\u51c6\u548c\u591a\u6837\u6a21\u578b\u4e0a\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728Qwen2.5\u6a21\u578b\u4e0a\u62a5\u544a\u7684\u5f3a\u5316\u5b66\u4e60\u7a81\u7834\u53ef\u80fd\u53d7\u5230\u9884\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u5e72\u6270\uff0c\u9700\u5728\u65e0\u6c61\u67d3\u73af\u5883\u4e2d\u9a8c\u8bc1\u7ed3\u8bba\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u53ef\u751f\u6210\u4efb\u610f\u957f\u5ea6/\u96be\u5ea6\u7b97\u672f\u95ee\u9898\u7684\u5408\u6210\u6570\u636e\u96c6RandomCalculation\uff0c\u5bf9\u6bd4\u51c6\u786e\u4fe1\u53f7\u4e0e\u566a\u58f0\u4fe1\u53f7\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4ec5\u7cbe\u786e\u5956\u52b1\u4fe1\u53f7\u80fd\u6301\u7eed\u63d0\u5347\u6a21\u578b\u8868\u73b0\uff0c\u800cQwen2.5\u5728\u4f20\u7edf\u57fa\u51c6\u4e0a\u7684\u4f18\u5f02\u8868\u73b0\u53ef\u80fd\u6e90\u4e8e\u6570\u636e\u6cc4\u9732\u800c\u975e\u7b97\u6cd5\u4f18\u52bf\u3002", "conclusion": "\u9700\u5efa\u7acb\u6297\u6c61\u67d3\u8bc4\u4f30\u57fa\u51c6\u5e76\u5728\u591a\u6a21\u578b\u5bb6\u65cf\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u7814\u7a76\u7ed3\u8bba\u7684\u666e\u9002\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.10548", "pdf": "https://arxiv.org/pdf/2507.10548", "abs": "https://arxiv.org/abs/2507.10548", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.", "AI": {"tldr": "\u63d0\u51faEmRACE-3K\u6570\u636e\u96c6\u89e3\u51b3VLMs\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u63a8\u7406\u5c40\u9650\uff0c\u901a\u8fc7\u76d1\u7763+\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u4f7f\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u5f53\u524dVLMs\u5728\u4e3b\u52a8\u4ea4\u4e92\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u7a0b\u89c4\u5212\u65b9\u9762\u5b58\u5728\u660e\u663e\u7f3a\u9677", "method": "\u4f7f\u7528Unreal Engine\u6784\u5efa3,000+\u5177\u8eab\u4efb\u52a1\uff0c\u5efa\u7acb\u4e09\u7ef4\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u91c7\u7528\u76d1\u7763\u5b66\u4e60+\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03Qwen2.5-VL-7B\u6a21\u578b", "result": "\u96f6\u6837\u672c\u6210\u529f\u7387\u4f4e\u4e8e20%\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u5728\u5bfc\u822a/\u64cd\u4f5c/\u591a\u9636\u6bb5\u4efb\u52a1\u4e2d\u5747\u5b9e\u73b0\u663e\u8457\u63d0\u5347", "conclusion": "EmRACE-3K\u6709\u6548\u4fc3\u8fdb\u5177\u8eab\u63a8\u7406\u80fd\u529b\u53d1\u5c55\uff0c\u8bc1\u660e\u6570\u636e\u96c6\u5bf9\u63d0\u5347\u6a21\u578b\u4ea4\u4e92\u51b3\u7b56\u80fd\u529b\u7684\u5173\u952e\u4f5c\u7528"}}
