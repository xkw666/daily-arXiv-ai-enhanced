<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 3]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization](https://arxiv.org/abs/2511.00010)
*Jiajun Zhang,Jianke Zhang,Zeyu Cui,Jiaxi Yang,Lei Zhang,Binyuan Hui,Qiang Liu,Zilei Wang,Liang Wang,Junyang Lin*

Main category: cs.CL

TL;DR: 提出了PlotCraft基准测试和PlotCraftor模型，显著提升复杂数据可视化任务中LLM的代码生成性能（困难任务提升超50%）


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂结构化数据可视化任务中的能力缺乏系统性评估与优化，需构建专门基准与数据集填补空白

Method: 1. 创建包含1k任务、7类场景、48种图表类型的PlotCraft基准 2. 通过协同代理框架合成高质量SynthVis-30K数据集 3. 开发专用模型PlotCraftor

Result: PlotCraftor在VisEval、PandasPlotBench及PlotCraft中达到商用模型水平，困难任务性能提升超50%

Conclusion: 通过系统性基准构建与专用数据集开发，显著提升了LLM在复杂可视化场景的代码生成能力，填补该领域评估与优化空白

Abstract: Recent Large Language Models (LLMs) have demonstrated remarkable profi-
ciency in code generation. However, their ability to create complex visualiza-
tions for scaled and structured data remains largely unevaluated and
underdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark
featuring 1k challenging visualization tasks that cover a wide range of topics,
such as fi- nance, scientific research, and sociology. The benchmark is
structured around seven high-level visualization tasks and encompasses 48
distinct chart types. Cru- cially, it is the first to systematically evaluate
both single-turn generation and multi-turn refinement across a diverse spectrum
of task complexities. Our com- prehensive evaluation of 23 leading LLMs on
PlotCraft reveals obvious per- formance deficiencies in handling sophisticated
visualization tasks. To bridge this performance gap, we develope SynthVis-30K,
a large-scale, high-quality dataset of complex visualization code synthesized
via a collaborative agent frame- work. Building upon this dataset, we develope
PlotCraftor, a novel code gener- ation model that achieves strong capabilities
in complex data visualization with a remarkably small size. Across VisEval,
PandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance
comparable to that of leading propri- etary approaches. Especially, on hard
task, Our model achieves over 50% per- formance improvement. We will release
the benchmark, dataset, and code at
https://github.com/Speakn0w/PlotCraft-Benchmark.

</details>


### [2] [Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference](https://arxiv.org/abs/2511.00115)
*Haoyuan Li,Yuanbo Tong,Yuchen Li,Zirui Wang,Chunhou Liu,Jiamou Liu*

Main category: cs.CL

TL;DR: 提出ProtoMBTI框架，通过原型理论与LLM流程结合改进MBTI性格分类，在准确率、可解释性和跨数据集泛化方面均有提升。


<details>
  <summary>Details</summary>
Motivation: 传统硬标签分类无法反映人格判断的渐进性与原型特性，需构建认知对齐的推理框架。

Method: 1. 通过LLM多维度增强构建平衡语料库
2. LoRA微调轻量编码器学习特征嵌入
3. 检索-聚合-修正-保留循环推理机制

Result: 在Kaggle和Pandora基准测试中，MBTI四维度和16类型任务均优于基线模型，跨数据集泛化能力显著。

Conclusion: 将心理原型推理融入文本性格建模，可同步提升模型精度、可解释性和迁移能力。

Abstract: Personality recognition from text is typically cast as hard-label
classification, which obscures the graded, prototype-like nature of human
personality judgments. We present ProtoMBTI, a cognitively aligned framework
for MBTI inference that operationalizes prototype theory within an LLM-based
pipeline. First, we construct a balanced, quality-controlled corpus via
LLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).
Next, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative
embeddings and to standardize a bank of personality prototypes. At inference,
we retrieve top-k prototypes for a query post and perform a
retrieve--reuse--revise--retain cycle: the model aggregates prototype evidence
via prompt-based voting, revises when inconsistencies arise, and, upon correct
prediction, retains the sample to continually enrich the prototype library.
Across Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both
the four MBTI dichotomies and the full 16-type task, and exhibits robust
cross-dataset generalization. Our results indicate that aligning the inference
process with psychological prototype reasoning yields gains in accuracy,
interpretability, and transfer for text-based personality modeling.

</details>


### [3] [ParaScopes: What do Language Models Activations Encode About Future Text?](https://arxiv.org/abs/2511.00180)
*Nicky Pochinkov,Yulia Volkova,Anna Vasileva,Sai V R Chereddy*

Main category: cs.CL

TL;DR: 提出Residual Stream Decoders框架，通过解码残差流激活信息实现语言模型段落级/文档级规划监控，验证其在小型模型中可解码5+未来token的信息有效性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法局限于token级或特定概念分析，无法适应语言模型处理长时程任务时的规划信息监控需求。

Method: 开发残差流解码器框架，采用多种探测方法分析模型激活中编码的段落级和文档级规划信息。

Result: 实验证明该方法在小型模型中可解码相当于5+未来token的信息量，显著提升长时规划监控能力。

Conclusion: 该框架为深入理解语言模型的长时规划编码机制及开发更有效的监控工具奠定了基础。

Abstract: Interpretability studies in language models often investigate forward-looking
representations of activations. However, as language models become capable of
doing ever longer time horizon tasks, methods for understanding activations
often remain limited to testing specific concepts or tokens. We develop a
framework of Residual Stream Decoders as a method of probing model activations
for paragraph-scale and document-scale plans. We test several methods and find
information can be decoded equivalent to 5+ tokens of future context in small
models. These results lay the groundwork for better monitoring of language
models and better understanding how they might encode longer-term planning
information.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [4] [Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images](https://arxiv.org/abs/2511.00702)
*Alberto Di Biase*

Main category: cs.GR

TL;DR: 将医学影像中的扩散张量成像技术应用于绘画风格渲染，通过纤维追踪算法模拟艺术家笔触布局


<details>
  <summary>Details</summary>
Motivation: 探索医学领域DTI技术向艺术领域迁移应用的可行性，尝试将生物组织纤维追踪原理转化为数字艺术创作方法

Method: 使用结构张量替代梯度作为方向信息，开发类似DTI纤维追踪的笔触布局算法

Result: 成功应用于肖像画和通用图像，验证了通过tractography框架指导艺术创作的可行性

Conclusion: 该研究为跨领域技术迁移提供了创新范例，证明医学成像算法在艺术渲染中具有应用潜力

Abstract: Doctors and researchers routinely use diffusion tensor imaging (DTI) and
tractography to visualize the fibrous structure of tissues in the human body.
This paper explores the connection of these techniques to the painterly
rendering of images. Using a tractography algorithm the presented method can
place brush strokes that mimic the painting process of human artists,
analogously to how fibres are tracked in DTI. The analogue to the diffusion
tensor for image orientation is the structural tensor, which can provide better
local orientation information than the gradient alone. I demonstrate this
technique in portraits and general images, and discuss the parallels between
fibre tracking and brush stroke placement, and frame it in the language of
tractography. This work presents an exploratory investigation into the
cross-domain application of diffusion tensor imaging techniques to painterly
rendering of images. All the code is available at
https://github.com/tito21/st-python

</details>


### [5] [Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning](https://arxiv.org/abs/2511.00898)
*Heng Zhang,Jing Liu,Jiajun Wu,Haochen You,Lubin Gan,Yuling Shi,Xiaodong Gu,Zijian Zhang,Shuai Chen,Wenjun Huang,Jin Huang*

Main category: cs.GR

TL;DR: 提出无需训练的DuoGLM双视角框架，通过局部关系感知模板和全局拓扑角色推理，显著提升大语言模型在图结构理解中的表现。零样本节点分类准确率提升14.3%，跨域AUC提升7.6%。


<details>
  <summary>Details</summary>
Motivation: 现有方法将图拓扑编码为静态特征，缺乏结构到语义的显式推理机制，导致在桥接节点/中心节点等结构重要位置表现不佳，尤其在零样本场景下问题突出。

Method: 设计双视角框架：1) 局部视角构建节点与邻居的语义交互模板；2) 全局视角通过拓扑角色推断生成结构位置功能描述，形成互补推理机制。

Result: 在8个基准数据集上验证：零样本节点分类准确率提升14.3%，跨领域迁移任务AUC提升7.6%，显著优于现有方法。

Conclusion: 显式角色推理机制能有效增强LLMs的图结构理解能力，证实拓扑角色与语义关联的显式建模对图学习任务的关键作用。

Abstract: Large Language Models have emerged as a promising approach for graph learning
due to their powerful reasoning capabilities. However, existing methods exhibit
systematic performance degradation on structurally important nodes such as
bridges and hubs. We identify the root cause of these limitations. Current
approaches encode graph topology into static features but lack reasoning
scaffolds to transform topological patterns into role-based interpretations.
This limitation becomes critical in zero-shot scenarios where no training data
establishes structure-semantics mappings. To address this gap, we propose
DuoGLM, a training-free dual-perspective framework for structure-aware graph
reasoning. The local perspective constructs relation-aware templates capturing
semantic interactions between nodes and neighbors. The global perspective
performs topology-to-role inference to generate functional descriptions of
structural positions. These complementary perspectives provide explicit
reasoning mechanisms enabling LLMs to distinguish topologically similar but
semantically different nodes. Extensive experiments across eight benchmark
datasets demonstrate substantial improvements. DuoGLM achieves 14.3\% accuracy
gain in zero-shot node classification and 7.6\% AUC improvement in cross-domain
transfer compared to existing methods. The results validate the effectiveness
of explicit role reasoning for graph understanding with LLMs.

</details>


### [6] [G2rammar: Bilingual Grammar Modeling for Enhanced Text-attributed Graph Learning](https://arxiv.org/abs/2511.00911)
*Heng Zheng,Haochen You,Zijun Liu,Zijian Zhang,Lubin Gan,Hao Zhang,Wenjun Huang,Jin Huang*

Main category: cs.GR

TL;DR: 提出G2rammar双语语法框架，通过结构语法和语义语法增强文本属性图的理解，提升语言模型对图结构的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有图语言方法缺乏语法标注，无法明确节点结构角色，限制了语言模型对图拓扑的推理能力。

Method: 设计结构语法(通过中心性和邻域模式定义拓扑角色)与语义语法(通过文本信息量捕捉内容关系)，采用结构预训练+语义微调的两阶段学习框架。

Result: 在真实数据集实验中，G2rammar持续优于现有基线模型，语法框架使模型准确率平均提升3.2%。

Conclusion: 显式编码图结构和语义的语法规则，有效解决了传统图线性化方法缺乏语法角色标注的核心缺陷。

Abstract: Text-attributed graphs require models to effectively integrate both
structural topology and semantic content. Recent approaches apply large
language models to graphs by linearizing structures into token sequences
through random walks. These methods create concise graph vocabularies to
replace verbose natural language descriptions. However, they overlook a
critical component that makes language expressive: grammar. In natural
language, grammar assigns syntactic roles to words and defines their functions
within sentences. Similarly, nodes in graphs play distinct structural roles as
hubs, bridges, or peripheral members. Current graph language methods provide
tokens without grammatical annotations to indicate these structural or semantic
roles. This absence limits language models' ability to reason about graph
topology effectively. We propose \textbf{G2rammar}, a bilingual grammar
framework that explicitly encodes both structural and semantic grammar for
text-attributed graphs. Structural grammar characterizes topological roles
through centrality and neighborhood patterns. Semantic grammar captures content
relationships through textual informativity. The framework implements two-stage
learning with structural grammar pre-training followed by semantic grammar
fine-tuning. Extensive experiments on real-world datasets demonstrate that
G2rammar consistently outperforms competitive baselines by providing language
models with the grammatical context needed to understand graph structures.

</details>


### [7] [An Adjoint Method for Differentiable Fluid Simulation on Flow Maps](https://arxiv.org/abs/2511.01259)
*Zhiqi Li,Jinjin He,Barnabás Börcsök,Taiyuan Zhang,Duowen Chen,Tao Du,Ming C. Lin,Greg Turk,Bo Zhu*

Main category: cs.GR

TL;DR: 提出基于双向流映射的伴随求解器，通过共享流映射实现高效可微分流体模拟，内存占用仅6.53GB（192^3分辨率）


<details>
  <summary>Details</summary>
Motivation: 传统伴随方法需要存储中间变量并对数值步骤求导，新方法利用流映射共享特性突破计算瓶颈

Method: 1. 利用流映射双向传输流体冲量变量和伴随变量
2. 构建长期-短期时间稀疏流映射表示
3. 直接在流映射上求解伴随方程

Result: 实现192^3分辨率下6.53GB内存占用，保持涡流追踪精度，支持涡流识别/预测/控制新任务

Conclusion: 该方法为涡流动力学的精确建模与控制开辟新可能，特别适用于需要长程精确微分的流体模拟场景

Abstract: This paper presents a novel adjoint solver for differentiable fluid
simulation based on bidirectional flow maps. Our key observation is that the
forward fluid solver and its corresponding backward, adjoint solver share the
same flow map as the forward simulation. In the forward pass, this map
transports fluid impulse variables from the initial frame to the current frame
to simulate vortical dynamics. In the backward pass, the same map propagates
adjoint variables from the current frame back to the initial frame to compute
gradients. This shared long-range map allows the accuracy of gradient
computation to benefit directly from improvements in flow map construction.
Building on this insight, we introduce a novel adjoint solver that solves the
adjoint equations directly on the flow map, enabling long-range and accurate
differentiation of incompressible flows without differentiating intermediate
numerical steps or storing intermediate variables, as required in conventional
adjoint methods. To further improve efficiency, we propose a long-short
time-sparse flow map representation for evolving adjoint variables. Our
approach has low memory usage, requiring only 6.53GB of data at a resolution of
$192^3$ while preserving high accuracy in tracking vorticity, enabling new
differentiable simulation tasks that require precise identification,
prediction, and control of vortex dynamics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Object-Aware 4D Human Motion Generation](https://arxiv.org/abs/2511.00248)
*Shurui Gui,Deep Anil Patel,Xiner Li,Martin Renqiang Min*

Main category: cs.CV

TL;DR: 提出基于3D高斯表示与运动扩散先验的MSDI框架，通过LLM语义引导和MSDS优化，实现物理合理的零样本人体运动生成


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型因缺乏3D物理先验导致运动变形、语义违规和物理不一致问题

Method: 结合LLM的空间语义分析与MSDS运动评分蒸馏，通过预训练运动扩散模型优化人体运动轨迹

Result: 生成符合3D空间约束的自然人体运动，在分布外数据上展现良好泛化能力

Conclusion: 本框架为4D内容生成提供了可扩展解决方案，无需重新训练即可实现物理合理的人机交互运动

Abstract: Recent advances in video diffusion models have enabled the generation of
high-quality videos. However, these videos still suffer from unrealistic
deformations, semantic violations, and physical inconsistencies that are
largely rooted in the absence of 3D physical priors. To address these
challenges, we propose an object-aware 4D human motion generation framework
grounded in 3D Gaussian representations and motion diffusion priors. With
pre-generated 3D humans and objects, our method, Motion Score Distilled
Interaction (MSDI), employs the spatial and prompt semantic information in
large language models (LLMs) and motion priors through the proposed Motion
Diffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs
enables our spatial-aware motion optimization, which distills score gradients
from pre-trained motion diffusion models, to refine human motion while
respecting object and semantic constraints. Unlike prior methods requiring
joint training on limited interaction datasets, our zero-shot approach avoids
retraining and generalizes to out-of-distribution object aware human motions.
Experiments demonstrate that our framework produces natural and physically
plausible human motions that respect 3D spatial context, offering a scalable
solution for realistic 4D generation.

</details>


### [9] [Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery](https://arxiv.org/abs/2511.00362)
*Momen Khandoker Ope,Akif Islam,Mohd Ruhul Ameen,Abu Saleh Musa Miah,Md Rashedul Islam,Jungpil Shin*

Main category: cs.CV

TL;DR: Oitijjo-3D框架利用Google街景和生成式AI技术，低成本实现文化遗产3D数字化保护，解决发展中国家资源技术双重挑战。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国文化遗产面临传统3D数字化方法（如摄影测量/LiDAR）成本高、需专家的困境，导致大量建筑遗产缺乏数字保护。

Method: 两阶段AI流程：1) Gemini 2.5 Flash多模态合成结构纹理；2) Hexagen神经网络实现图像到3D几何重建。

Result: 系统秒级生成高保真3D模型（如Ahsan Manzil等案例），速度远超传统方法，且无需专业设备/专家监督。

Conclusion: 将开放图像转化为数字遗产，重构了资源有限国家文化保护的范式——AI辅助的社区驱动文化延续行动。

Abstract: Cultural heritage restoration in Bangladesh faces a dual challenge of limited
resources and scarce technical expertise. Traditional 3D digitization methods,
such as photogrammetry or LiDAR scanning, require expensive hardware, expert
operators, and extensive on-site access, which are often infeasible in
developing contexts. As a result, many of Bangladesh's architectural treasures,
from the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to
decay and inaccessible in digital form. This paper introduces Oitijjo-3D, a
cost-free generative AI framework that democratizes 3D cultural preservation.
By using publicly available Google Street View imagery, Oitijjo-3D reconstructs
faithful 3D models of heritage structures through a two-stage pipeline -
multimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture
synthesis, and neural image-to-3D generation through Hexagen for geometry
recovery. The system produces photorealistic, metrically coherent
reconstructions in seconds, achieving significant speedups compared to
conventional Structure-from-Motion pipelines, without requiring any specialized
hardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,
Choto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both
visual and structural fidelity while drastically lowering economic and
technical barriers. By turning open imagery into digital heritage, this work
reframes preservation as a community-driven, AI-assisted act of cultural
continuity for resource-limited nations.

</details>
