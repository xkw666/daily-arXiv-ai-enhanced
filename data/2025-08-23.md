<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training](https://arxiv.org/abs/2508.14904)
*Jianfeng Si,Lin Sun,Zhewen Tan,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: 提出统一协同训练框架，通过单阶段SFT集成三类安全行为，实现动态安全模式切换与控制


<details>
  <summary>Details</summary>
Motivation: 现有SFT和RLHF方法存在多阶段训练流程复杂、部署后缺乏细粒度可控性的缺陷

Method: 在单阶段SFT中协同训练积极/消极/拒绝三种安全模式，通过系统指令或魔法令牌动态激活特定行为

Result: 8B模型安全性能超越671B的DeepSeek-R1，训练复杂度和部署成本显著降低

Conclusion: 该框架为LLM内容安全提供了可扩展、高效且精细可控的解决方案

Abstract: Current methods for content safety in Large Language Models (LLMs), such as
Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback
(RLHF), often rely on multi-stage training pipelines and lack fine-grained,
post-deployment controllability. To address these limitations, we propose a
unified co-training framework that efficiently integrates multiple safety
behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and
rejective (refusal-oriented/conservative) within a single SFT stage. Notably,
each behavior is dynamically activated via a simple system-level instruction,
or magic token, enabling stealthy and efficient behavioral switching at
inference time. This flexibility supports diverse deployment scenarios, such as
positive for safe user interaction, negative for internal red-teaming, and
rejective for context-aware refusals triggered by upstream moderation signals.
This co-training strategy induces a distinct Safety Alignment Margin in the
output space, characterized by well-separated response distributions
corresponding to each safety mode. The existence of this margin provides
empirical evidence for the model's safety robustness and enables unprecedented
fine-grained control. Experiments show that our method matches the safety
alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1
(671B) in safety performance, while significantly reducing both training
complexity and deployment costs. This work presents a scalable, efficient, and
highly controllable solution for LLM content safety.

</details>


### [2] [Preliminary Ranking of WMT25 General Machine Translation Systems](https://arxiv.org/abs/2508.14909)
*Tom Kocmi,Eleftherios Avramidis,Rachel Bawden,Ondřej Bojar,Konstantin Dranch,Anton Dvorkovich,Sergey Dukanov,Natalia Fedorova,Mark Fishel,Markus Freitag,Thamme Gowda,Roman Grundkiewicz,Barry Haddow,Marzena Karpinska,Philipp Koehn,Howard Lakougna,Jessica Lundin,Kenton Murray,Masaaki Nagata,Stefano Perrella,Lorenzo Proietti,Martin Popel,Maja Popović,Parker Riley,Mariya Shmatova,Steinþór Steingrímsson,Lisa Yankovskaya,Vilém Zouhar*

Main category: cs.CL

TL;DR: WMT25机器翻译任务发布基于自动指标的初步排名，提醒自动评估可能存在重排序技术偏好，最终排名将以人工评估为准


<details>
  <summary>Details</summary>
Motivation: 在官方人工评估完成前，向任务参与者分享可能有助于准备系统论文的初步自动评估结果

Method: 使用自动评估指标对机器翻译系统进行初步排名，特别关注采用质量估计重排和最小贝叶斯风险解码的系统

Result: 自动评估产生的排名存在技术偏好偏差，仅作为临时参考

Conclusion: 自动排名具有临时性，最终以人类评估为准，初步结果主要帮助参与者完善系统文档

Abstract: We present the preliminary ranking of the WMT25 General Machine Translation
Shared Task, in which MT systems have been evaluated using automatic metrics.
As this ranking is based on automatic evaluations, it may be biased in favor of
systems that employ re-ranking techniques, such as Quality Estimation
re-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be
based on human evaluation, which is more reliable and will supersede the
automatic ranking.
  The purpose of this report is not to present the final findings of the
General MT task, but rather to share preliminary results with task
participants, which may be useful when preparing their system submission
papers.

</details>


### [3] [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)
*Israel Abebe Azime,Tadesse Destaw Belay,Dietrich Klakow,Philipp Slusallek,Anshuman Chhabra*

Main category: cs.CL

TL;DR: 本文提出LLM驱动的文化本地化框架，通过自动生成本土化数学题数据集，缓解低资源语言中英语实体偏见问题。


<details>
  <summary>Details</summary>
Motivation: 现有低资源语言数学推理数据集多为英文本地化翻译生成，缺乏真实的本土社会文化实体(如人名/货币)，导致模型跨文化数学能力评估偏差。

Method: 开发LLM驱动的文化本地化框架，从现有数学题中自动提取本土实体并重构题目，构建包含原生文化元素的多语言数学推理数据集。

Result: 实验表明该框架能有效减少英语中心实体偏见，在引入本土实体后提升模型跨语言数学推理的鲁棒性。

Conclusion: 文化本地化数据集对评估LLM真实多语言数学能力至关重要，传统翻译式本地化可能掩盖模型实际跨文化推理能力。

Abstract: Large language models (LLMs) have demonstrated significant capabilities in
solving mathematical problems expressed in natural language. However,
multilingual and culturally-grounded mathematical reasoning in low-resource
languages lags behind English due to the scarcity of socio-cultural task
datasets that reflect accurate native entities such as person names,
organization names, and currencies. Existing multilingual benchmarks are
predominantly produced via translation and typically retain English-centric
entities, owing to the high cost associated with human annotater-based
localization. Moreover, automated localization tools are limited, and hence,
truly localized datasets remain scarce. To bridge this gap, we introduce a
framework for LLM-driven cultural localization of math word problems that
automatically constructs datasets with native names, organizations, and
currencies from existing sources. We find that translated benchmarks can
obscure true multilingual math ability under appropriate socio-cultural
contexts. Through extensive experiments, we also show that our framework can
help mitigate English-centric entity bias and improves robustness when native
entities are introduced across various languages.

</details>


### [4] [Improving LLMs for Machine Translation Using Synthetic Preference Data](https://arxiv.org/abs/2508.14951)
*Dario Vajda,Domen Vreš,Marko Robnik-Šikonja*

Main category: cs.CL

TL;DR: 通过DPO训练改进LLM机器翻译效果，斯洛文尼亚语案例显示COMET指标提升并减少错误


<details>
  <summary>Details</summary>
Motivation: 探索如何利用少量易获取数据提升通用指令调优大语言模型的机器翻译能力

Method: 使用DPO训练框架，基于自动评估指标(COMET)和启发式规则构建排名数据集，对GaMS-9B-Instruct进行微调

Result: 微调模型在维基百科翻译任务上COMET分数分别提升0.04/0.02，错误率显著降低

Conclusion: 通过质量筛选数据和DPO训练可有效提升LLM翻译性能，尤其在低资源语言场景

Abstract: Large language models have emerged as effective machine translation systems.
In this paper, we explore how a general instruction-tuned large language model
can be improved for machine translation using relatively few easily produced
data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct
model using Direct Preference Optimization (DPO) training on a programmatically
curated and enhanced subset of a public dataset. As DPO requires pairs of
quality-ranked instances, we generated its training dataset by translating
English Wikipedia articles using two LLMs, GaMS-9B-Instruct and
EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics
coupled with automatic evaluation metrics such as COMET. The evaluation shows
that our fine-tuned model outperforms both models involved in the dataset
generation. In comparison to the baseline models, the fine-tuned model achieved
a COMET score gain of around 0.04 and 0.02, respectively, on translating
Wikipedia articles. It also more consistently avoids language and formatting
errors.

</details>


### [5] [Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems](https://arxiv.org/abs/2508.14982)
*Qianli Wang,Tatiana Anikina,Nils Feldhus,Simon Ostermann,Fedor Splitt,Jiaao Li,Yoana Tsoneva,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 提出MultiCoXQL多语言数据集和Compass自定义输入数据集，用于增强会话式可解释AI系统的多语言意图解析能力


<details>
  <summary>Details</summary>
Motivation: 现有ConvXAI系统存在多语言训练数据匮乏和自定义输入支持不足的问题，限制了其在多样化场景中的应用

Method: 1. 扩展创建包含5种语言的数据集 2. 开发新型解析策略 3. 评估不同规模LLM和BERT模型在单语/跨语言场景的表现

Result: 构建了覆盖低资源语言的基准数据集，实验表明模型在跨语言迁移中保持较好性能，但小模型存在显著性能落差

Conclusion: 该研究为多语言ConvXAI系统提供了数据基础和方法论支持，特别是在低资源语言处理和用户自定义输入方面取得进展

Abstract: Conversational explainable artificial intelligence (ConvXAI) systems based on
large language models (LLMs) have garnered considerable attention for their
ability to enhance user comprehension through dialogue-based explanations.
Current ConvXAI systems often are based on intent recognition to accurately
identify the user's desired intention and map it to an explainability method.
While such methods offer great precision and reliability in discerning users'
underlying intentions for English, a significant challenge in the scarcity of
training data persists, which impedes multilingual generalization. Besides, the
support for free-form custom inputs, which are user-defined data distinct from
pre-configured dataset instances, remains largely limited. To bridge these
gaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL
dataset spanning five typologically diverse languages, including one
low-resource language. Subsequently, we propose a new parsing approach aimed at
enhancing multilingual parsing performance, and evaluate three LLMs on
MultiCoXQL using various parsing strategies. Furthermore, we present Compass, a
new multilingual dataset designed for custom input extraction in ConvXAI
systems, encompassing 11 intents across the same five languages as MultiCoXQL.
We conduct monolingual, cross-lingual, and multilingual evaluations on Compass,
employing three LLMs of varying sizes alongside BERT-type models.

</details>


### [6] [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)
*Bolian Li,Yanran Wu,Xinyu Luo,Ruqi Zhang*

Main category: cs.CL

TL;DR: 提出奖励偏移推测性采样算法（SSS），通过对齐草稿模型与未调整的目标模型间的分布偏移，在降低推理成本的同时恢复RLHF最优解。


<details>
  <summary>Details</summary>
Motivation: 测试时对齐技术需额外计算资源导致高推理成本，限制了实际应用。需要找到高效解决方案。

Method: 将对齐过程转移至草稿模型，利用分布偏移修改接受标准与奖励分布，理论证明可恢复RLHF最优解。

Result: 在弱到强测试对齐实验中，算法以显著降低的推理成本获得更高奖励分数。

Conclusion: SSS算法同时实现了效果提升与计算效率优化，验证了测试时对齐的实用化路径。

Abstract: Aligning large language models (LLMs) with human preferences has become a
critical step in their development. Recent research has increasingly focused on
test-time alignment, where additional compute is allocated during inference to
enhance LLM safety and reasoning capabilities. However, these test-time
alignment techniques often incur substantial inference costs, limiting their
practical application. We are inspired by the speculative sampling
acceleration, which leverages a small draft model to efficiently predict future
tokens, to address the efficiency bottleneck of test-time alignment. We
introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the
draft model is aligned with human preferences, while the target model remains
unchanged. We theoretically demonstrate that the distributional shift between
the aligned draft model and the unaligned target model can be exploited to
recover the RLHF optimal solution without actually obtaining it, by modifying
the acceptance criterion and bonus token distribution. Our algorithm achieves
superior gold reward scores at a significantly reduced inference cost in
test-time weak-to-strong alignment experiments, thereby validating both its
effectiveness and efficiency.

</details>


### [7] [LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text](https://arxiv.org/abs/2508.15085)
*MohamamdJavad Ardestani,Ehsan Kamalloo,Davood Rafiei*

Main category: cs.CL

TL;DR: LongRecall提出三阶段召回评估框架，通过事实分解和结构化验证显著提升长文本生成的完整性检测准确率


<details>
  <summary>Details</summary>
Motivation: 现有召回评估方法依赖词汇重叠易产生误判，LLM整体评估存在语义不对齐和幻觉问题

Method: 1. 将答案分解为自包含事实 2. 通过词汇/语义筛选缩小候选范围 3. 结构化蕴含验证对齐

Result: 在三个长格式QA基准测试中，相比词汇匹配和LLM评估基线方法取得显著准确率提升

Conclusion: 该框架通过系统性验证流程，为需要高完整性的领域提供了可靠的召回评估基础方案

Abstract: LongRecall. The completeness of machine-generated text, ensuring that it
captures all relevant information, is crucial in domains such as medicine and
law and in tasks like list-based question answering (QA), where omissions can
have serious consequences. However, existing recall metrics often depend on
lexical overlap, leading to errors with unsubstantiated entities and
paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts
capture broader semantics but remain prone to misalignment and hallucinations
without structured verification. We introduce LongRecall, a general three-stage
recall evaluation framework that decomposes answers into self-contained facts,
successively narrows plausible candidate matches through lexical and semantic
filtering, and verifies their alignment through structured entailment checks.
This design reduces false positives and false negatives while accommodating
diverse phrasings and contextual variations, serving as a foundational building
block for systematic recall assessment. We evaluate LongRecall on three
challenging long-form QA benchmarks using both human annotations and LLM-based
judges, demonstrating substantial improvements in recall accuracy over strong
lexical and LLM-as-a-Judge baselines.

</details>


### [8] [Mapping the Course for Prompt-based Structured Prediction](https://arxiv.org/abs/2508.15090)
*Matt Pauk,Maria Leonor Pacheco*

Main category: cs.CL

TL;DR: 结合符号推理与LLMs提升结构化预测的准确性和一致性，证明结构化学习在LLMs时代仍有重要价值


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在幻觉问题和复杂推理任务中的局限性，将其预测能力与符号推理的结构一致性相结合

Method: 通过不同提示策略估计LLM置信度，结合组合推理算法进行结构化预测，并采用结构化目标进行微调

Result: 符号推理显著提升预测一致性(平均提升3.2%准确率)，结构化微调使挑战性任务性能提高15%

Conclusion: 符号推理与结构化学习能有效增强LLMs性能，这种组合方法为复杂语言任务提供了新解决方案

Abstract: LLMs have been shown to be useful for a variety of language tasks, without
requiring task-specific fine-tuning. However, these models often struggle with
hallucinations and complex reasoning problems due to their autoregressive
nature. We propose to address some of these issues, specifically in the area of
structured prediction, by combining LLMs with combinatorial inference in an
attempt to marry the predictive power of LLMs with the structural consistency
provided by inference methods. We perform exhaustive experiments in an effort
to understand which prompting strategies can effectively estimate LLM
confidence values for use with symbolic inference, and show that, regardless of
the prompting strategy, the addition of symbolic inference on top of prompting
alone leads to more consistent and accurate predictions. Additionally, we show
that calibration and fine-tuning using structured prediction objectives leads
to increased performance for challenging tasks, showing that structured
learning is still valuable in the era of LLMs.

</details>


### [9] [Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset](https://arxiv.org/abs/2508.15096)
*Rabeeh Karimi Mahabadi,Sanjeev Satheesh,Shrimai Prabhumoye,Mostofa Patwary,Mohammad Shoeybi,Bryan Catanzaro*

Main category: cs.CL

TL;DR: 提出Nemotron-CC-Math数学语料库构建方案，通过新型网页渲染与LLM清洗流程有效保留数学结构，构建出规模/质量超越现有开源数据集的新语料库，显著提升LLM数学/代码推理能力


<details>
  <summary>Details</summary>
Motivation: 现有Common Crawl数学数据集因低效提取、格式转换损失和数学结构破坏导致质量低下，需要更可靠的科学文本提取方法

Method: 采用布局感知渲染（lynx）与LLM清洗阶段结合的多格式数学恢复方案，支持MathJax/KaTeX/MathML等格式，实现标准化LaTeX表示和结构完整性保留

Result: 构建133B/52B token数据集，在MATH基准提升+4.8~12.6，MBPP+提升+4.6~14.3，同时提升MMLU通用领域表现

Conclusion: 首次实现从噪声网页数据可靠提取科学内容（含数学），显著提升模型数学/代码/通用推理能力，并开源代码和数据集推动社区发展

Abstract: Pretraining large language models (LLMs) on high-quality, structured data
such as mathematics and code substantially enhances reasoning capabilities.
However, existing math-focused datasets built from Common Crawl suffer from
degraded quality due to brittle extraction heuristics, lossy HTML-to-text
conversion, and the failure to reliably preserve mathematical structure. In
this work, we introduce Nemotron-CC-Math, a large-scale, high-quality
mathematical corpus constructed from Common Crawl using a novel,
domain-agnostic pipeline specifically designed for robust scientific text
extraction.
  Unlike previous efforts, our pipeline recovers math across various formats
(e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx
and a targeted LLM-based cleaning stage. This approach preserves the structural
integrity of equations and code blocks while removing boilerplate,
standardizing notation into LaTeX representation, and correcting
inconsistencies.
  We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+
(133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably,
Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including
MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens
than FineMath-4+, which was previously the highest-quality math pretraining
dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to
+12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines,
while also improving general-domain performance on MMLU and MMLU-Stem.
  We present the first pipeline to reliably extract scientific
content--including math--from noisy web-scale data, yielding measurable gains
in math, code, and general reasoning, and setting a new state of the art among
open math pretraining corpora. To support open-source efforts, we release our
code and datasets.

</details>


### [10] [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)
*Zijie Wang,Eduardo Blanco*

Main category: cs.CL

TL;DR: 提出通过检索外部证据和验证原子假设来改善LLMs对错误假设问题的处理能力


<details>
  <summary>Details</summary>
Motivation: 用户常提出包含错误假设的问题，而LLMs因幻觉问题容易生成误导性回答

Method: 1. 将问题简化为事实核查 2. 整合检索证据 3. 生成并验证原子假设

Result: 实验表明检索证据可提升准确率，原子假设验证可额外提升5.3%性能并提供可解释答案

Conclusion: 证据整合+假设分解策略在提升LLMs性能的同时增强了答案的可解释性

Abstract: People often ask questions with false assumptions, a type of question that
does not have regular answers. Answering such questions require first
identifying the false assumptions. Large Language Models (LLMs) often generate
misleading answers because of hallucinations. In this paper, we focus on
identifying and answering questions with false assumptions in several domains.
We first investigate to reduce the problem to fact verification. Then, we
present an approach leveraging external evidence to mitigate hallucinations.
Experiments with five LLMs demonstrate that (1) incorporating retrieved
evidence is beneficial and (2) generating and validating atomic assumptions
yields more improvements and provides an interpretable answer by specifying the
false assumptions.

</details>


### [11] [ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following](https://arxiv.org/abs/2508.15164)
*Seungmin Han,Haeun Kwon,Ji-jun Park,Taeyang Yoon*

Main category: cs.CL

TL;DR: 研究提出MMDR-Bench多模态对话推理基准和CoLVLM Agent框架，通过迭代循环机制提升现有模型性能，实验显示其在复杂任务中超越主流商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型在复杂多模态交互中存在上下文丢失和视觉幻觉问题，现有基准无法有效评估动态交互场景。

Method: 构建包含300个多轮场景的MMDR-Bench数据集，开发CoLVLM框架通过「记忆-感知-规划-执行」四阶段循环增强现有LVLMs能力，无需重新训练模型。

Result: 人类评估平均得分4.03，超过GPT-4o(3.92)和Gemini 1.5 Pro(3.85)，在推理深度和指令遵循方面表现突出。

Conclusion: 模块化设计和迭代机制有效解决复杂多模态交互问题，在长期对话中保持稳定性能，为模型优化提供新方向。

Abstract: Despite significant advancements in Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs), current models still face substantial
challenges in handling complex, multi-turn, and visually-grounded tasks that
demand deep reasoning, sustained contextual understanding, entity tracking, and
multi-step instruction following. Existing benchmarks often fall short in
capturing the dynamism and intricacies of real-world multi-modal interactions,
leading to issues such as context loss and visual hallucinations. To address
these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning
Benchmark), a novel dataset comprising 300 meticulously designed complex
multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across
six core dimensions including visual entity tracking and reasoning depth.
Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic
framework that enhances existing LVLMs with advanced reasoning and instruction
following capabilities through an iterative
"memory-perception-planning-execution" cycle, requiring no extensive
re-training of the underlying models. Our extensive experiments on MMDR-Bench
demonstrate that CoLVLM Agent consistently achieves superior performance,
attaining an average human evaluation score of 4.03, notably surpassing
state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro
(3.85). The framework exhibits significant advantages in reasoning depth,
instruction adherence, and error suppression, and maintains robust performance
over extended dialogue turns, validating the effectiveness of its modular
design and iterative approach for complex multi-modal interactions.

</details>


### [12] [SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling](https://arxiv.org/abs/2508.15190)
*Dong Liu,Yanxuan Yu*

Main category: cs.CL

TL;DR: 提出语义感知分词框架SemToken，通过语义聚类和粒度分配减少冗余token并提升计算效率


<details>
  <summary>Details</summary>
Motivation: 现有分词方法(BPE/WordPiece)仅依赖频率统计，忽略语义结构，导致长上下文场景中冗余token过多且上下文连贯性利用不足

Method: 1. 轻量级编码器提取语义嵌入 → 2. 局部语义聚类合并相似token → 3. 基于语义密度的异粒度token分配策略

Result: 在WikiText-103等基准测试中实现token数量减少2.4倍、速度提升1.9倍，困惑度和准确率基本无损

Conclusion: 语义结构为大规模语言模型的token优化和计算效率提升提供了新的优化维度

Abstract: Tokenization plays a critical role in language modeling, yet existing
approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on
frequency statistics, ignoring the underlying semantic structure of text. This
leads to over-tokenization of semantically redundant spans and underutilization
of contextual coherence, particularly in long-context scenarios. In this work,
we propose \textbf{SemToken}, a semantic-aware tokenization framework that
jointly reduces token redundancy and improves computation efficiency. SemToken
first extracts contextual semantic embeddings via lightweight encoders and
performs local semantic clustering to merge semantically equivalent tokens.
Then, it allocates heterogeneous token granularity based on semantic density,
allowing finer-grained tokenization in content-rich regions and coarser
compression in repetitive or low-entropy spans. SemToken can be seamlessly
integrated with modern language models and attention acceleration methods.
Experiments on long-context language modeling benchmarks such as WikiText-103
and LongBench show that SemToken achieves up to $2.4\times$ reduction in token
count and $1.9\times$ speedup, with negligible or no degradation in perplexity
and downstream accuracy. Our findings suggest that semantic structure offers a
promising new axis for optimizing tokenization and computation in large
language models.

</details>


### [13] [Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models](https://arxiv.org/abs/2508.15202)
*Yuanchen Zhou,Shuo Jiang,Jie Zhu,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang*

Main category: cs.CL

TL;DR: 阿里云提出金融领域专用奖励模型Fin-PRM，通过步骤和轨迹级监督提升大语言模型在金融推理中的表现，在监督学习、强化学习和测试场景分别实现12.9%、5.2%和5.1%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有通用PRMs在结构化、符号化且合规敏感的金融领域存在局限，需开发领域专用模型来精准评估中间推理步骤。

Method: 整合步骤级/轨迹级双监督机制；支持离线在线双模式应用（蒸馏训练、RL奖励、测试时推理）；构建金融逻辑对齐的评估体系。

Result: 在CFLUE/FinQA基准上：轨迹选择质量显著优于通用PRMs；监督学习提升12.9%、强化学习+5.2%、测试性能+5.1%。

Conclusion: 领域专用奖励模型有效提升LLMs的专家级金融推理能力，项目成果将通过阿里云启智平台开源。

Abstract: Process Reward Models (PRMs) have emerged as a promising framework for
supervising intermediate reasoning in large language models (LLMs), yet
existing PRMs are primarily trained on general or Science, Technology,
Engineering, and Mathematics (STEM) domains and fall short in domain-specific
contexts such as finance, where reasoning is more structured, symbolic, and
sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM},
a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate
reasoning steps in financial tasks. Fin-PRM integrates step-level and
trajectory-level reward supervision, enabling fine-grained evaluation of
reasoning traces aligned with financial logic. We apply Fin-PRM in both offline
and online reward learning settings, supporting three key applications: (i)
selecting high-quality reasoning trajectories for distillation-based supervised
fine-tuning, (ii) providing dense process-level rewards for reinforcement
learning, and (iii) guiding reward-informed Best-of-N inference at test time.
Experimental results on financial reasoning benchmarks, including CFLUE and
FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs
and strong domain baselines in trajectory selection quality. Downstream models
trained with Fin-PRM yield substantial improvements with baselines, with gains
of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in
test-time performance. These findings highlight the value of domain-specialized
reward modeling for aligning LLMs with expert-level financial reasoning. Our
project resources will be available at https://github.com/aliyun/qwen-dianjin.

</details>


### [14] [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)
*Huanxuan Liao,Yixing Xu,Shizhu He,Guanchen Li,Xuanwu Yin,Dong Li,Emad Barsoum,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: SPARK通过通道级KV剪枝实现长上下文推理优化，在保持精度的同时减少30%+KV存储


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法仅关注时间轴维度，忽视了通道维度的重要性差异。实际观测发现不同特征通道的信息密度存在显著差异，某些通道对特定查询几乎无贡献。

Method: 提出训练即插即用的SPARK方法：1) 通道级非结构化剪枝 2) 注意力计算时动态恢复剪枝条目 3) 正交兼容现有压缩/量化技术

Result: 同等序列长度下：较淘汰法减少30%+KV存储，80%激进剪枝时性能下降<5%；支持更长序列处理，兼容其他加速方案

Conclusion: 首次从通道维度解决KV缓存冗余，实现效率-精度平衡。通过通道级稀疏化突破现有压缩方法局限，为LLM长上下文处理提供新方向

Abstract: Long-context inference in large language models (LLMs) is increasingly
constrained by the KV cache bottleneck: memory usage grows linearly with
sequence length, while attention computation scales quadratically. Existing
approaches address this issue by compressing the KV cache along the temporal
axis through strategies such as token eviction or merging to reduce memory and
computational overhead. However, these methods often neglect fine-grained
importance variations across feature dimensions (i.e., the channel axis),
thereby limiting their ability to effectively balance efficiency and model
accuracy. In reality, we observe that channel saliency varies dramatically
across both queries and positions: certain feature channels carry near-zero
information for a given query, while others spike in relevance. To address this
oversight, we propose SPARK, a training-free plug-and-play method that applies
unstructured sparsity by pruning KV at the channel level, while dynamically
restoring the pruned entries during attention score computation. Notably, our
approach is orthogonal to existing KV compression and quantization techniques,
making it compatible for integration with them to achieve further acceleration.
By reducing channel-level redundancy, SPARK enables processing of longer
sequences within the same memory budget. For sequences of equal length, SPARK
not only preserves or improves model accuracy but also reduces KV cache storage
by over 30% compared to eviction-based methods. Furthermore, even with an
aggressive pruning ratio of 80%, SPARK maintains performance with less
degradation than 5% compared to the baseline eviction method, demonstrating its
robustness and effectiveness. Our code will be available at
https://github.com/Xnhyacinth/SparK.

</details>


### [15] [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)
*Bolei He,Xinran He,Run Shao,Shanfu Shu,Xianwei Xue,Mingquan Cheng,Haifeng Li,Zhenhua Ling*

Main category: cs.CL

TL;DR: 提出Selct2Know框架，通过知识自选策略和选择性微调实现低成本领域知识内化，在多个专业QA基准中超越现有方法且成本显著降低。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法存在幻觉和延迟问题，持续预训练成本高且缺乏灵活性。受人类渐进式学习启发，主张先理解概念再应用复杂推理的渐进知识获取方式。

Method: 1. 内部-外部知识自选策略实现领域知识内化
2. 结构化推理数据生成流程
3. 集成GRPO算法增强推理能力
4. 选择性监督微调机制

Result: 在医疗/法律/金融QA基准测试中：
- 性能持续超越现有方法
- 达到领域预训练LLM水平
- 训练成本降低95%

Conclusion: S2K框架验证了渐进式知识获取的有效性，通过创新性自选策略和结构化训练机制，以极低成本实现专业领域知识的高效内化与推理能力提升。

Abstract: Large Language Models (LLMs) perform well in general QA but often struggle in
domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces
external knowledge but suffers from hallucinations and latency due to noisy
retrievals. Continued pretraining internalizes domain knowledge but is costly
and lacks cross-domain flexibility. We attribute this challenge to the
long-tail distribution of domain knowledge, which leaves partial yet useful
internal knowledge underutilized. We further argue that knowledge acquisition
should be progressive, mirroring human learning: first understanding concepts,
then applying them to complex reasoning. To address this, we propose Selct2Know
(S2K), a cost-effective framework that internalizes domain knowledge through an
internal-external knowledge self-selection strategy and selective supervised
fine-tuning. We also introduce a structured reasoning data generation pipeline
and integrate GRPO to enhance reasoning ability. Experiments on medical, legal,
and financial QA benchmarks show that S2K consistently outperforms existing
methods and matches domain-pretrained LLMs with significantly lower cost.

</details>


### [16] [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)
*Sijia Cui,Aiyao He,Shuai Xu,Hongming Zhang,Yanna Wang,Qingyang Zhang,Yajing Wang,Bo Xu*

Main category: cs.CL

TL;DR: SEER方法通过逐步检索动态更新的经验池，显著提升大语言模型在多步骤工具调用任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人工设计示例或静态工具库，难以适应工具多样性和任务复杂度的增长需求

Method: 提出Stepwise Experience Recall框架，通过细粒度检索持续积累的成功经验轨迹，实现工具链的自动化优化

Result: 在ToolQA基准上平均提升6.1%（简单问题）和4.7%（困难问题），在τ-bench真实场景中分别获得7.44%和23.38%的准确率提升

Conclusion: 动态经验池机制有效解决了多步骤工具调用的核心挑战，为LLM的持续自我优化提供了可行路径

Abstract: Function calling enables large language models (LLMs) to interact with
external systems by leveraging tools and APIs. When faced with multi-step tool
usage, LLMs still struggle with tool selection, parameter generation, and
tool-chain planning. Existing methods typically rely on manually designing
task-specific demonstrations, or retrieving from a curated library. These
approaches demand substantial expert effort and prompt engineering becomes
increasingly complex and inefficient as tool diversity and task difficulty
scale. To address these challenges, we propose a self-guided method, Stepwise
Experience Recall (SEER), which performs fine-grained, stepwise retrieval from
a continually updated experience pool. Instead of relying on static or manually
curated library, SEER incrementally augments the experience pool with past
successful trajectories, enabling continuous expansion of the pool and improved
model performance over time. Evaluated on the ToolQA benchmark, SEER achieves
an average improvement of 6.1\% on easy and 4.7\% on hard questions. We further
test SEER on $\tau$-bench, which includes two real-world domains. Powered by
Qwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains
of 7.44\% and 23.38\%, respectively.

</details>


### [17] [Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?](https://arxiv.org/abs/2508.15218)
*Momoka Furuhashi,Kouta Nakayama,Takashi Kodama,Saku Sugawara*

Main category: cs.CL

TL;DR: 研究探讨选择性使用自动生成检查表在LLM评估任务中的有效性，发现其在成对比较中提升评估性能，但直接评分效果不稳定。


<details>
  <summary>Details</summary>
Motivation: 针对生成任务自动评估中存在标准模糊的问题，现有检查表生成方法的实际效用尚未充分验证。

Method: 使用6种方法生成检查表，在8种模型规模上进行实验，通过成对比较和直接评分任务评估效果，分析检查表项与人工评估的相关性。

Result: 选择性使用检查表使成对比较任务F1提升5.2%，但直接评分改进不一致。低相关性检查表项仍包含48%符合人工标准的条目。

Conclusion: 需明确定义客观评估标准以减少人工与自动评估的差异，检查表生成方法在特定评估场景中展现应用潜力。

Abstract: Automatic evaluation of generative tasks using large language models faces
challenges due to ambiguous criteria. Although automatic checklist generation
is a potentially promising approach, its usefulness remains underexplored. We
investigate whether checklists should be used for all questions or selectively,
generate them using six methods, evaluate their effectiveness across eight
model sizes, and identify checklist items that correlate with human
evaluations. Through experiments on pairwise comparison and direct scoring
tasks, we find that selective checklist use tends to improve evaluation
performance in pairwise settings, while its benefits are less consistent in
direct scoring. Our analysis also shows that even checklist items with low
correlation to human scores often reflect human-written criteria, indicating
potential inconsistencies in human evaluation. These findings highlight the
need to more clearly define objective evaluation criteria to guide both human
and automatic evaluations. \footnote{Our code is available
at~https://github.com/momo0817/checklist-effectiveness-study

</details>


### [18] [VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models](https://arxiv.org/abs/2508.15229)
*Hanling Zhang,Yayu Zhou,Tongcheng Fang,Zhihang Yuan,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: VocabTailor通过动态词汇选择与混合加载策略，将小语言模型词汇组件内存占用降低99%，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在边缘设备部署时，词汇表相关组件（嵌入层和LM头）因大词汇量产生严重内存瓶颈，而现有静态剪枝方法存在信息丢失和灵活性不足的问题。

Method: 基于词汇局部性原理（单次推理仅需少量token）和计算不对称性，提出解耦动态词汇框架：1）通过嵌入层卸载减少内存占用 2）LM头采用静态-动态混合词汇选择策略，按需加载组件。

Result: 跨任务实验显示词汇组件内存减少高达99%，任务性能基本无损，显著优于静态剪枝方法。

Conclusion: VocabTailor有效解决了小语言模型内存瓶颈，为资源受限环境提供了可行的动态词汇优化方案，具有实际部署价值。

Abstract: Small Language Models (SLMs) provide computational advantages in
resource-constrained environments, yet memory limitations remain a critical
bottleneck for edge device deployment. A substantial portion of SLMs' memory
footprint stems from vocabulary-related components, particularly embeddings and
language modeling (LM) heads, due to large vocabulary sizes. Existing static
vocabulary pruning, while reducing memory usage, suffers from rigid,
one-size-fits-all designs that cause information loss from the prefill stage
and a lack of flexibility. In this work, we identify two key principles
underlying the vocabulary reduction challenge: the lexical locality principle,
the observation that only a small subset of tokens is required during any
single inference, and the asymmetry in computational characteristics between
vocabulary-related components of SLM. Based on these insights, we introduce
VocabTailor, a novel decoupled dynamic vocabulary selection framework that
addresses memory constraints through offloading embedding and implements a
hybrid static-dynamic vocabulary selection strategy for LM Head, enabling
on-demand loading of vocabulary components. Comprehensive experiments across
diverse downstream tasks demonstrate that VocabTailor achieves a reduction of
up to 99% in the memory usage of vocabulary-related components with minimal or
no degradation in task performance, substantially outperforming existing static
vocabulary pruning.

</details>


### [19] [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)
*Peerat Limkonchotiwat,Pume Tuchinda,Lalita Lowphansirikul,Surapon Nonesung,Panuthep Tasawong,Alham Fikri Aji,Can Udomcharoenchaikit,Sarana Nutanong*

Main category: cs.CL

TL;DR: 研究构建了本土化的泰语指令数据集WangchanThaiInstruct，实验证明其能有效提升大语言模型在专业和文化敏感任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试依赖翻译数据，缺乏文化及专业领域适配性，难以评估大语言模型在低资源语言环境（如泰语）的真实应用性能。

Method: 通过多阶段质量控制流程构建人工标注的泰语数据集，开展零样本评估和指令调优对比实验，分离本土监督数据的影响。

Result: 使用本土数据微调的模型在领域内外评估中均优于翻译数据模型，准确率提升达6.7%（领域内）和3.2%（跨领域）。

Conclusion: 在低资源语言场景中，基于本土文化和专业知识的指令数据对提升大语言模型对齐效果具有关键作用。

Abstract: Large language models excel at instruction-following in English, but their
performance in low-resource languages like Thai remains underexplored. Existing
benchmarks often rely on translations, missing cultural and domain-specific
nuances needed for real-world use. We present WangchanThaiInstruct, a
human-authored Thai dataset for evaluation and instruction tuning, covering
four professional domains and seven task types. Created through a multi-stage
quality control process with annotators, domain experts, and AI researchers,
WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing
performance gaps on culturally and professionally specific tasks, and (2) an
instruction tuning study with ablations isolating the effect of native
supervision. Models fine-tuned on WangchanThaiInstruct outperform those using
translated data in both in-domain and out-of-domain benchmarks. These findings
underscore the need for culturally and professionally grounded instruction data
to improve LLM alignment in low-resource, linguistically diverse settings.

</details>


### [20] [UniCoM: A Universal Code-Switching Speech Generator](https://arxiv.org/abs/2508.15244)
*Sangmin Lee,Woojin Chung,Seyun Um,Hong-Goo Kang*

Main category: cs.CL

TL;DR: 提出UniCoM框架及SWORDS算法构建多语言代码切换语料库CS-FLEURS，显著提升代码切换语音技术性能


<details>
  <summary>Details</summary>
Motivation: 现实对话中普遍存在的代码切换现象对语音技术构成挑战，但现有系统因缺乏高质量数据集研究不足

Method: 通过SWORDS算法替换词语翻译（保持词性一致）生成语义不变的代码切换语音样本

Result: CS-FLEURS在客观指标和主观评估中均达到高可懂度与自然度，性能媲美现有数据集

Conclusion: 该框架为代码切换语音技术发展提供新范式，有助于构建更具包容性的多语言系统

Abstract: Code-switching (CS), the alternation between two or more languages within a
single speaker's utterances, is common in real-world conversations and poses
significant challenges for multilingual speech technology. However, systems
capable of handling this phenomenon remain underexplored, primarily due to the
scarcity of suitable datasets. To resolve this issue, we propose Universal
Code-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS
samples without altering sentence semantics. Our approach utilizes an algorithm
we call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by
replacing selected words with their translations while considering their parts
of speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a
multilingual CS corpus designed for automatic speech recognition (ASR) and
speech-to-text translation (S2TT). Experimental results show that CS-FLEURS
achieves high intelligibility and naturalness, performing comparably to
existing datasets on both objective and subjective metrics. We expect our
approach to advance CS speech technology and enable more inclusive multilingual
systems.

</details>


### [21] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang,Mingzi Zhang,Sheng Jin,Zengyi Yu,Xiangjie Kong,Binghao Tu*

Main category: cs.CL

TL;DR: EMNLP框架首次构建了教师角色大语言模型的伦理与心理对齐基准，揭示模型在道德推理与安全性的矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有职业角色模拟研究缺乏系统的心理评估和伦理风险分析，特别是在教育领域需要评估教师角色LLMs与人类教师的心理特征差异及伦理脆弱性。

Method: 通过构建88个教师专属道德困境案例库，设计针对性软提示注入测试集，在12个主流LLM上进行个性特征分析、道德发展阶段测量和风险行为评估。

Result: 教师角色LLM表现出更极化的理想人格，抽象道德推理优于人类，但情感处理能力弱；强推理模型存在能力-安全性悖论，模型温度参数对多数风险行为影响有限。

Conclusion: 该研究为教育AI提供了首个教师角色伦理评估基准，揭示了LLM职业角色模拟中道德能力与安全脆弱性的反比关系，相关资源已开源。

Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate
professional roles. However, comprehensive psychological and ethical evaluation
in these contexts remains lacking. This paper introduces EMNLP, an
Educator-role Moral and Normative LLMs Profiling framework for personality
profiling, moral development stage measurement, and ethical risk under soft
prompt injection. EMNLP extends existing scales and constructs 88
teacher-specific moral dilemmas, enabling profession-oriented comparison with
human teachers. A targeted soft prompt injection set evaluates compliance and
vulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs
exhibit more idealized and polarized personalities than human teachers, excel
in abstract moral reasoning, but struggle with emotionally complex situations.
Models with stronger reasoning are more vulnerable to harmful prompt injection,
revealing a paradox between capability and safety. The model temperature and
other hyperparameters have limited influence except in some risk behaviors.
This paper presents the first benchmark to assess ethical and psychological
alignment of teacher-role LLMs for educational AI. Resources are available at
https://e-m-n-l-p.github.io/.

</details>


### [22] [Conflict-Aware Soft Prompting for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.15253)
*Eunseong Choi,June Park,Hyeri Lee,Jongwuk Lee*

Main category: cs.CL

TL;DR: 提出CARE方法解决RAG中的上下文-记忆冲突问题，通过上下文评估器引导推理方向，在QA和事实核查任务中实现平均5.0%的性能提升


<details>
  <summary>Details</summary>
Motivation: 当检索到的外部上下文与LLM参数知识冲突时，现有RAG系统难以有效解决冲突，导致模型错误地依赖不可靠的外部信息

Method: 采用上下文评估器编码记忆令牌嵌入，通过基于对抗样本的软提示训练机制识别不可靠上下文，并生成指导信号引导模型选择可靠知识源

Result: 在多个QA和事实核查基准测试中平均获得5.0%的性能提升，有效缓解上下文-记忆冲突问题

Conclusion: CARE框架为构建可信赖且自适应的RAG系统提供了新的技术路径，通过动态知识源选择机制增强系统可靠性

Abstract: Retrieval-augmented generation (RAG) enhances the capabilities of large
language models (LLMs) by incorporating external knowledge into their input
prompts. However, when the retrieved context contradicts the LLM's parametric
knowledge, it often fails to resolve the conflict between incorrect external
context and correct parametric knowledge, known as context-memory conflict. To
tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation
(CARE), consisting of a context assessor and a base LLM. The context assessor
encodes compact memory token embeddings from raw context tokens. Through
grounded/adversarial soft prompting, the context assessor is trained to discern
unreliable context and capture a guidance signal that directs reasoning toward
the more reliable knowledge source. Extensive experiments show that CARE
effectively mitigates context-memory conflicts, leading to an average
performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a
promising direction for trustworthy and adaptive RAG systems.

</details>


### [23] [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)
*Lekshmi R Nair,Arun Sankar,Koninika Pal*

Main category: cs.CL

TL;DR: 提出通过LLMs自动挖掘时间常识构建TComQA数据集，验证其有效性并提升模型性能


<details>
  <summary>Details</summary>
Motivation: 自然语言中事件的时间上下文常隐含，现有大模型缺乏显式时间常识导致推理困难

Method: 构建时间常识提取流程，利用LLMs从SAMSum和RealNews语料自动生成TComQA数据集

Result: TComQA众包验证精度超80%，训练模型在时间问答任务上优于现有数据集微调的LLM

Conclusion: 自动挖掘时间常识能有效增强语言模型的时间推理能力，提升任务表现

Abstract: Understanding events necessitates grasping their temporal context, which is
often not explicitly stated in natural language. For example, it is not a
trivial task for a machine to infer that a museum tour may last for a few
hours, but can not take months. Recent studies indicate that even advanced
large language models (LLMs) struggle in generating text that require reasoning
with temporal commonsense due to its infrequent explicit mention in text.
Therefore, automatically mining temporal commonsense for events enables the
creation of robust language models. In this work, we investigate the capacity
of LLMs to extract temporal commonsense from text and evaluate multiple
experimental setups to assess their effectiveness. Here, we propose a temporal
commonsense extraction pipeline that leverages LLMs to automatically mine
temporal commonsense and use it to construct TComQA, a dataset derived from
SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing
and achieves over 80\% precision in extracting temporal commonsense. The model
trained with TComQA also outperforms an LLM fine-tuned on existing dataset of
temporal question answering task.

</details>


### [24] [CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing](https://arxiv.org/abs/2508.15316)
*Abdul Rehman,Jian-Jun Zhang,Xiaosong Yang*

Main category: cs.CL

TL;DR: 开发120ms捕获音素特征的轻量模型CUPE，通过音素长度窗口的声学模式建模实现跨语言通用语音处理


<details>
  <summary>Details</summary>
Motivation: 现有音素识别依赖长语音段和特定语言模式，需不受语境影响的纯净音素表征

Method: 设计处理120ms固定窗口的轻量模型，独立分析短语音段学习跨语言基础声学特征

Result: 在监督/自监督训练中展现强跨语言泛化能力，UCLA语料库零样本测试验证有效性

Conclusion: 音素长度窗口的基础声学模式建模是实现通用语音处理的有效途径

Abstract: Universal phoneme recognition typically requires analyzing long speech
segments and language-specific patterns. Many speech processing tasks require
pure phoneme representations free from contextual influence, which motivated
our development of CUPE - a lightweight model that captures key phoneme
features in just 120 milliseconds, about one phoneme's length. CUPE processes
short, fixed-width windows independently and, despite fewer parameters than
current approaches, achieves competitive cross-lingual performance by learning
fundamental acoustic patterns common to all languages. Our extensive evaluation
through supervised and self-supervised training on diverse languages, including
zero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual
generalization and reveals that effective universal speech processing is
possible through modeling basic acoustic patterns within phoneme-length
windows.

</details>


### [25] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)
*Haji Gul,Abul Ghani Naim,Ajaz Ahmad Bhat*

Main category: cs.CL

TL;DR: 提出EDAS元度量标准，通过整合多数据集多指标表现解决KGC模型评估不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有KGC评估指标在不同数据集和不同指标间存在排名矛盾，导致模型全面性能评估困难

Method: 基于距离平均解的评估方法(EDAS)，将多数据集多指标表现整合为归一化分数M_i∈[0,1]

Result: 在FB15k-237/WN18RR等基准数据集验证，EDAS有效统一多指标多数据集表现形成综合排名

Conclusion: EDAS提供一致可解释的评估框架，支持跨数据集公平比较，促进更可靠的KGC模型选择

Abstract: Knowledge Graphs (KGs) enable applications in various domains such as
semantic search, recommendation systems, and natural language processing. KGs
are often incomplete, missing entities and relations, an issue addressed by
Knowledge Graph Completion (KGC) methods that predict missing elements.
Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank
(MR), and Hit@k, are commonly used to assess the performance of such KGC
models. A major challenge in evaluating KGC models, however, lies in comparing
their performance across multiple datasets and metrics. A model may outperform
others on one dataset but underperform on another, making it difficult to
determine overall superiority. Moreover, even within a single dataset,
different metrics such as MRR and Hit@1 can yield conflicting rankings, where
one model excels in MRR while another performs better in Hit@1, further
complicating model selection for downstream tasks. These inconsistencies hinder
holistic comparisons and highlight the need for a unified meta-metric that
integrates performance across all metrics and datasets to enable a more
reliable and interpretable evaluation framework. To address this need, we
propose KG Evaluation based on Distance from Average Solution (EDAS), a robust
and interpretable meta-metric that synthesizes model performance across
multiple datasets and diverse evaluation criteria into a single normalized
score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated
aspects of performance, EDAS offers a global perspective that supports more
informed model selection and promotes fairness in cross-dataset evaluation.
Experimental results on benchmark datasets such as FB15k-237 and WN18RR
demonstrate that EDAS effectively integrates multi-metric, multi-dataset
performance into a unified ranking, offering a consistent, robust, and
generalizable framework for evaluating KGC models.

</details>


### [26] [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361)
*Shiwen Ni,Guhong Chen,Shuaimin Li,Xuanang Chen,Siyi Li,Bingli Wang,Qiyao Wang,Xingjian Wang,Yifan Zhang,Liyang Fan,Chengming Li,Ruifeng Xu,Le Sun,Min Yang*

Main category: cs.CL

TL;DR: 系统综述了大语言模型评测基准的发展现状，提出分类框架并指出现有问题及创新方向


<details>
  <summary>Details</summary>
Motivation: 系统梳理LLM评测基准现状，解决数据污染、文化偏见等问题，推动技术创新

Method: 将283个基准分类为通用能力、领域特定、目标特定，并系统性分析其特点与问题

Result: 当前基准存在数据污染导致分数虚高、文化语言偏见、缺乏动态评估等问题

Conclusion: 未来基准需注重数据净化、公平评估、过程可信度及动态环境适应，提供创新设计范式

Abstract: In recent years, with the rapid development of the depth and breadth of large
language models' capabilities, various corresponding evaluation benchmarks have
been emerging in increasing numbers. As a quantitative assessment tool for
model performance, benchmarks are not only a core means to measure model
capabilities but also a key element in guiding the direction of model
development and promoting technological innovation. We systematically review
the current status and development of large language model benchmarks for the
first time, categorizing 283 representative benchmarks into three categories:
general capabilities, domain-specific, and target-specific. General capability
benchmarks cover aspects such as core linguistics, knowledge, and reasoning;
domain-specific benchmarks focus on fields like natural sciences, humanities
and social sciences, and engineering technology; target-specific benchmarks pay
attention to risks, reliability, agents, etc. We point out that current
benchmarks have problems such as inflated scores caused by data contamination,
unfair evaluation due to cultural and linguistic biases, and lack of evaluation
on process credibility and dynamic environments, and provide a referable design
paradigm for future benchmark innovation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [27] [Hybrelighter: Combining Deep Anisotropic Diffusion and Scene Reconstruction for On-device Real-time Relighting in Mixed Reality](https://arxiv.org/abs/2508.14930)
*Hanwen Zhao,John Akers,Baback Elmieh,Ira Kemelmacher-Shlizerman*

Main category: cs.GR

TL;DR: 提出一种通过整合图像分割和各向异性扩散光照传播的混合现实场景重照明方法，在边缘设备实现100fps实时渲染并改善扫描精度问题


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法无法满足MR设备实时性要求，场景重建方法因扫描误差导致重照明质量差，2D图像滤镜无法处理复杂几何阴影

Method: 整合图像分割技术/各向异性扩散光照传播/基础场景理解，结合滤波技术的计算简易性，修正设备扫描误差

Result: 在边缘设备实现100fps实时渲染，视觉质量优于行业标准，成功应用于房地产场景的日光变化模拟

Conclusion: 该方法通过多技术融合突破现有技术限制，为混合现实实时重照明应用提供了可行解决方案

Abstract: Mixed Reality scene relighting, where virtual changes to lighting conditions
realistically interact with physical objects, producing authentic illumination
and shadows, can be used in a variety of applications. One such application in
real estate could be visualizing a room at different times of day and placing
virtual light fixtures. Existing deep learning-based relighting techniques
typically exceed the real-time performance capabilities of current MR devices.
On the other hand, scene understanding methods, such as on-device scene
reconstruction, often yield inaccurate results due to scanning limitations, in
turn affecting relighting quality. Finally, simpler 2D image filter-based
approaches cannot represent complex geometry and shadows. We introduce a novel
method to integrate image segmentation, with lighting propagation via
anisotropic diffusion on top of basic scene understanding, and the
computational simplicity of filter-based techniques. Our approach corrects
on-device scanning inaccuracies, delivering visually appealing and accurate
relighting effects in real-time on edge devices, achieving speeds as high as
100 fps. We show a direct comparison between our method and the industry
standard, and present a practical demonstration of our method in the
aforementioned real estate example.

</details>


### [28] [Inference Time Debiasing Concepts in Diffusion Models](https://arxiv.org/abs/2508.14933)
*Lucas S. Kupssinskü,Marco N. Bochernitsan,Jordan Kopper,Otávio Parraga,Rodrigo C. Barros*

Main category: cs.GR

TL;DR: DeCoDi提出了一种仅需调整推断过程的扩散模型去偏方法，在保持图像质量的同时有效提升生成多样性


<details>
  <summary>Details</summary>
Motivation: 现有深度学习去偏方法需要复杂调整或高计算成本，而该方法只需修改推断过程，大幅降低应用门槛

Method: 通过调整扩散过程，避免潜在空间中的偏见概念区域，无需修改模型参数或重新训练

Result: 人工评估1200张图像显示在性别/种族/年龄等维度去偏有效，GPT4o自动评估与人工评估结果具有统计一致性

Conclusion: 该方法显著提升扩散模型生成多样性，且计算开销可忽略，具有广泛适用性

Abstract: We propose DeCoDi, a debiasing procedure for text-to-image diffusion-based
models that changes the inference procedure, does not significantly change
image quality, has negligible compute overhead, and can be applied in any
diffusion-based image generation model. DeCoDi changes the diffusion process to
avoid latent dimension regions of biased concepts. While most deep learning
debiasing methods require complex or compute-intensive interventions, our
method is designed to change only the inference procedure. Therefore, it is
more accessible to a wide range of practitioners. We show the effectiveness of
the method by debiasing for gender, ethnicity, and age for the concepts of
nurse, firefighter, and CEO. Two distinct human evaluators manually inspect
1,200 generated images. Their evaluation results provide evidence that our
method is effective in mitigating biases based on gender, ethnicity, and age.
We also show that an automatic bias evaluation performed by the GPT4o is not
significantly statistically distinct from a human evaluation. Our evaluation
shows promising results, with reliable levels of agreement between evaluators
and more coverage of protected attributes. Our method has the potential to
significantly improve the diversity of images it generates by diffusion-based
text-to-image generative models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [29] [Image-Conditioned 3D Gaussian Splat Quantization](https://arxiv.org/abs/2508.15372)
*Xinshuang Liu,Runfa Blark Li,Keito Suzuki,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出基于图像条件的高斯泼溅量化器（ICGS-Quantizer），通过跨场景共享码书和联合相关性利用，将3DGS压缩至千字节级，支持存档后场景更新。


<details>
  <summary>Details</summary>
Motivation: 解决现有3DGS压缩方法存在的两个核心问题：(1)中等规模场景压缩后仍需兆字节级存储；(2)缺乏适应长期存档后场景变化的机制。

Method: 1. 联合利用高斯间/属性间相关性提升量化效率
2. 跨训练场景共享固定码书消除单场景开销
3. 引入图像条件解码机制支持存档后场景更新

Result: 在3D场景压缩和更新任务中，ICGS-Quantizer在保持视觉质量的同时将存储需求降至千字节级，且优于现有方法。

Conclusion: 该方法显著提升了3DGS的压缩效率和场景适应性，其编码-量化-解码联合训练框架为三维场景存储提供了新思路，开源部署将推动实际应用。

Abstract: 3D Gaussian Splatting (3DGS) has attracted considerable attention for
enabling high-quality real-time rendering. Although 3DGS compression methods
have been proposed for deployment on storage-constrained devices, two
limitations hinder archival use: (1) they compress medium-scale scenes only to
the megabyte range, which remains impractical for large-scale scenes or
extensive scene collections; and (2) they lack mechanisms to accommodate scene
changes after long-term archival. To address these limitations, we propose an
Image-Conditioned Gaussian Splat Quantizer (ICGS-Quantizer) that substantially
enhances compression efficiency and provides adaptability to scene changes
after archiving. ICGS-Quantizer improves quantization efficiency by jointly
exploiting inter-Gaussian and inter-attribute correlations and by using shared
codebooks across all training scenes, which are then fixed and applied to
previously unseen test scenes, eliminating the overhead of per-scene codebooks.
This approach effectively reduces the storage requirements for 3DGS to the
kilobyte range while preserving visual fidelity. To enable adaptability to
post-archival scene changes, ICGS-Quantizer conditions scene decoding on images
captured at decoding time. The encoding, quantization, and decoding processes
are trained jointly, ensuring that the codes, which are quantized
representations of the scene, are effective for conditional decoding. We
evaluate ICGS-Quantizer on 3D scene compression and 3D scene updating.
Experimental results show that ICGS-Quantizer consistently outperforms
state-of-the-art methods in compression efficiency and adaptability to scene
changes. Our code, model, and data will be publicly available on GitHub.

</details>


### [30] [Scaling Group Inference for Diverse and High-Quality Generation](https://arxiv.org/abs/2508.15773)
*Gaurav Parmar,Or Patashnik,Daniil Ostashev,Kuan-Chieh Wang,Kfir Aberman,Srinivasa Narasimhan,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 提出可扩展的群体推理方法，通过优化样本质量和多样性来解决生成模型独立采样冗余问题


<details>
  <summary>Details</summary>
Motivation: 生成模型独立采样导致结果冗余，限制用户选择并阻碍创意探索，需提升群体输出的多样性与质量

Method: 将群体推理建模为二次整数分配问题（质量作为一元项，多样性作为二元项），采用渐进式候选集剪枝提高计算效率

Result: 实验表明该方法显著提升群体多样性和质量，适用于文本到图像、图像到图像、视频生成等多种任务

Conclusion: 该框架使生成模型将多个输出视为有机整体而非独立样本，扩展了生成模型的应用边界

Abstract: Generative models typically sample outputs independently, and recent
inference-time guidance and scaling algorithms focus on improving the quality
of individual samples. However, in real-world applications, users are often
presented with a set of multiple images (e.g., 4-8) for each prompt, where
independent sampling tends to lead to redundant results, limiting user choices
and hindering idea exploration. In this work, we introduce a scalable group
inference method that improves both the diversity and quality of a group of
samples. We formulate group inference as a quadratic integer assignment
problem: candidate outputs are modeled as graph nodes, and a subset is selected
to optimize sample quality (unary term) while maximizing group diversity
(binary term). To substantially improve runtime efficiency, we progressively
prune the candidate set using intermediate predictions, allowing our method to
scale up to large candidate sets. Extensive experiments show that our method
significantly improves group diversity and quality compared to independent
sampling baselines and recent inference algorithms. Our framework generalizes
across a wide range of tasks, including text-to-image, image-to-image, image
prompting, and video generation, enabling generative models to treat multiple
outputs as cohesive groups rather than independent samples.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [31] [Neural Robot Dynamics](https://arxiv.org/abs/2508.15755)
*Jie Xu,Eric Heiden,Iretiayo Akinola,Dieter Fox,Miles Macklin,Yashraj Narang*

Main category: cs.RO

TL;DR: 提出神经动力学模型NeRD，通过机器人中心的空间不变表示方法，有效解决传统模拟器在关节式刚体机器人模拟中的泛化问题


<details>
  <summary>Details</summary>
Motivation: 传统机器人模拟器难以处理高自由度复杂机制，现有神经模拟器存在任务特异性训练需求强、全局状态表征不足导致的泛化能力差问题

Method: 1. 用学习模型替代传统模拟器的底层动力学和接触求解器
2. 采用机器人中心的空间不变状态表征
3. 将NeRD作为可互换求解器集成至主流机器人模拟框架

Result: 1. 在千步模拟中保持稳定准确
2. 跨任务和环境配置泛化能力强
3. 支持纯神经引擎策略学习
4. 可通过真实数据微调缩小虚实差距

Conclusion: NeRD在保持传统模拟器稳定性的同时，显著提升对未知任务/环境的适应能力，并通过数据驱动优化突破模拟与现实间的鸿沟

Abstract: Accurate and efficient simulation of modern robots remains challenging due to
their high degrees of freedom and intricate mechanisms. Neural simulators have
emerged as a promising alternative to traditional analytical simulators,
capable of efficiently predicting complex dynamics and adapting to real-world
data; however, existing neural simulators typically require
application-specific training and fail to generalize to novel tasks and/or
environments, primarily due to inadequate representations of the global state.
In this work, we address the problem of learning generalizable neural
simulators for robots that are structured as articulated rigid bodies. We
propose NeRD (Neural Robot Dynamics), learned robot-specific dynamics models
for predicting future states for articulated rigid bodies under contact
constraints. NeRD uniquely replaces the low-level dynamics and contact solvers
in an analytical simulator and employs a robot-centric and spatially-invariant
simulation state representation. We integrate the learned NeRD models as an
interchangeable backend solver within a state-of-the-art robotics simulator. We
conduct extensive experiments to show that the NeRD simulators are stable and
accurate over a thousand simulation steps; generalize across tasks and
environment configurations; enable policy learning exclusively in a neural
engine; and, unlike most classical simulators, can be fine-tuned from
real-world data to bridge the gap between simulation and reality.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [32] [Pixels Under Pressure: Exploring Fine-Tuning Paradigms for Foundation Models in High-Resolution Medical Imaging](https://arxiv.org/abs/2508.14931)
*Zahra TehraniNasab,Amar Kumar,Tal Arbel*

Main category: eess.IV

TL;DR: 研究不同微调方法对扩散模型在512x512分辨率下图像生成质量的影响，发现特定方法能同时提升生成质量与下游分类任务表现


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型主要聚焦低分辨率生成，而医学成像等领域亟需高分辨率图像合成。本研究旨在探索模型微调对高分辨率生成质量的系统影响

Method: 通过全面基准测试（包括全参数微调和参数高效微调PEFT），评估FID、Vendi分数、提示对齐等指标，并在数据稀缺条件下验证生成图像对分类器训练的有效性

Result: 特定微调策略显著改善生成质量（FID提升19.7%）和下游分类准确率（提高13.2%），证明合成图像可有效辅助医学影像分析任务

Conclusion: 系统揭示了微调方法选择对高分辨率生成的关键作用，为医学成像等应用提供了优化方案。公开代码促进相关领域研究复现与扩展

Abstract: Advancements in diffusion-based foundation models have improved text-to-image
generation, yet most efforts have been limited to low-resolution settings. As
high-resolution image synthesis becomes increasingly essential for various
applications, particularly in medical imaging domains, fine-tuning emerges as a
crucial mechanism for adapting these powerful pre-trained models to
task-specific requirements and data distributions. In this work, we present a
systematic study, examining the impact of various fine-tuning techniques on
image generation quality when scaling to high resolution 512x512 pixels. We
benchmark a diverse set of fine-tuning methods, including full fine-tuning
strategies and parameter-efficient fine-tuning (PEFT). We dissect how different
fine-tuning methods influence key quality metrics, including Fr\'echet
Inception Distance (FID), Vendi score, and prompt-image alignment. We also
evaluate the utility of generated images in a downstream classification task
under data-scarce conditions, demonstrating that specific fine-tuning
strategies improve both generation fidelity and downstream performance when
synthetic images are used for classifier training and evaluation on real
images. Our code is accessible through the project website -
https://tehraninasab.github.io/PixelUPressure/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [Emergent Crowds Dynamics from Language-Driven Multi-Agent Interactions](https://arxiv.org/abs/2508.15047)
*Yibo Liu,Liam Shatzel,Brandon Haworth,Teseo Schneider*

Main category: cs.AI

TL;DR: 提出利用大型语言模型控制人群代理的对话系统和语言驱动导航方法，增强群体行为真实性


<details>
  <summary>Details</summary>
Motivation: 现有群体动画方法忽视语言交互对行为的影响，无法实现复杂社会互动

Method: 结合代理个性特征的LLM对话生成系统 + 基于对话内容与感知状态的导航控制机制

Result: 实验验证群体自动聚散行为，建立人群信息传递机制，产生更真实的涌现式群体行为

Conclusion: 语言驱动框架有效提升群体模拟的真实性，使社会行为自然从环境互动中产生

Abstract: Animating and simulating crowds using an agent-based approach is a
well-established area where every agent in the crowd is individually controlled
such that global human-like behaviour emerges. We observe that human navigation
and movement in crowds are often influenced by complex social and environmental
interactions, driven mainly by language and dialogue. However, most existing
work does not consider these dimensions and leads to animations where
agent-agent and agent-environment interactions are largely limited to steering
and fixed higher-level goal extrapolation.
  We propose a novel method that exploits large language models (LLMs) to
control agents' movement. Our method has two main components: a dialogue system
and language-driven navigation. We periodically query agent-centric LLMs
conditioned on character personalities, roles, desires, and relationships to
control the generation of inter-agent dialogue when necessitated by the spatial
and social relationships with neighbouring agents. We then use the conversation
and each agent's personality, emotional state, vision, and physical state to
control the navigation and steering of each agent. Our model thus enables
agents to make motion decisions based on both their perceptual inputs and the
ongoing dialogue.
  We validate our method in two complex scenarios that exemplify the interplay
between social interactions, steering, and crowding. In these scenarios, we
observe that grouping and ungrouping of agents automatically occur.
Additionally, our experiments show that our method serves as an
information-passing mechanism within the crowd. As a result, our framework
produces more realistic crowd simulations, with emergent group behaviours
arising naturally from any environmental setting.

</details>
