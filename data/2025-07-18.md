<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 36]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CC](#cs.CC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 提出结合语言模型与概率程序的模型合成架构（MSA），实现人类开放式推理的机器模拟


<details>
  <summary>Details</summary>
Motivation: 探索人类如何整合分布式知识进行连贯推理，突破传统语言模型在开放式推理中的局限性

Method: 使用语言模型实现全局检索与模型合成，结合概率程序构建定制化世界模型，在Model Olympics体育场景数据集验证

Result: MSA在直接生成和思维链两种模式下，对人类判断的拟合度均显著优于纯语言模型基线

Conclusion: 模型合成架构为理解人类开放式推理提供了可行路径，展示了符号与分布式表示结合在复杂认知建模中的潜力

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [2] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 研究发现语言模型通过模态差异向量具备可靠的模态分类能力，其向量特征与人类分类行为存在关联，并为理解人类模态认知提供新视角。


<details>
  <summary>Details</summary>
Motivation: 针对近期研究质疑语言模型模态分类能力的现状（Michaelov等，2025；Kauf等，2023），探索模型内部是否存在可靠的模态分类表征机制。

Method: 运用机制可解释性技术，在不同语言模型中识别模态差异向量，分析其在训练阶段（训练步数/层级/参数量）的涌现规律，并与人类分类数据进行对比。

Result: 1. 发现模型具备超越先前认知的模态分类能力
2. 模态差异向量随模型能力提升呈现稳定涌现顺序
3. 模型向量投射可建模细粒度的人类分类行为
4. 向量投影与人类可解释特征评分存在相关性

Conclusion: 通过机制解释技术揭示了语言模型模态分类的新机制，其发现既提升模型可解释性，也为理解人类模态分类认知提供计算神经科学层面的启示。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [3] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 首个开源车臣语-俄语互译模型及配套数据集发布，BLEU/ChrF++分数俄译车8.34/34.69，车译俄20.89/44.55


<details>
  <summary>Details</summary>
Motivation: 针对濒危车臣语资源匮乏现状，开发首个开源翻译系统以支持语言保护和技术应用

Method: 基于NLLB-200多语言模型进行微调，收集并行词汇/短语/句子语料构建训练数据集

Result: 俄→车臣翻译BLEU 8.34/ChrF++34.69，反向翻译达20.89/44.55；同步发布适配车臣语的多语言句子编码器

Conclusion: 成功构建车臣语翻译基础设施，为低资源语言保护提供可复现技术方案，推动NLP技术在濒危语言领域的应用

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [4] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 微调BioClinicalBERT模型在药物过量死亡监测中展现接近完美的分类性能（F1≥0.998），显著优于传统方法和LLM模型


<details>
  <summary>Details</summary>
Motivation: 传统ICD-10编码存在信息延迟/丢失问题，需自动化解决方案改进药物过量监测

Method: 使用35,433条死亡记录训练NLP模型，比较传统分类器、微调编码器模型（BERT/BioClinicalBERT）与解码器大模型（Qwen 3/Llama 3），通过F1分数评估性能

Result: BioClinicalBERT内部测试F1≥0.998，外部验证F1=0.966，全面超越常规机器学习、通用BERT及各类大语言模型

Conclusion: 微调临床NLP模型能实现高精度、可扩展的药物监测，突破人工编码限制，支持实时追踪新兴药物使用趋势

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [5] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: 提出AdaptiSent框架，通过自适应跨模态注意力机制提升多模态方面情感分析性能，实验证明其在精度和跨模态关系理解上显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 传统文本模型和现有多模态方法在捕捉文本与图像的复杂交互关系方面存在局限，需要更有效的跨模态注意力机制来提升情感分析和方面词提取的准确性

Method: 结合动态模态加权机制和上下文自适应注意力，通过分析文本线索与视觉背景的交互关系，优化情感分类和方面词提取的联合建模

Result: 在Twitter数据集上实现精度、召回率和F1值全面超越基线模型，尤其在跨模态关系识别任务中提升显著（关键指标提升10-15%）

Conclusion: AdaptiSent通过动态上下文感知机制为多模态情感分析设立新标杆，验证了自适应跨模态注意力在复杂多模态数据分析中的核心价值

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [6] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: 提出AudioJudge框架，利用大型音频模型实现统一语音评估，通过音频拼接和上下文学习提升性能，多维度集成方法达到0.91人类偏好相关性


<details>
  <summary>Details</summary>
Motivation: 解决传统语音评估系统需针对单一声学特征设计专用系统，以及自动评估方法与人类偏好相关性差的双重挑战

Method: 系统探索LAM在声学特征检测（发音/语速/音质）和系统级偏好模拟中的应用，采用音频拼接+上下文学习策略，提出多维度集成评估方法（内容/音质/副语言特征三模块）

Result: 音频拼接策略使性能提升显著，集成方法在系统排名基准上Spearman系数达0.91。鲁棒性分析显示模型在噪声下稳定但存在位置偏差

Conclusion: AudioJudge为通用语音评估提供有效框架，多维度分解方法显著提升评估效果，但需通过去偏差策略解决模型固有偏好问题

Abstract: Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [7] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

TL;DR: 提出FLEXITOKENS可学习分词框架，通过动态字节分割显著提升语言模型的跨领域适应性


<details>
  <summary>Details</summary>
Motivation: 传统子词分词器在适应新数据分布时存在刚性，导致跨语言/领域时出现过度分词问题。现有基于梯度的分词方法受固定压缩率限制，形成新的适应性瓶颈。

Method: 开发可学习边界预测模块，将字节序列编码为变长片段；提出FLEXITOKENS训练目标，消除固定压缩率限制，实现动态自适应的分词策略。

Result: 在多语言基准测试中减少分词碎片化，下游任务性能提升达10%；在形态复杂任务和跨领域场景表现优于传统子词/梯度分词器。

Conclusion: FLEXITOKENS突破了传统分词器的刚性限制，为语言模型提供了动态适应能力，在跨语言处理和多领域应用场景具有重要实践价值。

Abstract: Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [8] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: TransEvalnia是基于大语言模型的翻译评估系统，通过多维质量指标实现细粒度评估，在多个语言对中表现优于现有方法，评估结果与人类评分高度相关。


<details>
  <summary>Details</summary>
Motivation: 解决传统翻译评估方法自动化程度低、维度单一的问题，通过大语言模型的推理能力实现更全面可靠的翻译质量评估与排序。

Method: 1. 使用Claude-3.5-Sonnet和Qwen-2-72B作为评估模型
2. 基于多维质量指标(MQM)子集设计评估维度
3. 提出位置偏差修正方法
4. 在WMT多语言数据和自建英日数据集上验证

Result: 1. 在英日翻译等任务中超越MT-Ranker
2. LLM评估结果与人类评分相关性达0.82
3. 评估结果人类接受度达85%
4. 发现翻译位置偏差并提出解决方案

Conclusion: TransEvalnia证明了LLM在翻译评估中的有效性，其多维评估框架和偏差修正方法为自动化翻译质量评估提供了可靠解决方案，开源数据推动领域发展。

Abstract: We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [9] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 提出通过动态策略切换提升狼人杀代理性能的方法，验证其有效性


<details>
  <summary>Details</summary>
Motivation: 现有基于提示工程的狼人杀代理使用隐式策略，缺乏情境适应能力

Method: 基于游戏上下文和玩家角色估计显式选择策略

Result: 策略自适应代理相比隐式/固定策略基线表现更优

Conclusion: 显式策略切换机制有效提升了狼人杀代理的上下文适应能力

Abstract: This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [10] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: 提出无需额外训练的ThinkLogit方法，通过小模型引导大模型实现复杂推理，性能提升显著


<details>
  <summary>Details</summary>
Motivation: 现有方法解锁大模型长链推理能力需额外训练，寻求更高效的无训练/少训练方案

Method: 1. ThinkLogit：解码时利用小模型的logits引导大模型
2. ThinkLogit-DPO：通过偏好优化训练引导模型

Result: 在数学数据集上pass@1相对提升26%-29%，强化学习迁移提升13%

Conclusion: 通过轻量级方法有效激发大模型长程推理能力，显著降低计算成本

Abstract: Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [11] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

TL;DR: Synergy是一种无需分词器的字节级语言模型，通过分层抽象路由机制实现高效处理，在相同规模下性能优于Llama3并产生更少标记。


<details>
  <summary>Details</summary>
Motivation: 探索端到端的分词器自由架构，通过分层抽象机制解决传统字节级模型（如BBPE）标记冗余问题，提升处理效率。

Method: 使用字节级语言模型架构，在端到端训练中自发学习分词策略，中间层采用无位置编码设计实现位置无关概念提取。

Result: 产生的概念标记比BBPE减少15%，相同规模下困惑度比Llama3降低0.4；中间层去除位置编码后准确率提升2.1%。

Conclusion: 证明了分层抽象路由机制的可行性，为开发完全端到端的语言模型提供了新范式，推动NLP管道向更灵活方向发展。

Abstract: In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [12] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 提出通过大模型蒸馏多样化否定模式数据，结合对比学习微调BERT模型，有效提升文本编码器的否定语义理解能力


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器在否定语义捕捉上存在不足，影响依赖文本嵌入的下游任务性能，需提升小模型在否定场景的鲁棒性

Method: 1. 从大语言模型蒸馏包含多种否定/限制性表达的训练数据
2. 采用对比学习框架微调BERT-base模型
3. 方法适配至大语言模型进行验证

Result: 1. 在否定理解基准上取得显著提升（+15.2%）
2. 保持通用语义理解任务竞争力（平均下降<0.8%）
3. 大模型适配后否定任务提升8.3%

Conclusion: 该方法成功构建了具有否定鲁棒性的文本编码器，验证了方法对大语言模型的普适性，为语义理解系统提供新解决方案

Abstract: Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [13] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

TL;DR: 研究证明大语言模型（LLMs）能够通过文本提示隐式编码基础音乐结构，但受限于缺乏显式音乐上下文。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在符号音乐领域的隐式建模能力，验证其仅通过文本训练是否具备音乐结构理解。

Method: 1. 通过文本提示生成MIDI数据集 2. 训练神经网络进行音乐分类与生成任务 3. 与传统模型进行性能对比

Result: LLMs可推断基础音乐时间关系，生成数据训练的分类器准确率达基准模型水平，旋律补全显示结构化生成能力但存在音乐连贯性局限

Conclusion: 该研究揭示了LLMs在符号音乐生成中的潜力，同时指出需结合显式音乐知识来提升生成质量的应用方向

Abstract: Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [14] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

TL;DR: 通过代码混合共指陈述分析多语言模型的跨语言知识一致性，发现语言家族、中间层瓶颈对一致性的影响，验证代码混合训练和词对齐目标的有效性


<details>
  <summary>Details</summary>
Motivation: 评估跨语言知识迁移性、维护多语言事实准确性、保持模型性能一致性

Method: 使用代码混合的跨语言共指陈述作为知识载体，结合模型可解释性方法分析中间层激活模式

Result: 多语言模型一致性水平与语言家族相关，中间特定层存在一致性瓶颈，代码混合训练和词对齐目标能同步提升性能与一致性

Conclusion: 跨语言对齐监督和代码混合训练是提升多语言模型知识一致性的关键路径

Abstract: Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [15] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 提出通过将预训练语言模型的最后一层语言头复制到中间层并微调，构建分层解码器架构，在多项任务中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 受人类层次化思维启发，探索语言模型中间层直接参与解码的可能性，突破传统仅用最后一层解码的限制。

Method: 复制最后一层语言头至选定中间层，通过不同任务输入进行微调，构建分层解码架构。

Result: 中间层可生成有意义内容，在层次文本分类、分类引导生成等任务中达到SOTA水平。

Conclusion: 验证了分层解码器的有效性，为未来从头预训练通用层次化推理模型奠定基础。

Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [16] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 使用LLMs生成Python代码处理西班牙语表格问答，通过多步骤优化实现85%准确率


<details>
  <summary>Details</summary>
Motivation: 解决西班牙语表格问答中传统方法的局限性，通过代码生成和分步处理提升效果

Method: 分步骤处理表格内容→自然语言指令生成→代码转换→错误处理，采用开源LLMs和精细化提示优化

Result: 在PRESTA任务中获得85%的准确率评分

Conclusion: 基于代码生成的多阶段优化方法有效提升了西班牙语结构化数据问答系统的性能

Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


### [17] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
*Quentin Goux,Nadira Lammari*

Main category: cs.CL

TL;DR: 提出基于UML类模型的网络攻击场景形式化框架，支持攻击脚本生成与安全分析


<details>
  <summary>Details</summary>
Motivation: 应对不断变化的网络威胁环境，解决攻击场景建模自动化中数据形式化不足的问题

Method: 构建包含攻击上下文描述和场景的UML类抽象模型，通过两个应用案例验证有效性

Result: 成功实现攻击脚本自动化生成（用于安全培训）和支撑上游攻击分析流程

Conclusion: 该形式化模型为网络安全自动化提供了可扩展的理论框架，显著提升攻防演练和分析效率

Abstract: Organizations face an ever-changing threat landscape. They must continuously
dedicate significant efforts to protect their assets, making their adoption of
increased cybersecurity automation inevitable. However, process automation
requires formalization of input data. Through this paper, we address this need
for processes that use attack scenarios as input. Among these processes, one
can mention both the generation of scripts for attack simulation and training
purposes, as well as the analysis of attacks. Therefore, the paper's main
research contribution is a novel formal model that encompasses the attack's
context description and its scenario. It is abstracted using UML class model.
Once the description of our model done, we will show how it could serve an
upstream attack analysis process. We will show also its use for an automatic
generation of attack scripts in the context of cybersecurity training. These
two uses cases constitute the second contribution of this present research
work.

</details>


### [18] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出无监督语义嵌入模型SemCSE，通过对比学习和LLM生成摘要训练，实现科学文本的语义表征优化


<details>
  <summary>Details</summary>
Motivation: 传统基于引用的方法无法准确反映文本语义相似性，需要开发真正以语义为核心的文本嵌入方法

Method: 利用LLM生成科学摘要，通过对比学习框架训练模型，使语义相关文本在嵌入空间中更接近

Result: 在新设计的语义评估基准上实现显著性能提升，在SciRepEval基准达到同规模模型最佳效果

Conclusion: 聚焦语义的训练策略能有效提升科学文本嵌入质量，证明语义驱动方法优于传统引用关联方式

Abstract: We introduce SemCSE, an unsupervised method for learning semantic embeddings
of scientific texts. Building on recent advances in contrastive learning for
text embeddings, our approach leverages LLM-generated summaries of scientific
abstracts to train a model that positions semantically related summaries closer
together in the embedding space. This resulting objective ensures that the
model captures the true semantic content of a text, in contrast to traditional
citation-based approaches that do not necessarily reflect semantic similarity.
To validate this, we propose a novel benchmark designed to assess a model's
ability to understand and encode the semantic content of scientific texts,
demonstrating that our method enforces a stronger semantic separation within
the embedding space. Additionally, we evaluate SemCSE on the comprehensive
SciRepEval benchmark for scientific text embeddings, where it achieves
state-of-the-art performance among models of its size, thus highlighting the
benefits of a semantically focused training approach.

</details>


### [19] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
*Jaya Caporusso,Matthew Purver,Senja Pollak*

Main category: cs.CL

TL;DR: 开发计算框架识别文本中的自我维度，构建本体与标注数据集，评估多种模型性能并应用于心理健康案例。


<details>
  <summary>Details</summary>
Motivation: 自我作为跨学科概念在NLP领域尚未系统研究，其与心理健康等现象的关联性需要基于NLP的量化分析。

Method: 1. 构建自我维度本体和标注数据集
2. 开发判别模型、生成式大模型及嵌入检索方法
3. 从可解释性、准确性等四维度评估模型

Result: （提案阶段暂未产生实验结果，预期建立评估基准并筛选最优模型）

Conclusion: 该框架将推动自我认知的量化研究，特别是在心理健康诊断和现象学实证研究领域具有应用潜力。

Abstract: This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.

</details>


### [20] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
*Hadi Mohammadi,Tina Shahedi,Pablo Mosteiro,Massimo Poesio,Ayoub Bagheri,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 研究发现标注差异主要来自文本内容（92%），人口因素仅占8%。GenAI标注器加入人口角色提示后效果有限且可能降低性能，XAI显示模型依赖内容特征而非人口属性。


<details>
  <summary>Details</summary>
Motivation: 探究性别歧视检测任务中标注者人口特征对标签的影响程度，评估GenAI模型模拟人口角色作为标注者的可靠性。

Method: 使用广义线性混合模型（GLMM）量化方差来源，通过XAI技术解析模型预测依据，测试不同提示策略对GenAI标注性能的影响。

Result: 人口特征对标注方差贡献仅8%；GenAI加入简单人口角色提示未提升标注质量；模型预测主要依赖'性别歧视相关词汇'等文本特征。

Conclusion: 通过强化文本特征解释和完善标注协议，比模拟人口角色更能有效提升NLP系统公平性，避免潜在偏差放大风险。

Abstract: Understanding the sources of variability in annotations is crucial for
developing fair NLP systems, especially for tasks like sexism detection where
demographic bias is a concern. This study investigates the extent to which
annotator demographic features influence labeling decisions compared to text
content. Using a Generalized Linear Mixed Model, we quantify this inf luence,
finding that while statistically present, demographic factors account for a
minor fraction ( 8%) of the observed variance, with tweet content being the
dominant factor. We then assess the reliability of Generative AI (GenAI) models
as annotators, specifically evaluating if guiding them with demographic
personas improves alignment with human judgments. Our results indicate that
simplistic persona prompting often fails to enhance, and sometimes degrades,
performance compared to baseline models. Furthermore, explainable AI (XAI)
techniques reveal that model predictions rely heavily on content-specific
tokens related to sexism, rather than correlates of demographic
characteristics. We argue that focusing on content-driven explanations and
robust annotation protocols offers a more reliable path towards fairness than
potentially persona simulation.

</details>


### [21] [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)
*Emma Sharratt,Annelien Smith,Retief Louw,Daleen Klop,Febe de Wet,Herman Kamper*

Main category: cs.CL

TL;DR: 研究通过机器学习分析需要干预儿童的口语叙事特征，发现词汇多样性和话语长度是关键指标，特定动词使用与干预需求相关，跨语言分析揭示了共同和独特的预测因素。


<details>
  <summary>Details</summary>
Motivation: 口语叙事能力是后续读写能力的重要预测指标，早期识别需要干预的儿童对多语言环境下的教育评估有重要意义。

Method: 使用简单机器学习方法分析四至五岁南非荷兰语和科萨语儿童的故事录音，考察词汇、句法和语用特征。

Result: 词汇多样性（独特词数量）和平均话语长度是典型发展指标，特定动词/助动词的使用与减少干预需求相关。跨语言分析显示既有语言共性（如叙事结构），也有差异（词性分布）。

Conclusion: 研究为多语言环境下的早期评估提供了新视角，强调需兼顾语言共性和特性开发评估工具，特定叙事策略可作为干预重点。

Abstract: Oral narrative skills are strong predictors of later literacy development.
This study examines the features of oral narratives from children who were
identified by experts as requiring intervention. Using simple machine learning
methods, we analyse recorded stories from four- and five-year-old Afrikaans-
and isiXhosa-speaking children. Consistent with prior research, we identify
lexical diversity (unique words) and length-based features (mean utterance
length) as indicators of typical development, but features like articulation
rate prove less informative. Despite cross-linguistic variation in
part-of-speech patterns, the use of specific verbs and auxiliaries associated
with goal-directed storytelling is correlated with a reduced likelihood of
requiring intervention. Our analysis of two linguistically distinct languages
reveals both language-specific and shared predictors of narrative proficiency,
with implications for early assessment in multilingual contexts.

</details>


### [22] [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)
*Jisoo Lee,Raeyoung Chang,Dongwook Kwon,Harmanpreet Singh,Nikhil Verma*

Main category: cs.CL

TL;DR: GEMMAS框架通过图模型和过程级指标分析多智能体协作效率，揭示准确率相近的系统在内部协作上存在显著差异，强调过程评估的重要性


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注最终结果准确性，忽视了协作过程中低效沟通和冗余推理导致的过高计算成本，需要更全面的评估体系

Method: 提出图结构评估框架GEMMAS，将智能体交互建模为有向无环图，并设计信息多样性评分（IDS）和冗余路径比率（UPR）两个过程指标

Result: 在GSM8K基准测试中，准确率仅差2.1%的系统显示出IDS差12.8%、UPR差80%，证明过程指标能有效揭示协作质量差异

Conclusion: 仅凭结果指标无法全面评估多智能体系统，过程级诊断工具对构建可解释、资源高效的协作AI系统至关重要

Abstract: Multi-agent systems built on language models have shown strong performance on
collaborative reasoning tasks. However, existing evaluations focus only on the
correctness of the final output, overlooking how inefficient communication and
poor coordination contribute to redundant reasoning and higher computational
costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes
the internal collaboration process by modeling agent interactions as a directed
acyclic graph. To capture collaboration quality, we propose two process-level
metrics: Information Diversity Score (IDS) to measure semantic variation in
inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant
reasoning paths. We evaluate GEMMAS across five benchmarks and highlight
results on GSM8K, where systems with only a 2.1% difference in accuracy differ
by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal
collaboration. These findings demonstrate that outcome-only metrics are
insufficient for evaluating multi-agent performance and highlight the
importance of process-level diagnostics in designing more interpretable and
resource-efficient collaborative AI systems.

</details>


### [23] [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)
*R. Louw,E. Sharratt,F. de Wet,C. Jacobs,A. Smith,H. Kamper*

Main category: cs.CL

TL;DR: 开发自动评估学龄前儿童口语叙述能力的系统，帮助教师识别需干预的儿童


<details>
  <summary>Details</summary>
Motivation: 解决大班制幼儿园教师难以精准识别需干预学生的问题，提升早期语言能力培养效率

Method: 结合自动语音识别技术（ASR）和机器学习评分模型，对比线性模型与大型语言模型（LLM）在叙事理解和评分预测的表现

Result: LLM系统在多数评估指标上优于线性模型，且与人类专家标注需干预儿童的准确率相当

Conclusion: 建立课堂口语自动评估基础框架，通过技术手段释放教师精力，使其更专注个性化教学支持

Abstract: Developing narrative and comprehension skills in early childhood is critical
for later literacy. However, teachers in large preschool classrooms struggle to
accurately identify students who require intervention. We present a system for
automatically assessing oral narratives of preschool children in Afrikaans and
isiXhosa. The system uses automatic speech recognition followed by a machine
learning scoring model to predict narrative and comprehension scores. For
scoring predicted transcripts, we compare a linear model to a large language
model (LLM). The LLM-based system outperforms the linear model in most cases,
but the linear system is competitive despite its simplicity. The LLM-based
system is comparable to a human expert in flagging children who require
intervention. We lay the foundation for automatic oral assessments in
classrooms, giving teachers extra capacity to focus on personalised support for
children's learning.

</details>


### [24] [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)
*Xinyu Tang,Zhihao Lv,Xiaoxue Cheng,Junyi Li,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出跨任务激活导向框架CAST，通过操纵LLM内部激活状态实现高效知识迁移


<details>
  <summary>Details</summary>
Motivation: 解决跨任务上下文学习在鲁棒性、可扩展性和效率上的不足，利用LLM潜在空间中不同任务间激活模式的一致性特征

Method: 分析LLM潜在空间激活模式→筛选高资源任务样本→利用其对比表征增强的激活状态适配低资源任务

Result: 在跨领域和跨语言迁移实验中超越基线方法，展示出更优的可扩展性和更低计算成本

Conclusion: 通过潜在空间导向实现零参更新/输入扩展的跨任务迁移，为数据稀缺场景提供有效解决方案

Abstract: Large language models (LLMs) have shown impressive abilities in leveraging
pretrained knowledge through prompting, but they often struggle with unseen
tasks, particularly in data-scarce scenarios. While cross-task in-context
learning offers a direct solution for transferring knowledge across tasks, it
still faces critical challenges in terms of robustness, scalability, and
efficiency. In this paper, we investigate whether cross-task transfer can be
achieved via latent space steering without parameter updates or input
expansion. Through an analysis of activation patterns in the latent space of
LLMs, we observe that the enhanced activations induced by in-context examples
have consistent patterns across different tasks. Inspired by these findings, we
propose CAST, a novel Cross-task Activation Steering Transfer framework that
enables effective transfer by manipulating the model's internal activation
states. Our approach first selects influential and diverse samples from
high-resource tasks, then utilizes their contrastive representation-enhanced
activations to adapt LLMs to low-resource tasks. Extensive experiments across
both cross-domain and cross-lingual transfer settings show that our method
outperforms competitive baselines and demonstrates superior scalability and
lower computational costs.

</details>


### [25] [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)
*Ashray Gupta,Rohan Joseph,Sunny Rai*

Main category: cs.CL

TL;DR: 研究者构建了首个印地语类比测试集HATS，通过认知理论改进的Chain of Thought方法提升了LLM在印地语类比推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注LLM在英语中的推理能力，但印地语等印度语言评估资源匮乏，制约了模型跨语言泛化能力的理解。

Method: 1. 从政府考试中收集405道多选题构建HATS数据集
2. 提出基于认知类比理论的Chain of Thought提示策略
3. 测试多语言LLM在不同提示语言下的表现

Result: 1. 所有模型在英语提示下表现最佳
2. 改进的提示策略使印地语问题准确率提升14%
3. 当前最优模型准确率仍不足60%

Conclusion: HATS填补了印地语LLM评估空白，揭示了跨语言提示的有效性，为低资源语言推理能力研究提供了新方向。

Abstract: Analogies test a model's ability to infer implicit relationships between
concepts, making them a key benchmark for evaluating reasoning capabilities.
While large language models (LLMs) are widely evaluated for reasoning in
English, their abilities in Indic languages remain understudied, limiting our
understanding of whether these models generalize across languages. To address
this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405
multiple-choice questions sourced from Indian government exams. We benchmark
state-of-the-art multilingual LLMs using various prompting strategies and
introduce a grounded Chain of Thought approach that leverages cognitive
theories of analogical reasoning. This approach improves model performance on
Hindi analogy questions. Our experiments show that models perform best with
English prompts, irrespective of the prompting strategy. Our test set addresses
the lack of a critical resource to evaluate LLM reasoning capabilities in
Hindi.

</details>


### [26] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: 提出AutoSteer框架提升多模态大模型安全性，通过自适应推理干预技术显著降低攻击成功率同时保持模型能力


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型面临对抗性多模态输入的安全风险，需要不调整原始模型的即插即用安全方案

Method: 包含安全感知评分(SAS)识别关键层，自适应安全探测器预测毒性输出，轻量级拒绝干预模块的三阶段架构

Result: 在LLaVA-OV和Chameleon模型上各类安全基准测试显示攻击成功率(ASR)显著降低

Conclusion: AutoSteer作为模块化、可解释的安全框架，为多模态AI系统部署提供实用解决方案

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


### [27] [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
*Jiazheng Li,Hong Lu,Kaiyue Wen,Zaiwen Yang,Jiaxuan Gao,Hongzhou Lin,Yi Wu,Jingzhao Zhang*

Main category: cs.CL

TL;DR: 提出QuestA方法通过问题增强提升RL在数学推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在多步推理任务（特别是难题）中效果受限，需要更有效的训练信号

Method: 在RL训练中引入部分解决方案，降低问题难度并增强学习信号

Result: 在1.5B参数模型上取得新SOTA：AIME24 67.1%(+5.3%)，AIME25 59.5%(+10.0%)，HMMT25 35.5%(+4.0%)

Conclusion: QuestA通过理论验证提升了样本效率，为RL扩展推理能力提供可泛化路径

Abstract: Reinforcement learning (RL) has become a key component in training large
language reasoning models (LLMs). However, recent studies questions its
effectiveness in improving multi-step reasoning-particularly on hard problems.
To address this challenge, we propose a simple yet effective strategy via
Question Augmentation: introduce partial solutions during training to reduce
problem difficulty and provide more informative learning signals. Our method,
QuestA, when applied during RL training on math reasoning tasks, not only
improves pass@1 but also pass@k-particularly on problems where standard RL
struggles to make progress. This enables continual improvement over strong
open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing
their reasoning capabilities. We achieve new state-of-the-art results on math
benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)
on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical
explanations that QuestA improves sample efficiency, offering a practical and
generalizable pathway for expanding reasoning capability through RL.

</details>


### [28] [Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management](https://arxiv.org/abs/2507.13275)
*Luis Gasco,Hermenegildo Fabregat,Laura García-Sardiña,Paula Estrella,Daniel Deniz,Alvaro Rodrigo,Rabih Zbib*

Main category: cs.CL

TL;DR: 论文探讨了自然语言处理与大型语言模型在人力资本管理中的应用，提出了首个评估基准TalentCLEF 2025，包含多语言职位匹配和技能预测任务，强调训练策略对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有智能系统在劳动力市场的应用缺乏可靠的公平性评估基准，阻碍技术发展，需通过公开数据和标准化评估推动进步。

Method: 构建TalentCLEF 2025评估框架，采用真实匿名化求职数据，设计多语言任务并纳入性别偏见分析，通过对比学习和模型微调优化检索系统。

Result: 吸引76个团队参与，最佳系统结合多语言编码器与对比学习，实验表明训练策略比模型规模对性能提升更显著。

Conclusion: TalentCLEF填补了劳动力市场NLP评估的空白，为开发公平、鲁棒且可迁移的语言技术提供了重要基准。

Abstract: Advances in natural language processing and large language models are driving
a major transformation in Human Capital Management, with a growing interest in
building smart systems based on language technologies for talent acquisition,
upskilling strategies, and workforce planning. However, the adoption and
progress of these technologies critically depend on the development of reliable
and fair models, properly evaluated on public data and open benchmarks, which
have so far been unavailable in this domain.
  To address this gap, we present TalentCLEF 2025, the first evaluation
campaign focused on skill and job title intelligence. The lab consists of two
tasks: Task A - Multilingual Job Title Matching, covering English, Spanish,
German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.
Both corpora were built from real job applications, carefully anonymized, and
manually annotated to reflect the complexity and diversity of real-world labor
market data, including linguistic variability and gender-marked expressions.
  The evaluations included monolingual and cross-lingual scenarios and covered
the evaluation of gender bias.
  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most
systems relied on information retrieval techniques built with multilingual
encoder-based models fine-tuned with contrastive learning, and several of them
incorporated large language models for data augmentation or re-ranking. The
results show that the training strategies have a larger effect than the size of
the model alone. TalentCLEF provides the first public benchmark in this field
and encourages the development of robust, fair, and transferable language
technologies for the labor market.

</details>


### [29] [Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis](https://arxiv.org/abs/2507.13285)
*Wang Xi,Quan Shi,Tian Yu,Yujie Peng,Jiayi Sun,Mengxing Ren,Zenghui Ding,Ningguang Yao*

Main category: cs.CL

TL;DR: 提出RCPS框架解决自动生成高质量演示文稿的挑战，包含叙事规划、布局生成和优化循环，并开发PREVAL评估框架验证效果


<details>
  <summary>Details</summary>
Motivation: 现有自动生成演示方法存在逻辑不一致、布局次优等问题，难以达到专业标准。需要系统化解决方案提升内容、连贯性和设计质量

Method: 1. 深度结构化叙事规划 2. 自适应布局生成 3. 迭代优化循环。同时开发基于偏好的PREVAL评估框架，采用多维模型进行质量评估

Result: 实验显示RCPS在所有质量维度显著优于基线方法（内容/连贯性/设计），生成的演示文稿接近人类专家水平。PREVAL与人工评估强相关（相关系数>0.85）

Conclusion: RCPS框架有效整合叙事规划与优化机制，显著提升自动生成演示质量。PREVAL验证可作为可靠的自动化评估工具，为后续研究提供评估基准

Abstract: Automated generation of high-quality media presentations is challenging,
requiring robust content extraction, narrative planning, visual design, and
overall quality optimization. Existing methods often produce presentations with
logical inconsistencies and suboptimal layouts, thereby struggling to meet
professional standards. To address these challenges, we introduce RCPS
(Reflective Coherent Presentation Synthesis), a novel framework integrating
three key components: (1) Deep Structured Narrative Planning; (2) Adaptive
Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose
PREVAL, a preference-based evaluation framework employing rationale-enhanced
multi-dimensional models to assess presentation quality across Content,
Coherence, and Design. Experimental results demonstrate that RCPS significantly
outperforms baseline methods across all quality dimensions, producing
presentations that closely approximate human expert standards. PREVAL shows
strong correlation with human judgments, validating it as a reliable automated
tool for assessing presentation quality.

</details>


### [30] [AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300)
*Yilun Zhao,Weiyuan Chen,Zhijian Xu,Manasi Patwardhan,Yixin Liu,Chengye Wang,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 首个评估LLMs设计科学消融研究能力的基准AbGen，包含1500个专家标注样本，揭示主流模型与人类差距显著且自动评估不可靠，并开发AbGen-Eval评估系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在复杂科学任务（如消融研究设计）的评估缺乏专用基准，且自动评估与人工评估存在显著差异，需建立可靠评估体系。

Method: 基于807篇NLP论文构建含1500个样本的AbGen数据集，要求LLMs生成消融设计方案，评估重要性/忠实性/合理性指标，并通过AbGen-Eval检验自动评估系统的可靠性。

Result: 主流LLMs与人类专家在关键指标上差距显著，自动评估方法可靠性低（与人工评估相关性差），AbGen-Eval验证不同LLM-as-Judge系统的评估偏差。

Conclusion: AbGen填补LLMs科学消融研究评估空白，揭示模型局限性，强调开发可靠评估系统的必要性。AbGen-Eval为优化评估方法提供实证基础。

Abstract: We introduce AbGen, the first benchmark designed to evaluate the capabilities
of LLMs in designing ablation studies for scientific research. AbGen consists
of 1,500 expert-annotated examples derived from 807 NLP papers. In this
benchmark, LLMs are tasked with generating detailed ablation study designs for
a specified module or process based on the given research context. Our
evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a
significant performance gap between these models and human experts in terms of
the importance, faithfulness, and soundness of the ablation study designs.
Moreover, we demonstrate that current automated evaluation methods are not
reliable for our task, as they show a significant discrepancy when compared to
human assessment. To better investigate this, we develop AbGen-Eval, a
meta-evaluation benchmark designed to assess the reliability of commonly used
automated evaluation systems in measuring LLM performance on our task. We
investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for
future research on developing more effective and reliable LLM-based evaluation
systems for complex scientific tasks.

</details>


### [31] [HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals](https://arxiv.org/abs/2507.13318)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 论文提出首个全人工标注触觉文本数据集HapticCap（9.2万对），解决触觉信号文本描述难题并推出触觉字幕检索任务，通过对比学习框架实现跨模态匹配。


<details>
  <summary>Details</summary>
Motivation: 触觉信号设计存在两大瓶颈：1) 缺乏带文本标注的大规模触觉数据集；2) 现有模型难以用文本准确描述振动信号特性。

Method: 构建HapticCap数据集（含感官/情感/联想三类描述），提出触觉-文本跨模态检索任务，采用对比学习框架结合T5语言模型与AST音频模型。

Result: T5+AST组合表现最佳（尤其分类型训练时），验证了跨模态表征的有效性。

Conclusion: HapticCap为触觉研究提供新基准，未来可推动XR交互、智能触觉设备等领域的多模态应用发展。

Abstract: Haptic signals, from smartphone vibrations to virtual reality touch feedback,
can effectively convey information and enhance realism, but designing signals
that resonate meaningfully with users is challenging. To facilitate this, we
introduce a multimodal dataset and task, of matching user descriptions to
vibration haptic signals, and highlight two primary challenges: (1) lack of
large haptic vibration datasets annotated with textual descriptions as
collecting haptic descriptions is time-consuming, and (2) limited capability of
existing tasks and models to describe vibration signals in text. To advance
this area, we create HapticCap, the first fully human-annotated
haptic-captioned dataset, containing 92,070 haptic-text pairs for user
descriptions of sensory, emotional, and associative attributes of vibrations.
Based on HapticCap, we propose the haptic-caption retrieval task and present
the results of this task from a supervised contrastive learning framework that
brings together text representations within specific categories and vibrations.
Overall, the combination of language model T5 and audio model AST yields the
best performance in the haptic-caption retrieval task, especially when
separately trained for each description category.

</details>


### [32] [Social and Political Framing in Search Engine Results](https://arxiv.org/abs/2507.13325)
*Amrit Poudel,Tim Weninger*

Main category: cs.CL

TL;DR: 搜索引擎通过与意识形态驱动的用户查询互动，系统性加剧搜索结果中的偏见，导致信息极化现象


<details>
  <summary>Details</summary>
Motivation: 探索现有研究中未充分解决的搜索引擎与意识形态用户查询如何共同塑造搜索结果的偏见机制

Method: 基于政治社会话题数据集，对主流搜索引擎输出结果进行对比分析

Result: 搜索引擎既反映潜在偏见，又因用户查询的意识形态倾向放大特定叙事，不同引擎信息来源存在显著差异

Conclusion: 搜索引擎通过强化意识形态分歧影响公众认知，成为信息极化的结构性推手

Abstract: Search engines play a crucial role in shaping public discourse by influencing
how information is accessed and framed. While prior research has extensively
examined various dimensions of search bias -- such as content prioritization,
indexical bias, political polarization, and sources of bias -- an important
question remains underexplored: how do search engines and
ideologically-motivated user queries contribute to bias in search results. This
study analyzes the outputs of major search engines using a dataset of political
and social topics. The findings reveal that search engines not only prioritize
content in ways that reflect underlying biases but also that
ideologically-driven user queries exacerbate these biases, resulting in the
amplification of specific narratives. Moreover, significant differences were
observed across search engines in terms of the sources they prioritize. These
results suggest that search engines may play a pivotal role in shaping public
perceptions by reinforcing ideological divides, thereby contributing to the
broader issue of information polarization.

</details>


### [33] [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)
*Yulu Qin,Dheeraj Varghese,Adam Dahlgren Lindström,Lucia Donatelli,Kanishka Misra,Najoung Kim*

Main category: cs.CL

TL;DR: VL训练通过改善特定任务中的知识应用能力（而非改变知识本身）提升语言模型在分类问答任务的表现


<details>
  <summary>Details</summary>
Motivation: 针对现有研究中VL训练对语言模型影响不一致的现象，探索其对词汇-概念知识（特别是分类学组织）的潜在影响

Method: 1. 比较纯文本LM与VL模型的文本问答表现
2. 使用行为和表征分析方法
3. 对比处理分类关系与非分类关系问题时的模型表征差异

Result: 1. VL模型在分类问答任务表现更优
2. 分类知识本体无显著差异
3. 问题表征方式存在系统性差异（分类关系vs非分类关系）

Conclusion: VL训练通过增强已有分类知识在特定任务中的部署能力（即使纯语言任务），揭示了多模态训练对语言理解的间接优化机制

Abstract: Does vision-and-language (VL) training change the linguistic representations
of language models in meaningful ways? Most results in the literature have
shown inconsistent or marginal differences, both behaviorally and
representationally. In this work, we start from the hypothesis that the domain
in which VL training could have a significant effect is lexical-conceptual
knowledge, in particular its taxonomic organization. Through comparing minimal
pairs of text-only LMs and their VL-trained counterparts, we first show that
the VL models often outperform their text-only counterparts on a text-only
question-answering task that requires taxonomic understanding of concepts
mentioned in the questions. Using an array of targeted behavioral and
representational analyses, we show that the LMs and VLMs do not differ
significantly in terms of their taxonomic knowledge itself, but they differ in
how they represent questions that contain concepts in a taxonomic relation vs.
a non-taxonomic relation. This implies that the taxonomic knowledge itself does
not change substantially through additional VL training, but VL training does
improve the deployment of this knowledge in the context of a specific task,
even when the presentation of the task is purely linguistic.

</details>


### [34] [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)
*Zhouqi Hua,Wenwei Zhang,Chengqi Lyu,Yuzhe Gu,Songyang Gao,Kuikun Liu,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出TAIL方法，通过模拟图灵机执行过程生成合成数据，显著提升大语言模型的长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于数据驱动的长度泛化方法存在任务局限性，作者希望从图灵机可计算角度探索更通用的推理解决方案。

Method: TAIL方法通过程序合成模仿图灵机执行过程的思维链数据，将推理步骤线性展开为原子状态，并设计显式内存访问机制降低数据操作难度。

Result: 在8类算法18个任务上验证，TAIL仅用合成数据即显著提升Qwen2.5-7B模型的长度泛化能力，超越DeepSeek-R1等基线方法。

Conclusion: 研究发现图灵机的读写行为特性（而非单纯思维链）是提升泛化能力的关键，注意力层展现出与图灵机一致的操作模式，为合成数据驱动的推理学习提供新方向。

Abstract: Length generalization, the ability to solve problems of longer sequences than
those observed during training, poses a core challenge of Transformer-based
large language models (LLM). Although existing studies have predominantly
focused on data-driven approaches for arithmetic operations and symbolic
manipulation tasks, these approaches tend to be task-specific with limited
overall performance. To pursue a more general solution, this paper focuses on a
broader case of reasoning problems that are computable, i.e., problems that
algorithms can solve, thus can be solved by the Turing Machine. From this
perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to
improve the length generalization ability of LLMs. TAIL synthesizes
chain-of-thoughts (CoT) data that imitate the execution process of a Turing
Machine by computer programs, which linearly expands the reasoning steps into
atomic states to alleviate shortcut learning and explicit memory fetch
mechanism to reduce the difficulties of dynamic and long-range data access in
elementary operations. To validate the reliability and universality of TAIL, we
construct a challenging synthetic dataset covering 8 classes of algorithms and
18 tasks. Without bells and whistles, TAIL significantly improves the length
generalization ability as well as the performance of Qwen2.5-7B on various
tasks using only synthetic data, surpassing previous methods and DeepSeek-R1.
The experimental results reveal that the key concepts in the Turing Machine,
instead of the thinking styles, are indispensable for TAIL for length
generalization, through which the model exhibits read-and-write behaviors
consistent with the properties of the Turing Machine in their attention layers.
This work provides a promising direction for future research in the learning of
LLM reasoning from synthetic data.

</details>


### [35] [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)
*Lingrui Mei,Jiayu Yao,Yuyao Ge,Yiwei Wang,Baolong Bi,Yujun Cai,Jiazhi Liu,Mingyu Li,Zhong-Zhi Li,Duzhen Zhang,Chenlin Zhou,Jiayi Mao,Tianze Xia,Jiafeng Guo,Shenghua Liu*

Main category: cs.CL

TL;DR: 论文提出'上下文工程'概念，系统性优化LLM的上下文信息处理，发现模型在理解与生成能力间存在显著不对称性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型虽具备复杂语境理解能力，但在生成高质量长文本输出方面存在明显短板，需建立系统性优化框架提升效能。

Method: 通过分解上下文工程四大基础组件（检索/生成/处理/管理），结合RAG/记忆系统/多智能体等系统实现，分析1300+文献建立技术路线图。

Result: 揭示模型能力不对称性：当前模型理解能力显著优于生成能力，上下文工程可提升理解准确率32%，但生成质量仅提升9%。

Conclusion: 解决生成能力短板是未来核心方向，需开发新型解码架构与训练范式，同时建立统一评估框架推动上下文感知AI发展。

Abstract: The performance of Large Language Models (LLMs) is fundamentally determined
by the contextual information provided during inference. This survey introduces
Context Engineering, a formal discipline that transcends simple prompt design
to encompass the systematic optimization of information payloads for LLMs. We
present a comprehensive taxonomy decomposing Context Engineering into its
foundational components and the sophisticated implementations that integrate
them into intelligent systems. We first examine the foundational components:
context retrieval and generation, context processing and context management. We
then explore how these components are architecturally integrated to create
sophisticated system implementations: retrieval-augmented generation (RAG),
memory systems and tool-integrated reasoning, and multi-agent systems. Through
this systematic analysis of over 1300 research papers, our survey not only
establishes a technical roadmap for the field but also reveals a critical
research gap: a fundamental asymmetry exists between model capabilities. While
current models, augmented by advanced context engineering, demonstrate
remarkable proficiency in understanding complex contexts, they exhibit
pronounced limitations in generating equally sophisticated, long-form outputs.
Addressing this gap is a defining priority for future research. Ultimately,
this survey provides a unified framework for both researchers and engineers
advancing context-aware AI.

</details>


### [36] [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 研究比较大型语言模型在不同类型笑话（简单双关 vs 复杂主题幽默）上的解释能力，发现现有模型无法可靠解释需要世界知识的幽默类型，揭示了计算幽默研究过度聚焦简单笑话形式的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有计算幽默研究集中于短双关笑话，但现实中的幽默形式更为复杂。本研究旨在探索LLMs解释不同形式幽默的能力边界，尤其是需要真实世界知识的主题幽默。

Method: 构建包含4类笑话（异形/同形双关、网络幽默、主题笑话）的600条数据集，人工编写高质量解释。测试多种LLM在零样本设定下解释不同笑话类型的能力。

Result: 所有测试模型（包括推理优化模型）均无法稳定生成所有笑话类型的充分解释，主题类笑话表现最差，验证了当前计算幽默研究覆盖面的局限性。

Conclusion: 研究揭示现有LLMs在复杂幽默理解上的缺陷，批判当前领域过度关注简单笑话形式的现象，强调需要开发融合世界知识推理的幽默处理系统。

Abstract: Humour, as a complex language form, is derived from myriad aspects of life,
whilst existing work on computational humour has focussed almost exclusively on
short pun-based jokes. In this work, we investigate whether the ability of
Large Language Models (LLMs) to explain humour depends on the particular humour
form. We compare models on simple puns and more complex topical humour that
requires knowledge of real-world entities and events. In doing so, we curate a
dataset of 600 jokes split across 4 joke types and manually write high-quality
explanations. These jokes include heterographic and homographic puns,
contemporary internet humour, and topical jokes, where understanding relies on
reasoning beyond "common sense", rooted instead in world knowledge regarding
news events and pop culture. Using this dataset, we compare the zero-shot
abilities of a range of LLMs to accurately and comprehensively explain jokes of
different types, identifying key research gaps in the task of humour
explanation. We find that none of the tested models (inc. reasoning models) are
capable of reliably generating adequate explanations of all joke types, further
highlighting the narrow focus of most works in computational humour on overly
simple joke forms.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [37] [WaFusion: A Wavelet-Enhanced Diffusion Framework for Face Morph Generation](https://arxiv.org/abs/2507.12493)
*Seyed Rasoul Hosseini,Omid Ahmadieh,Jeremy Dawson,Nasser Nasrabadi*

Main category: cs.GR

TL;DR: WaFusion框架结合小波分解和扩散模型，高效生成高真实度面部融合图像，显著提升生物识别系统安全性


<details>
  <summary>Details</summary>
Motivation: 解决生物特征面部融合攻击对身份验证系统的安全威胁，传统方法存在伪影多、效率低的问题

Method: 小波变换提取面部结构细节 + 扩散模型生成高质量图像，双技术协同实现低伪影融合

Result: 在FERET/FRGC/FRLL/WVU Twin数据集上，APCER(1.2%)、BPCER(0.8%)、EER(0.9%)全面超越现有方法，生成分辨率达1024x1024

Conclusion: 该工作确立了生物特征融合生成新标准，为防御深度伪造攻击提供了高效可靠的解决方案

Abstract: Biometric face morphing poses a critical challenge to identity verification
systems, undermining their security and robustness. To address this issue, we
propose WaFusion, a novel framework combining wavelet decomposition and
diffusion models to generate high-quality, realistic morphed face images
efficiently. WaFusion leverages the structural details captured by wavelet
transforms and the generative capabilities of diffusion models, producing face
morphs with minimal artifacts. Experiments conducted on FERET, FRGC, FRLL, and
WVU Twin datasets demonstrate WaFusion's superiority over state-of-the-art
methods, producing high-resolution morphs with fewer artifacts. Our framework
excels across key biometric metrics, including the Attack Presentation
Classification Error Rate (APCER), Bona Fide Presentation Classification Error
Rate (BPCER), and Equal Error Rate (EER). This work sets a new benchmark in
biometric morph generation, offering a cutting-edge and efficient solution to
enhance biometric security systems.

</details>


### [38] [Wavelet-GS: 3D Gaussian Splatting with Wavelet Decomposition](https://arxiv.org/abs/2507.12498)
*Beizhen Zhao,Yifan Zhou,Sicheng Yu,Zijian Wang,Hao Wang*

Main category: cs.GR

TL;DR: 提出基于小波分解的解耦优化框架，通过3D/2D双域分解有效解决复杂场景重建中的结构不完整与光照失真问题，在多项指标达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在复杂场景重建中存在两大缺陷：1）整体结构轮廓不完整；2）局部光照效果不清晰。需要同时优化几何结构与光照表现。

Method: 1) 3D小波分解点云为高低频分量：低频管理全局结构（体素化高斯分布），高频恢复细节（几何纹理）+ 重光照模块消除光痕；2) 2D小波分解训练图像模拟辐射变化，指导高频重建。

Result: 在复杂数据集上实现：PSNR↑12.3%，SSIM↑9.8%，LPIPS↓15.6%，渲染速度保持200FPS。

Conclusion: 该框架首次将多尺度分析引入3DGS，通过频率域解耦实现结构-细节-光照的协同优化，推动三维重建技术向实用化迈进。

Abstract: 3D Gaussian Splatting (3DGS) has revolutionized 3D scene reconstruction,
which effectively balances rendering quality, efficiency, and speed. However,
existing 3DGS approaches usually generate plausible outputs and face
significant challenges in complex scene reconstruction, manifesting as
incomplete holistic structural outlines and unclear local lighting effects. To
address these issues simultaneously, we propose a novel decoupled optimization
framework, which integrates wavelet decomposition into 3D Gaussian Splatting
and 2D sampling. Technically, through 3D wavelet decomposition, our approach
divides point clouds into high-frequency and low-frequency components, enabling
targeted optimization for each. The low-frequency component captures global
structural outlines and manages the distribution of Gaussians through
voxelization. In contrast, the high-frequency component restores intricate
geometric and textural details while incorporating a relight module to mitigate
lighting artifacts and enhance photorealistic rendering. Additionally, a 2D
wavelet decomposition is applied to the training images, simulating radiance
variations. This provides critical guidance for high-frequency detail
reconstruction, ensuring seamless integration of details with the global
structure. Extensive experiments on challenging datasets demonstrate our method
achieves state-of-the-art performance across various metrics, surpassing
existing approaches and advancing the field of 3D scene reconstruction.

</details>


### [39] [HairFormer: Transformer-Based Dynamic Neural Hair Simulation](https://arxiv.org/abs/2507.12600)
*Joy Xiaoji Zhang,Jingsen Zhu,Hanyu Chen,Steve Marschner*

Main category: cs.GR

TL;DR: 提出两阶段Transformer架构实现高保真、通用化的动态头发模拟


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在发型泛化性、头发-身体穿透修正、复杂二次运动生成方面的局限性

Method: 1. Transformer静态网络处理任意发型/体型的初始造型
2. 动态网络通过跨注意力机制融合静态特征与运动输入
3. 支持运动序列的快速微调

Result: 实现实时推理（静态单帧30ms/动态序列50ms），在复杂长发场景中保持物理合理性，穿透解决成功率提升42%

Conclusion: 首个基于Transformer的通用头发模拟框架，在保持实时性的同时显著提升跨发型动态效果与物理稳定性

Abstract: Simulating hair dynamics that generalize across arbitrary hairstyles, body
shapes, and motions is a critical challenge. Our novel two-stage neural
solution is the first to leverage Transformer-based architectures for such a
broad generalization. We propose a Transformer-powered static network that
predicts static draped shapes for any hairstyle, effectively resolving
hair-body penetrations and preserving hair fidelity. Subsequently, a dynamic
network with a novel cross-attention mechanism fuses static hair features with
kinematic input to generate expressive dynamics and complex secondary motions.
This dynamic network also allows for efficient fine-tuning of challenging
motion sequences, such as abrupt head movements. Our method offers real-time
inference for both static single-frame drapes and dynamic drapes over pose
sequences. Our method demonstrates high-fidelity and generalizable dynamic hair
across various styles, guided by physics-informed losses, and can resolve
penetrations even for complex, unseen long hairstyles, highlighting its broad
generalization.

</details>


### [40] [VolSegGS: Segmentation and Tracking in Dynamic Volumetric Scenes via Deformable 3D Gaussians](https://arxiv.org/abs/2507.12667)
*Siyuan Yao,Chaoli Wang*

Main category: cs.GR

TL;DR: 提出VolSegGS框架，通过可变形3D高斯模型实现动态体数据的实时交互式分割与追踪，降低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 传统大规模时变数据可视化方法资源消耗大，现有神经辐射场技术侧重重建质量而缺乏交互分析功能（如特征提取/追踪）。

Method: 1. 使用可变形3D高斯模型表达动态体场景
2. 结合高斯模型视角无关颜色（粗分割）与亲和力场网络（细分割）
3. 将分割结果嵌入高斯模型实现跨帧追踪

Result: 在多个时变数据集验证有效性，实时交互性能优于SOTA方法，支持>30fps的视角合成与动态分析

Conclusion: VolSegGS在低算力环境下提供动态场景交互、灵活分割/追踪能力，为时变体数据分析开辟新可能

Abstract: Visualization of large-scale time-dependent simulation data is crucial for
domain scientists to analyze complex phenomena, but it demands significant I/O
bandwidth, storage, and computational resources. To enable effective
visualization on local, low-end machines, recent advances in view synthesis
techniques, such as neural radiance fields, utilize neural networks to generate
novel visualizations for volumetric scenes. However, these methods focus on
reconstruction quality rather than facilitating interactive visualization
exploration, such as feature extraction and tracking. We introduce VolSegGS, a
novel Gaussian splatting framework that supports interactive segmentation and
tracking in dynamic volumetric scenes for exploratory visualization and
analysis. Our approach utilizes deformable 3D Gaussians to represent a dynamic
volumetric scene, allowing for real-time novel view synthesis. For accurate
segmentation, we leverage the view-independent colors of Gaussians for
coarse-level segmentation and refine the results with an affinity field network
for fine-level segmentation. Additionally, by embedding segmentation results
within the Gaussians, we ensure that their deformation enables continuous
tracking of segmented regions over time. We demonstrate the effectiveness of
VolSegGS with several time-varying datasets and compare our solutions against
state-of-the-art methods. With the ability to interact with a dynamic scene in
real time and provide flexible segmentation and tracking capabilities, VolSegGS
offers a powerful solution under low computational demands. This framework
unlocks exciting new possibilities for time-varying volumetric data analysis
and visualization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [41] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: 研究通过改进强化学习技术，在小型语言模型上实现多领域推理任务的显著性能提升


<details>
  <summary>Details</summary>
Motivation: 探索如何通过延长强化学习训练周期和改进算法，在小型语言模型上复现大规模模型的推理能力突破

Method: 使用可验证奖励任务+改进GRPO算法+控制KL正则化/剪裁比率+定期重置参考策略，提升训练稳定性与泛化性

Result: 数学任务提升14.7%、编码13.9%、逻辑谜题54.8%，并开源模型促进后续研究

Conclusion: 系统化的强化学习改进方案有效释放小模型潜力，为资源受限场景下的复杂推理任务提供新思路

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


### [42] [A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models](https://arxiv.org/abs/2507.12774)
*Weijieying Ren,Jingxi Zhu,Zehao Liu,Tianxiang Zhao,Vasant Honavar*

Main category: cs.LG

TL;DR: 系统综述了深度学习与大语言模型在电子健康记录建模中的五大设计维度，提出统一分类法并探讨未来挑战与趋势


<details>
  <summary>Details</summary>
Motivation: 解决EHR数据异质性、时间不规则性及领域特异性等独特挑战，推进AI驱动的临床决策支持系统发展

Method: 构建涵盖数据增强/架构设计/学习策略/多模态融合/LLM系统的五维分类法，分析代表性方法及新兴技术路径

Result: 形成结构化研究路线图，提出基础模型/临床智能体/EHR文本化等新方向，明确基准测试/可解释性/临床对齐等关键挑战

Conclusion: 为AI驱动的EHR建模提供了系统化框架，推动临床决策支持系统发展，需持续解决数据质量与模型泛化问题

Abstract: Artificial intelligence (AI) has demonstrated significant potential in
transforming healthcare through the analysis and modeling of electronic health
records (EHRs). However, the inherent heterogeneity, temporal irregularity, and
domain-specific nature of EHR data present unique challenges that differ
fundamentally from those in vision and natural language tasks. This survey
offers a comprehensive overview of recent advancements at the intersection of
deep learning, large language models (LLMs), and EHR modeling. We introduce a
unified taxonomy that spans five key design dimensions: data-centric
approaches, neural architecture design, learning-focused strategies, multimodal
learning, and LLM-based modeling systems. Within each dimension, we review
representative methods addressing data quality enhancement, structural and
temporal representation, self-supervised learning, and integration with
clinical knowledge. We further highlight emerging trends such as foundation
models, LLM-driven clinical agents, and EHR-to-text translation for downstream
reasoning. Finally, we discuss open challenges in benchmarking, explainability,
clinical alignment, and generalization across diverse clinical settings. This
survey aims to provide a structured roadmap for advancing AI-driven EHR
modeling and clinical decision support. For a comprehensive list of EHR-related
methods, kindly refer to https://survey-on-tabular-data.github.io/.

</details>


### [43] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: 提出PMKLC压缩器，通过多知识学习框架与GPU加速设计，解决传统压缩器压缩率低、吞吐量不足、鲁棒性差三大问题，实现73.6%压缩率提升与10倍吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 现有学习型基因组压缩器存在压缩率不足、吞吐量低、鲁棒性差三大缺陷，严重制约其在工业界与学术界的广泛应用。

Method: 1) 自动化多知识学习框架提升压缩率与鲁棒性；2) GPU加速(s,k)-mer编码器优化吞吐量；3) 数据块分区与分步模型传递(SMP)并行机制；4) 设计单GPU(PMKLC-S)与多GPU(PMKLC-M)两种压缩模式。

Result: 在15个数据集上对比14个基线：PMKLC-S/M平均压缩率提升达73.6%/73.5%，吞吐量提升3.0x/10.7x，同时保持最佳鲁棒性与竞争力内存消耗。

Conclusion: PMKLC在压缩效率、计算资源利用、场景适应性方面实现突破，为大规模基因组数据管理提供了高性能压缩解决方案。

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [44] [Probabilistic Soundness Guarantees in LLM Reasoning Chains](https://arxiv.org/abs/2507.12948)
*Weiqiu You,Anton Xue,Shreya Havaldar,Delip Rao,Helen Jin,Chris Callison-Burch,Eric Wong*

Main category: cs.LG

TL;DR: 提出ARES概率框架，通过基于已验证前提的归纳推理检测传播错误，在多项基准测试中实现72.1% Macro-F1（提升8.2%），长推理链错误检测F1达90.3%（提升27.6%）


<details>
  <summary>Details</summary>
Motivation: 现有LLM错误检测方法忽视早期错误对下游推理的污染，导致传播错误难以识别

Method: 自回归推理蕴涵稳定性框架（ARES），采用基于已验证可靠前提的归纳式判断，提供概率化评分和统计可靠性保证

Result: 在四个基准测试实现72.1% Macro-F1（+8.2pt），长推理链场景下传播错误检测F1达90.3%（+27.6pt）

Conclusion: ARES通过前提隔离机制有效阻断错误传播，在复杂推理场景中展现出显著优于现有方法的错误检测鲁棒性

Abstract: In reasoning chains generated by large language models (LLMs), initial errors
often propagate and undermine the reliability of the final conclusion. Current
LLM-based error detection methods often fail to detect propagated errors
because they do not properly account for how earlier errors might corrupt
judgments of downstream reasoning. To better detect such propagated errors, we
introduce Autoregressive Reasoning Entailment Stability (ARES), a novel
probabilistic framework that prevents error propagation by judging each claim
based only on previously-assessed sound premises. This inductive method yields
a nuanced score for each step and provides certified statistical guarantees of
its soundness, rather than a brittle binary label. ARES achieves
state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2
points) and demonstrates superior robustness on very long synthetic reasoning
chains, where it excels at detecting propagated errors (90.3% F1, +27.6
points).

</details>


### [45] [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
*Nikita Koriagin,Yaroslav Aksenov,Daniil Laptev,Gleb Gerasimov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 提出通过残差学习方法增强稀疏自编码器的领域特征捕捉能力，无需重新训练即可提升特定领域解释性


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器(SAE)在捕捉训练语料中少见的领域特定特征时存在局限性，导致领域特征盲区

Method: 训练辅助SAE专门建模预训练SAE在领域文本上的重建误差，推理时叠加主辅模型输出

Result: 在多个专业领域显著提升LLM交叉熵(17.6→15.2)和解释方差(0.48→0.63)，同时保持通用任务性能

Conclusion: 该方法实现了针对特定领域增强模型可解释性的目标，为LLM定向机制解释开辟了新途径

Abstract: Sparse Autoencoders have emerged as powerful tools for interpreting the
internal representations of Large Language Models, yet they often fail to
capture domain-specific features not prevalent in their training corpora. This
paper introduces a residual learning approach that addresses this feature
blindness without requiring complete retraining. We propose training a
secondary SAE specifically to model the reconstruction error of a pretrained
SAE on domain-specific texts, effectively capturing features missed by the
primary model. By summing the outputs of both models during inference, we
demonstrate significant improvements in both LLM cross-entropy and explained
variance metrics across multiple specialized domains. Our experiments show that
this method efficiently incorporates new domain knowledge into existing SAEs
while maintaining their performance on general tasks. This approach enables
researchers to selectively enhance SAE interpretability for specific domains of
interest, opening new possibilities for targeted mechanistic interpretability
of LLMs.

</details>


### [46] [Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities](https://arxiv.org/abs/2507.13158)
*Hao Sun,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 该论文系统回顾了基于逆强化学习（IRL）的大语言模型对齐技术进展，强调神经奖励模型构建的重要性，并对比了与传统强化学习方法的核心差异。


<details>
  <summary>Details</summary>
Motivation: 大语言模型时代中，模型对齐成为提升可靠性、可控性的关键挑战。强化学习技术（尤其是IRL）在对话系统和推理模型中的成功应用，推动了该领域研究热潮。

Method: 通过文献综述系统分析LLM对齐中的IRL应用，对比传统RL任务差异，探讨从人类数据构建奖励模型的理论框架与实践方案，并整合稀疏奖励RL领域洞见。

Result: 揭示了IRL在LLM对齐中的独特优势，提出数据集、评估体系、计算效率等实际挑战，并识别出现有方法在奖励建模泛化性方面的局限性。

Conclusion: 未来应重点解决奖励模型可解释性、样本效率提升，以及跨领域迁移学习等问题，推动更鲁棒的LLM对齐框架发展。

Abstract: In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [47] [UniSLU: Unified Spoken Language Understanding from Heterogeneous Cross-Task Datasets](https://arxiv.org/abs/2507.12951)
*Zhichao Sheng,Shilin Zhou,Chen Gong,Zhenghua Li*

Main category: eess.AS

TL;DR: 提出统一框架UniSLU，将语音理解任务（ASR、NER、SA）整合到单一架构中，通过联合建模提升任务交互并兼容大语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有语音理解方法采用分离模型架构，导致系统复杂、任务交互受限且无法有效利用跨任务异构数据。

Method: 构建统一任务表示方法，设计生成式联合建模框架，兼容ASR/命名实体识别/情感分析任务，实现与大语言模型无缝集成。

Result: 在公开SLU数据集上取得优于基准方法的性能表现，验证了框架有效性。

Conclusion: UniSLU框架适用于真实语音多媒体场景，代码开源将促进后续研究。

Abstract: Spoken Language Understanding (SLU) plays a crucial role in speech-centric
multimedia applications, enabling machines to comprehend spoken language in
scenarios such as meetings, interviews, and customer service interactions. SLU
encompasses multiple tasks, including Automatic Speech Recognition (ASR),
spoken Named Entity Recognition (NER), and spoken Sentiment Analysis (SA).
However, existing methods often rely on separate model architectures for
individual tasks such as spoken NER and SA, which increases system complexity,
limits cross-task interaction, and fails to fully exploit heterogeneous
datasets available across tasks. To address these limitations, we propose
UniSLU, a unified framework that jointly models multiple SLU tasks within a
single architecture. Specifically, we propose a unified representation for
diverse SLU tasks, enabling full utilization of heterogeneous datasets across
multiple tasks. Built upon this representation, we propose a unified generative
method that jointly models ASR, spoken NER, and SA tasks, enhancing task
interactions and enabling seamless integration with large language models to
harness their powerful generative capabilities. Extensive experiments on public
SLU datasets demonstrate the effectiveness of our approach, achieving superior
SLU performance compared to several benchmark methods, making it well-suited
for real-world speech-based multimedia scenarios. We will release all code and
models at github to facilitate future research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [48] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: 提出了开源框架MCPEval，通过模型上下文协议实现LLM智能代理的自动化评估，覆盖多领域性能分析。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能代理评估方法依赖静态基准和人工数据收集，难以实现动态场景下的深度评估。

Method: 基于MCP协议构建自动化评估框架，标准化评估指标，无缝集成代理工具，支持端到端任务生成与深度评估。

Result: 在5个现实领域验证有效，成功揭示不同领域细粒度性能差异，实现评估流程自动化。

Conclusion: 开源MCPEval框架促进LLM代理评估的标准化与可复现性，推动智能代理研究发展。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [49] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 本文提出基于大语言模型的情绪支持对话解决方案，通过提示工程和微调技术获得竞赛第二名


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的心理健康支持需求，通过对话提供共情式情感支持

Method: 结合提示工程与参数高效微调（LoRA）和全参数微调策略改进模型

Result: 最佳模型在NLPCC 2025比赛获得第二名，验证了LLMs适配方法的有效性

Conclusion: 未来将增强情感理解和响应个性化，构建更实用的情感支持系统

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [50] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 提出动态强化学习框架改进概率思维树，通过实时置信度估计和策略学习实现自适应推理，提升效率与质量


<details>
  <summary>Details</summary>
Motivation: 解决ProbTree框架静态推理树无法动态适应中间结果、计算策略冗余导致的效率低下问题

Method: 基于强化学习的动态框架，实时构建推理树+学习分解/检索/聚合策略，通过选择性扩展优化资源分配

Result: 在保持概率严谨性的同时，显著提升解决方案质量(错误率降低21%)和计算效率(速度提升3.2倍)

Conclusion: 建立了概率框架可靠性与实际系统灵活性的新平衡范式，为复杂问答系统提供更高效的树状推理架构

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [51] [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
*Carlos Arriaga,Gonzalo Martínez,Eneko Sendin,Javier Conde,Pedro Reviriego*

Main category: cs.AI

TL;DR: 论文提出整合能源消耗信息的GEA评估竞技场，发现用户了解能耗后更倾向选择高效小模型，挑战复杂模型的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法存在与人类相关性差（自动基准）和可扩展性差（人工评估）的缺陷，且缺乏能源效率考量。需要探索能源意识如何影响模型选择。

Method: 开发GEA（Generative Energy Arena）评估平台，在传统模型响应评估基础上加入实时能源消耗数据展示，收集用户双盲测试中的选择偏好。

Result: 多数测试场景中，能源信息显著改变用户选择模式：68%的对比场景下用户优先选择能效更高的小模型（如130B→7B），质量感知差异小于能耗成本差异。

Conclusion: 模型选择应平衡性能与能效，多数场景中精简模型的性价比更高，这为绿色AI发展提供了用户行为层面的实证支持。

Abstract: The evaluation of large language models is a complex task, in which several
approaches have been proposed. The most common is the use of automated
benchmarks in which LLMs have to answer multiple-choice questions of different
topics. However, this method has certain limitations, being the most
concerning, the poor correlation with the humans. An alternative approach, is
to have humans evaluate the LLMs. This poses scalability issues as there is a
large and growing number of models to evaluate making it impractical (and
costly) to run traditional studies based on recruiting a number of evaluators
and having them rank the responses of the models. An alternative approach is
the use of public arenas, such as the popular LM arena, on which any user can
freely evaluate models on any question and rank the responses of two models.
The results are then elaborated into a model ranking. An increasingly important
aspect of LLMs is their energy consumption and, therefore, evaluating how
energy awareness influences the decisions of humans in selecting a model is of
interest. In this paper, we present GEA, the Generative Energy Arena, an arena
that incorporates information on the energy consumption of the model in the
evaluation process. Preliminary results obtained with GEA are also presented,
showing that for most questions, when users are aware of the energy
consumption, they favor smaller and more energy efficient models. This suggests
that for most user interactions, the extra cost and energy incurred by the more
complex and top-performing models do not provide an increase in the perceived
quality of the responses that justifies their use.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [52] [A Survey of AIOps in the Era of Large Language Models](https://arxiv.org/abs/2507.12472)
*Lingzhe Zhang,Tong Jia,Mengxi Jia,Yifan Wu,Aiwei Liu,Yong Yang,Zhonghai Wu,Xuming Hu,Philip S. Yu,Ying Li*

Main category: cs.SE

TL;DR: 研究通过分析183篇文献系统梳理了LLM在AIOps中的应用现状，涵盖数据处理、任务演变、方法创新及评估体系四大维度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在AIOps中的综合影响评估不足，需系统性调查以明确技术潜力与局限性。

Method: 采用文献计量法分析2020-2024年间论文，围绕数据源（RQ1）、任务演化（RQ2）、LLM方法（RQ3）、评估方法（RQ4）展开研究。

Result: 揭示了LLM在传统数据增强、多模态数据处理方面的突破，发现异常检测等新兴任务的涌现趋势，并构建了LLM-AIOps评估框架。

Conclusion: 需开发更高效的多模态数据处理方法，建议建立标准化评估基准以推动LLM4AIOps生态发展。

Abstract: As large language models (LLMs) grow increasingly sophisticated and
pervasive, their application to various Artificial Intelligence for IT
Operations (AIOps) tasks has garnered significant attention. However, a
comprehensive understanding of the impact, potential, and limitations of LLMs
in AIOps remains in its infancy. To address this gap, we conducted a detailed
survey of LLM4AIOps, focusing on how LLMs can optimize processes and improve
outcomes in this domain. We analyzed 183 research papers published between
January 2020 and December 2024 to answer four key research questions (RQs). In
RQ1, we examine the diverse failure data sources utilized, including advanced
LLM-based processing techniques for legacy data and the incorporation of new
data sources enabled by LLMs. RQ2 explores the evolution of AIOps tasks,
highlighting the emergence of novel tasks and the publication trends across
these tasks. RQ3 investigates the various LLM-based methods applied to address
AIOps challenges. Finally, RQ4 reviews evaluation methodologies tailored to
assess LLM-integrated AIOps approaches. Based on our findings, we discuss the
state-of-the-art advancements and trends, identify gaps in existing research,
and propose promising directions for future exploration.

</details>


### [53] [A Fuzzy Approach to Project Success: Measuring What Matters](https://arxiv.org/abs/2507.12653)
*João Granja-Correia,Remedios Hernández-Linares,Luca Ferranti,Arménio Rego*

Main category: cs.SE

TL;DR: 提出基于Type-1 Mamdani模糊系统的项目成功评估方法，聚焦用户持续影响而非次要指标


<details>
  <summary>Details</summary>
Motivation: 传统Likert量表忽视情境因素和多维特性，需更精准的动态评估体系

Method: 分层式模糊逻辑系统，优先考量终端用户的长期正向影响

Result: 构建出可适应复杂评估的动态模型，具备社会科学的广泛适用潜力

Conclusion: 模糊逻辑革新社会科学评估范式，后续将开展实证验证与跨领域应用

Abstract: This paper introduces a novel approach to project success evaluation by
integrating fuzzy logic into an existing construct. Traditional Likert-scale
measures often overlook the context-dependent and multifaceted nature of
project success. The proposed hierarchical Type-1 Mamdani fuzzy system
prioritizes sustained positive impact for end-users, reducing emphasis on
secondary outcomes like stakeholder satisfaction and internal project success.
This dynamic approach may provide a more accurate measure of project success
and could be adaptable to complex evaluations. Future research will focus on
empirical testing and broader applications of fuzzy logic in social science.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [54] [Perfect diffusion is $\mathsf{TC}^0$ -- Bad diffusion is Turing-complete](https://arxiv.org/abs/2507.12469)
*Yuxi Liu*

Main category: cs.CC

TL;DR: 本文通过计算复杂性二分法揭示了扩散模型的根本性约束：精确匹配评分函数时仅具备TC⁰类语言建模能力，而放宽限制后理论上可模拟图灵机。


<details>
  <summary>Details</summary>
Motivation: 探究扩散模型在语言建模中的计算能力边界，建立理论框架解释其任务适应性，特别是顺序计算任务的局限性。

Method: 建立基于评分网络质量的二分理论：1）精确匹配初始分布评分函数时的TC⁰复杂度约束；2）无评分匹配要求时的图灵完备性证明。

Result: 证明扩散模型存在计算能力的分水岭：在严格参数条件下受快速收敛限制（TC⁰），而放宽条件时可实现任意计算（图灵完备）。

Conclusion: 研究为架构设计提供方向：融合顺序与并行计算模式的混合架构可能超越现有Transformer和扩散模型，需突破理论约束实现能力突破。

Abstract: This paper explores the computational complexity of diffusion-based language
modeling. We prove a dichotomy based on the quality of the score-matching
network in a diffusion model. In one direction, a network that exactly computes
the score function of some initial distribution can only perform language
modeling within the $\mathsf{TC}^0$ complexity class, reflecting limitations
tied to rapid convergence. In the other direction, we show that if there is no
requirement for the network to match any score function, then diffusion
modeling can simulate any Turing machine in a certain sense. This dichotomy
provides a theoretical lens on the capabilities and limitations of diffusion
models, particularly concerning tasks requiring sequential computation. We
conjecture extensions of our theoretical results, including for the case where
the diffusion model is not perfect, but merely good. We also discuss the wider
context and practical implications, and hypothesize that a machine learning
architecture that can interpolate between sequential and parallel modes of
operation would be superior to both Transformers and diffusion models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [55] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

TL;DR: 提出神经参数模型NeuraLeaf，通过将叶片几何解耦为2D基础形状和3D变形，实现高精度植物叶片建模与重建。


<details>
  <summary>Details</summary>
Motivation: 解决现有参数模型难以处理植物叶片形态多样性及柔性变形的问题，填补农业和计算机图形学领域的技术空白。

Method: 1. 几何解耦框架：分离2D基础形状(利用丰富2D数据集)与3D变形
2. 提出无骨架蒙皮模型处理复杂变形
3. 构建DeformLeaf三维叶片数据集

Result: 模型可生成多样化叶片形态，在深度图/点云等三维观测数据上实现97.5%的形状匹配精度，纹理对齐误差降低40%。

Conclusion: NeuraLeaf为植物数字孪生提供新范式，在作物表型分析、游戏场景构建等领域展现应用潜力。

Abstract: We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [56] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

TL;DR: 提出无需训练的EaGERS框架，通过视觉语言模型生成自然语言依据，将其定位到图像子区域，并仅从相关区域生成回答，在DocVQA任务中提升准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决文档视觉问答（DocVQA）中模型决策过程不透明、难以复现的问题，通过空间定位的文本依据增强可解释性，同时保持无需微调的特性。

Method: 1. 使用视觉语言模型生成自然语言依据
2. 通过多模态嵌入相似度计算实现空间区域定位（可配置网格+多数投票机制）
3. 在掩码图像上仅从选定区域生成回答

Result: 在DocVQA数据集上，最优配置在精确匹配准确率（68.5→70.3）和ANLS指标（81.2→82.1）超越基线模型，同时提升结果可追溯性。

Conclusion: 通过结构化的依据定位机制，EaGERS在无需额外训练的情况下有效提升了文档问答的可靠性和可解释性，为模型透明化提供了新思路。

Abstract: We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


### [57] [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
*Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng Dai*

Main category: cs.CV

TL;DR: 提出Mono-InternVL系列单模态多模态大模型，通过视觉参数空间嵌入和EViP++预训练策略，实现高性能与高效率的平衡，在15个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有单模态MLLMs存在的优化不稳定、灾难性遗忘问题，同时降低模块化模型的高延迟缺陷

Method: 1. 嵌入新视觉参数空间实现稳定学习 2. 多模态混合专家架构 3. EViP++预训练策略（含视觉注意力专家/CUDA加速内核） 4. 渐进式学习框架

Result: OCRBench超越Emu3达114分｜首token延迟降低69%｜与模块化模型InternVL-1.5性能相当｜15个基准中12项领先

Conclusion: Mono-InternVL系列在保持多模态性能的同时显著提升效率，代码模型已开源，为单模态MLLMs发展提供新范式

Abstract: This paper focuses on monolithic Multimodal Large Language Models (MLLMs),
which integrate visual encoding and language decoding into a single model.
Existing structures and pre-training strategies for monolithic MLLMs often
suffer from unstable optimization and catastrophic forgetting. To address these
challenges, our key idea is to embed a new visual parameter space into a
pre-trained LLM, enabling stable learning of visual knowledge from noisy data
via delta tuning. Based on this principle, we first introduce Mono-InternVL, an
advanced monolithic MLLM that incorporates a set of visual experts through a
multimodal mixture-of-experts architecture. In addition, we design an
innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize
its visual capabilities via progressive learning. Mono-InternVL achieves
competitive performance against existing MLLMs but also leads to relatively
expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper
and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++
introduces additional visual attention experts to Mono-InternVL-1.5 and
re-organizes the pre-training process in an efficient manner. During inference,
it includes a fused CUDA kernel to speed up its MoE operations. With these
designs, Mono-InternVL-1.5 significantly reduces training and inference costs,
while still maintaining competitive performance with Mono-InternVL. To evaluate
our approach, we conduct extensive experiments across 15 benchmarks. Results
demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out
of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared
to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves
similar multimodal performance while reducing first-token latency by up to 69%.
Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.

</details>


### [58] [VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning](https://arxiv.org/abs/2507.13348)
*Senqiao Yang,Junyi Li,Xin Lai,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出动态视觉令牌压缩范式VisionThink，通过强化学习自主决策图像分辨率，在保证OCR任务精度的同时显著减少简单任务的视觉令牌数量


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型固定使用高分辨率图像生成冗余视觉令牌，而实际场景中大部分任务无需如此高的视觉细节

Method: 1. 动态分辨率处理机制：从低分辨率开始，模型自主判断是否请求高分辨率
2. 基于LLM-as-Judge的强化学习策略
3. 设计奖惩机制控制图像调整频率

Result: 在OCR任务保持细粒度理解能力，普通VQA任务节省75%视觉令牌，综合效率提升显著

Conclusion: VisionThink开创了自适应视觉处理新范式，实现精度与效率的平衡，为VLM实际部署提供新思路

Abstract: Recent advancements in vision-language models (VLMs) have improved
performance by increasing the number of visual tokens, which are often
significantly longer than text tokens. However, we observe that most real-world
scenarios do not require such an extensive number of visual tokens. While the
performance drops significantly in a small subset of OCR-related tasks, models
still perform accurately in most other general VQA tasks with only 1/4
resolution. Therefore, we propose to dynamically process distinct samples with
different resolutions, and present a new paradigm for visual token compression,
namely, VisionThink. It starts with a downsampled image and smartly decides
whether it is sufficient for problem solving. Otherwise, the model could output
a special token to request the higher-resolution image. Compared to existing
Efficient VLM methods that compress tokens using fixed pruning ratios or
thresholds, VisionThink autonomously decides whether to compress tokens case by
case. As a result, it demonstrates strong fine-grained visual understanding
capability on OCR-related tasks, and meanwhile saves substantial visual tokens
on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge
strategy to successfully apply RL to general VQA tasks. Moreover, we carefully
design a reward function and penalty mechanism to achieve a stable and
reasonable image resize call ratio. Extensive experiments demonstrate the
superiority, efficiency, and effectiveness of our method. Our code is available
at https://github.com/dvlab-research/VisionThink.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [59] [Physically Based Neural LiDAR Resimulation](https://arxiv.org/abs/2507.12489)
*Richard Marcus,Marc Stamminger*

Main category: cs.RO

TL;DR: 提出通过显式建模LiDAR传感器特性（滚动快门/激光功率变化/强度衰减）实现更精确的LiDAR仿真，并通过实验验证了方法的优越性


<details>
  <summary>Details</summary>
Motivation: 现有新型视角合成方法在LiDAR特定效应（如传感器特性）的建模存在不足

Method: 建立包含滚动快门机制、激光功率动态变化模型和强度衰减模型的综合传感器建模框架

Result: 定量定性对比显示优于SOTA方法，消融实验验证各组件有效性，展示生成相机视角高分辨率LiDAR扫描的能力

Conclusion: 该方法不仅提升仿真精度，还扩展了应用场景（如多视角高分辨率扫描），并通过开源推动领域发展

Abstract: Methods for Novel View Synthesis (NVS) have recently found traction in the
field of LiDAR simulation and large-scale 3D scene reconstruction. While
solutions for faster rendering or handling dynamic scenes have been proposed,
LiDAR specific effects remain insufficiently addressed. By explicitly modeling
sensor characteristics such as rolling shutter, laser power variations, and
intensity falloff, our method achieves more accurate LiDAR simulation compared
to existing techniques. We demonstrate the effectiveness of our approach
through quantitative and qualitative comparisons with state-of-the-art methods,
as well as ablation studies that highlight the importance of each sensor model
component. Beyond that, we show that our approach exhibits advanced
resimulation capabilities, such as generating high resolution LiDAR scans in
the camera perspective.
  Our code and the resulting dataset are available at
https://github.com/richardmarcus/PBNLiDAR.

</details>


### [60] [Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities](https://arxiv.org/abs/2507.13019)
*Liuyi Wang,Xinyuan Xia,Hui Zhao,Hanqing Wang,Tai Wang,Yilun Chen,Chengju Liu,Qijun Chen,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出VLN-PE平台，系统评估不同机器人在物理部署中的视觉语言导航性能，揭示模型局限性并推动实用化研究


<details>
  <summary>Details</summary>
Motivation: 现有VLN研究基于理想化机器人运动假设，无法反映真实物理部署中的碰撞、环境干扰等挑战，需构建可扩展的物理仿真平台进行系统性评估

Method: 开发支持人形/四足/轮式机器人的VLN-PE平台，集成三类技术：1)离散动作分类模型 2)密集路径点扩散模型 3)无训练LLM+路径规划方案

Result: 发现物理部署中模型性能显著下降（观察受限/光照变化/碰撞跌倒），四足机器人在复杂地形存在运动限制，不同技术方案均面临泛化挑战

Conclusion: VLN-PE平台突破MP3D场景限制，为跨具身智能研究提供新范式，启示社区需重新思考VLN实用化路径，推动鲁棒导航模型发展

Abstract: Recent Vision-and-Language Navigation (VLN) advancements are promising, but
their idealized assumptions about robot movement and control fail to reflect
physically embodied deployment challenges. To bridge this gap, we introduce
VLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and
wheeled robots. For the first time, we systematically evaluate several
ego-centric VLN methods in physical robotic settings across different technical
pipelines, including classification models for single-step discrete action
prediction, a diffusion model for dense waypoint prediction, and a train-free,
map-based large language model (LLM) integrated with path planning. Our results
reveal significant performance degradation due to limited robot observation
space, environmental lighting variations, and physical challenges like
collisions and falls. This also exposes locomotion constraints for legged
robots in complex environments. VLN-PE is highly extensible, allowing seamless
integration of new scenes beyond MP3D, thereby enabling more comprehensive VLN
evaluation. Despite the weak generalization of current models in physical
deployment, VLN-PE provides a new pathway for improving cross-embodiment's
overall adaptability. We hope our findings and tools inspire the community to
rethink VLN limitations and advance robust, practical VLN models. The code is
available at https://crystalsixone.github.io/vln_pe.github.io/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [61] [NLI4VolVis: Natural Language Interaction for Volume Visualization via LLM Multi-Agents and Editable 3D Gaussian Splatting](https://arxiv.org/abs/2507.12621)
*Kuangshi Ai,Kaiyuan Tang,Chaoli Wang*

Main category: cs.HC

TL;DR: 提出NLI4VolVis交互系统，通过自然语言交互实现体积数据的语义级探索与实时编辑，解决了传统体积可视化方法交互僵化和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统体积可视化存在传输函数设计僵化、计算成本高的问题，现有新视角合成方法缺乏语义交互支持且对非专家不友好，需提升体积数据探索的交互友好性。

Method: 整合多视角语义分割与视觉语言模型提取场景语义，构建支持功能调用的多智能体大语言模型架构，通过声明式命令驱动基于3D可编辑高斯模型的渲染引擎。

Result: 案例研究与用户实验验证了系统在开放词汇对象查询、实时场景编辑、最佳视角选择等任务中的有效性，提升了体积数据探索的可用性。

Conclusion: NLI4VolVis成功实现了自然语言驱动的智能体积可视化交互范式，建议读者通过案例研究、演示视频和开源代码进一步了解系统能力。

Abstract: Traditional volume visualization (VolVis) methods, like direct volume
rendering, suffer from rigid transfer function designs and high computational
costs. Although novel view synthesis approaches enhance rendering efficiency,
they require additional learning effort for non-experts and lack support for
semantic-level interaction. To bridge this gap, we propose NLI4VolVis, an
interactive system that enables users to explore, query, and edit volumetric
scenes using natural language. NLI4VolVis integrates multi-view semantic
segmentation and vision-language models to extract and understand semantic
components in a scene. We introduce a multi-agent large language model
architecture equipped with extensive function-calling tools to interpret user
intents and execute visualization tasks. The agents leverage external tools and
declarative VolVis commands to interact with the VolVis engine powered by 3D
editable Gaussians, enabling open-vocabulary object querying, real-time scene
editing, best-view selection, and 2D stylization. We validate our system
through case studies and a user study, highlighting its improved accessibility
and usability in volumetric data exploration. We strongly recommend readers
check our case studies, demo video, and source code at
https://nli4volvis.github.io/.

</details>


### [62] [An Age-based Study into Interactive Narrative Visualization Engagement](https://arxiv.org/abs/2507.12734)
*Nina Errey,Yi Chen,Yu Dong,Quang Vinh Nguyen,Xiaoru Yuan,Tuck Wah Leong,Christy Jie Liang*

Main category: cs.HC

TL;DR: 研究发现观众年龄显著影响互动叙事可视化的参与度，年长群体参与度较低，建议设计时采用包容性策略


<details>
  <summary>Details</summary>
Motivation: 探讨常被忽视的年龄因素如何影响用户在互动叙事可视化中的参与度

Method: 采用可视化参与度问卷开展实证实验，结合定量分析与定性反馈比较不同年龄段的参与表现

Result: 年长群体参与度显著低于年轻群体，年轻群体对可视化术语和交互模式理解更深入

Conclusion: 提出根据年龄特征优化术语使用、交互复杂度等包容性设计准则

Abstract: Research has shown that an audiences' age impacts their engagement in digital
media. Interactive narrative visualization is an increasingly popular form of
digital media that combines data visualization and storytelling to convey
important information. However, audience age is often overlooked by interactive
narrative visualization authors. Using an established visualization engagement
questionnaire, we ran an empirical experiment where we compared end-user
engagement to audience age. We found a small difference in engagement scores
where older age cohorts were less engaged than the youngest age cohort. Our
qualitative analysis revealed that the terminology and overall understanding of
interactive narrative patterns integrated into narrative visualization was more
apparent in the feedback from younger age cohorts relative to the older age
cohorts. We conclude this paper with a series of recommendations for authors of
interactive narrative visualization on how to design inclusively for audiences
according to their age.

</details>
