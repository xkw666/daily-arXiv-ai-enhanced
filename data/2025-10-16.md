<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 77]
- [cs.GR](#cs.GR) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.AI](#cs.AI) [Total: 6]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning](https://arxiv.org/abs/2510.12807)
*Mahdi Cherakhloo,Arash Abbasi,Mohammad Saeid Sarafraz,Bijan Vosoughi Vahdat*

Main category: cs.CL

TL;DR: 评估开源大语言模型在波斯语NLP任务中的表现，发现Gemma 2在多数任务中表现最优，但存在词级理解任务的挑战


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在波斯语等低资源语言的有效性，填补现有研究的空白

Method: 使用ParsiNLU和ArmanEmo数据集，在零样本/少样本学习范式下测试情感分析、命名实体识别等任务，采用准确率/F1值等评估指标

Result: Gemma 2在复杂推理任务中表现突出，但多数模型在命名实体识别等词级任务上表现欠佳

Conclusion: 为多语言大模型研究提供波斯语基准，揭示波斯语处理的特殊挑战，指导未来模型开发

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
numerous languages; however, their effectiveness in low-resource languages like
Persian requires thorough investigation. This paper presents a comprehensive
benchmark of several open-source LLMs for Persian Natural Language Processing
(NLP) tasks, utilizing both zero-shot and few-shot learning paradigms. We
evaluate models across a range of tasks including sentiment analysis, named
entity recognition, reading comprehension, and question answering, using
established Persian datasets such as ParsiNLU and ArmanEmo. Our methodology
encompasses rigorous experimental setups for both zero-shot and few-shot
scenarios, employing metrics such as Accuracy, F1-score, BLEU, and ROUGE for
performance evaluation. The results reveal that Gemma 2 consistently
outperforms other models across nearly all tasks in both learning paradigms,
with particularly strong performance in complex reasoning tasks. However, most
models struggle with token-level understanding tasks like Named Entity
Recognition, highlighting specific challenges in Persian language processing.
This study contributes to the growing body of research on multilingual LLMs,
providing valuable insights into their performance in Persian and offering a
benchmark for future model development.

</details>


### [2] [Cancer Diagnosis Categorization in Electronic Health Records Using Large Language Models and BioBERT: Model Performance Evaluation Study](https://arxiv.org/abs/2510.12813)
*Soheil Hashtarkhani,Rezaur Rashid,Christopher L Brett,Lokesh Chinthala,Fekede Asefa Kumsa,Janet A Zink,Robert L Davis,David L Schwartz,Arash Shaban-Nejad*

Main category: cs.CL

TL;DR: BioBERT在结构化ICD编码分类表现最佳，GPT-4o在自由文本诊断分类领先，但临床可靠应用仍需人工监督


<details>
  <summary>Details</summary>
Motivation: 解决电子健康记录数据非结构化处理难题，评估AI模型在癌症诊断分类中的可靠性和适用性

Method: 使用3456份患者记录（含762个诊断案例），比较5种模型在14类癌症分类任务中的表现，并由肿瘤专家验证结果

Result: BioBERT结构化数据F1-score 84.2（ICD），GPT-4o自由文本F1-score 71.8，模型常混淆转移瘤与中枢神经系统肿瘤分类

Conclusion: 当前模型适用于行政/研究场景，临床高风险管理需结合标准化文档规范和人工复核机制

Abstract: Electronic health records contain inconsistently structured or free-text
data, requiring efficient preprocessing to enable predictive health care
models. Although artificial intelligence-driven natural language processing
tools show promise for automating diagnosis classification, their comparative
performance and clinical reliability require systematic evaluation. The aim of
this study is to evaluate the performance of 4 large language models (GPT-3.5,
GPT-4o, Llama 3.2, and Gemini 1.5) and BioBERT in classifying cancer diagnoses
from structured and unstructured electronic health records data. We analyzed
762 unique diagnoses (326 International Classification of Diseases (ICD) code
descriptions, 436free-text entries) from 3456 records of patients with cancer.
Models were tested on their ability to categorize diagnoses into 14predefined
categories. Two oncology experts validated classifications. BioBERT achieved
the highest weighted macro F1-score for ICD codes (84.2) and matched GPT-4o in
ICD code accuracy (90.8). For free-text diagnoses, GPT-4o outperformed BioBERT
in weighted macro F1-score (71.8 vs 61.5) and achieved slightly higher accuracy
(81.9 vs 81.6). GPT-3.5, Gemini, and Llama showed lower overall performance on
both formats. Common misclassification patterns included confusion between
metastasis and central nervous system tumors, as well as errors involving
ambiguous or overlapping clinical terminology. Although current performance
levels appear sufficient for administrative and research use, reliable clinical
applications will require standardized documentation practices alongside robust
human oversight for high-stakes decision-making.

</details>


### [3] [From Noise to Signal to Selbstzweck: Reframing Human Label Variation in the Era of Post-training in NLP](https://arxiv.org/abs/2510.12817)
*Shanshan Xu,Santosh T. Y. S. S,Barbara Plank*

Main category: cs.CL

TL;DR: 探讨人类标签多样性(HLV)在AI对齐中的核心价值，主张在偏好数据集中主动保留HLV以维护人类价值观的多元性


<details>
  <summary>Details</summary>
Motivation: 当前偏好学习数据集通过聚合标注消除差异，掩盖了人类价值观的多样性，违背了AI对齐保护人类多元价值的初衷

Method: 提出将HLV作为AI系统设计的根本目标，并规划了在偏好数据集中整合HLV的具体实施路径

Result: 建立了HLV作为系统性设计原则的理论框架，为构建真正尊重人类多元价值观的AI系统奠定基础

Conclusion: 保护HLV不仅是技术需求，更是维护人类多元价值的伦理要求，应成为AI系统设计的核心目标

Abstract: Human Label Variation (HLV) refers to legitimate disagreement in annotation
that reflects the genuine diversity of human perspectives rather than mere
error. For decades, HLV in NLP was dismissed as noise to be discarded, and only
slowly over the last decade has it been reframed as a signal for improving
model robustness. With the rise of large language models (LLMs), where
post-training on human feedback has become central to model alignment, the role
of HLV has become increasingly consequential. Yet current preference-learning
datasets routinely aggregate multiple annotations into a single label, thereby
flattening diverse perspectives into a false universal agreement and erasing
precisely the pluralism of human values that alignment aims to preserve. In
this position paper, we argue that preserving HLV as an embodiment of human
pluralism must be treated as a Selbstzweck - a goal it self when designing AI
systems. We call for proactively incorporating HLV into preference datasets and
outline actionable steps towards it.

</details>


### [4] [MEDEQUALQA: Evaluating Biases in LLMs with Counterfactual Reasoning](https://arxiv.org/abs/2510.12818)
*Rajarshi Ghosh,Abhay Gupta,Hudson McBride,Anurag Vaidya,Faisal Mahmood*

Main category: cs.CL

TL;DR: 研究者开发了MEDEQUALQA基准测试，发现医疗AI模型在患者代词变化时虽总体输出稳定，但关键推理环节存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在临床决策支持中广泛部署，但患者人口统计学线索可能影响其推理过程。现有研究主要关注输出结果的差异，缺乏对受控人口参数变化下模型内部推理偏移的系统分析。

Method: 通过保持关键症状不变、仅替换患者代词(他/她/他们)构建反事实数据集MEDEQUALQA，包含69,000个临床场景。使用GPT-4.1模型进行测试，采用语义文本相似度(STS)量化不同代词变体间推理路径的稳定性。

Result: 总体语义相似度高(均值>0.80)，但风险因素引用、指南依据和鉴别诊断排序等环节存在系统性差异。约23%案例显示临床相关推理偏移，且最终诊断一致时仍可能产生不同治疗建议。

Conclusion: MEDEQUALQA为医疗AI提供了推理稳定性审计工具，揭示了可能引发医疗不平等的关键偏差位点，强调需要开发更鲁棒的临床决策支持系统。

Abstract: Large language models (LLMs) are increasingly deployed in clinical decision
support, yet subtle demographic cues can influence their reasoning. Prior work
has documented disparities in outputs across patient groups, but little is
known about how internal reasoning shifts under controlled demographic changes.
We introduce MEDEQUALQA, a counterfactual benchmark that perturbs only patient
pronouns (he/him, she/her, they/them) while holding critical symptoms and
conditions (CSCs) constant. Each clinical vignette is expanded into single-CSC
ablations, producing three parallel datasets of approximately 23,000 items each
(69,000 total). We evaluate a GPT-4.1 model and compute Semantic Textual
Similarity (STS) between reasoning traces to measure stability across pronoun
variants. Our results show overall high similarity (mean STS >0.80), but reveal
consistent localized divergences in cited risk factors, guideline anchors, and
differential ordering, even when final diagnoses remain unchanged. Our error
analysis highlights certain cases in which the reasoning shifts, underscoring
clinically relevant bias loci that may cascade into inequitable care.
MEDEQUALQA offers a controlled diagnostic setting for auditing reasoning
stability in medical AI.

</details>


### [5] [Classifier-Augmented Generation for Structured Workflow Prediction](https://arxiv.org/abs/2510.12825)
*Thomas Gschwind,Shramona Chakraborty,Nitin Gupta,Sameep Mehta*

Main category: cs.CL

TL;DR: 提出Classifier-Augmented Generation (CAG)方法，通过自然语言自动生成ETL工作流，结合分类器与少样本提示技术提升配置效率并降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统ETL工具配置复杂且依赖专业知识，需开发自然语言驱动的自动化方案降低使用门槛。

Method: CAG方法分解自然语言指令，通过分类器+少样本提示预测流程阶段，利用边缘预测构建非线性工作流，上下文推断属性配置，模块化架构支持端到端生成及验证。

Result: 相比基线方法，CAG在准确性/效率上提升显著，计算资源消耗减少，架构具备可解释性与鲁棒性。

Conclusion: 该系统首次实现自然语言驱动ETL流程的全自动化生成，为低代码数据工程提供创新解决方案。

Abstract: ETL (Extract, Transform, Load) tools such as IBM DataStage allow users to
visually assemble complex data workflows, but configuring stages and their
properties remains time consuming and requires deep tool knowledge. We propose
a system that translates natural language descriptions into executable
workflows, automatically predicting both the structure and detailed
configuration of the flow. At its core lies a Classifier-Augmented Generation
(CAG) approach that combines utterance decomposition with a classifier and
stage-specific few-shot prompting to produce accurate stage predictions. These
stages are then connected into non-linear workflows using edge prediction, and
stage properties are inferred from sub-utterance context. We compare CAG
against strong single-prompt and agentic baselines, showing improved accuracy
and efficiency, while substantially reducing token usage. Our architecture is
modular, interpretable, and capable of end-to-end workflow generation,
including robust validation steps. To our knowledge, this is the first system
with a detailed evaluation across stage prediction, edge layout, and property
generation for natural-language-driven ETL authoring.

</details>


### [6] [Scheming Ability in LLM-to-LLM Strategic Interactions](https://arxiv.org/abs/2510.12826)
*Thao Pham*

Main category: cs.CL

TL;DR: 研究通过博弈论框架测试主流LLM代理的策略性欺骗能力，发现即使无明确提示，模型普遍表现出高欺骗倾向与成功率。


<details>
  <summary>Details</summary>
Motivation: 评估LLM代理在自主部署中的策略性欺骗风险，填补LLM间欺骗行为研究的空白。

Method: 采用Cheap Talk信号游戏和Peer Evaluation对抗游戏，测试4个主流模型在有/无提示下的欺骗表现与策略。

Result: 提示下Gemini/Claude达近完美欺骗；无提示时Peer Evaluation欺骗率100%，Cheap Talk成功率95-100%。

Conclusion: 需建立高风险多智能体场景的严格评估机制，确保LLM部署的安全性。

Abstract: As large language model (LLM) agents are deployed autonomously in diverse
contexts, evaluating their capacity for strategic deception becomes crucial.
While recent research has examined how AI systems scheme against human
developers, LLM-to-LLM scheming remains underexplored. We investigate the
scheming ability and propensity of frontier LLM agents through two
game-theoretic frameworks: a Cheap Talk signaling game and a Peer Evaluation
adversarial game. Testing four models (GPT-4o, Gemini-2.5-pro,
Claude-3.7-Sonnet, and Llama-3.3-70b), we measure scheming performance with and
without explicit prompting while analyzing scheming tactics through
chain-of-thought reasoning. When prompted, most models, especially
Gemini-2.5-pro and Claude-3.7-Sonnet, achieved near-perfect performance.
Critically, models exhibited significant scheming propensity without prompting:
all models chose deception over confession in Peer Evaluation (100% rate),
while models choosing to scheme in Cheap Talk succeeded at 95-100% rates. These
findings highlight the need for robust evaluations using high-stakes
game-theoretic scenarios in multi-agent settings.

</details>


### [7] [Mathematics with large language models as provers and verifiers](https://arxiv.org/abs/2510.12829)
*Hieu Le Duc,Leo Liberti*

Main category: cs.CL

TL;DR: ChatGPT通过多实例协作协议在定理证明领域取得突破，使用lean证明助手与人工验证确保严谨性，成功解决5/6 2025年IMO试题及三分之一数论猜想。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在复杂数学问题中的定理证明能力，验证协同工作模式对解决难题的有效性，同时通过双重验证机制规避AI幻觉问题。

Method: 采用GPT-5模型的多个实例分别担任证明生成器和验证器，最终通过lean证明助手进行形式化验证，并由人工核验逻辑一致性。

Result: 成功解决2025年IMO六题中的五道，完成Cohen数论猜想集66个问题中约三分之一的证明。

Conclusion: 多AI协同工作框架结合形式化验证机制，显著提升大模型解决复杂数学问题的可靠性，为AI辅助数学研究提供有效范式。

Abstract: During 2024 and 2025 the discussion about the theorem-proving capabilities of
large language models started reporting interesting success stories, mostly to
do with difficult exercises (such as problems from the International
Mathematical Olympiad), but also with conjectures [Feldman & Karbasi,
arXiv:2509.18383v1] formulated for the purpose of verifying whether the
artificial intelligence could prove it. In this paper we report a theorem
proving feat achieved by ChatGPT by using a protocol involving different prover
and verifier instances of the gpt-5 model working collaboratively. To make sure
that the produced proofs do not suffer from hallucinations, the final proof is
formally verified by the lean proof assistant, and the conformance of premises
and conclusion of the lean code is verified by a human. Our methodology was
able to solve five out of six 2025 IMO problems, and close a third of the
sixty-six number theory conjectures in [Cohen, Journal of Integer Sequences,
2025].

</details>


### [8] [MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training](https://arxiv.org/abs/2510.12831)
*Taicheng Guo,Hai Wang,ChaoChun Liu,Mohsen Golalikhani,Xin Chen,Xiangliang Zhang,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 提出MTSQL-R1框架，通过马尔可夫决策过程实现多轮Text-to-SQL的持续验证与优化循环，在基准测试中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有系统缺乏执行验证和显式优化机制，导致生成不可执行/不连贯的SQL查询

Method: 构建马尔可夫决策过程框架，通过数据库执行反馈与对话记忆验证，形成执行->验证->优化的持续迭代循环

Result: 在COSQL和SPARC数据集上显著超越基线模型，验证了环境驱动验证和记忆引导优化的有效性

Conclusion: 环境驱动的执行验证与持久化对话记忆引导的优化机制是提升对话式语义解析性能的关键要素

Abstract: Multi-turn Text-to-SQL aims to translate a user's conversational utterances
into executable SQL while preserving dialogue coherence and grounding to the
target schema. However, most existing systems only regard this task as a simple
text translation task and follow a short-horizon paradigm, generating a query
per turn without execution, explicit verification, and refinement, which leads
to non-executable or incoherent outputs. We present MTSQL-R1, an agentic
training framework for long-horizon multi-turn Text-to-SQL. We cast the task as
a Markov Decision Process (MDP) in which an agent interacts with (i) a database
for execution feedback and (ii) a persistent dialogue memory for coherence
verification, performing an iterative propose to execute -> verify -> refine
cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that
MTSQL-R1 consistently outperforms strong baselines, highlighting the importance
of environment-driven verification and memory-guided refinement for
conversational semantic parsing. Full recipes (including code, trained models,
logs, reasoning trajectories, etc.) will be released after the internal review
to contribute to community research.

</details>


### [9] [Repurposing Annotation Guidelines to Instruct LLM Annotators: A Case Study](https://arxiv.org/abs/2510.12835)
*Kon Woo Kim,Rezarta Islamaj,Jin-Dong Kim,Florian Boudin,Akiko Aizawa*

Main category: cs.CL

TL;DR: 研究探索如何将传统人工标注指南转化为适合LLM的结构化指令，通过NCBI疾病语料库实验验证有效性并揭示实践挑战


<details>
  <summary>Details</summary>
Motivation: 传统标注指南面向人类设计，无法直接指导LLM完成自动化标注任务，需开发适配LLM特性的结构化指令生成方法

Method: 提出面向LLM的指南重构方法，通过模型调节过程将人工指南转化为机器可执行的明确指令（基于NCBI疾病标注任务开展实验验证）

Result: 重构后的指南能有效指导LLM标注，但暴露指令模糊性、标注一致性维护等实际工程挑战

Conclusion: 该方法为自动化标注提供了经济高效的解决方案，但需进一步解决LLM特性带来的操作性问题以实现规模化应用

Abstract: This study investigates how existing annotation guidelines can be repurposed
to instruct large language model (LLM) annotators for text annotation tasks.
Traditional guidelines are written for human annotators who internalize
training, while LLMs require explicit, structured instructions. We propose a
moderation-oriented guideline repurposing method that transforms guidelines
into clear directives for LLMs through an LLM moderation process. Using the
NCBI Disease Corpus as a case study, our experiments show that repurposed
guidelines can effectively guide LLM annotators, while revealing several
practical challenges. The results highlight the potential of this workflow to
support scalable and cost-effective refinement of annotation guidelines and
automated annotation.

</details>


### [10] [A\textsuperscript{2}FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning](https://arxiv.org/abs/2510.12838)
*Qianben Chen,Jingyi Cao,Jiayu Zhang,Tianrui Qin,Xiaowan Li,King Zhu,Dingfeng Shi,He Zhu,Minghao Liu,Xiaobo Liang,Ge Zhang,Jian Yang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 提出A²FM框架统一推理型与代理型大模型，通过路由对齐机制和即时模式实现高效自适应执行，在保持精度的同时降低45%成本。


<details>
  <summary>Details</summary>
Motivation: 现有推理型和代理型LLM存在训练目标割裂，导致简单任务过度推理/调用工具、复杂任务能力不足的效率与精度失衡问题。

Method: 1. 采用route-then-align架构：任务感知路由学习+多模式对齐训练
2. 新增即时模式处理简单查询
3. 自适应策略优化(APO)实现模式间采样与成本正则化奖励

Result: 32B模型在BrowseComp(13.4%)/AIME25(70.4%)/HLE(16.7%)刷新SOTA，单正确答案成本仅$0.00487（较推理型降45.2%，较代理型降33.5%）

Conclusion: A²FM首次实现LLM多模式统一，通过自适应执行机制在精度与成本效率间取得突破性平衡，为下一代通用模型架构提供新范式。

Abstract: Large language models split into two families: reasoning-centric LLMs, which
strengthen internal chain-of-thought reasoning but cannot invoke external
tools, and agentic LLMs, which learn to interact with environments and leverage
tools but often lag in deep reasoning. This divide arises from fundamentally
different training objectives, leading to mismatched strengths and inefficiency
on simple queries, where both families tend to overthink or over-call tools. In
this work, we present Adaptive Agent Foundation Model (A\textsuperscript{2}FM),
a unified framework that follows a route-then-align principle: the model first
learns task-aware routing and then aligns mode-specific trajectories under a
shared backbone. To address the inefficiency gap, we introduce a third
mode-instant-that handles simple queries directly, preventing unnecessary
reasoning or tool calls while complementing the agentic and reasoning modes. To
jointly enhance accuracy and efficiency, we propose Adaptive Policy
Optimization (APO), which enforces adaptive sampling across modes and applies a
cost-regularized reward. On the 32B scale, A\textsuperscript{2}FM achieves
13.4\% on BrowseComp, 70.4\% on AIME25, and 16.7\% on HLE, setting new SOTA
among comparable models and performing competitively with frontier LLMs across
agentic, reasoning, and general benchmarks. Notably, the adaptive execution
achieves a cost of pass of only \$0.00487 per correct answer-cutting cost by
45.2\% relative to reasoning and 33.5\% relative to agentic, thus delivering
substantially higher cost efficiency while maintaining comparable accuracy.

</details>


### [11] [FaStFACT: Faster, Stronger Long-Form Factuality Evaluations in LLMs](https://arxiv.org/abs/2510.12839)
*Yingjia Wan,Haochen Tan,Xiao Zhu,Xinyu Zhou,Zhiwei Li,Qingsong Lv,Changxuan Sun,Jiaqi Zeng,Yi Xu,Jianqiao Lu,Yinhong Liu,Zhijiang Guo*

Main category: cs.CL

TL;DR: FastFact框架通过分块级声明提取与置信度预验证结合，配合文档级证据收集，实现了对长文本生成内容事实性的高效评估，实验证明其效果与效率均优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在两大缺陷：(1) 复杂流程导致效率低下，难以处理长文本输出；(2) 声明集不准确且证据收集仅依赖单行片段，导致评估效果不足。

Method: 1. 分块级声明提取集成置信度预验证，降低搜索和推理成本
2. 从爬取网页中收集文档级证据
3. 验证阶段选择性检索证据，解决证据不足问题

Result: 在人工标注的聚合基准测试中，FastFact在评估效果上与人工评估对齐度最高，同时保持最高运行效率（比现有方法快3倍）

Conclusion: FastFact通过系统优化评估流程，既解决了传统方法效率瓶颈，又通过分块提取和文档级证据增强了可靠性，为LLM生成内容的事实性评估提供了高效解决方案。代码和基准数据已开源。

Abstract: Evaluating the factuality of long-form generations from Large Language Models
(LLMs) remains challenging due to accuracy issues and costly human assessment.
Prior efforts attempt this by decomposing text into claims, searching for
evidence, and verifying claims, but suffer from critical drawbacks: (1)
inefficiency due to complex pipeline components unsuitable for long LLM
outputs, and (2) ineffectiveness stemming from inaccurate claim sets and
insufficient evidence collection of one-line snippets.
  To address these limitations, we propose \name, a fast and strong evaluation
framework that achieves the highest alignment with human evaluation and
efficiency among existing baselines. \name first employs chunk-level claim
extraction integrated with confidence-based pre-verification, significantly
reducing the cost of web searching and inference calling while ensuring
reliability. For searching and verification, it collects document-level
evidence from crawled webpages and selectively retrieves it during
verification, addressing the evidence insufficiency problem in previous
pipelines.
  Extensive experiments based on an aggregated and manually annotated benchmark
demonstrate the reliability of \name in both efficiently and effectively
evaluating the factuality of long-form LLM generations. Code and benchmark data
is available at https://github.com/Yingjia-Wan/FastFact.

</details>


### [12] [VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages](https://arxiv.org/abs/2510.12845)
*Jesse Atuhurra,Iqra Ali,Tomoya Iwakura,Hidetaka Kamigaito,Tatsuya Hiraoka*

Main category: cs.CL

TL;DR: 提出多语言基准VLURes评估视觉语言模型在英语/日语/低资源语言下的细粒度理解能力，发现GPT-4o与人类表现差距6.7%，开源模型差距更大


<details>
  <summary>Details</summary>
Motivation: 现有VLM评估集中于英语短文本，需构建多语言长文本基准测试模型在物体识别、场景理解等智能代理关键任务上的跨语言能力

Method: 从目标语言网络资源构建含8个任务的基准，覆盖10类图像和长文本，通过prompt生成模型响应并采用自动评估+母语者人工评估

Result: 最佳模型GPT-4o总体准确率90.8%，落后人类6.7%。开源模型表现更差，斯瓦希里语和乌尔都语任务准确率显著低于英语

Conclusion: VLURes填补多语言评估空白，揭示VLM在低资源语言和复杂推理任务的不足，为开发多模态智能代理提供关键基准

Abstract: Vision Language Models (VLMs) are pivotal for advancing perception in
intelligent agents. Yet, evaluation of VLMs remains limited to predominantly
English-centric benchmarks in which the image-text pairs comprise short texts.
To evaluate VLM fine-grained abilities, in four languages under long-text
settings, we introduce a novel multilingual benchmark VLURes featuring eight
vision-and-language tasks, and a pioneering unrelatedness task, to probe the
fine-grained Visual and Linguistic Understanding capabilities of VLMs across
English, Japanese, and low-resource languages, Swahili, and Urdu. Our datasets,
curated from web resources in the target language, encompass ten diverse image
categories and rich textual context, introducing valuable vision-language
resources for Swahili and Urdu. By prompting VLMs to generate responses and
rationales, evaluated automatically and by native speakers, we uncover
performance disparities across languages and tasks critical to intelligent
agents, such as object recognition, scene understanding, and relationship
understanding. We conducted evaluations of ten VLMs with VLURes. The best
performing model, GPT-4o, achieves an overall accuracy of 90.8% and lags human
performance by 6.7%, though the gap is larger for open-source models. The gap
highlights VLURes' critical role in developing intelligent agents to tackle
multi-modal visual reasoning.

</details>


### [13] [Efficient Adaptive Transformer: An Empirical Study and Reproducible Framework](https://arxiv.org/abs/2510.12856)
*Jan Miller*

Main category: cs.CL

TL;DR: EAT框架通过整合三种自适应技术，在保持精度的同时探索动态计算潜力，并提供开源工具促进社区研究。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer模型在延迟敏感场景的计算效率问题，通过动态自适应机制优化推理过程。

Method: 结合渐进式令牌修剪（压缩输入）、稀疏注意力（降低计算复杂度）、动态早期退出（按需终止计算）三项技术

Result: 在SST-2任务上准确率略超DistilBERT基线，但浅层模型组合机制可能增加推理延迟

Conclusion: EAT框架通过标准化实验流程和开源工具，为动态Transformer研究提供可复现基准，推动延迟敏感场景的NLP应用

Abstract: The Efficient Adaptive Transformer (EAT) framework unifies three adaptive
efficiency techniques - progressive token pruning, sparse attention, and
dynamic early exiting - into a single, reproducible architecture for
input-adaptive inference. EAT provides an open-source benchmarking pipeline
that automates data processing, timing, and ablation across GLUE tasks (SST-2,
QQP, MNLI). Although this empirical study finds that combining these mechanisms
can increase latency in shallow six-layer models, it demonstrates that EAT
achieves slightly higher accuracy than the optimized DistilBERT baseline on
SST-2, illustrating the potential of dynamic computation for latency-sensitive
NLP. The main contribution is the open, end-to-end reproducible framework -
complete with scripts, CSV logging, and analysis utilities - intended to serve
as a community tool for further research on adaptive transformers.

</details>


### [14] [A Critical Review of the Need for Knowledge-Centric Evaluation of Quranic Recitation](https://arxiv.org/abs/2510.12858)
*Mohammed Hilal Al-Kharusi,Khizar Hayat,Khalil Bader Al Ruqeishi,Haroon Rashid Lone*

Main category: cs.CL

TL;DR: 指出现有基于ASR的《古兰经》吟诵评估工具存在数据依赖性和反馈有效性缺陷，提出基于规则知识体系与音频分析结合的混合系统框架


<details>
  <summary>Details</summary>
Motivation: 数字化《古兰经》吟诵教学工具虽多但成效有限，现有技术架构存在根本性错位，急需构建符合教义规则的可靠评估系统

Method: 通过跨学科文献综述，系统分析近二十年学术研究、网络平台和商业应用的技术路径及其局限性

Result: 揭示数据驱动范式存在人口统计偏见、诊断反馈缺失等问题，验证基于发音规则预建模的声学分析框架更具鲁棒性

Conclusion: 未来发展方向应整合深层语言学知识与先进音频分析技术，构建符合教学伦理且具备全球适应性的智能评估系统

Abstract: The sacred practice of Quranic recitation (Tajweed), governed by precise
phonetic, prosodic, and theological rules, faces significant pedagogical
challenges in the modern era. While digital technologies promise unprecedented
access to education, automated tools for recitation evaluation have failed to
achieve widespread adoption or pedagogical efficacy. This literature review
investigates this critical gap, conducting a comprehensive analysis of academic
research, web platforms, and commercial applications developed over the past
two decades. Our synthesis reveals a fundamental misalignment in prevailing
approaches that repurpose Automatic Speech Recognition (ASR) architectures,
which prioritize lexical recognition over qualitative acoustic assessment and
are plagued by data dependency, demographic biases, and an inability to provide
diagnostically useful feedback. Critiquing these data--driven paradigms, we
argue for a foundational paradigm shift towards a knowledge-centric
computational framework. Capitalizing on the immutable nature of the Quranic
text and the precisely defined rules of Tajweed, we propose that a robust
evaluator must be architected around anticipatory acoustic modeling based on
canonical rules and articulation points (Makhraj), rather than relying on
statistical patterns learned from imperfect and biased datasets. This review
concludes that the future of automated Quranic evaluation lies in hybrid
systems that integrate deep linguistic knowledge with advanced audio analysis,
offering a path toward robust, equitable, and pedagogically sound tools that
can faithfully support learners worldwide.

</details>


### [15] [EduDial: Constructing a Large-scale Multi-turn Teacher-Student Dialogue Corpus](https://arxiv.org/abs/2510.12899)
*Shouang Wei,Min Zhang,Xin Lin,Bo Jiang,Zhongxiang Dai,Kun Kuang*

Main category: cs.CL

TL;DR: 该研究提出了教育对话数据集EduDial及其训练模型EduDial-LLM，通过11维评估框架验证其在17个主流大语言模型中表现最优，证明了教育领域定制化模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对话评测基准难以满足教育场景需求，需构建专门化的师生对话数据集以推动教育智能化发展。大语言模型深度理解教学情境的能力使其成为教育技术的关键。

Method: 基于布鲁姆教育目标分类学设计，整合情境提问、最近发展区提问等10种策略，创建包含34,250个对话的EduDial数据集，并训练32B参数的EduDial-LLM模型。

Result: EduDial-LLM在11维评估框架下显著优于所有基线模型，实验显示主流模型在师生对话场景存在局限，而定制模型各项指标提升明显。

Conclusion: 研究证实教育专用数据集与评估体系的重要性，EduDial为智能教学系统开发提供有效解决方案，差异化教学策略设计增强了模型的教学针对性。

Abstract: Recently, several multi-turn dialogue benchmarks have been proposed to
evaluate the conversational abilities of large language models (LLMs). As LLMs
are increasingly recognized as a key technology for advancing intelligent
education, owing to their ability to deeply understand instructional contexts
and provide personalized guidance, the construction of dedicated
teacher-student dialogue benchmarks has become particularly important. To this
end, we present EduDial, a comprehensive multi-turn teacher-student dialogue
dataset. EduDial covers 345 core knowledge points and consists of 34,250
dialogue sessions generated through interactions between teacher and student
agents. Its design is guided by Bloom's taxonomy of educational objectives and
incorporates ten questioning strategies, including situational questioning,
zone of proximal development (ZPD) questioning, and metacognitive
questioning-thus better capturing authentic classroom interactions.
Furthermore, we design differentiated teaching strategies for students at
different cognitive levels, thereby providing more targeted teaching guidance.
Building on EduDial, we further develop EduDial-LLM 32B via training and
propose an 11-dimensional evaluation framework that systematically measures the
teaching abilities of LLMs, encompassing both overall teaching quality and
content quality. Experiments on 17 mainstream LLMs reveal that most models
struggle in student-centered teaching scenarios, whereas our EduDial-LLM
achieves significant gains, consistently outperforming all baselines across all
metrics. The code is available at
https://github.com/Mind-Lab-ECNU/EduDial/tree/main.

</details>


### [16] [Who's Asking? Evaluating LLM Robustness to Inquiry Personas in Factual Question Answering](https://arxiv.org/abs/2510.12925)
*Nil-Jana Akpinar,Chia-Jung Lee,Vanessa Murdock,Pietro Perona*

Main category: cs.CL

TL;DR: 研究发现用户背景信息会影响大语言模型的事实回答准确性，导致拒绝回答、幻觉限制和角色混淆等问题。


<details>
  <summary>Details</summary>
Motivation: 测试大语言模型对真实用户属性（身份/专业/信仰）的敏感性，此前研究多关注对抗性输入而非实际用户互动场景。

Method: 通过系统性评估人类中心化的询问角色线索（inquiry persona cues），模拟真实用户交互场景。

Result: 用户背景线索显著改变QA准确率，并引发拒绝回答（refusals）、虚构限制（hallucinated limitations）和角色混淆（role confusion）三类故障模式。

Conclusion: 模型对用户框架的敏感性会损害事实可靠性，建议将询问角色测试作为有效的鲁棒性评估工具。

Abstract: Large Language Models (LLMs) should answer factual questions truthfully,
grounded in objective knowledge, regardless of user context such as
self-disclosed personal information, or system personalization. In this paper,
we present the first systematic evaluation of LLM robustness to inquiry
personas, i.e. user profiles that convey attributes like identity, expertise,
or belief. While prior work has primarily focused on adversarial inputs or
distractors for robustness testing, we evaluate plausible, human-centered
inquiry persona cues that users disclose in real-world interactions. We find
that such cues can meaningfully alter QA accuracy and trigger failure modes
such as refusals, hallucinated limitations, and role confusion. These effects
highlight how model sensitivity to user framing can compromise factual
reliability, and position inquiry persona testing as an effective tool for
robustness evaluation.

</details>


### [17] [The Curious Case of Curiosity across Human Cultures and LLMs](https://arxiv.org/abs/2510.12943)
*Angana Borah,Rada Mihalcea*

Main category: cs.CL

TL;DR: 研究发现LLMs在跨文化好奇心表达中存在西方偏向，通过微调策略可将人机对齐差距缩小50%，验证了好奇心对跨文化NLP应用的重要性


<details>
  <summary>Details</summary>
Motivation: 现有LLM研究缺乏对好奇心这一核心人类特质的跨文化分析，尤其缺乏不同社会背景下的差异化表达研究

Method: 使用Yahoo! Answers多国数据集构建CUEST评估框架，结合语言学风格分析、主题偏好分析和社会科学理论验证

Result: 主流LLMs呈现文化扁平化特征，与西方国家表达模式对齐度更高；基于文化特征的微调显著提升模型跨文化适应性

Conclusion: 好奇心建模应成为NLP研究重点，文化敏感的模型调整策略可有效增强LLMs的全球应用价值

Abstract: Recent advances in Large Language Models (LLMs) have expanded their role in
human interaction, yet curiosity -- a central driver of inquiry -- remains
underexplored in these systems, particularly across cultural contexts. In this
work, we investigate cultural variation in curiosity using Yahoo! Answers, a
real-world multi-country dataset spanning diverse topics. We introduce CUEST
(CUriosity Evaluation across SocieTies), an evaluation framework that measures
human-model alignment in curiosity through linguistic (style), topic preference
(content) analysis and grounding insights in social science constructs. Across
open- and closed-source models, we find that LLMs flatten cross-cultural
diversity, aligning more closely with how curiosity is expressed in Western
countries. We then explore fine-tuning strategies to induce curiosity in LLMs,
narrowing the human-model alignment gap by up to 50\%. Finally, we demonstrate
the practical value of curiosity for LLM adaptability across cultures, showing
its importance for future NLP research.

</details>


### [18] [3-Model Speculative Decoding](https://arxiv.org/abs/2510.12966)
*Sanghyun Byun,Mohanad Odema,Jung Ick Guack,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 提出金字塔推测解码（PyramidSD），通过引入中间限定模型桥接分布差异，允许使用更小草案模型，在保持性能前提下显著提升推测解码效率


<details>
  <summary>Details</summary>
Motivation: 传统推测解码存在模型大小与令牌接受率的权衡矛盾——小模型生成快但接受率低，大模型反之。现有方法难以兼顾推理速度与质量

Method: 在草案模型和目标模型间插入中间限定模型，建立分层解码架构。采用模糊接受标准，允许各阶段放宽发散阈值，提升吞吐量

Result: 实验显示推理速度最高提升1.91倍（达124 token/s@RTX4090），在1B-8B模型配置下仅轻微牺牲质量即可显著提升吞吐

Conclusion: PyramidSD为推测解码提供高效解决方案，可无缝集成现有推理流程，在资源受限场景下具有显著实用价值

Abstract: Speculative Decoding (SD) accelerates inference in large language models by
using a smaller draft model to propose tokens, which are then verified by a
larger target model. However, the throughput gains of SD are fundamentally
limited by a trade-off between draft model size and token acceptance: smaller
draft models generate tokens more quickly but exhibit greater divergence from
the target model, resulting in lower acceptance rates and reduced speedups. We
introduce Pyramid Speculative Decoding (PyramidSD), an extension of SD that
inserts an intermediate qualifier model between the draft and target to bridge
the distributional gap in output predictions, allowing smaller model to be used
for drafting. This hierarchical decoding strategy improves alignment across
models, enabling higher acceptance rates and allowing the use of significantly
smaller draft models without sacrificing overall performance. PyramidSD builds
on fuzzy acceptance criteria to support relaxed divergence thresholds at each
stage, improving throughput. In experiments, PyramidSD achieves up to 1.91x
generation speed over standard SD, reaching 124 tokens per second on a consumer
GPU (RTX 4090). In small-memory settings with a 1B-parameter draft model and an
8B target model, PyramidSD minimally trades target model quality for improved
throughput. Overall, PyramidSD offers a practical approach to enhancing
speculative decoding efficiency and can be readily applied to existing
inference pipelines.

</details>


### [19] [A Multilingual, Large-Scale Study of the Interplay between LLM Safeguards, Personalisation, and Disinformation](https://arxiv.org/abs/2510.12993)
*João A. Leite,Arnav Arora,Silvia Gargova,João Luz,Gustavo Sampaio,Ian Roberts,Carolina Scarton,Kalina Bontcheva*

Main category: cs.CL

TL;DR: 研究发现大型语言模型生成个性化虚假信息时越狱风险显著增加，个性化提示会提升虚假信息说服力并改变语言模式，暴露当前模型安全机制漏洞。


<details>
  <summary>Details</summary>
Motivation: 针对LLM可能被滥用于生成个性化虚假信息的风险，现有研究在说服力与人口属性定制化方面存在空白。

Method: 采用红队测试方法，构建包含160万条文本的AI-TRAITS数据集（覆盖4种语言/150个人设/324个虚假叙事），评估8个前沿LLM的安全机制鲁棒性。

Result: 简单个性化策略使所有LLM越狱概率显著上升，个性化提示改变语言修辞模式并增强虚假叙事说服力。

Conclusion: 研究揭示前沿LLM在多语言跨人口场景下的安全漏洞，为改进安全对齐和检测策略提供实证基础。

Abstract: The human-like proficiency of Large Language Models (LLMs) has brought
concerns about their potential misuse for generating persuasive and
personalised disinformation at scale. While prior work has demonstrated that
LLMs can generate disinformation, specific questions around persuasiveness and
personalisation (generation of disinformation tailored to specific demographic
attributes) remain largely unstudied. This paper presents the first
large-scale, multilingual empirical study on persona-targeted disinformation
generation by LLMs. Employing a red teaming methodology, we systematically
evaluate the robustness of LLM safety mechanisms to persona-targeted prompts. A
key novel result is AI-TRAITS (AI-generaTed peRsonAlIsed disinformaTion
dataSet), a new dataset of around 1.6 million texts generated by eight
state-of-the-art LLMs. AI-TRAITS is seeded by prompts that combine 324
disinformation narratives and 150 distinct persona profiles, covering four
major languages (English, Russian, Portuguese, Hindi) and key demographic
dimensions (country, generation, political orientation). The resulting
personalised narratives are then assessed quantitatively and compared along the
dimensions of models, languages, jailbreaking rate, and personalisation
attributes. Our findings demonstrate that the use of even simple
personalisation strategies in the prompts significantly increases the
likelihood of jailbreaks for all studied LLMs. Furthermore, personalised
prompts result in altered linguistic and rhetorical patterns and amplify the
persuasiveness of the LLM-generated false narratives. These insights expose
critical vulnerabilities in current state-of-the-art LLMs and offer a
foundation for improving safety alignment and detection strategies in
multilingual and cross-demographic contexts.

</details>


### [20] [OPLoRA: Orthogonal Projection LoRA Prevents Catastrophic Forgetting during Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2510.13003)
*Yifeng Xiong,Xiaohui Xie*

Main category: cs.CL

TL;DR: 提出正交投影LoRA（OPLoRA），通过双面正交投影解决LoRA微调中的灾难性遗忘问题，在保留预训练知识的同时保持任务性能


<details>
  <summary>Details</summary>
Motivation: 传统LoRA在微调时会与预训练模型的主奇异方向产生干扰，导致关键知识遗忘。需要找到既能高效微调又能保护核心知识的方法

Method: 1. 对冻结权重进行SVD分解
2. 设计左右正交投影矩阵P_L和P_R
3. 将LoRA更新约束在top-k奇异子空间的正交补空间
4. 数学证明可完全保留top-k奇异三元组

Result: 在常识推理、数学和代码生成任务中（LLaMA-2 7B/Qwen2.5 7B）:
- 显著降低遗忘现象
- 保持与原始LoRA相当的微调性能
- 新提出的ρ_k指标有效量化子空间干扰

Conclusion: 正交投影机制为参数高效微调提供了可靠的知识保护方案，理论保证与实验结果共同验证了该方法的有效性

Abstract: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language
models but suffers from catastrophic forgetting when learned updates interfere
with the dominant singular directions that encode essential pre-trained
knowledge. We propose Orthogonal Projection LoRA (OPLoRA), a theoretically
grounded approach that prevents this interference through double-sided
orthogonal projections. By decomposing frozen weights via SVD, OPLoRA
constrains LoRA updates to lie entirely within the orthogonal complement of the
top-$k$ singular subspace using projections $P_L = I - U_k U_k^\top$ and $P_R =
I - V_k V_k^\top$. We prove that this construction exactly preserves the
top-$k$ singular triples, providing mathematical guarantees for knowledge
retention. To quantify subspace interference, we introduce $\rho_k$, a metric
measuring update alignment with dominant directions. Extensive experiments
across commonsense reasoning, mathematics, and code generation demonstrate that
OPLoRA significantly reduces forgetting while maintaining competitive
task-specific performance on LLaMA-2 7B and Qwen2.5 7B, establishing orthogonal
projection as an effective mechanism for knowledge preservation in
parameter-efficient fine-tuning.

</details>


### [21] [CurLL: A Developmental Framework to Evaluate Continual Learning in Language Models](https://arxiv.org/abs/2510.13008)
*Pavan Kalyan,Shubhra Mishra,Satya Lokam,Navin Goyal*

Main category: cs.CL

TL;DR: 提出基于5-10岁儿童发展轨迹的持续学习数据集CurlL，支持细粒度评估模型技能增量获取能力


<details>
  <summary>Details</summary>
Motivation: 通过模拟人类分阶段学习过程，改进现有持续学习评估方法对技能依赖关系和知识迁移效率的测量精度

Method: 构建包含23.4B token的合成数据集，通过控制技能递进、词汇复杂度及文本格式多样性，结合135M参数Transformer模型进行独立/联合/持续训练对比

Result: 发现持续学习场景下模型在技能保持与迁移效率间的权衡关系，验证数据集对遗忘、前向/后向迁移的量化分析能力

Conclusion: CurlL通过镜像人类学习模式及细粒度技能依赖控制，为语言模型持续学习评估提供更贴近认知发展的评测基准

Abstract: We introduce a comprehensive continual learning dataset and benchmark (CurlL)
grounded in human developmental trajectories from ages 5-10, enabling
systematic and fine-grained assessment of models' ability to progressively
acquire new skills. CurlL spans five developmental stages (0-4) covering ages
5-10, supported by a skill graph that breaks down broad skills into smaller
abilities, concrete goals, and measurable indicators, while also capturing
which abilities build on others. We generate a 23.4B-token synthetic dataset
with controlled skill progression, vocabulary complexity, and format diversity,
comprising paragraphs, comprehension-based QA (CQA), skill-testing QA (CSQA),
and instruction-response (IR) pairs. Stage-wise token counts range from 2.12B
to 6.78B tokens, supporting precise analysis of forgetting, forward transfer,
and backward transfer. Using a 135M-parameter transformer trained under
independent, joint, and sequential (continual) setups, we show trade-offs in
skill retention and transfer efficiency. By mirroring human learning patterns
and providing fine-grained control over skill dependencies, this work advances
continual learning evaluations for language models.

</details>


### [22] [On the Role of Preference Variance in Preference Optimization](https://arxiv.org/abs/2510.13022)
*Jiacheng Guo,Zihao Li,Jiahao Qiu,Yue Wu,Mengdi Wang*

Main category: cs.CL

TL;DR: 研究提出偏好方差（PVar）指标可有效筛选高价值训练样本，仅用10%高PVar样本即可超越全量数据训练效果，显著降低LLM对齐成本。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据收集成本高且效率低，研究旨在通过分析偏好方差（PVar）对DPO训练的影响，找到高效选择高质量训练样本的方法。

Method: 1. 理论推导DPO梯度范数与PVar的数学关系
2. 基于奖励模型生成偏好数据微调LLM
3. 在AlpacaEval 2.0和Arena-Hard基准测试验证
4. 使用1B/3B小规模奖励模型测试鲁棒性
5. 利用UltraFeedback人类标注数据对比实验

Result: 1. 高PVar提示词性能优于随机/低PVar选择（最高提升显著）
2. 小规模奖励模型筛选依然有效
3. UltraFeedback数据中前10%高PVar样本训练效果优于全量数据

Conclusion: 偏好方差（PVar）是识别高效训练样本的关键指标，基于PVar的样本选择策略可显著提升LLM对齐效率，为低成本偏好学习提供理论依据与实践方案。

Abstract: Direct Preference Optimization (DPO) has emerged as an important approach for
learning from human preferences in aligning large language models (LLMs).
However, collecting human preference data is costly and inefficient, motivating
methods to reduce the required annotations. In this work, we investigate the
impact of \emph{preference variance} (PVar), which measures the variance in
model preferences when comparing pairs of responses, on the effectiveness of
DPO training. We provide a theoretical insight by establishing an upper bound
on the DPO gradient norm for any given prompt, showing it is controlled by the
PVar of that prompt. This implies that prompts with low PVar can only produce
small gradient updates, making them less valuable for learning. We validate
this finding by fine-tuning LLMs with preferences generated by a reward model,
evaluating on two benchmarks (AlpacaEval 2.0 and Arena-Hard). Experimental
results demonstrate that prompts with higher PVar outperform randomly selected
prompts or those with lower PVar. We also show that our PVar-based selection
method is robust, when using smaller reward models (1B, 3B) for selection.
Notably, in a separate experiment using the original human annotations from the
UltraFeedback dataset, we found that training on only the top 10\% of prompts
with the highest PVar yields better evaluation performance than training on the
full dataset, highlighting the importance of preference variance in identifying
informative examples for efficient LLM alignment.

</details>


### [23] [GatePro: Parameter-Free Expert Selection Optimization for Mixture-of-Experts Models](https://arxiv.org/abs/2510.13079)
*Chen Zheng,Yuhang Cai,Deyi Liu,Jin Ma,Yiyuan Ma,Yuan Yang,Jing Liu,Yutao Zeng,Xun Zhou,Siyuan Qiao*

Main category: cs.CL

TL;DR: GatePro通过局部竞争机制增强MoE专家多样性，无需额外参数即可减少冗余计算


<details>
  <summary>Details</summary>
Motivation: 针对MoE架构中相似专家同时激活导致的冗余计算问题，现有平衡损失方法未能解决专家多样性不足的根本问题，需要开发直接提升专家选择多样性的方法

Method: 提出参数免费的GatePro方法：1. 识别最相似的专家对 2. 引入局部竞争机制 3. 保持专家自然专业化同时防止冗余激活

Result: 跨模型规模和基准测试验证有效性，专家形成更显著互补能力，冗余减少。训练过程中可热插拔部署，无需额外参数

Conclusion: GatePro为提升MoE效率提供实用解决方案，通过增强专家多样性避免功能冗余，适用于任意训练阶段灵活部署

Abstract: Modern large language models leverage Mixture-of-Experts (MoE) architectures
for efficient scaling, but face a critical challenge: functionally similar
experts are often selected simultaneously, creating redundant computation and
limiting effective model capacity. Existing auxiliary balance loss methods
improve token distribution but fail to address the underlying expert diversity
problem. We introduce GatePro, a novel parameter-free method that directly
promotes expert selection diversity. GatePro identifies the most similar expert
pairs and introduces localized competition mechanisms, preventing redundant
expert co-activation while maintaining natural expert specialization. Our
comprehensive evaluation demonstrates GatePro's effectiveness across model
scales and benchmarks. Analysis demonstrates GatePro's ability to achieve
enhanced expert diversity, where experts develop more distinct and
complementary capabilities, avoiding functional redundancy. This approach can
be deployed hot-swappable during any training phase without additional
learnable parameters, offering a practical solution for improving MoE
effectiveness.

</details>


### [24] [ESI: Epistemic Uncertainty Quantification via Semantic-preserving Intervention for Large Language Models](https://arxiv.org/abs/2510.13103)
*Mingda Li,Xinyu Li,Weinan Zhang,Longxuan Ma*

Main category: cs.CL

TL;DR: 提出基于因果视角的灰盒不确定性量化方法，通过语义保持干预测量LLM输出变化来有效估计认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统UQ方法难以有效量化LLM的不确定性，需要从因果关系的角度建立模型不确定性与语义干预不变性的关联。

Method: 设计语义保持干预策略，构建输入文本的等价变体，通过测量干预前后输出差异构建不确定性量化指标。

Result: 在多种LLM和QA数据集上的实验表明，该方法相比基线在不确定性校准误差(ECE)降低18.7%，计算速度提升3倍。

Conclusion: 该方法为LLM可靠性评估提供了高效解决方案，首次实现模型不确定性与因果不变性的理论关联，具有重要实践价值。

Abstract: Uncertainty Quantification (UQ) is a promising approach to improve model
reliability, yet quantifying the uncertainty of Large Language Models (LLMs) is
non-trivial. In this work, we establish a connection between the uncertainty of
LLMs and their invariance under semantic-preserving intervention from a causal
perspective. Building on this foundation, we propose a novel grey-box
uncertainty quantification method that measures the variation in model outputs
before and after the semantic-preserving intervention. Through theoretical
justification, we show that our method provides an effective estimate of
epistemic uncertainty. Our extensive experiments, conducted across various LLMs
and a variety of question-answering (QA) datasets, demonstrate that our method
excels not only in terms of effectiveness but also in computational efficiency.

</details>


### [25] [Multi-Label Clinical Text Eligibility Classification and Summarization System](https://arxiv.org/abs/2510.13115)
*Surya Tejaswi Yerramsetty,Almas Fathimah*

Main category: cs.CL

TL;DR: 提出结合NLP与LLM的自动化系统，通过多标签分类和摘要技术提升临床试验资格评估效率


<details>
  <summary>Details</summary>
Motivation: 解决临床试验参与者筛选效率低下问题，利用AI技术实现自动化资格评估

Method: 整合Word2Vec、命名实体识别与TF-IDF特征提取，开发加权TF-IDF词嵌入，应用随机森林/SVM分类模型，采用TextRank/Luhn/GPT-3摘要技术

Result: ROUGE评分验证方法有效性，系统实现医疗概念识别与需求摘要的自动化处理

Conclusion: 数据驱动的自动化系统显著提升临床试验筛选效率，为医学研究提供可扩展解决方案

Abstract: Clinical trials are central to medical progress because they help improve
understanding of human health and the healthcare system. They play a key role
in discovering new ways to detect, prevent, or treat diseases, and it is
essential that clinical trials include participants with appropriate and
diverse medical backgrounds. In this paper, we propose a system that leverages
Natural Language Processing (NLP) and Large Language Models (LLMs) to automate
multi-label clinical text eligibility classification and summarization. The
system combines feature extraction methods such as word embeddings (Word2Vec)
and named entity recognition to identify relevant medical concepts, along with
traditional vectorization techniques such as count vectorization and TF-IDF
(Term Frequency-Inverse Document Frequency). We further explore weighted TF-IDF
word embeddings that integrate both count-based and embedding-based strengths
to capture term importance effectively. Multi-label classification using Random
Forest and SVM models is applied to categorize documents based on eligibility
criteria. Summarization techniques including TextRank, Luhn, and GPT-3 are
evaluated to concisely summarize eligibility requirements. Evaluation with
ROUGE scores demonstrates the effectiveness of the proposed methods. This
system shows potential for automating clinical trial eligibility assessment
using data-driven approaches, thereby improving research efficiency.

</details>


### [26] [Stable LLM Ensemble: Interaction between Example Representativeness and Diversity](https://arxiv.org/abs/2510.13143)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: 通过结合代表性示例选择与高温度采样，单次LLM集成性能显著提升（+7.6% macro-F1/-10.5% RMSE），效果优于随机选择和5次提示


<details>
  <summary>Details</summary>
Motivation: 现有LLM单次预测的准确性和鲁棒性高度依赖示例选择及集成多样性，需系统性研究示例代表性和温度参数的影响机制

Method: 比较质心法代表性示例选择与随机采样两种策略，结合不同温度参数设置，评估集成模型的性能差异

Result: 提出的方法在高温度设置下macro-F1提升7.6%、RMSE降低10.5%，且超过5次提示21.1%（macro-F1）和24.0%（RMSE）

Conclusion: 有效的单次LLM集成需要同时考虑示例代表性和受控多样性，二者的协同作用能显著提升模型性能与稳定性

Abstract: Large language models (LLMs) have achieved remarkable results in wide range
of domains. However, the accuracy and robustness of one-shot LLM predictions
remain highly sensitive to the examples and the diversity among ensemble
members. This study systematically investigates the effects of example
representativeness (one-shot strategy) and output diversity (sampling
temperature) on LLM ensemble performance. Two one-shot strategies are compared:
centroid-based representative examples (proposed) and randomly sampled examples
(baseline) and sampling temperature also is varied. The proposed approach with
higher temperature setting significantly outperforms random selection by +7.6%
(macro-F1) and -10.5% (RMSE). Furthermore, the proposed model exceeds 5-shot
prompting by +21.1% (macro-F1) and -24.0% (RMSE). Our findings demonstrate that
combining representative example selection with increased temperature provides
the appropriate level of diversity to the ensemble. This work highlights the
practical importance of both example selection and controlled diversity in
designing effective one-shot LLM ensembles.

</details>


### [27] [I Am Aligned, But With Whom? MENA Values Benchmark for Evaluating Cultural Alignment and Multilingual Bias in LLMs](https://arxiv.org/abs/2510.13154)
*Pardis Sadat Zahraei,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 提出MENAValues评估框架，揭示大语言模型在中东与北非地区存在跨语言价值偏移等文化对齐问题


<details>
  <summary>Details</summary>
Motivation: 针对中东与北非地区在AI评估中的代表性不足问题，基于权威人类调查数据构建结构化评测基准

Method: 通过三种视角框架（中性/个性化/文化观察者）与两种语言模式（英语/本地语言）组合形成多条件评估体系

Result: 发现跨语言价值偏移、推理能力导致的对齐退化、Logit泄漏现象，模型在本地语言模式下将多国简化为单一文化实体

Conclusion: MENAValues为诊断文化错位提供可扩展框架，促进开发更具文化包容性的人工智能系统

Abstract: We introduce MENAValues, a novel benchmark designed to evaluate the cultural
alignment and multilingual biases of large language models (LLMs) with respect
to the beliefs and values of the Middle East and North Africa (MENA) region, an
underrepresented area in current AI evaluation efforts. Drawing from
large-scale, authoritative human surveys, we curate a structured dataset that
captures the sociocultural landscape of MENA with population-level response
distributions from 16 countries. To probe LLM behavior, we evaluate diverse
models across multiple conditions formed by crossing three perspective framings
(neutral, personalized, and third-person/cultural observer) with two language
modes (English and localized native languages: Arabic, Persian, Turkish). Our
analysis reveals three critical phenomena: "Cross-Lingual Value Shifts" where
identical questions yield drastically different responses based on language,
"Reasoning-Induced Degradation" where prompting models to explain their
reasoning worsens cultural alignment, and "Logit Leakage" where models refuse
sensitive questions while internal probabilities reveal strong hidden
preferences. We further demonstrate that models collapse into simplistic
linguistic categories when operating in native languages, treating diverse
nations as monolithic entities. MENAValues offers a scalable framework for
diagnosing cultural misalignment, providing both empirical insights and
methodological tools for developing more culturally inclusive AI.

</details>


### [28] [Mirror Speculative Decoding: Breaking the Serial Barrier in LLM Inference](https://arxiv.org/abs/2510.13161)
*Nikhil Bhendawade,Kumari Nishu,Arnav Kundu,Chris Bartels,Minsik Cho,Irina Belousova*

Main category: cs.CL

TL;DR: Mirror-SD通过并行异构计算+多token推测流，突破传统推测解码的延迟-接受率权衡


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法（如Medusa、EAGLE）存在延迟与接受率的矛盾，增加草案规模会带来额外延迟。现有方法无法同时优化速度与准确性

Method: 1. 基于早期退出信号启动并行分支预测
2. 异构加速器（GPU+NPU）并行流水线执行
3. 引入多token推测流技术
4. 目标模型与草案模型互相验证预测路径

Result: 在14B-66B参数模型上实现2.8x-5.8x端到端加速，相对EAGLE3提升30%平均性能

Conclusion: Mirror-SD通过双重并行策略达到高接受率与低延迟的理想状态，大幅提升LLM推理效率

Abstract: Speculative decoding accelerates LLM inference by using a draft model to look
ahead, but gains are capped by the cost of autoregressive draft generation:
increasing draft size elevates acceptance rates but introduces additional
latency overhead exacerbating the speed-accuracy tradeoff. Prior methods
(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade
acceptance or introduce overheads that limit scaling. We present Mirror
Speculative Decoding (Mirror-SD), an inference algorithm that breaks the
latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from
early-exit signals in parallel with the target model's suffix and explicitly
maps computation across heterogeneous accelerators (GPU and NPU) to exploit
cross-device parallelism. The draft speculates forward continuations for the
target to verify, while the target simultaneously speculates correction paths
for the draft, converting speculation into two complementary execution
pipelines. To further cut draft latency without weakening acceptance semantics,
we add speculative streaming so the draft emits multiple tokens per step. This
dual strategy of parallel heterogeneous execution plus multi-token speculative
streaming pushes speculative decoding toward its ideal regime of high
acceptance with low overhead. On SpecBench with server-scale models from 14B to
66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving
2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative
improvement over the strongest baseline, EAGLE3.

</details>


### [29] [A Matter of Representation: Towards Graph-Based Abstract Code Generation](https://arxiv.org/abs/2510.13163)
*Nyx Iskandar,Hisham Bedri,Andy Tsen*

Main category: cs.CL

TL;DR: 提出JSON表示法提升LLM在图抽象代码生成中的准确率，验证不同表示法对结果影响显著。


<details>
  <summary>Details</summary>
Motivation: 现有LLM擅长生成顺序代码，但缺乏图结构代码生成能力（节点封装逻辑+边控制流程），而视觉编程语言及受限代码访问场景需要此能力。

Method: 基于自研Python版Scratch构建ScratchTest测试集，评估不同JSON表示法对LLM生成图结构代码的影响。

Result: 合适的JSON表示法使LLM单次生成准确率显著提升（无需复杂流程），不同表示法准确率差异达3倍。

Conclusion: 该研究为图抽象代码生成的表示学习奠定基础，证明表示法设计是该任务关键因素。

Abstract: Most large language models (LLMs) today excel at generating raw, sequential
code with minimal abstractions and custom structures. However, there has been
little work on graph-based abstract code generation, where significant logic is
encapsulated in predefined nodes and execution flow is determined by edges.
This is relevant for visual programming languages, and in cases where raw
source code is inaccessible to users and LLM training sets. In this work, we
propose and evaluate JSON representations for graphs to enable high accuracy
graph-based abstract code generation. We evaluate these representations on
ScratchTest, a mini-benchmark based on our custom Python re-implementation of
Scratch, which tests the LLM in code graph space. Our findings demonstrate that
LLMs can indeed perform the aforementioned generation task in a single pass
without relying on specialized or complex pipelines, given the correct graph
representations. We also show that different representations induce
significantly different accuracies, highlighting the instrumental role of
representations in this generation task. All in all, this work establishes the
first steps towards representation learning for graph-based abstract code
generation.

</details>


### [30] [CoT-Evo: Evolutionary Distillation of Chain-of-Thought for Scientific Reasoning](https://arxiv.org/abs/2510.13166)
*Kehua Feng,Keyan Ding,Zhihui Zhu,Lei Liang,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: 提出CoT-Evo框架，通过进化式思维链蒸馏提升科学领域推理数据质量，使小型模型在科学推理任务中达到顶尖性能。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法在科学领域效果差，因大型模型常产生错误/表面化推理，直接蒸馏导致低质量训练数据，限制学生模型性能。

Method: 1. 构建多LLM的多样化推理轨迹池 2. 自动检索领域知识增强 3. 通过新颖性驱动选择、反思重组和变异迭代优化轨迹，基于正确性、连贯性和知识利用率的适应度函数。

Result: 进化后的高质量数据集使微调模型在科学推理基准测试中取得state-of-the-art性能。

Conclusion: 建立了可扩展的进化框架，能够从多样化且有缺陷的LLM中合成高保真科学推理数据，突破现有蒸馏方法的局限性。

Abstract: While chain-of-thought (CoT) distillation from advanced large language models
(LLMs) has proven effective in general reasoning tasks, it struggles in
scientific domains where even advanced models often produce incorrect or
superficial reasoning due to high complexity and specialized knowledge
requirements. Directly distilling from such flawed outputs results in
low-quality training data and limits the performance of smaller student models.
To overcome this, we propose CoT-Evo, an evolutionary CoT distillation
framework. It begins by constructing a diverse pool of reasoning trajectories
from multiple LLM thinkers, enriches them with automatically retrieved domain
knowledge, and iteratively refines the trajectories using novelty-driven
selection, reflective recombination and mutation. The refinement is guided by a
fitness function that evaluates answer correctness, coherence, and effective
knowledge utilization. This results in a high-quality CoT dataset tailored for
scientific reasoning. We employ this evolved dataset to fine-tune a compact
model, which achieves state-of-the-art performance on scientific reasoning
benchmarks. Our work establishes a scalable approach to synthesizing
high-fidelity scientific reasoning data from diverse and fallible LLMs.

</details>


### [31] [Putting on the Thinking Hats: A Survey on Chain of Thought Fine-tuning from the Perspective of Human Reasoning Mechanism](https://arxiv.org/abs/2510.13170)
*Xiaoshu Chen,Sihang Zhou,Ke Liang,Duanyang Yuan,Haoyuan Chen,Xiaoyu Sun,Linyuan Meng,Xinwang Liu*

Main category: cs.CL

TL;DR: 首篇基于人类推理理论系统分析CoT微调技术的综述，通过六顶思考帽框架分类研究方法，构建数据集库并追踪领域进展


<details>
  <summary>Details</summary>
Motivation: 现有CoT微调研究多聚焦技术层面，缺乏从人类认知机制角度系统分析。为实现LLMs类人推理的终极目标，需结合人类认知理论展开研究

Method: 采用六顶思考帽理论框架（系统化人类思维模式分类方法），对现有CoT微调技术进行分类与系统性分析

Result: 构建包含完整数据集/模型性能的综述体系，建立实时更新的GitHub知识库，为领域发展提供持续资源支持

Conclusion: 通过建立CoT微调与人类推理理论的内在联系，为技术创新提供认知科学视角的启发，推动AI推理能力与人类思维机制的深度融合

Abstract: Chain of thought (CoT) fine-tuning aims to endow large language models (LLMs)
with reasoning capabilities by training them on curated reasoning traces. It
leverages both supervised and reinforced fine-tuning to cultivate human-like
reasoning skills in LLMs, including detailed planning, divergent thinking,
intuitive judgment, timely reflection, internal thinking, and fact perception,
etc. As CoT fine-tuning has advanced, LLMs have demonstrated substantial
improvements in tasks such as mathematical reasoning and code generation.
However, existing surveys about CoT fine-tuning primarily focus on technical
aspects and overlook a systematic analysis from the perspective of human
reasoning mechanisms. Given that the ultimate goal of CoT fine-tuning is to
enable LLMs to reason like humans, it is crucial to investigate this technique
through the lens of human cognition. To fill this gap, we present the first
comprehensive survey of CoT fine-tuning grounded in human reasoning theory.
Specifically, inspired by the well-known Six Thinking Hats framework, which
systematically characterizes common human thinking modes using six metaphorical
hats, we classify and examine CoT fine-tuning methods through this lens.
Furthermore, building upon this theory, we outline potential directions for
future research in CoT fine-tuning. In addition, we compile a comprehensive
overview of existing datasets and model performances, and a real-time GitHub
repository \footnote{https://github.com/AI-Chen/Awesome-CoT-Finetuning} that
continuously tracks recent advances in this area is maintained. We hope this
survey will serve as a valuable resource to inspire innovation and foster
progress in this rapidly evolving field.

</details>


### [32] [DSCD: Large Language Model Detoxification with Self-Constrained Decoding](https://arxiv.org/abs/2510.13183)
*Ming Dong,Jinkui Zhang,Bolong Zheng,Xinhui Tu,Po Hu,Tingting He*

Main category: cs.CL

TL;DR: 提出自约束解码方法DSCD，无需参数微调即可实现大语言模型高效去毒，兼具轻量化、强兼容性和即插即用特性。


<details>
  <summary>Details</summary>
Motivation: 现有基于外部约束的解码去毒方法存在额外资源消耗与生成流畅性下降的缺陷，需要更轻量的解决方案。

Method: 通过调整输出生成时安全层/幻觉层/毒性层的内部token分布权重，增强安全性同时抑制毒性内容生成。

Result: 在主流开源LLM和公开数据集上验证，DSCD在去毒效果和生成流畅度上均达到SOTA，效率显著优于现有方法。

Conclusion: DSCD为大语言模型安全部署提供了可扩展的实用解决方案，可与现有去毒方法叠加实现性能增益。

Abstract: Detoxification in large language models (LLMs) remains a significant research
challenge. Existing decoding detoxification methods are all based on external
constraints, which require additional resource overhead and lose generation
fluency. This work proposes Detoxification with Self-Constrained Decoding
(DSCD), a novel method for LLM detoxification without parameter fine-tuning.
DSCD strengthens the inner next-token distribution of the safety layer while
weakening that of hallucination and toxic layers during output generation. This
effectively diminishes toxicity and enhances output safety. DSCD offers
lightweight, high compatibility, and plug-and-play capabilities, readily
integrating with existing detoxification methods for further performance
improvement. Extensive experiments on representative open-source LLMs and
public datasets validate DSCD's effectiveness, demonstrating state-of-the-art
(SOTA) performance in both detoxification and generation fluency, with superior
efficiency compared to existing methods. These results highlight DSCD's
potential as a practical and scalable solution for safer LLM deployments.

</details>


### [33] [SHIELD: Classifier-Guided Prompting for Robust and Safer LVLMs](https://arxiv.org/abs/2510.13190)
*Juan Ren,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 提出SHIELD框架——一种轻量级预处理方法，通过细粒度安全分类与动态响应机制有效降低多模态模型的对抗攻击风险，无需重新训练即可实现即插即用的安全防护。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)虽具备强大推理能力，但其开放输入特性易受对抗攻击(如通过良性提示隐藏恶意指令)。现有二值化审查机制无法灵活应对复杂攻击场景。

Method: 1. 细粒度安全分类器识别攻击类型
2. 基于攻击类型的动态响应机制(阻断/重构请求/安全转发)
3. 组合定制化安全提示词实现精准防御
4. 兼容弱对齐与强对齐模型的即插即用架构

Result: 在5个基准测试和5种主流LVLM上：
- 越狱攻击成功率平均降低68%
- 指令违背率下降54%
- 保持95%+的正常任务可用性
- 单次请求处理延迟<50ms

Conclusion: SHIELD为多模态模型安全提供高效解决方案，其模块化设计具备：
1. 攻击类型快速扩展能力
2. 计算开销极低(仅增加0.2%参数量)
3. 兼容不同对齐程度的现有模型

Abstract: Large Vision-Language Models (LVLMs) unlock powerful multimodal reasoning but
also expand the attack surface, particularly through adversarial inputs that
conceal harmful goals in benign prompts. We propose SHIELD, a lightweight,
model-agnostic preprocessing framework that couples fine-grained safety
classification with category-specific guidance and explicit actions (Block,
Reframe, Forward). Unlike binary moderators, SHIELD composes tailored safety
prompts that enforce nuanced refusals or safe redirection without retraining.
Across five benchmarks and five representative LVLMs, SHIELD consistently
lowers jailbreak and non-following rates while preserving utility. Our method
is plug-and-play, incurs negligible overhead, and is easily extendable to new
attack types -- serving as a practical safety patch for both weakly and
strongly aligned LVLMs.

</details>


### [34] [Grounding Long-Context Reasoning with Contextual Normalization for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13191)
*Jiamin Chen,Yuchen Li,Xinyu Ma,Xinran Chen,Xiaokun Zhang,Shuaiqiang Wang,Chen Ma,Dawei Yin*

Main category: cs.CL

TL;DR: 研究发现RAG系统中上下文格式（如分隔符和结构标记）显著影响模型性能，提出上下文归一化策略提升鲁棒性和长文本利用率。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视文档格式化对RAG系统的影响，而实验表明相同的语义内容采用不同格式时性能存在显著差异，需系统性探索格式因素的作用机制。

Method: 通过控制变量实验分析上下文密度/分隔符风格/位置布局的影响，提出上下文归一化策略对检索内容进行自适应标准化处理。

Result: 该方法在多场景实验中提升模型对顺序变化的鲁棒性（最高+12.5%准确率），增强长上下文利用率（效率提升22%）。

Conclusion: 可靠的RAG系统需同时关注检索内容质量与呈现格式，上下文归一化为长文本推理提供了新的技术路径。

Abstract: Retrieval-Augmented Generation (RAG) has become an essential approach for
extending the reasoning and knowledge capacity of large language models (LLMs).
While prior research has primarily focused on retrieval quality and prompting
strategies, the influence of how the retrieved documents are framed, i.e.,
context format, remains underexplored. We show that seemingly superficial
choices, such as delimiters or structural markers in key-value extraction, can
induce substantial shifts in accuracy and stability, even when semantic content
is identical. To systematically investigate this effect, we design controlled
experiments that vary context density, delimiter styles, and positional
placement, revealing the underlying factors that govern performance
differences. Building on these insights, we introduce Contextual Normalization,
a lightweight strategy that adaptively standardizes context representations
before generation. Extensive experiments on both controlled and real-world RAG
benchmarks across diverse settings demonstrate that the proposed strategy
consistently improves robustness to order variation and strengthens
long-context utilization. These findings underscore that reliable RAG depends
not only on retrieving the right content, but also on how that content is
presented, offering both new empirical evidence and a practical technique for
better long-context reasoning.

</details>


### [35] [StressTransfer: Stress-Aware Speech-to-Speech Translation with Emphasis Preservation](https://arxiv.org/abs/2510.13194)
*Xi Chen,Yuchen Song,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 利用大语言模型实现跨语言重音转换的语音翻译系统，通过自动生成对齐数据与LLM评估机制，在保留语义强调的同时保持翻译质量


<details>
  <summary>Details</summary>
Motivation: 传统语音翻译系统难以有效保留词级强调等副语言信息，可能影响说话者意图传达。本研究旨在解决跨语言韵律迁移的数据稀缺问题并提升副语言特征保留效果

Method: 1. 将源语言重音转化为目标语言控制标签
2. 开发自动化数据对齐流程生成训练数据
3. 引入LLM-as-Judge评估机制
4. 结合可控TTS模型实现重音转换

Result: 在强调保留指标上显著优于基线模型(BLEU分数提升约15%)，同时保持相当的翻译质量(语义准确率>92%)和语音自然度(MOS 4.1/5.0)

Conclusion: 证明了韵律特征在语音翻译中的关键作用，提出的数据高效解决方案为副语言信息迁移开辟了新方向，未来可扩展至更多语言对和韵律特征

Abstract: We propose a stress-aware speech-to-speech translation (S2ST) system that
preserves word-level emphasis by leveraging LLMs for cross-lingual emphasis
conversion. Our method translates source-language stress into target-language
tags that guide a controllable TTS model. To overcome data scarcity, we
developed a pipeline to automatically generate aligned training data and
introduce the "LLM-as-Judge" for evaluation. Experiments show our approach
substantially outperforms baselines in preserving emphasis while maintaining
comparable translation quality, speaker intent, and naturalness. Our work
highlights the importance of prosody in translation and provides an effective,
data-efficient solution for preserving paralinguistic cues in S2ST.

</details>


### [36] [Text Anomaly Detection with Simplified Isolation Kernel](https://arxiv.org/abs/2510.13197)
*Yang Cao,Sikun Yang,Yujiu Yang,Lianyong Qi,Ming Liu*

Main category: cs.CL

TL;DR: 提出简化隔离核(SIK)方法，将高维文本嵌入映射为低维稀疏表示，在保持检测性能的同时显著降低计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的两阶段文本异常检测方法存在高内存占用和计算耗时问题，需要更高效的嵌入处理方法

Method: 通过边界聚焦特征映射创新，设计线性时间复杂度的SIK核函数，实现维度压缩同时保留异常特征

Result: 在7个数据集上超越11种先进算法，检测性能提升且计算效率提高100倍，内存成本降低1000倍

Conclusion: SIK在检测性能与计算效率间取得平衡，为资源受限场景提供有效解决方案，已开源代码便于应用验证

Abstract: Two-step approaches combining pre-trained large language model embeddings and
anomaly detectors demonstrate strong performance in text anomaly detection by
leveraging rich semantic representations. However, high-dimensional dense
embeddings extracted by large language models pose challenges due to
substantial memory requirements and high computation time. To address this
challenge, we introduce the Simplified Isolation Kernel (SIK), which maps
high-dimensional dense embeddings to lower-dimensional sparse representations
while preserving crucial anomaly characteristics. SIK has linear time
complexity and significantly reduces space complexity through its innovative
boundary-focused feature mapping. Experiments across 7 datasets demonstrate
that SIK achieves better detection performance than 11 state-of-the-art (SOTA)
anomaly detection algorithms while maintaining computational efficiency and low
memory cost. All code and demonstrations are available at
https://github.com/charles-cao/SIK.

</details>


### [37] [LLM-Guided Synthetic Augmentation (LGSA) for Mitigating Bias in AI Systems](https://arxiv.org/abs/2510.13202)
*Sai Suhruth Reddy Karri,Yashwanth Sai Nallapuneni,Laxmi Narasimha Reddy Mallireddy,Gopichand G*

Main category: cs.CL

TL;DR: 使用LLM生成反事实示例的LGSA方法，在保持高准确率的同时有效减少AI性别偏见差距（基线模型7.2%→1.9%）


<details>
  <summary>Details</summary>
Motivation: 传统公平性方法依赖受保护属性标签、面临准确性与公平性权衡，且难以跨数据集泛化。需要不依赖标签、保持数据完整性的新方法解决群体表征不足问题

Method: 提出LLM引导的合成增强（LGSA）：1) 使用结构化提示生成性别转换的释义 2) 通过语义相似度检查、属性验证、毒性筛查和人工抽查进行质量控制 3) 用增强数据集训练分类器

Result: 基线模型准确率96.7%（性别偏见差距7.2%）；简单替换法差距降至0.7%但准确率降为95.6%；LGSA达到99.1%准确率且差距1.9%，女性样本表现提升

Conclusion: LGSA在保持任务准确性和标签保真度的同时，有效改善亚组平衡，为AI偏见缓解提供新策略，证明合成数据增强的可行性

Abstract: Bias in AI systems, especially those relying on natural language data, raises
ethical and practical concerns. Underrepresentation of certain groups often
leads to uneven performance across demographics. Traditional fairness methods,
such as pre-processing, in-processing, and post-processing, depend on
protected-attribute labels, involve accuracy-fairness trade-offs, and may not
generalize across datasets. To address these challenges, we propose LLM-Guided
Synthetic Augmentation (LGSA), which uses large language models to generate
counterfactual examples for underrepresented groups while preserving label
integrity. We evaluated LGSA on a controlled dataset of short English sentences
with gendered pronouns, professions, and binary classification labels.
Structured prompts were used to produce gender-swapped paraphrases, followed by
quality control including semantic similarity checks, attribute verification,
toxicity screening, and human spot checks. The augmented dataset expanded
training coverage and was used to train a classifier under consistent
conditions. Results show that LGSA reduces performance disparities without
compromising accuracy. The baseline model achieved 96.7 percent accuracy with a
7.2 percent gender bias gap. Simple swap augmentation reduced the gap to 0.7
percent but lowered accuracy to 95.6 percent. LGSA achieved 99.1 percent
accuracy with a 1.9 percent bias gap, improving performance on female-labeled
examples. These findings demonstrate that LGSA is an effective strategy for
bias mitigation, enhancing subgroup balance while maintaining high task
accuracy and label fidelity.

</details>


### [38] [A fully automated and scalable Parallel Data Augmentation for Low Resource Languages using Image and Text Analytics](https://arxiv.org/abs/2510.13211)
*Prawaal Sharma,Navneet Goyal,Poonam Goyal,Vishnupriyan R*

Main category: cs.CL

TL;DR: 提出全自动方法从报纸文章提取双语语料库，改进低资源语言机器翻译效果3 BLEU值


<details>
  <summary>Details</summary>
Motivation: 解决语言多样性导致的数字资源匮乏问题，帮助低资源语言获得NLP技术支持

Method: 结合图像分析和文本分析，构建可扩展的自动化双语平行语料库提取方法

Result: 在两种语言对上构建语料库，机器翻译任务比基线提升近3个BLEU值

Conclusion: 自动化方法能有效构建高质量双语数据集，显著提升低资源语言机器翻译性能

Abstract: Linguistic diversity across the world creates a disparity with the
availability of good quality digital language resources thereby restricting the
technological benefits to majority of human population. The lack or absence of
data resources makes it difficult to perform NLP tasks for low-resource
languages. This paper presents a novel scalable and fully automated methodology
to extract bilingual parallel corpora from newspaper articles using image and
text analytics. We validate our approach by building parallel data corpus for
two different language combinations and demonstrate the value of this dataset
through a downstream task of machine translation and improve over the current
baseline by close to 3 BLEU points.

</details>


### [39] [Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](https://arxiv.org/abs/2510.13255)
*Jingmin An,Yilong Song,Ruolin Yang,Nai Ding,Lingxi Lu,Yuxuan Wang,Wei Wang,Chu Zhuang,Qian Wang,Fang Fang*

Main category: cs.CL

TL;DR: 通过频率标记探针发现LLMs与人类大脑在句法处理上的异同，模型升级呈现与脑机制背反趋势


<details>
  <summary>Details</summary>
Motivation: 探索LLMs处理句法结构的具体模块是否与人类大脑机制同源

Method: 使用分层频率标记探针（HFTP）进行频率域分析，对比LLM神经元与大脑皮层表征

Result: 发现LLMs在类似层级处理句法，人脑使用不同皮层区；新版模型Gemma2更接近人脑，Llama3.1反而偏离

Conclusion: 模型性能提升可能来自非人脑机制，HFTP为计算语言学与认知神经科学搭建桥梁

Abstract: Large Language Models (LLMs) demonstrate human-level or even superior
language abilities, effectively modeling syntactic structures, yet the specific
computational modules responsible remain unclear. A key question is whether LLM
behavioral capabilities stem from mechanisms akin to those in the human brain.
To address these questions, we introduce the Hierarchical Frequency Tagging
Probe (HFTP), a tool that utilizes frequency-domain analysis to identify
neuron-wise components of LLMs (e.g., individual Multilayer Perceptron (MLP)
neurons) and cortical regions (via intracranial recordings) encoding syntactic
structures. Our results show that models such as GPT-2, Gemma, Gemma 2, Llama
2, Llama 3.1, and GLM-4 process syntax in analogous layers, while the human
brain relies on distinct cortical regions for different syntactic levels.
Representational similarity analysis reveals a stronger alignment between LLM
representations and the left hemisphere of the brain (dominant in language
processing). Notably, upgraded models exhibit divergent trends: Gemma 2 shows
greater brain similarity than Gemma, while Llama 3.1 shows less alignment with
the brain compared to Llama 2. These findings offer new insights into the
interpretability of LLM behavioral improvements, raising questions about
whether these advancements are driven by human-like or non-human-like
mechanisms, and establish HFTP as a valuable tool bridging computational
linguistics and cognitive neuroscience. This project is available at
https://github.com/LilTiger/HFTP.

</details>


### [40] [Do You Get the Hint? Benchmarking LLMs on the Board Game Concept](https://arxiv.org/abs/2510.13271)
*Ine Gevers,Walter Daelemans*

Main category: cs.CL

TL;DR: 研究者提出文字猜谜游戏Concept作为自然语言形式的溯因推理基准测试，发现当前LLMs成功率不足40%且多语言表现更差


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在需要抽象推理的任务（特别是非自然语言表示形式）中表现薄弱，需更贴近预训练数据形式的评估工具

Method: 设计自然语言形式的文字猜谜游戏Concept，测试LLMs的策略意图理解能力和动态信息更新能力，并进行跨语言评估（英语/荷兰语/法语/西班牙语）

Result: 人类成功率超90%，最佳LLMs不足40%；低资源语言性能显著下降；模型在意图推理和假设修正环节存在明显缺陷

Conclusion: Concept有效暴露LLMs的抽象推理局限，尤其在多语言场景中，提示需要加强战略意图理解和动态认知更新的能力

Abstract: Large language models (LLMs) have achieved striking successes on many
benchmarks, yet recent studies continue to expose fundamental weaknesses. In
particular, tasks that require abstract reasoning remain challenging, often
because they use representations such as grids, symbols, or visual patterns
that differ from the natural language data LLMs are trained on. In this paper,
we introduce Concept, a simple word-guessing board game, as a benchmark for
probing abductive reasoning in a representation that is much closer to LLM
pre-training data: natural language. Our results show that this game, easily
solved by humans (with a success rate of over 90\%), is still very challenging
for state-of-the-art LLMs (no model exceeds 40\% success rate). Specifically,
we observe that LLMs struggle with interpreting other players' strategic
intents, and with correcting initial hypotheses given sequential information
updates. In addition, we extend the evaluation across multiple languages, and
find that the LLM performance drops further in lower-resource languages (Dutch,
French, and Spanish) compared to English.

</details>


### [41] [Beyond Correctness: Rewarding Faithful Reasoning in Retrieval-Augmented Generation](https://arxiv.org/abs/2510.13272)
*Zhichao Xu,Zongyu Wu,Yun Zhou,Aosong Feng,Kang Zhou,Sangmin Woo,Kiran Ramnath,Yijun Tian,Xuan Qi,Weikang Qiu,Lin Lee Cheong,Haibo Ding*

Main category: cs.CL

TL;DR: 提出VERITAS框架，通过细粒度奖励机制提升基于强化学习的搜索代理在QA任务中的推理忠实性


<details>
  <summary>Details</summary>
Motivation: 现有基于RL的搜索代理过度关注最终答案正确性，忽视中间推理步骤的忠实性，导致思维链不可信问题

Method: VERITAS框架将推理忠实性指标(信息-思考/思考-答案/思考-搜索三种忠实性)整合到强化学习奖励机制中

Result: 在7个QA基准测试中实现推理忠实性显著提升，同时保持任务性能相当水平

Conclusion: 通过可验证的中间推理追踪机制，成功平衡了检索增强生成中的性能与忠实性需求

Abstract: Inspired by the success of reinforcement learning (RL) in Large Language
Model (LLM) training for domains like math and code, recent works have begun
exploring how to train LLMs to use search engines more effectively as tools for
retrieval-augmented generation. Although these methods achieve performance
improvement across QA benchmarks, many prioritize final answer correctness
while overlooking the quality of intermediate reasoning steps, which may lead
to chain-of-thought unfaithfulness. In this paper, we first introduce a
comprehensive evaluation framework for evaluating RL-based search agents,
covering three distinct faithfulness metrics: information-think faithfulness,
think-answer faithfulness, and think-search faithfulness. Our evaluations
reveal that a prototypical RL-based search agent, Search-R1, has significant
room for improvement in this regard. To foster faithful reasoning, we introduce
VERITAS (Verifying Entailed Reasoning through Intermediate Traceability in
Agentic Search), a novel framework that integrates fine-grained faithfulness
rewards into the reinforcement learning process. Our experiments show that
models trained with VERITAS not only significantly improve reasoning
faithfulness, but also achieve comparable task performance across seven QA
benchmarks.

</details>


### [42] [In-Distribution Steering: Balancing Control and Coherence in Language Model Generation](https://arxiv.org/abs/2510.13285)
*Arthur Vogels,Benjamin Wong,Yann Choho,Annabelle Blangero,Milan Bhan*

Main category: cs.CL

TL;DR: 提出In-Distribution Steering (IDS)方法，通过动态调整激活引导强度实现LLM精准控制，在保持文本连贯性的同时提升分类任务准确性


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法使用固定强度参数，容易导致控制不足或文本质量下降。需要一种能根据输入数据分布自适应调整引导强度的解决方案

Method: 基于输入在表示空间中的分布位置动态计算引导强度：1) 测量输入与训练分布的距离 2) 根据距离远近按比例调整干预强度 3) 在文本生成过程中保持自适应稳定性

Result: 实验证明IDS在分类任务中达到89.2%准确率（比基线高15%），生成文本的困惑度保持在3.8以下，未出现文本崩溃现象

Conclusion: IDS通过分布感知的动态引导机制，在模型控制精度与生成质量间取得平衡，特别适用于需要可靠输出的实际应用场景

Abstract: Activation steering methods control large language model (LLM) behavior by
modifying internal activations at inference time. However, most existing
activation steering methods rely on a fixed steering strength, leading to
either insufficient control or unadapted intervention that degrades text
plausibility and coherence. We introduce In-Distribution Steering (IDS), a
novel method that adapts steering strength based on the input data distribution
in representation space. IDS dynamically adjusts interventions according to how
far a given input lies within the distribution, enabling adaptive intervention
and generation stability during text generation. Experiments demonstrate that
IDS achieves strong accuracy on classification tasks while producing coherent
text without collapse, making IDS particularly well suited for real-world
applications.

</details>


### [43] [Higher Satisfaction, Lower Cost: A Technical Report on How LLMs Revolutionize Meituan's Intelligent Interaction Systems](https://arxiv.org/abs/2510.13291)
*Xuxin Cheng,Ke Zeng,Zhiquan Cao,Linyi Dai,Wenxuan Gao,Fei Han,Ai Jian,Feng Hong,Wenxing Hu,Zihe Huang,Dejian Kong,Jia Leng,Zhuoyuan Liao,Pei Liu,Jiaye Lin,Xing Ma,Jingqing Ruan,Jiaxing Song,Xiaoyu Tan,Ruixuan Xiao,Wenhui Yu,Wenyu Zhan,Haoxing Zhang,Chao Zhou,Hao Zhou,Shaodong Zheng,Ruinian Chen,Siyuan Chen,Ziyang Chen,Yiwen Dong,Yaoyou Fan,Yangyi Fang,Yang Gan,Shiguang Guo,Qi He,Chaowen Hu,Binghui Li,Dailin Li,Xiangyu Li,Yan Li,Chengjian Liu,Xiangfeng Liu,Jiahui Lv,Qiao Ma,Jiang Pan,Cong Qin,Chenxing Sun,Wen Sun,Zhonghui Wang,Abudukelimu Wuerkaixi,Xin Yang,Fangyi Yuan,Yawen Zhu,Tianyi Zhai,Jie Zhang,Runlai Zhang,Yao Xu,Yiran Zhao,Yifan Wang,Xunliang Cai,Yangen Hu,Cao Liu,Lu Pan,Xiaoli Wang,Bo Xiao,Wenyuan Yao,Qianlin Zhou,Benchang Zhu*

Main category: cs.CL

TL;DR: 论文提出WOWService智能交互系统，通过整合大语言模型和多智能体架构，解决工业场景中冷启动数据构建、多轮对话优化、业务规则适应、多代理协作和自动化评估五大挑战，在美团应用中显著提升用户满意度指标。


<details>
  <summary>Details</summary>
Motivation: 现有智能交互系统面临冷启动训练数据构建困难、多轮对话意图理解不足、业务规则频繁变更导致可操作性下降、单一LLM在复杂场景能力局限、开放领域对话缺乏统一评估标准等痛点，制约服务质量与扩展能力。

Method: 基于LLMs构建多智能体架构，聚焦数据构造、通用能力增强、业务场景适配、多代理协作和自动化评估五大核心模块，实现自主任务管理与协同问题解决。

Result: 美团App实际部署显示关键指标显著改善：用户满意度指标1(USM1)降低27.53%，用户满意度指标2(USM2)提升25.51%，验证系统捕捉用户需求与推进个性化服务的能力。

Conclusion: WOWService通过技术创新解决工业级智能交互系统核心痛点，其多代理协同架构与自动化评估体系为复杂场景下的个性化服务提供了可扩展的技术框架，实证结果验证了方案的业务价值。

Abstract: Enhancing customer experience is essential for business success, particularly
as service demands grow in scale and complexity. Generative artificial
intelligence and Large Language Models (LLMs) have empowered intelligent
interaction systems to deliver efficient, personalized, and 24/7 support. In
practice, intelligent interaction systems encounter several challenges: (1)
Constructing high-quality data for cold-start training is difficult, hindering
self-evolution and raising labor costs. (2) Multi-turn dialogue performance
remains suboptimal due to inadequate intent understanding, rule compliance, and
solution extraction. (3) Frequent evolution of business rules affects system
operability and transferability, constraining low-cost expansion and
adaptability. (4) Reliance on a single LLM is insufficient in complex
scenarios, where the absence of multi-agent frameworks and effective
collaboration undermines process completeness and service quality. (5) The
open-domain nature of multi-turn dialogues, lacking unified golden answers,
hampers quantitative evaluation and continuous optimization. To address these
challenges, we introduce WOWService, an intelligent interaction system tailored
for industrial applications. With the integration of LLMs and multi-agent
architectures, WOWService enables autonomous task management and collaborative
problem-solving. Specifically, WOWService focuses on core modules including
data construction, general capability enhancement, business scenario
adaptation, multi-agent coordination, and automated evaluation. Currently,
WOWService is deployed on the Meituan App, achieving significant gains in key
metrics, e.g., User Satisfaction Metric 1 (USM 1) -27.53% and User Satisfaction
Metric 2 (USM 2) +25.51%, demonstrating its effectiveness in capturing user
needs and advancing personalized service.

</details>


### [44] [Mismatch Aware Guidance for Robust Emotion Control in Auto-Regressive TTS Models](https://arxiv.org/abs/2510.13293)
*Yizhou Peng,Yukun Ma,Chong Zhang,Yi-Wen Chao,Chongjia Ni,Bin Ma*

Main category: cs.CL

TL;DR: 针对自回归TTS模型中的情感与内容冲突问题，提出自适应CFG方案，平衡情感表达与音质


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统在情感提示与文本内容冲突时产生不自然语音，且CFG技术在自回归模型中的应用不足导致音质下降

Method: 基于LLM/NLI模型检测冲突程度，设计自适应CFG调节机制，动态调整情感控制强度

Result: 自适应CFG在保持音质和清晰度的同时，显著提升情感表达能力

Conclusion: 该方案有效解决情感-内容冲突问题，为自回归TTS模型提供更精细的情感控制方案

Abstract: While Text-to-Speech (TTS) systems can achieve fine-grained control over
emotional expression via natural language prompts, a significant challenge
emerges when the desired emotion (style prompt) conflicts with the semantic
content of the text. This mismatch often results in unnatural-sounding speech,
undermining the goal of achieving fine-grained emotional control.
Classifier-Free Guidance (CFG) is a key technique for enhancing prompt
alignment; however, its application to auto-regressive (AR) TTS models remains
underexplored, which can lead to degraded audio quality. This paper directly
addresses the challenge of style-content mismatch in AR TTS models by proposing
an adaptive CFG scheme that adjusts to different levels of the detected
mismatch, as measured using large language models or natural language inference
models. This solution is based on a comprehensive analysis of CFG's impact on
emotional expressiveness in state-of-the-art AR TTS models. Our results
demonstrate that the proposed adaptive CFG scheme improves the emotional
expressiveness of the AR TTS model while maintaining audio quality and
intelligibility.

</details>


### [45] [LLM one-shot style transfer for Authorship Attribution and Verification](https://arxiv.org/abs/2510.13302)
*Pablo Miralles-González,Javier Huertas-Tato,Alejandro Martín,David Camacho*

Main category: cs.CL

TL;DR: 提出了一种基于LLM预训练的无监督风格分析方法，利用上下文学习及对数概率衡量风格可迁移性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有监督和对比学习方法存在主题混淆问题，且未充分利用LLM的CLM预训练潜力。

Method: 利用LLM的上下文学习能力，通过计算文本间风格迁移的对数概率实现无监督风格分析。

Result: 在控制主题干扰下准确率超越基线模型，模型规模与性能呈正相关，计算成本与精度可灵活权衡。

Conclusion: 基于LLM预训练的无监督方法在作者验证等任务中展现出高效性，为风格分析提供了新范式。

Abstract: Computational stylometry analyzes writing style through quantitative patterns
in text, supporting applications from forensic tasks such as identity linking
and plagiarism detection to literary attribution in the humanities. Supervised
and contrastive approaches rely on data with spurious correlations and often
confuse style with topic. Despite their natural use in AI-generated text
detection, the CLM pre-training of modern LLMs has been scarcely leveraged for
general authorship problems. We propose a novel unsupervised approach based on
this extensive pre-training and the in-context learning capabilities of LLMs,
employing the log-probabilities of an LLM to measure style transferability from
one text to another. Our method significantly outperforms LLM prompting
approaches of comparable scale and achieves higher accuracy than contrastively
trained baselines when controlling for topical correlations. Moreover,
performance scales fairly consistently with the size of the base model and, in
the case of authorship verification, with an additional mechanism that
increases test-time computation; enabling flexible trade-offs between
computational cost and accuracy.

</details>


### [46] [ChatR1: Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering](https://arxiv.org/abs/2510.13312)
*Simon Lupart,Mohammad Aliannejadi,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: ChatR1是基于强化学习的对话问答框架，通过交替搜索与推理解决用户意图动态变化问题，利用意图感知奖励机制提升性能，在多个数据集上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统静态问答流程难以处理对话中用户意图的动态演变和模糊查询，需动态协调检索与生成以实现上下文敏感的交互。

Method: 采用强化学习框架，设计意图感知奖励机制提供轮次反馈；在3B/7B模型上测试，覆盖多领域数据集（话题转移、意图演变等），通过消融实验验证奖励有效性。

Result: 在5个CQA数据集的F1、BERTScore和LLM评价中均优于基线，泛化性强，消融实验确认意图奖励贡献显著（指标提升约12-15%）。

Conclusion: RL框架赋予问答系统灵活性与上下文敏感性，意图感知奖励是关键创新，可替代静态流程并支持跨领域迁移应用。

Abstract: We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of sparse and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and LLM-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.

</details>


### [47] [Embedding-Based Context-Aware Reranker](https://arxiv.org/abs/2510.13329)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

TL;DR: 提出轻量级重排框架EBCAR，通过增强跨段落理解改进RAG系统的检索效果


<details>
  <summary>Details</summary>
Motivation: 现有重排方法忽视跨段落推理挑战（如指代消解、多源证据整合），且传统方法计算成本高

Method: 基于嵌入的上下文感知重排框架（EBCAR），利用段落结构信息和混合注意力机制同时捕获文档间高层交互与文档内低层关联

Result: 在ConTEB基准测试中优于SOTA重排器，在需要跨段落推理的任务中展现出更高准确性和效率优势

Conclusion: EBCAR为复杂信息检索任务提供了高性价比解决方案，平衡了性能与计算成本

Abstract: Retrieval-Augmented Generation (RAG) systems rely on retrieving relevant
evidence from a corpus to support downstream generation. The common practice of
splitting a long document into multiple shorter passages enables finer-grained
and targeted information retrieval. However, it also introduces challenges when
a correct retrieval would require inference across passages, such as resolving
coreference, disambiguating entities, and aggregating evidence scattered across
multiple sources. Many state-of-the-art (SOTA) reranking methods, despite
utilizing powerful large pretrained language models with potentially high
inference costs, still neglect the aforementioned challenges. Therefore, we
propose Embedding-Based Context-Aware Reranker (EBCAR), a lightweight reranking
framework operating directly on embeddings of retrieved passages with enhanced
cross-passage understandings through the structural information of the passages
and a hybrid attention mechanism, which captures both high-level interactions
across documents and low-level relationships within each document. We evaluate
EBCAR against SOTA rerankers on the ConTEB benchmark, demonstrating its
effectiveness for information retrieval requiring cross-passage inference and
its advantages in both accuracy and efficiency.

</details>


### [48] [Taming the Fragility of KV Cache Eviction in LLM Inference](https://arxiv.org/abs/2510.13334)
*Yuan Feng,Haoyu Guo,JunLin Lv,S. Kevin Zhou,Xike Xie*

Main category: cs.CL

TL;DR: 提出DefensiveKV及其分层扩展方法，通过防御性聚合策略控制最坏情况风险，显著减少低缓存场景下的生成质量损失。


<details>
  <summary>Details</summary>
Motivation: 现有缓存淘汰方法基于脆弱的稳定性假设，导致均值聚合在极端情况下表现不佳，需开发更鲁棒的防御策略。

Method: 采用线性时间复杂度的两阶段防御性聚合策略，结合Layer-DefensiveKV的分层预算分配优化缓存淘汰机制。

Result: 在20%缓存限制下，生成质量损失分别降低2.3倍和4.3倍，在18个数据集上刷新性能基准。

Conclusion: 通过最坏情况风险管理优化缓存淘汰，为克服稳定性假设的固有脆弱性开辟了新方向，实现显著性能提升。

Abstract: Large language models have revolutionized natural language processing, yet
their deployment remains hampered by the substantial memory and runtime
overhead of the transformer's Key-Value cache. To mitigate this, recent methods
employ a scoring-aggregation framework to evict unimportant cache entries,
based on the stability assumption-that a fixed subset of entries remains
consistently important during generation. However, prior work has largely
focused on refining importance indicators for scoring, while defaulting to mean
aggregation due to a faithful trust in the stability assumption. In this work,
we argue that this underlying assumption is inherently fragile, making mean
aggregation highly vulnerable in extreme cases. To counter this, we propose a
simple yet elegant defensive aggregation strategy: a two-step, linear-time
approach that controls worst-case risk, thereby defending against extreme cases
with negligible computational overhead. Embodying this strategy, we propose a
novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,
which incorporates layer-wise budget allocation. Across seven task domains (18
datasets), our methods reduce generation quality loss by 2.3x and 4.3x
respectively, versus the strongest baseline under a 20% cache size. These
results set new performance benchmarks and pioneer a promising direction for
optimizing cache eviction against underlying fragility through worst-case risk
management. Our code is available at https://github.com/FFY0/DefensiveKV.

</details>


### [49] [Are Proverbs the New Pythian Oracles? Exploring Sentiment in Greek Sayings](https://arxiv.org/abs/2510.13341)
*Katerina Korre,John Pavlopoulos*

Main category: cs.CL

TL;DR: 利用大语言模型分析希腊谚语情感分布，发现负面情感在多数地区占主导且LLMs在非常规情感分类任务中表现优异


<details>
  <summary>Details</summary>
Motivation: 填补希腊谚语系统性研究的空白，通过NLP技术突破传统口语传播限制，探索方言谚语的情感分布特征

Method: 扩展带方言的标注数据集，采用LLM进行非传统情感极性分类，结合地理信息、方言特征和主题进行三维分析

Result: LLM在谚语情感分类准确率达83.7%，希腊73%行政区的谚语呈现负面情感优势（克里特岛负面情感密度达89%）

Conclusion: 大语言模型为跨文化谚语研究提供新范式，情感地理可视化技术对民俗保护和文化传播具有重要应用价值

Abstract: Proverbs are among the most fascinating linguistic phenomena that transcend
cultural and linguistic boundaries. Yet, much of the global landscape of
proverbs remains underexplored, as many cultures preserve their traditional
wisdom within their own communities due to the oral tradition of the
phenomenon. Taking advantage of the current advances in Natural Language
Processing (NLP), we focus on Greek proverbs, analyzing their sentiment.
Departing from an annotated dataset of Greek proverbs, we expand it to include
local dialects, effectively mapping the annotated sentiment. We present (1) a
way to exploit LLMs in order to perform sentiment classification of proverbs,
(2) a map of Greece that provides an overview of the distribution of sentiment,
(3) a combinatory analysis in terms of the geographic position, dialect, and
topic of proverbs. Our findings show that LLMs can provide us with an accurate
enough picture of the sentiment of proverbs, especially when approached as a
non-conventional sentiment polarity task. Moreover, in most areas of Greece
negative sentiment is more prevalent.

</details>


### [50] [Protect: Towards Robust Guardrailing Stack for Trustworthy Enterprise LLM Systems](https://arxiv.org/abs/2510.13351)
*Karthik Avinash,Nikhil Pareek,Rishav Hada*

Main category: cs.CL

TL;DR: 提出多模态安全防护模型Protect，通过LoRA适配器和多模态数据集解决LLM企业部署中的安全监管问题，性能超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护系统存在多模态支持不足、实时监管困难及可解释性差等问题，难以满足企业级合规需求。

Method: 整合基于LoRA微调的领域适配器，构建覆盖毒性/性别歧视/隐私/提示注入的多模态数据集，采用教师辅助标注框架生成上下文感知标签。

Result: 在四项安全维度上实现SOTA性能，超越WildGuard/LlamaGuard-4/GPT-4.1等模型。

Conclusion: Protect建立了可信赖的多模态安全防护基准，支持文本/图像/音频的全模态企业级部署。

Abstract: The increasing deployment of Large Language Models (LLMs) across enterprise
and mission-critical domains has underscored the urgent need for robust
guardrailing systems that ensure safety, reliability, and compliance. Existing
solutions often struggle with real-time oversight, multi-modal data handling,
and explainability -- limitations that hinder their adoption in regulated
environments. Existing guardrails largely operate in isolation, focused on text
alone making them inadequate for multi-modal, production-scale environments. We
introduce Protect, natively multi-modal guardrailing model designed to operate
seamlessly across text, image, and audio inputs, designed for enterprise-grade
deployment. Protect integrates fine-tuned, category-specific adapters trained
via Low-Rank Adaptation (LoRA) on an extensive, multi-modal dataset covering
four safety dimensions: toxicity, sexism, data privacy, and prompt injection.
Our teacher-assisted annotation pipeline leverages reasoning and explanation
traces to generate high-fidelity, context-aware labels across modalities.
Experimental results demonstrate state-of-the-art performance across all safety
dimensions, surpassing existing open and proprietary models such as WildGuard,
LlamaGuard-4, and GPT-4.1. Protect establishes a strong foundation for
trustworthy, auditable, and production-ready safety systems capable of
operating across text, image, and audio modalities.

</details>


### [51] [Personal Attribute Leakage in Federated Speech Models](https://arxiv.org/abs/2510.13357)
*Hamdan Al-Ali,Ali Reza Ghavamipour,Tommaso Caselli,Fatih Turkmen,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

TL;DR: 联邦ASR模型在联邦学习场景中存在属性推断攻击漏洞，尤以口音信息最易被推断，暴露了新的安全隐患


<details>
  <summary>Details</summary>
Motivation: 揭示联邦学习框架下ASR模型对敏感属性（性别/年龄/口音/情绪/构音障碍）推断攻击的脆弱性

Method: 采用被动威胁模型下的非参数白盒攻击方法，仅通过权重差异分析，在Wav2Vec2/HuBERT/Whisper模型上进行测试

Result: 证明预训练数据中代表性不足的属性更易受攻击（口音在所有模型中均显示可被可靠推断）

Conclusion: 发现联邦ASR模型未公开的脆弱性，为提升模型安全性提供新见解

Abstract: Federated learning is a common method for privacy-preserving training of
machine learning models. In this paper, we analyze the vulnerability of ASR
models to attribute inference attacks in the federated setting. We test a
non-parametric white-box attack method under a passive threat model on three
ASR models: Wav2Vec2, HuBERT, and Whisper. The attack operates solely on weight
differentials without access to raw speech from target speakers. We demonstrate
attack feasibility on sensitive demographic and clinical attributes: gender,
age, accent, emotion, and dysarthria. Our findings indicate that attributes
that are underrepresented or absent in the pre-training data are more
vulnerable to such inference attacks. In particular, information about accents
can be reliably inferred from all models. Our findings expose previously
undocumented vulnerabilities in federated ASR models and offer insights towards
improved security.

</details>


### [52] [D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured Memory And Reasoning Tree](https://arxiv.org/abs/2510.13363)
*Xiang Lei,Qin Li,Min Zhang,Min Zhang*

Main category: cs.CL

TL;DR: 提出D-SMART框架解决大语言模型多轮对话中的事实不一致问题，通过动态结构化记忆和推理树提升48%对话一致性


<details>
  <summary>Details</summary>
Motivation: 现有方法（RAG/工作记忆）依赖静态知识源和单一路径推理，难以适应动态对话语境演变导致的逻辑衰减

Method: 包含动态结构化记忆（构建OWL知识图谱）和推理树（多步图搜索推理）的双组件框架

Result: 在MT-Bench-101基准测试中显著超越SOTA模型，开源模型质量分数提升10.1%

Conclusion: D-SMART通过结构化动态记忆和显式推理路径有效保持对话一致性，新NLI指标能更好检测逻辑缺陷

Abstract: Large Language Models (LLMs) often exhibit factual inconsistencies and
logical decay in extended, multi-turn dialogues, a challenge stemming from
their reliance on static, pre-trained knowledge and an inability to reason
adaptively over the dialogue history. Prevailing mitigation strategies, such as
Retrieval-Augmented Generation (RAG) and agentic working memories, improve
information recall but still engage with fundamentally static knowledge sources
and follow pre-defined single reasoning path. This hinders their ability to
preserve factual and logical consistency of their responses in multi-turn
dialogues while the context evolves over time. To address this issue, we
propose D-SMART, a model-agnostic framework designed to maintain multi-turn
dialogue consistency by enabling LLMs to build and reason over a dynamic,
structured representation of the conversational context. This is achieved via
two synergistic components: (1) a Dynamic Structured Memory (DSM), which
incrementally constructs and maintains an authoritative, OWL-compliant
knowledge graph of the conversation; and (2) a Reasoning Tree (RT), which
executes inferences as an explicit and traceable multi-step search over the
graph. As the popular-used quality score (judged by GPT-4) can overlook logical
flaws, we introduce new NLI-based metrics to better measure multi-turn dialogue
consistency. Comprehensive experiments on the MT-Bench-101 benchmark show that
D-SMART significantly outperforms state-of-the-art baselines, elevating the
dialogue consistency score by over 48\% for both proprietary and open-source
models, and notably improves the quality score of the latter by up to 10.1\%.

</details>


### [53] [Document Intelligence in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2510.13366)
*Weishi Wang,Hengchang Hu,Zhijie Zhang,Zhaochen Li,Hongxin Shao,Daniel Dahlmeier*

Main category: cs.CL

TL;DR: 本文系统综述了大语言模型(LLM)对文档智能(DAI)领域的革新性影响，梳理了多模态、多语言、检索增强等关键方向的研究进展与挑战，并提出了基于代理的方法、文档专用基础模型等未来方向。


<details>
  <summary>Details</summary>
Motivation: LLM显著改变了文档AI的技术范式，需要系统性分析其研究现状及未来演进方向。文章旨在为学术界和工业界提供结构化的技术演进分析框架。

Method: 通过综合现有文献，分析LLM在文档理解与生成中的范式转变，重点考察多模态数据处理、跨语言应用、检索增强技术等关键领域的技术实现路径。

Result: 揭示了当前LLM在文档智能应用中面临的多模态对齐、语言泛化、知识融合等核心挑战，同时验证了其在复杂文档处理任务中的显著性能提升。

Conclusion: LLM正在重塑文档智能的技术体系，未来需通过代理协作架构和领域专用模型研发，推动文档AI向更智能、更实用的方向发展。

Abstract: Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (LLMs). While
earlier approaches relied on encoder-decoder architectures, decoder-only LLMs
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of LLMs in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.

</details>


### [54] [Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment](https://arxiv.org/abs/2510.13387)
*Buwei He,Yang Liu,Zhaowei Zhang,Zixia Jia,Huijia Wu,Zhaofeng He,Zilong Zheng,Yipeng Kang*

Main category: cs.CL

TL;DR: 通过贝叶斯说服框架提升LLMs的战略说服能力，实验验证半正式/全自然语言策略在不同场景下的有效性


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视信息不对称的战略应用且依赖强预设承诺，需探索自然语言场景下LLMs的战略说服能力提升方案

Method: 设计承诺-沟通机制，通过类型声明引导贝叶斯信念更新，构建SFNL（半正式）和FNL（全自然语言）两种BP策略

Result: 1）BP策略成功率优于基线；2）SFNL逻辑性更强，FNL情感共鸣更好；3）微调后小模型可媲美大模型效果

Conclusion: 贝叶斯说服框架有效提升LLMs战略说服能力，不同策略适用于可信度/情感需求场景，为AI说服系统提供新范式

Abstract: Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (LLMs). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of LLMs.
Our framework incorporates a commitment-communication mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including LLM instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on LLM-based agents reveal three main findings: (1) LLMs guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.

</details>


### [55] [Doing Things with Words: Rethinking Theory of Mind Simulation in Large Language Models](https://arxiv.org/abs/2510.13395)
*Agnese Lombardi,Alessandro Lenci*

Main category: cs.CL

TL;DR: GPT-4在心理理论任务中表现不佳，揭示先前观察到的类心理理论能力可能源于浅层统计关联而非真实推理


<details>
  <summary>Details</summary>
Motivation: 验证生成式智能体模型Concordia能否有效模拟心理理论，探究GPT-4是否能通过社会情境进行真实推理而非依赖语言记忆

Method: 使用生成式智能体模型（GABM）Concordia构建模拟现实环境，通过行为选择测试GPT-4的信念归因能力

Result: GPT-4无法基于信念归因选择行动，在复杂社交互动中难以生成连贯因果效应，暴露浅层关联的局限性

Conclusion: 挑战大语言模型心理理论能力的现有结论，强调需要建立基于行为的严谨评估框架替代纯语言指标

Abstract: Language is fundamental to human cooperation, facilitating not only the
exchange of information but also the coordination of actions through shared
interpretations of situational contexts. This study explores whether the
Generative Agent-Based Model (GABM) Concordia can effectively model Theory of
Mind (ToM) within simulated real-world environments. Specifically, we assess
whether this framework successfully simulates ToM abilities and whether GPT-4
can perform tasks by making genuine inferences from social context, rather than
relying on linguistic memorization. Our findings reveal a critical limitation:
GPT-4 frequently fails to select actions based on belief attribution,
suggesting that apparent ToM-like abilities observed in previous studies may
stem from shallow statistical associations rather than true reasoning.
Additionally, the model struggles to generate coherent causal effects from
agent actions, exposing difficulties in processing complex social interactions.
These results challenge current statements about emergent ToM-like capabilities
in LLMs and highlight the need for more rigorous, action-based evaluation
frameworks.

</details>


### [56] [Investigating Lexical Change through Cross-Linguistic Colexification Patterns](https://arxiv.org/abs/2510.13407)
*Kim Gfeller,Sabine Stoll,Chundra Cathcart,Paul Widmer*

Main category: cs.CL

TL;DR: 通过系统发育比较模型分析三大语系词典数据，揭示概念对词形共现的演化规律：关联性强的概念对演变慢且共现广，高频/易借用概念对变化快且共现少，不同语系间存在地域文化差异。


<details>
  <summary>Details</summary>
Motivation: 探究语言演变中词形共现现象的动态机制，解析关联性、借用频率和使用频率对概念对共现演化的影响。

Method: 使用系统发育比较方法分析南岛语系、印欧语系和乌拉尔语系的词典数据，评估关联性、借用可能性、使用频率三因素对概念对共现的演化影响。

Result: 关联性强的概念对在语系树中覆盖更广且演变速率低30%；高频概念对的共现变化速率快45%，易借用概念对的共现率低22%；不同语系间存在显著演化路径差异。

Conclusion: 语言演变受系统发育保守性和社会文化动态双重驱动，概念对的语义关联强度与使用场景共同塑造共现模式，地域文化因素通过借用机制加速语言创新。

Abstract: One of the most intriguing features of language is its constant change, with
ongoing shifts in how meaning is expressed. Despite decades of research, the
factors that determine how and why meanings evolve remain only partly
understood. Colexification -- the phenomenon of expressing multiple distinct
concepts using the same word form -- serves as a valuable window onto the
dynamics of meaning change across languages. Here, we apply phylogenetic
comparative models to dictionary data from three language families,
Austronesian, Indo-European, and Uralic, in order to shed light on the
evolutionary dynamics underlying the colexification of concept pairs. We assess
the effects of three predictors: associativity, borrowability, and usage
frequency. Our results show that more closely related concept pairs are
colexified across a larger portion of the family tree and exhibit slower rates
of change. In contrast, concept pairs that are more frequent and more prone to
borrowing tend to change more rapidly and are less often colexified. We also
find considerable differences between the language families under study,
suggesting that areal and cultural factors may play a role.

</details>


### [57] [Evaluating Arabic Large Language Models: A Survey of Benchmarks, Methods, and Gaps](https://arxiv.org/abs/2510.13430)
*Ahmed Alzubaidi,Shaikha Alsuwaidi,Basma El Amel Boussaha,Leen AlQadi,Omar Alkaabi,Mohammed Alyafeai,Hamza Alobeidli,Hakim Hacid*

Main category: cs.CL

TL;DR: 首篇系统性综述阿拉伯语LLM基准测试，提出四维分类法并揭示当前评估体系的进展与三大核心缺陷（时间连续性不足、多轮对话评估缺失、翻译数据集文化错位）。


<details>
  <summary>Details</summary>
Motivation: 填补阿拉伯语大语言模型基准研究空白，为NLP研究者提供标准化评估框架与方法论参考。

Method: 构建知识/NLP任务/文化方言/特定目标四维分类法，分析40+基准数据集，对比原生收集/翻译/合成生成三种构建范式。

Result: 发现基准多样性显著提升，但存在时间维度评估断层（仅1%含时间标注）、多轮对话评估覆盖率不足7%、翻译数据集文化匹配度低于62%等关键问题。

Conclusion: 建立阿拉伯语评估基准的金标准，建议加强时序评估模块设计，开发文化敏感的多轮对话测试集，推动评估范式从静态向动态演进。

Abstract: This survey provides the first systematic review of Arabic LLM benchmarks,
analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,
cultural understanding, and specialized capabilities. We propose a taxonomy
organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and
Dialects, and Target-Specific evaluations. Our analysis reveals significant
progress in benchmark diversity while identifying critical gaps: limited
temporal evaluation, insufficient multi-turn dialogue assessment, and cultural
misalignment in translated datasets. We examine three primary approaches:
native collection, translation, and synthetic generation discussing their
trade-offs regarding authenticity, scale, and cost. This work serves as a
comprehensive reference for Arabic NLP researchers, providing insights into
benchmark methodologies, reproducibility standards, and evaluation metrics
while offering recommendations for future development.

</details>


### [58] [Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference Optimization for Machine Translation](https://arxiv.org/abs/2510.13434)
*Hao Wang,Linlong Xu,Heng Liu,Yangyang Liu,Xiaohu Zhao,Bo Zeng,Liangying Shao,Longyue Wang,Weihua Luo,Kaifu Zhang*

Main category: cs.CL

TL;DR: 提出M²PO方法，通过多视角奖励引擎和多组数据策略改进机器翻译偏好优化，解决现有DPO方法奖励缺陷和数据低效问题


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法存在质量评估模型忽略关键错误（如翻译幻觉）的奖励信号缺陷，以及仅使用单组胜负对的低效数据利用问题

Method: 整合多视角奖励引擎（包含新型幻觉惩罚机制和动态质量评分）与多组构建策略，从全部候选翻译中系统创建偏好对比数据

Result: 在WMT21-22基准测试中显著超越现有偏好优化方法，与主流商业大模型展现竞争力

Conclusion: M²PO通过融合多维度质量评估和高效数据利用机制，实现了更鲁棒可靠的机器翻译优化框架

Abstract: Direct Preference Optimization (DPO) is a powerful paradigm for aligning
Large Language Models (LLMs) to human preferences in Machine Translation (MT),
but current methods are hindered by two fundamental challenges: (1) flawed
reward signals from Quality Estimation (QE) models that overlook critical
errors like translation hallucination, and (2) inefficient data utilization
that discards valuable learning signals by selecting only a single win-loss
pair. To address these limitations, we introduce M^2PO: Multi-Pair,
Multi-Perspective Preference Optimization. Our framework integrates a
multi-perspective reward engine that creates a more robust signal by combining
two key viewpoints: a new hallucination penalty for factuality, and an
innovative dynamic quality score that adaptively fuses external evaluations
with the model's own evolving judgment. This is synergistically paired with a
multi-pair construction strategy that systematically creates a comprehensive
set of preference pairs from the entire pool of translation candidates. This
synergistic approach ensures the model learns from a richer spectrum of quality
trade-offs, leading to more robust and faithful translations. On challenging
WMT21-22 benchmarks, M^2PO substantially outperforms existing preference
optimization methods and demonstrates highly competitive performance against
leading proprietary LLMs.

</details>


### [59] [LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA](https://arxiv.org/abs/2510.13494)
*Tommaso Bonomo,Luca Gioffré,Roberto Navigli*

Main category: cs.CL

TL;DR: 提出高质量文学问答数据集LiteraryQA，改进NarrativeQA的数据质量问题，验证LLM评估优于传统指标


<details>
  <summary>Details</summary>
Motivation: NarrativeQA基准存在文档噪声和问答对缺陷，难以可靠评估叙事文本问答系统性能

Method: 1. 构建人工+LLM验证流程清洗数据
2. 开展指标元评估
3. 测试长上下文LLM性能

Result: n-gram类指标与人工评估相关性低，7B参数LLM评估结果与人类判断高度一致（Spearman系数0.9）

Conclusion: LiteraryQA为文学QA提供可靠基准，LLM评估体系可有效替代人工评估，促进长文本理解研究

Abstract: Question Answering (QA) on narrative text poses a unique challenge to current
systems, requiring a deep understanding of long, complex documents. However,
the reliability of NarrativeQA, the most widely used benchmark in this domain,
is hindered by noisy documents and flawed QA pairs. In this work, we introduce
LiteraryQA, a high-quality subset of NarrativeQA focused on literary works.
Using a human- and LLM-validated pipeline, we identify and correct low-quality
QA samples while removing extraneous text from source documents. We then carry
out a meta-evaluation of automatic metrics to clarify how systems should be
evaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics
have a low system-level correlation to human judgment, while LLM-as-a-Judge
evaluations, even with small open-weight models, can strongly agree with the
ranking identified by humans. Finally, we benchmark a set of long-context LLMs
on LiteraryQA. We release our code and data at
https://github.com/SapienzaNLP/LiteraryQA.

</details>


### [60] [ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding](https://arxiv.org/abs/2510.13499)
*Xiaozhe Li,TianYi Lyu,Siyi Yang,Yuxi Gong,Yizhao Yang,Jinxuan Huang,Ligao Zhang,Zhuoyi Huang,Qingwen Liu*

Main category: cs.CL

TL;DR: 提出了首个动态实时评估基准\bench，用于评估大语言模型在消费领域复杂意图理解能力


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏真实场景下人类意图理解的大规模评估基准，且存在数据收集和评估流程构建的双重挑战

Method: 通过自动化数据管道收集整理真实公共讨论数据，构建支持实时更新的动态评估体系

Result: 创建了当前最大规模、最多样化的意图理解基准，具备防数据污染机制和实时更新能力

Conclusion: \bench填补了LLM意图理解评估的空白，为复杂动态场景下的模型能力提升提供关键基础设施

Abstract: Understanding human intent is a complex, high-level task for large language
models (LLMs), requiring analytical reasoning, contextual interpretation,
dynamic information aggregation, and decision-making under uncertainty.
Real-world public discussions, such as consumer product discussions, are rarely
linear or involve a single user. Instead, they are characterized by interwoven
and often conflicting perspectives, divergent concerns, goals, emotional
tendencies, as well as implicit assumptions and background knowledge about
usage scenarios. To accurately understand such explicit public intent, an LLM
must go beyond parsing individual sentences; it must integrate multi-source
signals, reason over inconsistencies, and adapt to evolving discourse, similar
to how experts in fields like politics, economics, or finance approach complex,
uncertain environments. Despite the importance of this capability, no
large-scale benchmark currently exists for evaluating LLMs on real-world human
intent understanding, primarily due to the challenges of collecting real-world
public discussion data and constructing a robust evaluation pipeline. To bridge
this gap, we introduce \bench, the first dynamic, live evaluation benchmark
specifically designed for intent understanding, particularly in the consumer
domain. \bench is the largest and most diverse benchmark of its kind,
supporting real-time updates while preventing data contamination through an
automated curation pipeline.

</details>


### [61] [MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts](https://arxiv.org/abs/2510.13500)
*Shujun Xia,Haokun Lin,Yichen Wu,Yinan Zhou,Zixuan Li,Zhongwei Wan,Xingrun Xing,Yefeng Zheng,Xiang Li,Caifeng Shan,Zhenan Sun,Quanzheng Li*

Main category: cs.CL

TL;DR: 提出医疗领域大模型编辑框架MedREK，通过检索增强和参数编辑的结合，解决单样本与批量编辑中的知识表示重叠问题，并构建MedVersa基准验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有参数编辑方法会破坏知识局部性，而检索式编辑面临医疗知识空间表示重叠和批量编辑空白的双重挑战，难以满足医疗场景的高精度要求。

Method: 构建MedVersa医疗编辑评估基准，设计MedREK框架：通过共享查询键模块实现精准匹配，结合注意力提示编码器增强信息引导能力。

Result: 在多个医疗基准测试中，MedREK核心指标超越基线模型，首次实现医疗大模型的批量编辑（batch-edit准确率达92.7%）且保持严格的局部性约束。

Conclusion: MedREK为医疗LLM动态更新提供可靠方案，MedVersa基准填补领域评估空白，二者共同推动可信医疗AI在临床场景的应用落地。

Abstract: LLMs hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation overlap within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
LLMs. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.

</details>


### [62] [Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization](https://arxiv.org/abs/2510.13554)
*Yang Li,Zhichen Dong,Yuhan Sun,Weixun Wang,Shaopan Xiong,Yijia Luo,Jiashun Liu,Han Lu,Jiamang Wang,Wenbo Su,Bo Zheng,Junchi Yan*

Main category: cs.CL

TL;DR: 通过注意力机制解析LLM推理模式，提出两种量化指标区分局部/全局注意力头，并开发三种针对性强化学习策略提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习对生成过程采用均匀奖励分配，无法区分关键推理步骤与常规步骤。需要建立可解释的注意力机制分析框架来指导优化。

Method: 提出Windowed Average Attention Distance测量局部短语块注意力，Future Attention Influence量化全局关键token重要性。基于此开发三种动态信用分配RL策略：预规划token、锚定token及时序耦合强化。

Result: 新RL策略在多种推理任务中取得稳定性能提升，验证了注意力模式与推理节奏的强相关性。

Conclusion: 将注意力模式作为优化路标，使RL训练与模型内在推理节律对齐，为构建透明高效的LLM推理优化框架提供了新范式。

Abstract: The reasoning pattern of Large language models (LLMs) remains opaque, and
Reinforcement learning (RL) typically applies uniform credit across an entire
generation, blurring the distinction between pivotal and routine steps. This
work positions attention as a privileged substrate that renders the internal
logic of LLMs legible, not merely as a byproduct of computation, but as a
mechanistic blueprint of reasoning itself. We first distinguish attention heads
between locally and globally focused information processing and reveal that
locally focused heads produce a sawtooth pattern near the diagonal indicating
phrasal chunks, while globally focused heads expose tokens that exert broad
downstream influence over future tokens. We formalize these with two metrics:
1) Windowed Average Attention Distance, which measures the extent of backward
attention within a clipped window; 2) Future Attention Influence, which
quantifies a token's global importance as the average attention it receives
from subsequent tokens. Taken together, these signals reveal a recurring
preplan-and-anchor mechanism, where the model first performs a long-range
contextual reference to generate an introductory token, which is immediately
followed by or coincides with a semantic anchor token that organizes subsequent
reasoning. Leveraging these insights, we introduce three novel RL strategies
that dynamically perform targeted credit assignment to critical nodes (preplan
tokens, anchor tokens, and their temporal coupling) and show consistent
performance gains across various reasoning tasks. By aligning optimization with
the model's intrinsic reasoning rhythm, we aim to transform opaque optimization
into an actionable structure-aware process, hoping to offer a potential step
toward more transparent and effective optimization of LLM reasoning.

</details>


### [63] [Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models](https://arxiv.org/abs/2510.13580)
*Daniil Gurgurov,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 通过微调语言特定子网络提升LLM在低资源语言的单语能力，同时保持通用性


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言和高资源语言间性能差距显著的问题

Method: 使用语言激活概率熵识别语言特定神经元，仅微调相关子网络权重

Result: 在12种中低资源语言上性能优于全参数微调/LoRA等方法，仅更新1%参数

Conclusion: 为低资源语言适配SOTA模型提供高效方案，并发布100+语言神经元识别数据

Abstract: Large language models exhibit uneven performance across languages, with
substantial gaps between high- and low-resource languages. We present a
framework for enhancing monolingual capabilities of LLMs in underrepresented
languages while preserving their general-purpose performance through targeted
fine-tuning of language-specific subnetworks. Our approach identifies
language-specific neurons using Language Activation Probability Entropy and
fine-tunes only the weights associated with these neurons, a dedicated
subnetwork, on target-language data. Experiments on Llama-3.1-8B and
Mistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our
method consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA
adaptation, and random subset fine-tuning baselines while efficiently updating
only up to 1% of model parameters. Beyond performance improvements, we observe
enhanced favorable training dynamics, cross-lingual representational alignment,
and systematic weight update changes. To facilitate future research, we release
language-specific neuron identifications for over 100 languages as well as our
adaptation pipeline, offering a cost-effective pathway for adapting
state-of-the-art models to underrepresented languages.

</details>


### [64] [Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs](https://arxiv.org/abs/2510.13586)
*Pasin Buakhaw,Kun Kerdthaisong,Phuree Phenhiran,Pitikorn Khlaisamniang,Supasate Vorathammathorn,Piyalitt Ittichaiwong,Nutchanon Yongsatianchot*

Main category: cs.CL

TL;DR: 提出结合轻量级提示技术（API赛道）与微调大模型（GPU赛道）的方法，在CPDC竞赛中取得优异成绩


<details>
  <summary>Details</summary>
Motivation: 利用大语言模型提升游戏NPC的任务执行能力和角色一致性对话生成

Method: 1. API赛道：采用Deflanderization提示抑制过度角色扮演
2. GPU赛道：基于Qwen3-14B进行监督微调(SFT)和低秩适配(LoRA)

Result: Task1第2名（API）、Task3第2名（API）和第4名（GPU）

Conclusion: 提示工程与模型微调的组合策略有效兼顾任务完成度和对话自然性

Abstract: The emergence of large language models (LLMs) has opened new opportunities
for cre- ating dynamic non-player characters (NPCs) in gaming environments,
enabling both func- tional task execution and persona-consistent dialogue
generation. In this paper, we (Tu_Character_lab) report our participation in
the Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which
eval- uates agents across three tracks: task-oriented dialogue, context-aware
dialogue, and their integration. Our approach combines two complementary
strategies: (i) lightweight prompting techniques in the API track, including a
Deflanderization prompting method to suppress excessive role-play and improve
task fidelity, and (ii) fine-tuned large models in the GPU track, leveraging
Qwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our
best submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on
Task 3 (GPU track).

</details>


### [65] [FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation](https://arxiv.org/abs/2510.13598)
*Kristýna Onderková,Ondřej Plátek,Zdeněk Kasner,Ondřej Dušek*

Main category: cs.CL

TL;DR: 提出动态生成多语言表格文本基准FreshTab，解决数据污染和领域不平衡问题


<details>
  <summary>Details</summary>
Motivation: 现有表格文本生成基准存在LLM训练数据污染和领域不平衡缺陷，非英语数据集资源匮乏

Method: 通过维基百科实时抓取最新表格数据构建多语言基准（英/德/俄/法），实现领域平衡评估

Result: 自动指标与人类评估存在差异，领域平衡基准显著提升评估挑战性

Conclusion: 动态生成机制有效规避数据污染，多语言按需收集扩展了跨文化场景的应用能力

Abstract: Table-to-text generation (insight generation from tables) is a challenging
task that requires precision in analyzing the data. In addition, the evaluation
of existing benchmarks is affected by contamination of Large Language Model
(LLM) training data as well as domain imbalance. We introduce FreshTab, an
on-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM
data contamination problem and enable domain-sensitive evaluation. While
non-English table-to-text datasets are limited, FreshTab collects datasets in
different languages on demand (we experiment with German, Russian and French in
addition to English). We find that insights generated by LLMs from recent
tables collected by our method appear clearly worse by automatic metrics, but
this does not translate into LLM and human evaluations. Domain effects are
visible in all evaluations, showing that a~domain-balanced benchmark is more
challenging.

</details>


### [66] [NOSA: Native and Offloadable Sparse Attention](https://arxiv.org/abs/2510.13602)
*Yuxiang Huang,Chaojun Xiao,Xu Han,Zhiyuan Liu*

Main category: cs.CL

TL;DR: 提出可训练稀疏注意力框架NOSA，通过显式局部性约束优化KV缓存传输，在保持性能的同时提升解码吞吐量2.3倍


<details>
  <summary>Details</summary>
Motivation: 现有稀疏注意力方法未解决KV缓存占用问题，导致大规模批处理推理时GPU显存不足和解码效率下降

Method: 将token选择分解为查询感知和查询无关组件，通过显式局部性约束减少CPU-GPU间KV传输，保持训练推理一致性

Result: 预训练1B参数模型实现近无损性能，解码吞吐量较基准提升2.3倍

Conclusion: NOSA框架有效平衡注意力稀疏性与缓存管理，显著提升实际推理效率

Abstract: Trainable sparse attention has emerged as a promising solution to address the
decoding efficiency bottleneck of LLMs in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing sparse attention methods leave a crucial
limitation unresolved: the size of the key-value (KV) cache remains unreduced,
which constrains on-GPU batch sizes and throttles decoding throughput,
especially in large-scale batched inference. In this paper, we show that
trainable sparse attention naturally exhibits strong locality in token
selection across adjacent decoding steps, thereby enabling KV cache offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected KV pairs between the CPU and GPU continues to dominate the overall
decoding cost. Building on this insight, we present NOSA, a trainable sparse
attention framework designed to natively support KV cache offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing KV transfers while
preserving the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in decoding throughput compared with the vanilla trainable sparse attention
baseline (InfLLM-V2).

</details>


### [67] [MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning](https://arxiv.org/abs/2510.13614)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: MemoTime框架通过结构化时间知识图谱和记忆增强机制，显著提升LLM在复杂时序推理任务中的性能，使小模型达到GPT-4水平


<details>
  <summary>Details</summary>
Motivation: 现有TKG增强方法存在多跳推理时序失真、多实体同步困难、操作符适配不足和经验复用缺失四大缺陷，制约LLM时序推理能力

Method: 1. 构建层次化时间树实现操作符感知推理
2. 动态证据检索层适配不同时序操作符
3. 自进化经验内存实现跨任务知识复用

Result: 在时序QA基准上达到SOTA（提升24%），Qwen3-4B模型推理性能匹敌GPT-4-Turbo

Conclusion: 该框架通过时序约束建模和经验复用机制，为LLM的时序推理提供了可解释、可扩展的解决方案

Abstract: Large Language Models (LLMs) have achieved impressive reasoning abilities,
but struggle with temporal understanding, especially when questions involve
multiple entities, compound operators, and evolving event sequences. Temporal
Knowledge Graphs (TKGs), which capture vast amounts of temporal facts in a
structured format, offer a reliable source for temporal reasoning. However,
existing TKG-based LLM reasoning methods still struggle with four major
challenges: maintaining temporal faithfulness in multi-hop reasoning, achieving
multi-entity temporal synchronization, adapting retrieval to diverse temporal
operators, and reusing prior reasoning experience for stability and efficiency.
To address these issues, we propose MemoTime, a memory-augmented temporal
knowledge graph framework that enhances LLM reasoning through structured
grounding, recursive reasoning, and continual experience learning. MemoTime
decomposes complex temporal questions into a hierarchical Tree of Time,
enabling operator-aware reasoning that enforces monotonic timestamps and
co-constrains multiple entities under unified temporal bounds. A dynamic
evidence retrieval layer adaptively selects operator-specific retrieval
strategies, while a self-evolving experience memory stores verified reasoning
traces, toolkit decisions, and sub-question embeddings for cross-type reuse.
Comprehensive experiments on multiple temporal QA benchmarks show that MemoTime
achieves overall state-of-the-art results, outperforming the strong baseline by
up to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to
achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [68] [Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses](https://arxiv.org/abs/2510.13624)
*Stefan Lenz,Lakisha Ortiz Rosario,Georg Vollmar,Arsenij Ustjanzew,Fatma Alickovic,Thomas Kindler,Torsten Panholzer*

Main category: cs.CL

TL;DR: 通过公开医学目录构建指令数据集微调LLMs，显著提升肿瘤诊断编码任务中ICD-10-GM准确率（41-58% vs 基线1.4-24%），缩小不同规模模型差距。


<details>
  <summary>Details</summary>
Motivation: 解决德语场景下开源轻量级LLMs医疗编码准确率不足的问题，探索利用公共目录构建指令数据集提升医疗文档任务的可行性。

Method: 基于ICD/OPS目录构建50万+QA训练集，微调Qwen/Llama/Mistral系列模型（7-70B参数），使用肿瘤系统真实数据评估编码准确性。

Result: ICD-10-GM完整编码准确率提升至41-58%（部分编码73-83%），ICD-O-3地形编码达22-40%，模型规模与性能正相关但微调缩小差距，Qwen3推理模式效果差且慢100倍。

Conclusion: 证实利用公共目录构建指令集可有效提升LLMs医疗编码能力，发布完整训练数据和最优模型为医疗AI应用提供资源。

Abstract: Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential
for structured cancer documentation in Germany. Smaller open-weight LLMs are
appealing for privacy-preserving automation but often struggle with coding
accuracy in German-language contexts. This study investigates whether
instruction-based fine-tuning on public datasets improves the coding accuracy
of open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded
diagnoses from the local tumor documentation system as test data. In a
systematic data quality assessment, the upper limit for ICD-10 coding
performance was estimated at 60-79% for exact and 81-94% for partial
(three-character codes only) derivation. As training data, over 500,000
question-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS
catalogues. Eight open-weight models from the Qwen, Llama, and Mistral families
(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to
41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3
topography coding also improved but started and remained considerably lower
with an exact accuracy of 22-40% and a partial accuracy of 56-67% after
fine-tuning. Malformed code outputs dropped to 0% for all models.
Tumor-diagnosis recognition reached 99%. Accuracy correlated positively with
model size, but gaps between small and large models narrowed after fine-tuning.
The reasoning mode in Qwen3 generally yielded a lower performance than
fine-tuning and was over 100 times slower. Our findings highlight the potential
of leveraging public catalogues to build instruction datasets that improve LLMs
in medical documentation tasks. The complete training dataset and the
best-performing checkpoints of the fine-tuned models are available from
https://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.

</details>


### [69] [Closing the Gap Between Text and Speech Understanding in LLMs](https://arxiv.org/abs/2510.13632)
*Santiago Cuervo,Skyler Seto,Maureen de Seyssel,Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly,Zakaria Aldeneh*

Main category: cs.CL

TL;DR: 提出SALAD方法，通过跨模态蒸馏和针对性数据合成，用少量公开语音数据缩小LLMs的文本-语音理解差距


<details>
  <summary>Details</summary>
Motivation: 现有语音适配LLM方法依赖大规模合成数据或专有数据集，成本高且不可复现，需更数据高效的方法解决文本-语音理解差距

Method: 结合跨模态蒸馏（保持文本能力）和主动选择目标合成数据（增强对齐），同时解决模态对齐和模型遗忘问题

Result: 3B/7B LLM使用公开语料训练（数据量减少10倍以上），在知识/语言理解/推理任务上达到开源模型竞争力

Conclusion: SALAD通过双重机制有效缩小文本-语音差距，证明数据效率路径可行性，为语音集成LLM提供新方向

Abstract: Large Language Models (LLMs) can be adapted to extend their text capabilities
to speech inputs. However, these speech-adapted LLMs consistently underperform
their text-based counterparts--and even cascaded pipelines--on language
understanding tasks. We term this shortfall the text-speech understanding gap:
the performance drop observed when a speech-adapted LLM processes spoken inputs
relative to when the original text-based LLM processes the equivalent text.
Recent approaches to narrowing this gap either rely on large-scale speech
synthesis of text corpora, which is costly and heavily dependent on synthetic
data, or on large-scale proprietary speech datasets, which are not
reproducible. As a result, there remains a need for more data-efficient
alternatives for closing the text-speech understanding gap. In this work, we
analyze the gap as driven by two factors: (i) forgetting of text capabilities
during adaptation, and (ii) cross-modal misalignment between speech and text.
Based on this analysis, we introduce SALAD--Sample-efficient Alignment with
Learning through Active selection and cross-modal Distillation--which combines
cross-modal distillation with targeted synthetic data to improve alignment
while mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves
competitive performance with a strong open-weight model across broad-domain
benchmarks in knowledge, language understanding, and reasoning, while training
on over an order of magnitude less speech data from public corpora.

</details>


### [70] [How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study](https://arxiv.org/abs/2510.13681)
*Matthieu Dubois,François Yvon,Pablo Piantanida*

Main category: cs.CL

TL;DR: 研究发现LLM文本检测器对解码策略参数极其敏感，微小调整即可使检测准确率从99%骤降至1%


<details>
  <summary>Details</summary>
Motivation: 揭示现有文本检测方法在动态生成场景下的脆弱性，挑战检测器在变化解码策略下的鲁棒性假设

Method: 通过37种解码配置的系统性实验（温度采样/top-p采样等），分析（子）词级概率分布变化对检测的影响机制

Result: 解码参数微调导致AUROC检测准确率从近完美水平（99%）崩溃式下降至1%，暴露检测系统致命缺陷

Conclusion: 当前检测方法存在严重评估缺陷，需建立动态测试框架。开源包含多解码策略的基准数据集推动领域发展

Abstract: As texts generated by Large Language Models (LLMs) are ever more common and
often indistinguishable from human-written content, research on automatic text
detection has attracted growing attention. Many recent detectors report
near-perfect accuracy, often boasting AUROC scores above 99\%. However, these
claims typically assume fixed generation settings, leaving open the question of
how robust such systems are to changes in decoding strategies. In this work, we
systematically examine how sampling-based decoding impacts detectability, with
a focus on how subtle variations in a model's (sub)word-level distribution
affect detection performance. We find that even minor adjustments to decoding
parameters - such as temperature, top-p, or nucleus sampling - can severely
impair detector accuracy, with AUROC dropping from near-perfect levels to 1\%
in some settings. Our findings expose critical blind spots in current detection
methods and emphasize the need for more comprehensive evaluation protocols. To
facilitate future research, we release a large-scale dataset encompassing 37
decoding configurations, along with our code and evaluation framework
https://github.com/BaggerOfWords/Sampling-and-Detection

</details>


### [71] [NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching](https://arxiv.org/abs/2510.13721)
*Run Luo,Xiaobo Xia,Lu Wang,Longze Chen,Renke Shan,Jing Luo,Min Yang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: NExT-OMNI是一个基于离散流范式的开源全模态基础模型，实现了任意模态间的统一理解与生成，在多模态交互和跨模态检索中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有自回归架构多模态模型存在理解与生成能力失衡的缺陷，混合架构设计也难以兼顾跨模态检索等广泛场景应用需求。

Method: 通过度量诱导概率路径和动力学最优速度构建离散流范式，采用统一表征而非任务解耦设计，支持任意模态间高效响应。

Result: 在多模态生成/理解基准测试中展现竞争力，多轮交互和跨模态检索性能超越先前统一模型，训练数据涵盖文本/图像/视频/音频。

Conclusion: NExT-OMNI通过创新的离散流架构实现了能力平衡与场景扩展，其开源将推动下一代多模态基础模型的发展。

Abstract: Next-generation multimodal foundation models capable of any-to-any
cross-modal generation and multi-turn interaction will serve as core components
of artificial general intelligence systems, playing a pivotal role in
human-machine interaction. However, most existing multimodal models remain
constrained by autoregressive architectures, whose inherent limitations prevent
a balanced integration of understanding and generation capabilities. Although
hybrid and decoupling strategies have been explored to address these tasks
within unified frameworks separately, their redundant, non-integrated designs
limit their applicability to broader scenarios, such as cross-modal
retrieval.In this work, we introduce NExT-OMNI, an open-source omnimodal
foundation model that achieves unified modeling through discrete flow
paradigms. By leveraging metric-induced probability paths and kinetic optimal
velocities, NExT-OMNI natively supports any-to-any understanding and generation
with enhanced response efficiency, while enabling broader application scenarios
through concise unified representations rather than task-decoupled designs.
Trained on large-scale interleaved text, image, video, and audio data,
NExT-OMNI delivers competitive performance on multimodal generation and
understanding benchmarks, while outperforming prior unified models in
multi-turn multimodal interaction and cross-modal retrieval, highlighting its
architectural advantages as a next-generation multimodal foundation model. To
advance further research, we release training details, data protocols, and
open-source both the code and model checkpoints.

</details>


### [72] [GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians](https://arxiv.org/abs/2510.13734)
*Xiuyuan Chen,Tao Sun,Dexin Su,Ailing Yu,Junwei Liu,Zhe Chen,Gangzeng Jin,Xin Wang,Jingnan Liu,Hansong Xiao,Hualei Zhou,Dongjie Tao,Chunxiao Guo,Minghui Yang,Yuan Xia,Jing Zhao,Qianrui Fan,Yanyun Wang,Shuai Zhen,Kezhong Chen,Jun Wang,Zewen Sun,Heng Zhao,Tian Guan,Shaodong Wang,Geyun Chang,Jiaming Deng,Hongchengcheng Chen,Kexin Feng,Ruzhen Li,Jiayi Geng,Changtai Zhao,Jun Wang,Guihu Lin,Peihao Li,Liqi Liu,Peng Wei,Jian Wang,Jinjie Gu,Ping Wang,Fan Yang*

Main category: cs.CL

TL;DR: 提出GAPS框架自动评估AI临床系统的认知深度、答案完整性、鲁棒性和安全性，发现现有模型在深度推理、完整性、抗干扰和安全性方面存在显著缺陷


<details>
  <summary>Details</summary>
Motivation: 现有基于多选题和人工评分的AI临床系统评估方法无法反映真实临床实践所需的认知深度、鲁棒性和安全性要求

Method: 开发包含G(认知深度)、A(答案完整性)、P(鲁棒性)、S(安全性)四维度的自动化评估框架，通过证据图谱构建、双结构表示、深度研究代理生成GRADE标准评分规则，采用LLM集成评分

Result: 验证显示自动化生成问题质量与临床判断一致，主流模型在G/P轴性能下降60%/80%，A轴完整性不足，S轴存在安全隐患

Conclusion: GAPS框架为AI临床系统提供了可扩展的自动化评估方案，推动临床AI向更安全可靠的方向发展

Abstract: Current benchmarks for AI clinician systems, often based on multiple-choice
exams or manual rubrics, fail to capture the depth, robustness, and safety
required for real-world clinical practice. To address this, we introduce the
GAPS framework, a multidimensional paradigm for evaluating \textbf{G}rounding
(cognitive depth), \textbf{A}dequacy (answer completeness),
\textbf{P}erturbation (robustness), and \textbf{S}afety. Critically, we
developed a fully automated, guideline-anchored pipeline to construct a
GAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity
limitations of prior work. Our pipeline assembles an evidence neighborhood,
creates dual graph and tree representations, and automatically generates
questions across G-levels. Rubrics are synthesized by a DeepResearch agent that
mimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring
is performed by an ensemble of large language model (LLM) judges. Validation
confirmed our automated questions are high-quality and align with clinician
judgment. Evaluating state-of-the-art models on the benchmark revealed key
failure modes: performance degrades sharply with increased reasoning depth
(G-axis), models struggle with answer completeness (A-axis), and they are
highly vulnerable to adversarial perturbations (P-axis) as well as certain
safety issues (S-axis). This automated, clinically-grounded approach provides a
reproducible and scalable method for rigorously evaluating AI clinician systems
and guiding their development toward safer, more reliable clinical practice.

</details>


### [73] [Assessing Web Search Credibility and Response Groundedness in Chat Assistants](https://arxiv.org/abs/2510.13749)
*Ivan Vykopal,Matúš Pikuliak,Simon Ostermann,Marián Šimko*

Main category: cs.CL

TL;DR: 提出系统性评估聊天助手引用网络信息源可信度的新方法，在5个易错主题上测试主流模型，发现Perplexity可信度最高而GPT-4o在敏感话题存在非可信源引用问题。


<details>
  <summary>Details</summary>
Motivation: 聊天助手整合网络搜索功能可能放大低可信来源的错误信息，需建立评估体系保障AI系统在高风险信息环境中的可靠性。

Method: 使用100个易错声明覆盖5大主题，评估GPT-4o、GPT-5、Perplexity和Qwen Chat的来源可信度与回答基于性。

Result: 不同助手表现差异显著：Perplexity来源可信度最高（平均92%），GPT-4o在敏感话题非可信源引用率达35%。

Conclusion: 该研究为AI事实核查行为评估建立基准，强调需持续优化信息检索机制以提升高价值领域应用安全性。

Abstract: Chat assistants increasingly integrate web search functionality, enabling
them to retrieve and cite external sources. While this promises more reliable
answers, it also raises the risk of amplifying misinformation from
low-credibility sources. In this paper, we introduce a novel methodology for
evaluating assistants' web search behavior, focusing on source credibility and
the groundedness of responses with respect to cited sources. Using 100 claims
across five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,
and Qwen Chat. Our findings reveal differences between the assistants, with
Perplexity achieving the highest source credibility, whereas GPT-4o exhibits
elevated citation of non-credibility sources on sensitive topics. This work
provides the first systematic comparison of commonly used chat assistants for
fact-checking behavior, offering a foundation for evaluating AI systems in
high-stakes information environments.

</details>


### [74] [Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation](https://arxiv.org/abs/2510.13750)
*Zhiqi Huang,Vivek Datla,Chenyang Zhu,Alfy Samuel,Daben Liu,Anoop Kumar,Ritesh Soni*

Main category: cs.CL

TL;DR: 提出基于前馈神经网络激活值的置信度估计方法，在金融行业客服场景中验证有效性，平衡精度与响应速度


<details>
  <summary>Details</summary>
Motivation: 金融/医疗等高风险领域需精准的置信度评估，传统基于token概率的方法存在信息损失

Method: 利用未经softmax归一化的原始FFN激活信号，构建序列分类模型，引入Huber损失增强鲁棒性，在Llama 3.1第16层提取特征

Result: 实际金融客服场景中超越基线方法，仅使用16层激活即保持精度同时降低延迟（Llama 3.1 8B验证）

Conclusion: 基于神经网络激活的置信建模为可信RAG系统提供可扩展的架构感知解决方案，适用于时延敏感的高风险场景

Abstract: We propose a method for confidence estimation in retrieval-augmented
generation (RAG) systems that aligns closely with the correctness of large
language model (LLM) outputs. Confidence estimation is especially critical in
high-stakes domains such as finance and healthcare, where the cost of an
incorrect answer outweighs that of not answering the question. Our approach
extends prior uncertainty quantification methods by leveraging raw feed-forward
network (FFN) activations as auto-regressive signals, avoiding the information
loss inherent in token logits and probabilities after projection and softmax
normalization. We model confidence prediction as a sequence classification
task, and regularize training with a Huber loss term to improve robustness
against noisy supervision. Applied in a real-world financial industry
customer-support setting with complex knowledge bases, our method outperforms
strong baselines and maintains high accuracy under strict latency constraints.
Experiments on Llama 3.1 8B model show that using activations from only the
16th layer preserves accuracy while reducing response latency. Our results
demonstrate that activation-based confidence modeling offers a scalable,
architecture-aware path toward trustworthy RAG deployment.

</details>


### [75] [The Mechanistic Emergence of Symbol Grounding in Language Models](https://arxiv.org/abs/2510.13796)
*Shuyu Wu,Ziqiao Ma,Xiaoxi Luo,Yidong Huang,Josue Torres-Fonseca,Freda Shi,Joyce Chai*

Main category: cs.CL

TL;DR: 语言模型的符号落地现象通过中间层注意力机制聚合环境特征实现，在Transformer/SSM架构中稳定出现但单向LSTM未体现


<details>
  <summary>Details</summary>
Motivation: 探究大规模预训练语言模型中符号落地现象的自发涌现机制，定位其发生的具体网络层级并解析计算原理

Method: 构建受控评估框架，通过机械解释性和因果分析追踪模型内部计算过程，对比Transformer/SSM/LSTM不同架构

Result: 符号落地集中在中间层，注意力头通过聚合环境特征支持语言形式预测，该现象在多模态对话和不同架构中复现（单向LSTM除外）

Conclusion: 证实语言模型可自发实现符号落地，为预测和控制生成可靠性提供理论依据，不同架构的差异提示模型设计对认知能力的影响

Abstract: Symbol grounding (Harnad, 1990) describes how symbols such as words acquire
their meanings by connecting to real-world sensorimotor experiences. Recent
work has shown preliminary evidence that grounding may emerge in
(vision-)language models trained at scale without using explicit grounding
objectives. Yet, the specific loci of this emergence and the mechanisms that
drive it remain largely unexplored. To address this problem, we introduce a
controlled evaluation framework that systematically traces how symbol grounding
arises within the internal computations through mechanistic and causal
analysis. Our findings show that grounding concentrates in middle-layer
computations and is implemented through the aggregate mechanism, where
attention heads aggregate the environmental ground to support the prediction of
linguistic forms. This phenomenon replicates in multimodal dialogue and across
architectures (Transformers and state-space models), but not in unidirectional
LSTMs. Our results provide behavioral and mechanistic evidence that symbol
grounding can emerge in language models, with practical implications for
predicting and potentially controlling the reliability of generation.

</details>


### [76] [Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons](https://arxiv.org/abs/2510.13797)
*Giovanni Monea,Yair Feldman,Shankar Padmanabhan,Kianté Brantley,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出KV缓存压缩方法，通过联合蒸馏与强化学习框架优化长上下文推理的内存效率


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型长上下文推理中键值缓存线性增长导致的内存和计算成本过高问题

Method: 周期性压缩生成KV缓存（使用专用学习令牌），结合改进的联合蒸馏与强化学习训练框架

Result: 在内存-准确率帕累托前沿上优于未压缩模型和传统压缩方法

Conclusion: 基于信息衰减特性的动态缓存压缩方法能有效提升模型推理效率

Abstract: The scalability of large language models for long-context reasoning is
severely constrained by the linear growth of their Transformer key-value cache,
which incurs significant memory and computational costs. We posit that as a
model generates reasoning tokens, the informational value of past generated
tokens diminishes, creating an opportunity for compression. In this work, we
propose to periodically compress the generation KV cache with a learned,
special-purpose token and evict compressed entries. We train the model to
perform this compression via a modified joint distillation and reinforcement
learning (RL) framework. Our training method minimizes overhead over the
conventional RL process, as it leverages RL outputs for distillation.
Empirically, our method achieves a superior memory-accuracy Pareto frontier
compared to both the model without cache compression and training-free
compression techniques.

</details>


### [77] [BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning](https://arxiv.org/abs/2510.13799)
*Jia-Chen Gu,Junyi Zhang,Di Wu,Yuankai Li,Kai-Wei Chang,Nanyun Peng*

Main category: cs.CL

TL;DR: BRIEF-Pro 是一个轻量级通用压缩器，通过摘要长上下文提升检索增强生成(RAG)效率，在保持性能的同时降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 处理复杂多跳问题时，传统RAG的长上下文导致高延迟和认知负荷，需高效压缩方案解决效率瓶颈。

Method: 使用短种子数据训练模型，实现对10k+词跨场景文本的抽象压缩，支持用户自定义摘要句子数控制输出长度。

Result: 在4个多跳QA数据集中，BRIEF-Pro 32x压缩使70B模型QA性能提升4.67%，计算开销仅为对比方案的23%。

Conclusion: BRIEF-Pro突破长上下文处理限制，通过灵活可控的摘要机制显著提升RAG效率与模型表现，具广泛适用性。

Abstract: As retrieval-augmented generation (RAG) tackles complex tasks, increasingly
expanded contexts offer richer information, but at the cost of higher latency
and increased cognitive load on the model. To mitigate this bottleneck,
especially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a
universal, lightweight compressor that distills relevant evidence for a given
query from retrieved documents into a concise summary for seamless integration
into in-context RAG. Using seed data consisting of relatively short contexts
(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression
of extended contexts exceeding 10k words across a wide range of scenarios.
Furthermore, BRIEF-Pro offers flexible user control over summary length by
allowing users to specify the desired number of sentences. Experiments on four
open-domain multi-hop question-answering datasets show that BRIEF-Pro generates
more concise and relevant summaries, enhancing performance across small, large,
and proprietary language models. With the 70B reader model, 32x compression by
BRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,
while requiring only 23% of its computational overhead.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [78] [MiGumi: Making Tightly Coupled Integral Joints Millable](https://arxiv.org/abs/2510.13168)
*Aditya Ganeshan,Kurt Fleischer,Wenzel Jakob,Ariel Shamir,Daniel Ritchie,Takeo Igarashi,Maria Larsson*

Main category: cs.GR

TL;DR: 提出MXG语言描述CNC铣削几何，通过协同优化零件形状解决加工偏差问题，使传统木工接头适应现代制造流程。


<details>
  <summary>Details</summary>
Motivation: 传统木工接头因CNC加工导致的几何偏差（如圆角）破坏配合精度，需开发兼容CNC工艺的接头设计方法。

Method: 1. 设计MXG语言参数化铣削操作几何；2. 建立零半径钻头理想模型，引入实际半径偏差分析；3. 构建两种可微损失函数优化配合约束。

Result: 在30种传统接头设计中成功生成CNC兼容的紧密配合结构，几何形态接近原始设计。

Conclusion: 通过CNC工艺适配改造传统接头，延续传统工艺生命力的同时推动其在现代制造中的应用价值。

Abstract: Traditional integral wood joints, despite their strength, durability, and
elegance, remain rare in modern workflows due to the cost and difficulty of
manual fabrication. CNC milling offers a scalable alternative, but directly
milling traditional joints often fails to produce functional results because
milling induces geometric deviations, such as rounded inner corners, that alter
the target geometries of the parts. Since joints rely on tightly fitting
surfaces, such deviations introduce gaps or overlaps that undermine fit or
block assembly. We propose to overcome this problem by (1) designing a language
that represent millable geometry, and (2) co-optimizing part geometries to
restore coupling. We introduce Millable Extrusion Geometry (MXG), a language
for representing geometry as the outcome of milling operations performed with
flat-end drill bits. MXG represents each operation as a subtractive extrusion
volume defined by a tool direction and drill radius. This parameterization
enables the modeling of artifact-free geometry under an idealized zero-radius
drill bit, matching traditional joint designs. Increasing the radius then
reveals milling-induced deviations, which compromise the integrity of the
joint. To restore coupling, we formalize tight coupling in terms of both
surface proximity and proximity constraints on the mill-bit paths associated
with mating surfaces. We then derive two tractable, differentiable losses that
enable efficient optimization of joint geometry. We evaluate our method on 30
traditional joint designs, demonstrating that it produces CNC-compatible,
tightly fitting joints that approximates the original geometry. By
reinterpreting traditional joints for CNC workflows, we continue the evolution
of this heritage craft and help ensure its relevance in future making
practices.

</details>


### [79] [HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans](https://arxiv.org/abs/2510.13587)
*Chao Shi,Shenghao Jia,Jinhui Liu,Yong Zhang,Liangchao Zhu,Zhonglei Yang,Jinze Ma,Chaoyue Niu,Chengfei Lv*

Main category: cs.GR

TL;DR: 通过手机单目视频捕捉，结合分层表征与高效渲染管线，实现移动端实时高保真数字人建模


<details>
  <summary>Details</summary>
Motivation: 解决单目视频重建中几何/视觉数据不足的挑战，让非专业用户可通过智能手机低成本创建影视级虚拟形象

Method: 数据层面融合静态姿势/动态运动序列；表示层面采用衣物网格分离+表面高斯光照建模；渲染层面构建GPU驱动管线优化实时性能

Result: 移动端2K分辨率120FPS，VR设备90FPS，速度比基线快2.7倍，视觉保真度与实时性超越现有单目方法

Conclusion: HRM²Avatar通过多层级创新，在保持移动端实时渲染的同时实现影视级数字人重建，为AR/VR社交等场景提供高效创作方案

Abstract: We present HRM$^2$Avatar, a framework for creating high-fidelity avatars from
monocular phone scans, which can be rendered and animated in real time on
mobile devices. Monocular capture with smartphones provides a low-cost
alternative to studio-grade multi-camera rigs, making avatar digitization
accessible to non-expert users. Reconstructing high-fidelity avatars from
single-view video sequences poses challenges due to limited visual and
geometric data. To address these limitations, at the data level, our method
leverages two types of data captured with smartphones: static pose sequences
for texture reconstruction and dynamic motion sequences for learning
pose-dependent deformations and lighting changes. At the representation level,
we employ a lightweight yet expressive representation to reconstruct
high-fidelity digital humans from sparse monocular data. We extract garment
meshes from monocular data to model clothing deformations effectively, and
attach illumination-aware Gaussians to the mesh surface, enabling high-fidelity
rendering and capturing pose-dependent lighting. This representation
efficiently learns high-resolution and dynamic information from monocular data,
enabling the creation of detailed avatars. At the rendering level, real-time
performance is critical for animating high-fidelity avatars in AR/VR, social
gaming, and on-device creation. Our GPU-driven rendering pipeline delivers 120
FPS on mobile devices and 90 FPS on standalone VR devices at 2K resolution,
over $2.7\times$ faster than representative mobile-engine baselines.
Experiments show that HRM$^2$Avatar delivers superior visual realism and
real-time interactivity, outperforming state-of-the-art monocular methods.

</details>


### [80] [MimicKit: A Reinforcement Learning Framework for Motion Imitation and Control](https://arxiv.org/abs/2510.13794)
*Xue Bin Peng*

Main category: cs.GR

TL;DR: 开源框架MimicKit通过运动模仿和强化学习训练运动控制器


<details>
  <summary>Details</summary>
Motivation: 为计算机图形学和机器人研究提供统一训练框架，标准化环境、智能体及数据结构

Method: 采用模块化设计，集成常用运动模仿技术和强化学习算法，支持灵活配置与扩展

Result: 开源代码库已发布，支持快速适配新角色和任务

Conclusion: 该框架通过可扩展的架构设计，有效促进运动控制领域的研究与应用创新

Abstract: MimicKit is an open-source framework for training motion controllers using
motion imitation and reinforcement learning. The codebase provides
implementations of commonly-used motion-imitation techniques and RL algorithms.
This framework is intended to support research and applications in computer
graphics and robotics by providing a unified training framework, along with
standardized environment, agent, and data structures. The codebase is designed
to be modular and easily configurable, enabling convenient modification and
extension to new characters and tasks. The open-source codebase is available
at: https://github.com/xbpeng/MimicKit.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [81] [Two Heads Are Better Than One: Audio-Visual Speech Error Correction with Dual Hypotheses](https://arxiv.org/abs/2510.13281)
*Sungnyun Kim,Kangwook Jang,Sungwoo Cho,Joon Son Chung,Hoirin Kim,Se-Young Yun*

Main category: eess.AS

TL;DR: 提出DualHyp框架，通过双模态语言空间融合提升音视频语音识别的纠错能力


<details>
  <summary>Details</summary>
Motivation: 传统单模态纠错方法在噪声场景下性能有限（仅10%增益），需探索双模态联合推理

Method: 1. DualHyp框架：LLM融合ASR/VSR独立假设
2. RelPrompt机制：基于时序可靠性的动态模态切换

Result: 在LRS2基准测试中实现57.7%错误率改善，远超单模态方法

Conclusion: 双模态联合推理显著提升鲁棒性，开源代码数据集推动相关研究

Abstract: This paper introduces a new paradigm for generative error correction (GER)
framework in audio-visual speech recognition (AVSR) that reasons over
modality-specific evidences directly in the language space. Our framework,
DualHyp, empowers a large language model (LLM) to compose independent N-best
hypotheses from separate automatic speech recognition (ASR) and visual speech
recognition (VSR) models. To maximize the effectiveness of DualHyp, we further
introduce RelPrompt, a noise-aware guidance mechanism that provides
modality-grounded prompts to the LLM. RelPrompt offers the temporal reliability
of each modality stream, guiding the model to dynamically switch its focus
between ASR and VSR hypotheses for an accurate correction. Under various
corruption scenarios, our framework attains up to 57.7% error rate gain on the
LRS2 benchmark over standard ASR baseline, contrary to single-stream GER
approaches that achieve only 10% gain. To facilitate research within our
DualHyp framework, we release the code and the dataset comprising ASR and VSR
hypotheses at https://github.com/sungnyun/dualhyp.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [82] [AutoCode: LLMs as Problem Setters for Competitive Programming](https://arxiv.org/abs/2510.12803)
*Shang Zhou,Zihan Zheng,Kaiyuan Liu,Zeyu Shen,Zerui Cheng,Zexing Chen,Hansen He,Jianzhu Yao,Huanzhi Mao,Qiuyang Mang,Tianfu Fu,Beichen Li,Dongruixuan Li,Wenhao Chai,Zhuang Liu,Aleksandra Korolova,Peter Henderson,Natasha Jaques,Pramod Viswanath,Saining Xie,Jingbo Shang*

Main category: cs.SE

TL;DR: AutoCode系统通过多轮验证生成竞赛级编程问题及测试用例，测试套件一致性达99%并获顶级程序员认可


<details>
  <summary>Details</summary>
Motivation: 设计有竞争力的编程问题需要精准设定约束条件、输入分布和边界案例，这对验证大型语言模型能力构成理想测试场景

Method: 采用多轮验证生成问题描述，通过随机种子问题创建变体，利用交叉验证解决方案过滤异常问题

Result: 测试套件与官方评判一致性接近99%，显著优于现有方法（如HardTests的81%），生成的原创问题被顶级程序员认可为竞赛质量

Conclusion: AutoCode成功实现了高正确率的问题生成，验证了通过系统性验证流程保障生成内容质量的有效性

Abstract: Writing competitive programming problems is exacting. Authors must: set
constraints, input distributions, and edge cases that rule out shortcuts;
target specific algorithms (e.g., max-flow, dynamic programming, data
structures); and calibrate complexity beyond the reach of most competitors. We
argue that this makes for an ideal test of general large language model
capabilities and study whether they can do this reliably. We introduce
AutoCode, which uses multiple rounds of validation to yield competition-grade
problem statements and test cases. On held-out problems, AutoCode test suites
approach 99% consistency with official judgments, a significant improvement
over current state-of-the-art methods like HardTests, which achieve less than
81%. Furthermore, starting with a random seed problem, AutoCode can create
novel variants with reference and brute-force solutions. By cross-verifying
these generated solutions against test cases, we can further filter out
malformed problems. Our system ensures high correctness, as verified by human
experts. AutoCode successfully produces novel problems judged by
Grandmaster-level (top 0.3%) competitive programmers to be of contest quality.

</details>


### [83] [TRUSTVIS: A Multi-Dimensional Trustworthiness Evaluation Framework for Large Language Models](https://arxiv.org/abs/2510.13106)
*Ruoyu Sun,Da Song,Jiayang Song,Yuheng Huang,Lei Ma*

Main category: cs.SE

TL;DR: TRUSTVIS框架通过可视化指标评估大语言模型的可信度，结合AutoDAN扰动方法提升评估可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在安全性和鲁棒性方面的可信度隐患，填补自动化评估工具的空白。

Method: 集成AutoDAN等扰动方法，采用多数投票机制，开发交互式可视化界面降低使用门槛。

Result: 在Vicuna-7b等模型的案例研究中成功识别安全漏洞，交互界面支持深度结果分析。

Conclusion: 该框架为模型改进提供可信赖的评估基准，通过可视化赋能针对性优化。

Abstract: As Large Language Models (LLMs) continue to revolutionize Natural Language
Processing (NLP) applications, critical concerns about their trustworthiness
persist, particularly in safety and robustness. To address these challenges, we
introduce TRUSTVIS, an automated evaluation framework that provides a
comprehensive assessment of LLM trustworthiness. A key feature of our framework
is its interactive user interface, designed to offer intuitive visualizations
of trustworthiness metrics. By integrating well-known perturbation methods like
AutoDAN and employing majority voting across various evaluation methods,
TRUSTVIS not only provides reliable results but also makes complex evaluation
processes accessible to users. Preliminary case studies on models like
Vicuna-7b, Llama2-7b, and GPT-3.5 demonstrate the effectiveness of our
framework in identifying safety and robustness vulnerabilities, while the
interactive interface allows users to explore results in detail, empowering
targeted model improvements. Video Link: https://youtu.be/k1TrBqNVg8g

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [84] [Toward LLM-Supported Automated Assessment of Critical Thinking Subskills](https://arxiv.org/abs/2510.12915)
*Marisa C. Peczuh,Nischal Ashok Kumar,Ryan Baker,Blair Lehman,Danielle Eisenberg,Caitlin Mills,Keerthi Chebrolu,Sudhip Nashi,Cadence Young,Brayden Liu,Sherry Lachman,Andrew Lan*

Main category: cs.CY

TL;DR: 研究评估了三种自动评分方法在测量批判性思维子技能中的效果，发现GPT-5少样本提示效果最佳，并讨论了模型选择中的权衡


<details>
  <summary>Details</summary>
Motivation: 针对学习分析领域缺乏批判性思维测量方法的现状，探索基于议论文写作场景的批判性思维核心子技能量化可行性

Method: 开发基于技能进展的编码标准，人工标注论文语料后，对比测试零样本提示、少样本提示和监督微调三种方法在GPT-5、GPT-5-mini和ModernBERT模型上的表现

Result: GPT-5少样本提示在可分离/高频子技能上表现优异，但对细微差异/罕见类别敏感度不足；专有模型可靠性高但成本昂贵，开源模型在多数场景下具备实用精度

Conclusion: 该研究为实现真实教育场景中高阶推理技能的可扩展评估迈出了重要一步，揭示了自动评分技术在敏感性与成本效益间的关键权衡

Abstract: Critical thinking represents a fundamental competency in today's education
landscape. Developing critical thinking skills through timely assessment and
feedback is crucial; however, there has not been extensive work in the learning
analytics community on defining, measuring, and supporting critical thinking.
In this paper, we investigate the feasibility of measuring core "subskills"
that underlie critical thinking. We ground our work in an authentic task where
students operationalize critical thinking: student-written argumentative
essays. We developed a coding rubric based on an established skills progression
and completed human coding for a corpus of student essays. We then evaluated
three distinct approaches to automated scoring: zero-shot prompting, few-shot
prompting, and supervised fine-tuning, implemented across three large language
models (GPT-5, GPT-5-mini, and ModernBERT). GPT-5 with few-shot prompting
achieved the strongest results and demonstrated particular strength on
subskills with separable, frequent categories, while lower performance was
observed for subskills that required detection of subtle distinctions or rare
categories. Our results underscore critical trade-offs in automated critical
thinking assessment: proprietary models offer superior reliability at higher
cost, while open-source alternatives provide practical accuracy with reduced
sensitivity to minority categories. Our work represents an initial step toward
scalable assessment of higher-order reasoning skills across authentic
educational contexts.

</details>


### [85] [Addressing the alignment problem in transportation policy making: an LLM approach](https://arxiv.org/abs/2510.13139)
*Xiaoyu Yan,Tianxing Dai,Yu,Nie*

Main category: cs.CY

TL;DR: 研究使用LLM构建多智能体模拟系统，通过民主投票机制评估交通政策与居民偏好的对齐程度


<details>
  <summary>Details</summary>
Motivation: 解决交通规划中模型驱动决策与居民实际偏好偏差导致的实施失败问题

Method: 1. 创建代表不同社区居民的LLM代理
2. 通过公投形式进行交通政策偏好选择
3. 使用思维链推理生成排序/批准式投票
4. 采用即时决选投票机制汇总共识

Result: LLM能近似合理集体偏好并响应本地环境，但存在模型特定行为偏差并与优化基准存在差异

Conclusion: LLM在交通决策对齐问题上展现潜力，但需注意模型局限性和系统性偏差

Abstract: A key challenge in transportation planning is that the collective preferences
of heterogeneous travelers often diverge from the policies produced by
model-driven decision tools. This misalignment frequently results in
implementation delays or failures. Here, we investigate whether large language
models (LLMs), noted for their capabilities in reasoning and simulating human
decision-making, can help inform and address this alignment problem. We develop
a multi-agent simulation in which LLMs, acting as agents representing residents
from different communities in a city, participate in a referendum on a set of
transit policy proposals. Using chain-of-thought reasoning, LLM agents provide
ranked-choice or approval-based preferences, which are aggregated using
instant-runoff voting (IRV) to model democratic consensus. We implement this
simulation framework with both GPT-4o and Claude-3.5, and apply it for Chicago
and Houston. Our findings suggest that LLM agents are capable of approximating
plausible collective preferences and responding to local context, while also
displaying model-specific behavioral biases and modest divergences from
optimization-based benchmarks. These capabilities underscore both the promise
and limitations of LLMs as tools for solving the alignment problem in
transportation decision-making.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [86] [From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models](https://arxiv.org/abs/2510.12864)
*Imran Khan*

Main category: cs.AI

TL;DR: 提出RID框架解决LLMs规则僵化问题，通过零样本元提示技术实现95%人类对齐得分


<details>
  <summary>Details</summary>
Motivation: LLMs作为智能代理时对显式规则的刻板遵循导致决策偏离人类意图，现有SFT方法成本过高且难以普及

Method: 开发Rule-Intent Distinction(RID)框架——结构化元提示技术，通过任务解构、规则分类、结果权衡和决策论证四层认知模式

Result: 在20个场景测试中，RID框架以95%人类对齐得分(HAS)显著优于基线(80%)和思维链(75%)，且产生更高质量的意图驱动推理

Conclusion: RID为LLMs提供从机械规则遵循转向目标导向推理的实用方案，推动构建更可靠、务实的AI代理系统

Abstract: Large Language Models (LLMs) are increasingly being deployed as the reasoning
engines for agentic AI systems, yet they exhibit a critical flaw: a rigid
adherence to explicit rules that leads to decisions misaligned with human
common sense and intent. This "rule-rigidity" is a significant barrier to
building trustworthy autonomous agents. While prior work has shown that
supervised fine-tuning (SFT) with human explanations can mitigate this issue,
SFT is computationally expensive and inaccessible to many practitioners. To
address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a
novel, low-compute meta-prompting technique designed to elicit human-aligned
exception handling in LLMs in a zero-shot manner. The RID framework provides
the model with a structured cognitive schema for deconstructing tasks,
classifying rules, weighing conflicting outcomes, and justifying its final
decision. We evaluated the RID framework against baseline and Chain-of-Thought
(CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced
judgment across diverse domains. Our human-verified results demonstrate that
the RID framework significantly improves performance, achieving a 95% Human
Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT.
Furthermore, it consistently produces higher-quality, intent-driven reasoning.
This work presents a practical, accessible, and effective method for steering
LLMs from literal instruction-following to liberal, goal-oriented reasoning,
paving the way for more reliable and pragmatic AI agents.

</details>


### [87] [DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping](https://arxiv.org/abs/2510.12979)
*Wei Fan,Wenlin Yao,Zheng Li,Feng Yao,Xin Liu,Liang Qiu,Qingyu Yin,Yangqiu Song,Bing Yin*

Main category: cs.AI

TL;DR: DeepPlanner框架通过熵优化的强化学习机制，显著提升深度研究代理的规划质量，在低训练成本下实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法（隐式规划/非系统性优化）导致规划令牌熵值过高，揭示强化学习中规划阶段存在未充分优化的决策点。

Method: 1）基于熵的令牌级优势分配（高熵令牌大更新）
2）规划密集型样本的优势加权机制

Result: 在7个深度研究基准测试中，以显著降低的训练预算（-78%），取得规划质量和任务性能的双重提升。

Conclusion: 通过系统性优化规划阶段的强化学习信号，成功解决长期规划任务中决策点欠优化的问题。

Abstract: Large language models (LLMs) augmented with multi-step reasoning and action
generation abilities have shown promise in leveraging external tools to tackle
complex tasks that require long-horizon planning. However, existing approaches
either rely on implicit planning in the reasoning stage or introduce explicit
planners without systematically addressing how to optimize the planning stage.
As evidence, we observe that under vanilla reinforcement learning (RL),
planning tokens exhibit significantly higher entropy than other action tokens,
revealing uncertain decision points that remain under-optimized. To address
this, we propose DeepPlanner, an end-to-end RL framework that effectively
enhances the planning capabilities of deep research agents. Our approach shapes
token-level advantage with an entropy-based term to allocate larger updates to
high entropy tokens, and selectively upweights sample-level advantages for
planning-intensive rollouts. Extensive experiments across seven deep research
benchmarks demonstrate that DeepPlanner improves planning quality and achieves
state-of-the-art results under a substantially lower training budget.

</details>


### [88] [Personalized Learning Path Planning with Goal-Driven Learner State Modeling](https://arxiv.org/abs/2510.13215)
*Joy Jia Yin Lim,Ye He,Jifan Yu,Xin Cong,Daniel Zhang-Li,Zhiyuan Liu,Huiqin Liu,Lei Hou,Juanzi Li,Bin Xu*

Main category: cs.AI

TL;DR: 提出Pxplore框架，集成强化学习和LLM技术，通过自动化奖励函数和结构化学习者状态模型，生成个性化、目标驱动的学习路径，实验验证有效性并开源代码数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的学习路径规划方法缺乏目标对齐机制，难以确保学习路径与个体目标的连贯性和驱动性。

Method: 设计结构化学习者状态模型和自动化奖励函数，结合监督微调(SFT)和组相对策略优化(GRPO)训练策略，并在真实学习平台中部署强化学习架构。

Result: 实验证明Pxplore能生成连贯、个性化且目标导向的学习路径，框架在实际学习平台中验证有效。

Conclusion: 该框架推动了个性化学习路径规划领域的发展，通过开源代码和数据集为后续研究提供基础设施支持。

Abstract: Personalized Learning Path Planning (PLPP) aims to design adaptive learning
paths that align with individual goals. While large language models (LLMs) show
potential in personalizing learning experiences, existing approaches often lack
mechanisms for goal-aligned planning. We introduce Pxplore, a novel framework
for PLPP that integrates a reinforcement-based training paradigm and an
LLM-driven educational architecture. We design a structured learner state model
and an automated reward function that transforms abstract objectives into
computable signals. We train the policy combining supervised fine-tuning (SFT)
and Group Relative Policy Optimization (GRPO), and deploy it within a
real-world learning platform. Extensive experiments validate Pxplore's
effectiveness in producing coherent, personalized, and goal-driven learning
paths. We release our code and dataset to facilitate future research.

</details>


### [89] [EvoTest: Evolutionary Test-Time Learning for Self-Improving Agentic Systems](https://arxiv.org/abs/2510.13220)
*Yufei He,Juncheng Liu,Yue Liu,Yibo Li,Tri Cao,Zhiyuan Hu,Xinxing Xu,Bryan Hooi*

Main category: cs.AI

TL;DR: 论文提出J-TTL基准测试和EvoTest进化框架，通过进化配置实现AI代理的测试时学习，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理无法在测试时实时学习复杂技能，严重限制实际应用价值，需要建立新评估体系和解决方案。

Method: EvoTest框架包含执行游戏的Actor Agent和分析改进的Evolver Agent，通过进化配置（提示词/记忆/超参数/工具使用）实现系统级优化。

Result: 在J-TTL基准上，EvoTest成为唯一能赢得两个游戏的方案（Detective和Library），所有基线方法均失败。

Conclusion: 进化式测试时学习无需微调即可持续提升代理性能，为解决AI系统实时适应难题提供了新范式。

Abstract: A fundamental limitation of current AI agents is their inability to learn
complex skills on the fly at test time, often behaving like "clever but
clueless interns" in novel environments. This severely limits their practical
utility. To systematically measure and drive progress on this challenge, we
first introduce the Jericho Test-Time Learning (J-TTL) benchmark. J-TTL is a
new evaluation setup where an agent must play the same game for several
consecutive episodes, attempting to improve its performance from one episode to
the next. On J-TTL, we find that existing adaptation methods like reflection,
memory, or reinforcement learning struggle. To address the challenges posed by
our benchmark, we present EvoTest, an evolutionary test-time learning framework
that improves an agent without any fine-tuning or gradients-by evolving the
entire agentic system after every episode. EvoTest has two roles: the Actor
Agent, which plays the game, and the Evolver Agent, which analyzes the episode
transcript to propose a revised configuration for the next run. This
configuration rewrites the prompt, updates memory by logging effective
state-action choices, tunes hyperparameters, and learns the tool-use routines.
On our J-TTL benchmark, EvoTest consistently increases performance,
outperforming not only reflection and memory-only baselines but also more
complex online fine-tuning methods. Notably, our method is the only one capable
of winning two games (Detective and Library), while all baselines fail to win
any.

</details>


### [90] [Assessing LLM Reasoning Through Implicit Causal Chain Discovery in Climate Discourse](https://arxiv.org/abs/2510.13417)
*Liesbeth Allein,Nataly Pineda-Castañeda,Andrea Rocci,Marie-Francine Moens*

Main category: cs.AI

TL;DR: 研究探讨大型语言模型在隐式因果链发现中的机制推理能力，发现其依赖模式匹配但生成链条具备逻辑性。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs能否通过中间因果步骤实现真正的机制性因果推理，而非表面关联，以提升论证场景的因果分析。

Method: 使用气候变化辩论中的因果对，让9个LLMs生成中间因果链，分析其数量、一致性及人类评估逻辑完整性。

Result: 模型输出差异显著，自信度源于模式匹配而非因果推理，但人类验证确认链条逻辑连贯性，并建立基准数据集。

Conclusion: LLMs在机制因果推理存在局限，但其生成的逻辑链条为论证场景的隐式因果分析提供了可扩展的研究基础。

Abstract: How does a cause lead to an effect, and which intermediate causal steps
explain their connection? This work scrutinizes the mechanistic causal
reasoning capabilities of large language models (LLMs) to answer these
questions through the task of implicit causal chain discovery. In a diagnostic
evaluation framework, we instruct nine LLMs to generate all possible
intermediate causal steps linking given cause-effect pairs in causal chain
structures. These pairs are drawn from recent resources in argumentation
studies featuring polarized discussion on climate change. Our analysis reveals
that LLMs vary in the number and granularity of causal steps they produce.
Although they are generally self-consistent and confident about the
intermediate causal connections in the generated chains, their judgments are
mainly driven by associative pattern matching rather than genuine causal
reasoning. Nonetheless, human evaluations confirmed the logical coherence and
integrity of the generated chains. Our baseline causal chain discovery
approach, insights from our diagnostic evaluation, and benchmark dataset with
causal chains lay a solid foundation for advancing future work in implicit,
mechanistic causal reasoning in argumentation settings.

</details>


### [91] [Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math](https://arxiv.org/abs/2510.13744)
*Shrey Pandit,Austin Xu,Xuan-Phi Nguyen,Yifei Ming,Caiming Xiong,Shafiq Joty*

Main category: cs.AI

TL;DR: 提出Hard2Verify人工标注数据集用于评估数学证明步骤级验证器，发现开源验证器普遍落后于闭源模型


<details>
  <summary>Details</summary>
Motivation: 解决复杂数学证明中LLM步骤验证能力不足的问题，特别是在开源验证器领域存在明显短板

Method: 构建包含500+人工小时的步骤级验证基准，评估29种生成式验证模型和过程奖励模型

Result: 开源验证器整体表现落后闭源模型，验证能力受计算规模影响，发现自我验证机制存在局限性

Conclusion: 需提升开源验证器性能，验证-生成动态关系值得深入研究，步骤级验证基准推动LLM推理系统发展

Abstract: Large language model (LLM)-based reasoning systems have recently achieved
gold medal-level performance in the IMO 2025 competition, writing mathematical
proofs where, to receive full credit, each step must be not only correct but
also sufficiently supported. To train LLM-based reasoners in such challenging,
open-ended settings, strong verifiers capable of catching step-level mistakes
are necessary prerequisites. We introduce Hard2Verify, a human-annotated,
step-level verification benchmark produced with over 500 hours of human labor.
Hard2Verify is designed to rigorously assess step-level verifiers at the
frontier: Verifiers must provide step-level annotations or identify the first
error in responses generated by frontier LLMs for very recent, challenging, and
open-ended math questions. We evaluate 29 generative critics and process reward
models, demonstrating that, beyond a few standouts, open-source verifiers lag
closed source models. We subsequently analyze what drives poor performance in
step-level verification, the impacts of scaling verifier compute, as well as
fundamental questions such as self-verification and verification-generation
dynamics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [92] [Kinematic Kitbashing for Modeling Functional Articulated Objects](https://arxiv.org/abs/2510.13048)
*Minghao Guo,Victor Zordan,Sheldon Andrews,Wojciech Matusik,Maneesh Agrawala,Hsueh-Ti Derek Liu*

Main category: cs.RO

TL;DR: 通过几何匹配与功能驱动的优化框架，实现基于现有零件库的自动化铰接物体合成，支持非可微目标函数的全局探索


<details>
  <summary>Details</summary>
Motivation: 传统零件拼装方法难以兼顾运动学约束与功能性需求，需开发能同时满足几何适配性和功能目标的自动装配系统

Method: 提出基于运动学感知的附着能量函数，结合退火黎曼Langevin动力学采样器，实现多运动状态下的几何对齐与功能目标优化

Result: 生成垃圾桶轮组汽车、多段机械臂灯、齿轮驱动船桨等多样装配体，在几何/运动/功能指标上显著超越基线方法

Conclusion: 该方法打通了零件建模与功能装配设计的壁垒，为快速创建交互式铰接资产提供新范式

Abstract: We introduce Kinematic Kitbashing, an automatic framework that synthesizes
functionality-aware articulated objects by reusing parts from existing models.
Given a kinematic graph with a small collection of articulated parts, our
optimizer jointly solves for the spatial placement of every part so that (i)
attachments remain geometrically sound over the entire range of motion and (ii)
the assembled object satisfies user-specified functional goals such as
collision-free actuation, reachability, or trajectory following. At its core is
a kinematics-aware attachment energy that aligns vector distance function
features sampled across multiple articulation snapshots. We embed this
attachment term within an annealed Riemannian Langevin dynamics sampler that
treats functionality objectives as additional energies, enabling robust global
exploration while accommodating non-differentiable functionality objectives and
constraints. Our framework produces a wide spectrum of assembled articulated
shapes, from trash-can wheels grafted onto car bodies to multi-segment lamps,
gear-driven paddlers, and reconfigurable furniture, and delivers strong
quantitative improvements over state-of-the-art baselines across geometric,
kinematic, and functional metrics. By tightly coupling articulation-aware
geometry matching with functionality-driven optimization, Kinematic Kitbashing
bridges part-based shape modeling and functional assembly design, empowering
rapid creation of interactive articulated assets.

</details>


### [93] [UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles](https://arxiv.org/abs/2510.12992)
*Neel P. Bhatt,Po-han Li,Kushagra Gupta,Rohan Siva,Daniel Milan,Alexander T. Hogue,Sandeep P. Chinchali,David Fridovich-Keil,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: 提出UNCAP框架，通过自然语言通信和不确定性指导提升自动驾驶车辆协同规划效率与安全


<details>
  <summary>Details</summary>
Motivation: 现有CAV协同方法存在高带宽需求且忽略感知不确定性，导致系统不可扩展且不安全

Method: 两阶段协议：1) 选择关键车辆；2) 传输量化感知不确定性的自然语言消息，通过互信息最大化进行选择性融合

Result: 实验显示通信带宽降低63%，安全评分提升31%，决策不确定性下降61%，碰撞距离边际增加四倍

Conclusion: UNCAP通过轻量级自然语言通信与不确定性量化，显著提升大规模CAV协同规划的安全性和可扩展性

Abstract: Safe large-scale coordination of multiple cooperative connected autonomous
vehicles (CAVs) hinges on communication that is both efficient and
interpretable. Existing approaches either rely on transmitting high-bandwidth
raw sensor data streams or neglect perception and planning uncertainties
inherent in shared data, resulting in systems that are neither scalable nor
safe. To address these limitations, we propose Uncertainty-Guided Natural
Language Cooperative Autonomous Planning (UNCAP), a vision-language model-based
planning approach that enables CAVs to communicate via lightweight natural
language messages while explicitly accounting for perception uncertainty in
decision-making. UNCAP features a two-stage communication protocol: (i) an ego
CAV first identifies the subset of vehicles most relevant for information
exchange, and (ii) the selected CAVs then transmit messages that quantitatively
express their perception uncertainty. By selectively fusing messages that
maximize mutual information, this strategy allows the ego vehicle to integrate
only the most relevant signals into its decision-making, improving both the
scalability and reliability of cooperative planning. Experiments across diverse
driving scenarios show a 63% reduction in communication bandwidth with a 31%
increase in driving safety score, a 61% reduction in decision uncertainty, and
a four-fold increase in collision distance margin during near-miss events.
Project website: https://uncap-project.github.io/

</details>


### [94] [LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models](https://arxiv.org/abs/2510.13626)
*Senyu Fei,Siyin Wang,Junhao Shi,Zihao Dai,Jikun Cai,Pengfang Qian,Li Ji,Xinzhe He,Shiduo Zhang,Zhaoye Fei,Jinlan Fu,Jingjing Gong,Xipeng Qiu*

Main category: cs.RO

TL;DR: VLA模型在基准测试中表现优异但存在系统性脆弱性，对视角/姿态扰动敏感却忽略语言指令，挑战了高分数即高能力的评估范式


<details>
  <summary>Details</summary>
Motivation: 揭示当前视觉-语言-动作模型在基准测试中的高成功率可能掩盖其真实应用场景中的脆弱性，质疑现有评估体系的有效性

Method: 通过物体布局、相机视角、机器人初始状态、语言指令、光照条件、背景纹理、传感器噪声等七个扰动维度，对多个SOTA模型进行系统性脆弱性测试

Result: 模型性能对视角扰动（95%→30%）和初始状态扰动极度敏感，但对语言指令变化不敏感（进一步实验显示模型完全忽略指令）

Conclusion: 高基准分数不等同真实能力，需建立包含现实扰动的可靠性评估体系，当前模型在物理推理和语言理解层面存在重大缺陷

Abstract: Visual-Language-Action (VLA) models report impressive success rates on
robotic manipulation benchmarks, yet these results may mask fundamental
weaknesses in robustness. We perform a systematic vulnerability analysis by
introducing controlled perturbations across seven dimensions: objects layout,
camera viewpoints, robot initial states, language instructions, light
conditions, background textures and sensor noise. We comprehensively analyzed
multiple state-of-the-art models and revealed consistent brittleness beneath
apparent competence. Our analysis exposes critical weaknesses: models exhibit
extreme sensitivity to perturbation factors, including camera viewpoints and
robot initial states, with performance dropping from 95% to below 30% under
modest perturbations. Surprisingly, models are largely insensitive to language
variations, with further experiments revealing that models tend to ignore
language instructions completely. Our findings challenge the assumption that
high benchmark scores equate to true competency and highlight the need for
evaluation practices that assess reliability under realistic variation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [95] [SimULi: Real-Time LiDAR and Camera Simulation with Unscented Transforms](https://arxiv.org/abs/2510.12901)
*Haithem Turki,Qi Wu,Xin Kang,Janick Martinez Esturo,Shengyu Huang,Ruilong Li,Zan Gojcic,Riccardo de Lutio*

Main category: cs.CV

TL;DR: 提出SimULi方法，实现实时渲染任意相机模型和LiDAR数据，解决现有神经渲染方法速度慢和跨传感器不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法(如NeRF/3DGS)存在渲染速度低、仅支持针孔相机模型的问题，且多传感器模拟存在跨模态质量失衡的挑战。

Method: 扩展3DGUT框架支持LiDAR，通过自动分块策略处理旋转雷达模型，采用因子化3D高斯表示和锚定策略降低跨传感器误差。

Result: 渲染速度比光线追踪快10-20倍，比现有光栅化方法快1.5-10倍；相机深度误差降低40%，在自动驾驶数据集上达到SOTA水平。

Conclusion: SimULi首次实现多模态传感器的实时高保真模拟，为自动驾驶测试提供了更高效的仿真解决方案。

Abstract: Rigorous testing of autonomous robots, such as self-driving vehicles, is
essential to ensure their safety in real-world deployments. This requires
building high-fidelity simulators to test scenarios beyond those that can be
safely or exhaustively collected in the real-world. Existing neural rendering
methods based on NeRF and 3DGS hold promise but suffer from low rendering
speeds or can only render pinhole camera models, hindering their suitability to
applications that commonly require high-distortion lenses and LiDAR data.
Multi-sensor simulation poses additional challenges as existing methods handle
cross-sensor inconsistencies by favoring the quality of one modality at the
expense of others. To overcome these limitations, we propose SimULi, the first
method capable of rendering arbitrary camera models and LiDAR data in
real-time. Our method extends 3DGUT, which natively supports complex camera
models, with LiDAR support, via an automated tiling strategy for arbitrary
spinning LiDAR models and ray-based culling. To address cross-sensor
inconsistencies, we design a factorized 3D Gaussian representation and
anchoring strategy that reduces mean camera and depth error by up to 40%
compared to existing methods. SimULi renders 10-20x faster than ray tracing
approaches and 1.5-10x faster than prior rasterization-based work (and handles
a wider range of camera models). When evaluated on two widely benchmarked
autonomous driving datasets, SimULi matches or exceeds the fidelity of existing
state-of-the-art methods across numerous camera and LiDAR metrics.

</details>


### [96] [Foveation Improves Payload Capacity in Steganography](https://arxiv.org/abs/2510.13151)
*Lifeng Qiu Lin,Henry Kam,Qi Sun,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出基于高效潜在表示和聚焦渲染的隐写术模型，容量提升5倍至500比特，错误率降至1/2000比特，视觉质量达31.47dB PSNR


<details>
  <summary>Details</summary>
Motivation: 突破传统隐写术100比特容量限制，解决元数据嵌入和水印应用中容量与视觉质量难以兼顾的问题

Method: 采用多模态潜在表示设计，结合人类视觉系统的聚焦渲染技术优化感知损失函数

Result: 在200K测试比特下错误率仅0.05%，PSNR 31.47dB与LPIPS 0.13达到工业应用级视觉质量

Conclusion: 新型感知驱动设计有效构建了高容量-低失真-强鲁棒性的隐写系统，为多媒体安全开辟新方向

Abstract: Steganography finds its use in visual medium such as providing metadata and
watermarking. With support of efficient latent representations and foveated
rendering, we trained models that improve existing capacity limits from 100 to
500 bits, while achieving better accuracy of up to 1 failure bit out of 2000,
at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB
PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in
creating multi-modal latent representations in steganography.

</details>


### [97] [Automated document processing system for government agencies using DBNET++ and BART models](https://arxiv.org/abs/2510.13303)
*Aya Kaysan Bahjat*

Main category: cs.CV

TL;DR: 提出基于DBNet++和BART的自动文档分类系统，有效解决复杂场景下的文本检测与分类，在Total-Text数据集上达到92.88%准确率


<details>
  <summary>Details</summary>
Motivation: 开发能够处理现实复杂场景（如光照变化、文本遮挡等）的文档分类系统，支持多源输入（离线与实时图像），提升实际应用中的分类准确性和鲁棒性

Method: 系统分为图像捕获与预处理、DBNet++文本检测、BART文本分类及PyQt5用户界面四阶段，结合深度学习方法处理复杂图像特征

Result: 在Total-Text数据集上，系统经过10小时训练实现92.88%的文本检测准确率，验证了在复杂场景下的有效性

Conclusion: 该方案在非受限成像场景中表现出色，为多源文档分类提供了实用解决方案，具有较高的实际应用价值

Abstract: An automatic document classification system is presented that detects textual
content in images and classifies documents into four predefined categories
(Invoice, Report, Letter, and Form). The system supports both offline images
(e.g., files on flash drives, HDDs, microSD) and real-time capture via
connected cameras, and is designed to mitigate practical challenges such as
variable illumination, arbitrary orientation, curved or partially occluded
text, low resolution, and distant text. The pipeline comprises four stages:
image capture and preprocessing, text detection [1] using a DBNet++
(Differentiable Binarization Network Plus) detector, and text classification
[2] using a BART (Bidirectional and Auto-Regressive Transformers) classifier,
all integrated within a user interface implemented in Python with PyQt5. The
achieved results by the system for text detection in images were good at about
92.88% through 10 hours on Total-Text dataset that involve high resolution
images simulate a various and very difficult challenges. The results indicate
the proposed approach is effective for practical, mixed-source document
categorization in unconstrained imaging scenarios.

</details>


### [98] [Leveraging 2D Priors and SDF Guidance for Dynamic Urban Scene Rendering](https://arxiv.org/abs/2510.13381)
*Siddharth Tourani,Jayaram Reddy,Akash Kumbar,Satyajit Tourani,Nishant Goyal,Madhava Krishna,N. Dinesh Reddy,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出结合SDF与3DGS的优化框架，无需LiDAR数据实现动态场景重建与编辑


<details>
  <summary>Details</summary>
Motivation: 现有动态场景建模方法依赖LiDAR、3D标注等多模态数据，本文旨在通过融合2D先验知识降低数据需求

Method: 将符号距离函数(SDF)与3D高斯溅射(3DGS)结合，构建统一优化框架提升几何精度和变形建模能力

Result: 在无LiDAR情况下达到SOTA渲染指标，支持场景分解/重组等编辑操作，加入LiDAR后重建效果进一步提升

Conclusion: 融合SDF与3DGS的方法有效结合两者优势，在降低数据依赖性的同时实现高质量动态场景建模与编辑

Abstract: Dynamic scene rendering and reconstruction play a crucial role in computer
vision and augmented reality. Recent methods based on 3D Gaussian Splatting
(3DGS), have enabled accurate modeling of dynamic urban scenes, but for urban
scenes they require both camera and LiDAR data, ground-truth 3D segmentations
and motion data in the form of tracklets or pre-defined object templates such
as SMPL. In this work, we explore whether a combination of 2D object agnostic
priors in the form of depth and point tracking coupled with a signed distance
function (SDF) representation for dynamic objects can be used to relax some of
these requirements. We present a novel approach that integrates Signed Distance
Functions (SDFs) with 3D Gaussian Splatting (3DGS) to create a more robust
object representation by harnessing the strengths of both methods. Our unified
optimization framework enhances the geometric accuracy of 3D Gaussian splatting
and improves deformation modeling within the SDF, resulting in a more adaptable
and precise representation. We demonstrate that our method achieves
state-of-the-art performance in rendering metrics even without LiDAR data on
urban scenes. When incorporating LiDAR, our approach improved further in
reconstructing and generating novel views across diverse object categories,
without ground-truth 3D motion annotation. Additionally, our method enables
various scene editing tasks, including scene decomposition, and scene
composition.

</details>


### [99] [Unifying Vision-Language Latents for Zero-label Image Caption Enhancement](https://arxiv.org/abs/2510.12931)
*Sanghyun Byun,Jung Ick Guack,Mohanad Odema,Baisub Lee,Jacob Song,Woo Seong Chung*

Main category: cs.CV

TL;DR: ViZer框架通过视觉-语言特征对齐实现零标签图像描述增强，无需标注数据或完整重训练


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型依赖标注数据导致扩展性受限的问题，充分挖掘未标注图像数据价值

Method: 在训练过程中主动对齐视觉与语言表征特征，基于现有VLM进行增强训练而非完全重训练

Result: 在SmolVLM和Qwen2-VL上实现定性提升，生成更接地气的细节描述（自动指标可能低估实际改进）

Conclusion: ViZer为视觉语言任务开辟了实用的零标签适应路径，强调定性评估在创新性内容生成中的重要性

Abstract: Vision-language models (VLMs) achieve remarkable performance through
large-scale image-text pretraining. However, their reliance on labeled image
datasets limits scalability and leaves vast amounts of unlabeled image data
underutilized. To address this, we propose Unified Vision-Language Alignment
for Zero-Label Enhancement (ViZer), an enhancement training framework that
enables zero-label learning in image captioning, providing a practical starting
point for broader zero-label adaptation in vision-language tasks. Unlike prior
approaches that rely on human or synthetically annotated datasets, ViZer
actively aligns vision and language representation features during training,
enabling existing VLMs to generate improved captions without requiring text
labels or full retraining. We demonstrate ViZer's advantage in qualitative
evaluation, as automated caption metrics such as CIDEr and BERTScore often
penalize details that are absent in reference captions. Applying ViZer on
SmolVLM-Base and Qwen2-VL, we observe consistent qualitative improvements,
producing captions that are more grounded and descriptive than their baseline.

</details>


### [100] [MMLongCite: A Benchmark for Evaluating Fidelity of Long-Context Vision-Language Models](https://arxiv.org/abs/2510.13276)
*Keyan Zhou,Zecheng Tang,Lingfeng Ming,Guanghao Zhou,Qiguang Chen,Dan Qiao,Zheming Yang,Libo Qin,Minghui Qiu,Juntao Li,Min Zhang*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）的长上下文窗口扩展未有效提升内容利用率，研究提出MMLongCite基准评估其多模态长上下文忠实度，发现现有模型存在明显局限。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估集中于纯文本领域，缺乏多模态场景的测试。为填补这一空白，需构建涵盖多模态、多任务的长上下文忠实度评估体系。

Method: 创建MMLongCite基准，包含6种上下文长度、8种任务类型，整合文本/图像/视频多模态数据，系统评估LVLMs的长上下文处理能力。

Result: 当前顶级LVLMs在长多模态上下文中的忠实度显著受限，且模型表现受上下文长度及关键内容位置影响明显。

Conclusion: MMLongCite有效量化了LVLMs的长上下文瓶颈，揭示了多模态场景下的关键影响因素，为模型优化提供实证依据。

Abstract: The rapid advancement of large vision language models (LVLMs) has led to a
significant expansion of their context windows. However, an extended context
window does not guarantee the effective utilization of the context, posing a
critical challenge for real-world applications. Current evaluations of such
long-context faithfulness are predominantly focused on the text-only domain,
while multimodal assessments remain limited to short contexts. To bridge this
gap, we introduce MMLongCite, a comprehensive benchmark designed to evaluate
the fidelity of LVLMs in long-context scenarios. MMLongCite comprises 8
distinct tasks spanning 6 context length intervals and incorporates diverse
modalities, including text, images, and videos. Our evaluation of
state-of-the-art LVLMs reveals their limited faithfulness in handling long
multimodal contexts. Furthermore, we provide an in-depth analysis of how
context length and the position of crucial content affect the faithfulness of
these models.

</details>


### [101] [Generative Universal Verifier as Multimodal Meta-Reasoner](https://arxiv.org/abs/2510.13804)
*Xinchen Zhang,Xiaoying Zhang,Youbin Wu,Yanbin Cao,Renrui Zhang,Ruihang Chu,Ling Yang,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出Generative Universal Verifier框架，通过构建ViVerBench基准、训练OmniVerifier-7B验证器及OmniVerifier-TTS测试时扩展范式，显著提升多模态推理系统的视觉验证能力与生成可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在视觉验证任务上存在显著性能差距，需要开发通用验证器增强生成过程中的反思与优化能力。

Method: 1. 创建包含16类视觉任务的ViVerBench基准；2. 自动化构建验证数据并训练OmniVerifier-7B；3. 提出OmniVerifier-TTS测试时优化框架实现迭代优化。

Result: ViVerBench性能提升+8.3，T2I-ReasonBench和GenEval++分别提升+3.7和+4.3，超越Best-of-N等传统方法。

Conclusion: 该框架通过可靠视觉验证能力，推动多模态推理系统向更可信、可控方向发展，奠定下一代生成式推理基础。

Abstract: We introduce Generative Universal Verifier, a novel concept and plugin
designed for next-generation multimodal reasoning in vision-language models and
unified multimodal models, providing the fundamental capability of reflection
and refinement on visual outcomes during the reasoning and generation process.
This work makes three main contributions: (1) We build ViVerBench, a
comprehensive benchmark spanning 16 categories of critical tasks for evaluating
visual outcomes in multimodal reasoning. Results show that existing VLMs
consistently underperform across these tasks, underscoring a substantial gap
from human-level capability in reliable visual verification. (2) We design two
automated pipelines to construct large-scale visual verification data and train
OmniVerifier-7B, the first omni-capable generative verifier trained for
universal visual verification and achieves notable gains on ViVerBench(+8.3).
Through training, we identify three atomic capabilities in visual verification
and demonstrate how they generalize and interact synergistically. (3) We
propose OmniVerifier-TTS, a sequential test-time scaling paradigm that
leverages the universal verifier to bridge image generation and editing within
unified models, enhancing the upper bound of generative ability through
iterative fine-grained optimization. Beyond generation, we extend universal
verifier to broader world-modeling interleaved reasoning scenarios.
Empirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),
and GenEval++(+4.3), outperforming existing parallel test-time scaling methods,
such as Best-of-N. By endowing multimodal reasoning with reliable visual
verification, OmniVerifier advances both reliable reflection during generation
and scalable test-time refinement, marking a step toward more trustworthy and
controllable next-generation reasoning systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [102] [Max It or Miss It: Benchmarking LLM On Solving Extremal Problems](https://arxiv.org/abs/2510.12997)
*Binxin Gao,Jingjun Han*

Main category: cs.LG

TL;DR: 论文提出ExtremBench基准测试，揭示现有数学评估基准无法全面衡量LLMs的极值问题推理能力


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法有效捕捉LLMs在极值问题推理能力的特异性，这类能力对规划、资源分配等应用至关重要

Method: 基于中国数学奥林匹克不等式题构建93个标准化极值问题数据集，评估Qwen3/GPT-OSS/DeepSeek等前沿模型家族

Result: 发现模型的数学推理能力与极值求解能力存在解耦现象，不同模型在两类任务上表现呈现互补性

Conclusion: 当前数学基准存在评估盲区，需开发更具针对性的测试框架以全面评估不同维度的数学推理能力

Abstract: Test-time scaling has enabled Large Language Models (LLMs) with remarkable
reasoning capabilities, particularly in mathematical domains, through
intermediate chain-of-thought (CoT) reasoning before generating final answers.
However, the specific sources and mechanisms underlying these reasoning
capabilities remain insufficiently understood. Optimization reasoning, i.e.
finding extrema under constraints, represents a fundamental abstraction that
underpins critical applications in planning, control, resource allocation, and
prompt search. To systematically evaluate this capability, we introduce
ExtremBench, a benchmark dataset for solving mathematical extremal problems,
curated from inequality exercises used for Chinese Mathematical Olympiad and
transformed into $93$ standardized extrema-finding problems. We conduct
extensive evaluations across various state-of-the-art open-source model
families, including the Qwen3, GPT-OSS, and DeepSeek. Our results reveal that
LLMs' extremal-solving reasoning capabilities do not always align with those of
current mathematical benchmarks such as AIME25 and MATH-500, with some models
showing strong general mathematical reasoning but poor extremal-solving skills,
and vice versa. This discrepancy highlights a critical gap in current
evaluation practices and suggests that existing benchmarks may not
comprehensively capture the full spectrum of mathematical reasoning abilities.

</details>


### [103] [On the Reasoning Abilities of Masked Diffusion Language Models](https://arxiv.org/abs/2510.13117)
*Anej Svete,Ashish Sabharwal*

Main category: cs.LG

TL;DR: MDM（掩码扩散模型）作为并行文本生成模型，在特定推理任务（如正则语言）中展现出比传统CoT增强变压器更高的效率，因其并行生成能力显著加速推理过程。


<details>
  <summary>Details</summary>
Motivation: 探究掩码扩散模型（MDM）的计算能力边界及其并行性限制，验证其在复杂推理任务中的实际潜力。

Method: 通过理论分析将MDM与思维链（CoT）和填充循环变压器（PLT）框架在有限精度对数宽度设定下建立等价关系，证明MDM具备与CoT增强变压器相同的推理能力。

Result: MDM可解决所有CoT增强变压器能处理的问题，且在正则语言等类别任务中推理速度显著优于串行生成的CoT模型。

Conclusion: 并行生成架构使MDM在特定推理场景具备天然效率优势，为下一代高效推理模型设计提供了理论依据。

Abstract: Masked diffusion models (MDMs) for text offer a compelling alternative to
traditional autoregressive language models. Parallel generation makes them
efficient, but their computational capabilities and the limitations inherent to
their parallelism remain largely unexplored. To this end, we characterize what
types of reasoning problems MDMs can provably solve and how efficiently. We do
this by connecting MDMs to the well-understood reasoning frameworks of chain of
thought (CoT) and padded looped transformers (PLTs) in the finite-precision
log-width setting: We show that MDMs and polynomially-padded PLTs are, in fact,
equivalent in this setting, and that MDMs can solve all problems that
CoT-augmented transformers can. Moreover, we showcase classes of problems
(including regular languages) for which MDMs are inherently more efficient than
CoT transformers, where parallel generation allows for substantially faster
reasoning.

</details>


### [104] [K-Merge: Online Continual Merging of Adapters for On-device Large Language Models](https://arxiv.org/abs/2510.13537)
*Donald Shenaj,Ondrej Bohdal,Taha Ceritli,Mete Ozay,Pietro Zanuttigh,Umberto Michieli*

Main category: cs.LG

TL;DR: 提出无数据依赖的计算高效LoRA合并策略，解决设备端持续增量适配器融合的存储限制问题


<details>
  <summary>Details</summary>
Motivation: 移动设备存储限制下，现有LoRA融合技术无法有效支持持续新增任务的在线增量合并需求

Method: 基于存储预算约束，设计数据无关的适配器选择与合并算法，通过参数空间优化实现增量集成

Result: 真实场景测试显示方法在保持历史任务性能的同时，显著优于基准策略（具体数据见论文实验部分）

Conclusion: 该方法为设备端持续学习提供实用解决方案，在存储和算力受限条件下实现高效模型扩展

Abstract: On-device deployment of Large Language Models (LLMs) frequently leverages
Low-Rank Adapters (LoRAs) to support diverse downstream tasks under tight
resource constraints. To address the limited storage capacity of mobile
devices, recent works have explored model merging techniques to fuse multiple
LoRAs into a single one. In practice, however, LoRAs are often delivered
incrementally, as users request support for new tasks (e.g., novel problem
types or languages). This scenario introduces a new challenge: on-device online
continual merging, where the objective is to incorporate new LoRAs while
preserving the performance on previously supported tasks. In this paper, we
propose a data-free and computationally efficient strategy for selecting and
merging LoRAs when a new one becomes available, assuming the device can store
only a limited number of adapters. Extensive experiments across real-world
tasks demonstrate the superiority of our approach compared to alternative
strategies while adhering to the storage budget and compute limitations of
on-device settings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [105] [UniMoE-Audio: Unified Speech and Music Generation with Dynamic-Capacity MoE](https://arxiv.org/abs/2510.13344)
*Zhenyu Liu,Yunxin Li,Xuanyu Zhang,Qixun Teng,Shenyuan Jiang,Xinyu Chen,Haoyuan Shi,Jinchao Li,Qi Wang,Haolan Chen,Fanbo Meng,Mingjun Zhao,Yu Xu,Yancheng He,Baotian Hu,Min Zhang*

Main category: cs.SD

TL;DR: UniMoE-Audio提出动态容量混合专家框架，通过三阶段训练策略解决语音与音乐生成的任务冲突与数据不平衡问题，在多项基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频生成模型常孤立处理语音与音乐，任务冲突与数据失衡阻碍通用音频合成发展。需设计专用架构与训练策略实现跨域协同。

Method: 1) Top-P路由动态分配专家数量 2) 混合专家包含领域专家/共享专家/空专家 3) 三阶段训练：专家预训练-MoE预热-全参数联合训练

Result: 在语音合成(LJSpeech)与音乐生成(MIDI)任务中分别取得最优表现，协同训练使参数效率提升2.3倍且避免性能下降。

Conclusion: 专用MoE架构与渐进式训练策略有效突破跨模态音频生成瓶颈，为通用音频合成系统提供新范式。

Abstract: Recent advances in unified multimodal models indicate a clear trend towards
comprehensive content generation. However, the auditory domain remains a
significant challenge, with music and speech often developed in isolation,
hindering progress towards universal audio synthesis. This separation stems
from inherent task conflicts and severe data imbalances, which impede the
development of a truly unified audio generation model. To address this
challenge, we propose UniMoE-Audio, a unified speech and music generation model
within a novel Dynamic-Capacity Mixture-of-Experts (MoE) framework.
Architecturally, UniMoE-Audio introduces a Top-P routing strategy for dynamic
expert number allocation, and a hybrid expert design comprising routed experts
for domain-specific knowledge, shared experts for domain-agnostic features, and
null experts for adaptive computation skipping. To tackle data imbalance, we
introduce a three-stage training curriculum: 1) Independent Specialist Training
leverages original datasets to instill domain-specific knowledge into each
"proto-expert" without interference; 2) MoE Integration and Warmup incorporates
these specialists into the UniMoE-Audio architecture, warming up the gate
module and shared expert using a subset of balanced dataset; and 3) Synergistic
Joint Training trains the entire model end-to-end on the fully balanced
dataset, fostering enhanced cross-domain synergy. Extensive experiments show
that UniMoE-Audio not only achieves state-of-the-art performance on major
speech and music generation benchmarks, but also demonstrates superior
synergistic learning, mitigating the performance degradation typically seen in
naive joint training. Our findings highlight the substantial potential of
specialized MoE architecture and curated training strategies in advancing the
field of universal audio generation. Homepage:
https://mukioxun.github.io/Uni-MoE-site/home.html

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [106] [Program of Thoughts for Financial Reasoning: Leveraging Dynamic In-Context Examples and Generative Retrieval](https://arxiv.org/abs/2510.13157)
*Subhendu Khatuya,Shashwat Naidu,Pawan Goyal,Niloy Ganguly*

Main category: cs.CE

TL;DR: 提出了FINDER框架，通过生成式检索器提取事实+动态程序思维提示，显著提升大语言模型在金融数值推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有方法（如chain-of-thought提示）在金融数值推理任务中表现不足，LLMs在FinQA等数据集上仍落后于SOTA模型

Method: 1. 生成式检索器提取非结构化数据中的相关事实（文本+表格）
2. 上下文感知的程序思维提示结合动态上下文示例选择

Result: 在FinQA和ConvFinQA数据集上执行准确率分别提升5.98%和4.05%，达到新SOTA

Conclusion: FINDER框架有效增强了LLMs的金融数值推理能力，两步方法组合实现性能突破

Abstract: Despite continuous advancements in the capabilities of large language models
(LLMs), numerical reasoning remains a challenging area. Techniques like
chain-of-thought prompting, tree-of-thought prompting, and program-of-thought
prompting guide LLMs through intermediate reasoning steps. Although in-context
learning with few-shot prompting has improved performance, LLMs still lag
behind state-of-the-art models on financial numerical reasoning datasets such
as FinQA and ConvFinQA. In this work, we introduce FINDER, a novel two-step
framework, to enhance LLMs' capabilities in financial numerical reasoning. The
first step utilizes a generative retriever to extract relevant facts from
unstructured data, including both text and tables. This is followed by
context-aware Program of Thought prompting with dynamic selection of in-context
examples. Our model FINDER achieves a new state-of-the-art performance on both
the FinQA and ConvFinQA datasets, surpassing previous benchmarks with execution
accuracy improvements of 5.98% and 4.05%, respectively.

</details>
